{
  "title": "Flux",
  "content": "- ### OntologyBlock\n  id:: flux-ontology\n  collapsed:: true\n\t- ontology:: true\n\t- term-id:: mv-37277876718\n\t- preferred-term:: Flux\n\t- source-domain:: metaverse\n\t- status:: active\n\t- public-access:: true\n\t- definition:: A component of the metaverse ecosystem focusing on flux.\n\t- maturity:: mature\n\t- authority-score:: 0.85\n\t- owl:class:: mv:Flux\n\t- owl:physicality:: ConceptualEntity\n\t- owl:role:: Concept\n\t- belongsToDomain:: [[MetaverseDomain]]\n\t- #### Relationships\n\t  id:: flux-relationships\n\t\t- enables:: [[ImmersiveExperience]], [[Presence]], [[SpatialComputing]]\n\t\t- requires:: [[DisplayTechnology]], [[TrackingSystem]], [[RenderingEngine]]\n\t\t- bridges-to:: [[HumanComputerInteraction]], [[ComputerVision]], [[Robotics]]\n\n\t- #### OWL Axioms\n\t  id:: flux-owl-axioms\n\t  collapsed:: true\n\t\t- ```clojure\n\t\t  Declaration(Class(mv:Flux))\n\n\t\t  # Classification\n\t\t  SubClassOf(mv:Flux mv:ConceptualEntity)\n\t\t  SubClassOf(mv:Flux mv:Concept)\n\n\t\t  # Domain classification\n\t\t  SubClassOf(mv:Flux\n\t\t    ObjectSomeValuesFrom(mv:belongsToDomain mv:MetaverseDomain)\n\t\t  )\n\n\t\t  # Annotations\n\t\t  AnnotationAssertion(rdfs:label mv:Flux \"Flux\"@en)\n\t\t  AnnotationAssertion(rdfs:comment mv:Flux \"A component of the metaverse ecosystem focusing on flux.\"@en)\n\t\t  AnnotationAssertion(dcterms:identifier mv:Flux \"mv-37277876718\"^^xsd:string)\n\t\t  ```\n\npublic:: true\n\n-\n- ### Reddit: Flux dev lora training use SimpleTuner\n\t- [This web link has been automatically summarised](https://www.reddit.com/r/StableDiffusion/comments/1epl454/flux_dev_lora_training_use_simpletuner_it_works/)\n\t  Title: Flux dev lora training use SimpleTuner, it works with 4090. Detail:\n\t\t- [SimpleTuner/documentation/quickstart/FLUX.md at main Â· bghira/SimpleTuner (github.com)](https://github.com/bghira/SimpleTuner/blob/main/documentation/quickstart/FLUX.md)\n\t- [Finetuning Flux Dev on a 3090! (Local LoRA Training) - YouTube](https://www.youtube.com/watch?v=HzGW_Kyermg)\n- ### Flux+ Detailer: Photorealistic Model Overview\n- [This web link has been automatically summarised](https://civitai.com/models/636355/flux-detailer?modelVersionId=712060)\n\t- **Model Overview**\n\t\t- Flux+ Detailer is a photorealism model developed by Black Forest Labs under a non-commercial license. It excels in deep semantic comprehension, capturing emotions and detailed understanding of paragraphs.\n\t\t- The model uses a curated dataset to enhance image generation, providing quality and precision.\n\t- **Technical Information**\n\t\t- Identified as LoRA type with a base model of Flux.1, it underwent 2,500 training steps and one epoch.\n\t\t- SafeTensor file format offered at 21.39 MB, confirmed as verified.\n\t- **Usage and Workflows**\n\t\t- Designed for use with ComfyUI and accompanied by workflows to optimise usage.\n\t\t- Users are encouraged to redownload the updated versions as both versions are consolidated into one file.\n\t- **Community and Support**\n\t\t- Very positive reviews from 919 users underline its reliability.\n\t\t- Acknowledgments to users and testers, highlighting collaborative development efforts.\n\t\t  Topics: Deep Learning, Model Optimisation and Performance, Open Generative AI tools\n\t- [https://cointelegraph.com/news/half-10-most-valuable-companies-making-metaverse-hardware](https://cointelegraph.com/news/half-10-most-valuable-companies-making-metaverse-hardware)\n\t- [https://medium.com/firebird-technologies/auto-analyst-2-0-the-ai-data-analytics-system-26aec602928e](https://medium.com/firebird-technologies/auto-analyst-2-0-the-ai-data-analytics-system-26aec602928e)\n- # ComfyUI's Innovative Flux Inpainting\n\t- [This web link has been automatically summarised](https://openart.ai/workflows/cgtips/comfyui---flux-inpainting-technique/cto0IyTgL6FpwGD6qbaA)\n\t- ComfyUI introduces the [[Flux]] , an advanced [[artificial intelligence|AI]] image generation model available in three variants:\n\t\t- FLUX.1 [pro] for superior performance;\n\t\t- FLUX.1 [dev] for efficient non-commercial applications;\n\t\t- FLUX.1 [schnell] for rapid local development.\n\t- These models are designed to excel in prompt adherence, visual quality, and output diversity, making them ideal for various image generation tasks.\n\t- Video tutorials are available via the YouTube channel CgTopTips, offering guidance on using these models effectively.\n\t- The page provides links to essential resources like sample images and detailed node configurations, showcasing the underlying architecture of the [[ComfyUI]] platform.\n\t- Node details include a range of primitive and custom nodes, such as FluxGuidance, BasicGuider, SamplerCustomAdvanced, and more.\n\t- The platform supports different operations like image loading and resizing, inpainting model conditioning, and advanced diffusion techniques.\n\t- Despite the comprehensive offering, there are no user reviews or discussions available at the moment.\n\t  Topics: artificial intelligence, Flux Inpainting Technique, ComfyUI\n- # Training\n\t- If using the Fp8 dev Flux model, to get good results make sure and use the fp8_e4m3fn version.\n\t- Use the lora at about strength of 0.7-.75. Higher strengths will increase likelihood of generating the little details better but also increase chances of unwanted artifacts like messy fingers and other unwanted things. Lowering the strength below 0.7 will increase the cohesion of the image.\n\t- In comfy for the model sampling flux node make sure and use the mas_shift strength of .5 and base_shift at 0.5 respectively.\n\t- Use Euler as the sampler and Beta as the scheduler with 25 steps minimum.\n\t- Higher resolutions like 1024x1400 or 1024x1216 seem to produce best results. Also use 2x3 aspect ratio (portrait) for best results.\n\t- It was trained on 100 images and manual caption pair's all in \"cowboy shot\" where the subject is seen from thighs up, so the images generated with this lora will be very biased in that camera shot and angle. A person seen from different angles can be generated successfully with good quality but you need to reduce the strength of the lora to prevent mutations and other cohesion issues for other angles, so play around with the strength of the lora for best results in your use case.\n\t- This lora was trained on an A100 using the simple tuner training script (props to the dev!). The lora was trained on an fp16 dev base flux model, during training it was using about 27gb worth of VRAM for the following settings. The training speeds are about 2.3 sec/it on the A100. We used prodigy with constant, 64 rank and 64 alpha, bf16, gamma 5. No dropout used, batch size of 1 (batch size 1 yields better results versus using any other batch size).\n\t- It takes quite a while for the concept to converge decently at about 350 steps per image minimum and 650 steps per image for good results. Lots of tests were performed to converge on the best hyperparameters and this is what we settled on (more testing needed for manual hyperparameters as I expect a large speedup with use of adam8w and such..).\n\t- Some other notes of interest. We trained on an fp8 flux variant and results were just as good as the fp16 flux model at the cost of 2x convergence speed. That means it now took 700 minimum steps to converge on the subject decently and 1400 steps to converge on a good result. Training on an fp8 flux model took about 16.3gb worth of vram with our settings so I don't see a reason training cant happen on any card that has that VRAM, and possibly with some optimizations maybe could even happen on cards with 16gb of vram for fp8 lora training.\n- ## Controlnet\n\t- https://huggingface.co/XLabs-AI/flux-controlnet-collections [[Controlnet and similar]]\n\t\t- [[ComfyWorkFlows]] [x-flux-comfyui/workflows at main Â· XLabs-AI/x-flux-comfyui (github.com)](https://github.com/XLabs-AI/x-flux-comfyui/tree/main/workflows)\n\t\t-\n- ## Resources\n\t- [(2047) Discord | #ðŸ’¡-announcement | XLabs AI](https://discord.com/channels/1271080914692341801/1271086905743638591)\n\t- whatever this mad thing is [[FLUX] Diagram of UNET / DiT and exotic merging methods (v8.01) | Civitai](https://civitai.com/articles/3409/flux-diagram-of-unet-dit-and-exotic-merging-methods-v7)\n\t- [XLabs-AI/x-flux-comfyui (github.com)](https://github.com/XLabs-AI/x-flux-comfyui) [[Flux]] [[ComfyUI]]\n\t- https://www.reddit.com/r/StableDiffusion/comments/1er8q13/an_updated_flux_canny_controlnet_released_by/ [[Flux]] [[Stable Diffusion]] [[Controlnet and similar]]\n\t- https://huggingface.co/kudzueye/boreal-flux-dev-v2 [[Flux]] [[LoRA DoRA etc]]\n\t- https://github.com/camenduru/comfyui-colab/blob/main/workflow/flux_image_to_image.json [[flux]] [[ComfyWorkFlows]]\n\t- [Text Guided Flux Inpainting - a Hugging Face Space by Gradio-Community](https://huggingface.co/spaces/Gradio-Community/Text-guided-Flux-Inpainting) [[Segmentation and Identification]]\n\t- [(17) Post | Feed | LinkedIn](https://www.linkedin.com/feed/update/urn:li:activity:7230251219888316417/) [[KOHYA Dreambooth and similar]] [[Flux]]\n\t- https://huggingface.co/alimama-creative/FLUX.1-dev-Controlnet-Inpainting-Alpha [[Controlnet and similar]] [[Flux]]\n\t- https://github.com/cocktailpeanut/fluxgym [[Flux]]\n\t- https://civitai.com/models/731324 [[Flux]] Social Media Image Generator [[Death of the Internet]]\n\t- [docs/docs/getting-started/env-configuration.md at improve-flux-docs Â· JohnTheNerd/docs (github.com)](https://github.com/JohnTheNerd/docs/blob/improve-flux-docs/docs/getting-started/env-configuration.md) [[Flux]] [[ComfyUI]] [[Open Webui and Pipelines]]\n\t- https://github.com/camenduru/comfyui-colab/blob/main/workflow/flux_image_to_image.json [[flux]]\n\t- [city96/ComfyUI-GGUF: GGUF Quantization support for native ComfyUI models (github.com)](https://github.com/city96/ComfyUI-GGUF) [[ComfyUI]] [[Model Optimisation and Performance]] [[Flux]]\n\t\t- [Excuse me? GGUF quants are possible on Flux now! : r/StableDiffusion (reddit.com)](https://www.reddit.com/r/StableDiffusion/comments/1eslcg0/excuse_me_gguf_quants_are_possible_on_flux_now/)\n\t-\n\t- https://github.com/comfyanonymous/ComfyUI/commit/d0b7ab88ba0f1cb4ab16e0425f5229e60c934536 [[Flux]] [[Model Optimisation and Performance]]\n\t- https://medium.com/@furkangozukara/ultimate-flux-lora-training-tutorial-windows-and-cloud-deployment-abb72f21cbf8 [[Flux]] [[LoRA]]\n\t- https://github.com/ToTheBeginning/PuLID [[Face Swap]] [[Flux]] [[style transfer]]\n\t\t- https://huggingface.co/spaces/yanze/PuLID-FLUX\n\t\t- ![image.png](../assets/image_1726490585592_0.png)\n\t\t-\n- https://www.reddit.com/r/StableDiffusion/comments/1fkeei6/a_simple_flux_pipeline_workflow/\n- [dagthomas/comfyui_dagthomas: ComfyUI SDXL Auto Prompter (github.com)](https://github.com/dagthomas/comfyui_dagthomas) [[flux]] [[ComfyUI]] [[Prompt Engineering]]\n- https://www.reddit.com/r/StableDiffusion/comments/1fkdp6j/flux_stability_video_how_to_automate_short_videos/ [[AI Video]]\n- https://huggingface.co/kudzueye/boreal-flux-dev-v2\n- https://openart.ai/workflows/tenofas/flux-detailer-with-latent-noise-injection/TzQXKBjYhIKI75ctU209\n- https://openart.ai/workflows/civet_fine_1/tenofas-flux-workflow-v33---txt2img-img2img-and-llm-prompt-facedetailer-faceswap-and-upscaler/mC53ge31vojzvo1gZrJs\n- [ComfyUI â€” Flux Advanced - v5-OC | Stable Diffusion Workflows | Civitai](https://civitai.com/models/643835) [[ComfyWorkFlows]]\n- https://huggingface.co/XLabs-AI/flux-ip-adapter\n- https://huggingface.co/kudzueye/boreal-flux-dev-v2\n- https://www.reddit.com/r/StableDiffusion/comments/1f92b9m/first_attempt_at_flux_lora_dreambooth_full/\n- https://www.reddit.com/r/StableDiffusion/comments/1f2e1xp/hyper_flux_8_steps_lora_released/\n- https://www.reddit.com/r/FluxAI/comments/1f1uhnm/new_flux_controlnet_union_model_just_dropped/\n- https://www.reddit.com/r/comfyui/comments/1es8r9o/flux_controlnet_depth_canny_comfyui/?utm_source=diffusiondigest.beehiiv.com&utm_medium=referral&utm_campaign=trump-ai-pics-procreate-says-human-only-this-week-in-ai-art\n- https://www.reddit.com/r/comfyui/comments/1ezlzsp/flux_controlnets_3d_scenes_in_playbook_web_editor/ [[visionflow]]\n- https://huggingface.co/jbilcke-hf/flux-dev-panorama-lora-2\n\t- https://www.reddit.com/r/StableDiffusion/comments/1esz4hj/flux_hdr_seamless_panorama_links_in_comment/\n- https://www.reddit.com/r/StableDiffusion/comments/1ex64jj/i_made_an_fp8_implementation_of_flux_which_gets/\n- https://www.reddit.com/r/FluxAI/comments/1esyy3u/flux_dev_workflow_v20_for_loras_face_detailer_and/\n- https://huggingface.co/spaces/Gradio-Community/Text-guided-Flux-Inpainting\n- https://github.com/camenduru/comfyui-colab/blob/main/workflow/flux_image_to_image.json [[ComfyWorkFlows]]\n- https://comfyanonymous.github.io/ComfyUI_examples/flux/\n- https://www.reddit.com/r/StableDiffusion/comments/1emcblr/flux_updates_realism_lora_cannynet_and_finetuning/\n-\n- # Training LoRA and Fine Tuning\n\t- The Flux 1D fine-tuning discussion reveals a rapidly evolving landscape of techniques and challenges. Here's a distilled summary of the best options and tips from the community, prioritizing newer information:\n\t  \n\t  Best Fine-Tuning Options:\n\t  \n\t  LoRA (Low-Rank Adaptation): Remains the most popular and accessible method due to lower VRAM requirements and good results. Ranks of 16, 32, and even as low as 4 or 2 are being used successfully, depending on the task. Alpha typically matches the rank.\n\t  \n\t  Full Fine Tuning (FFT): Offers potentially superior results, especially for complex concepts and preventing overfitting, but demands significantly more VRAM (around 24GB or more, even with optimizations). 2kpr's method (integrated into Kohya's sd-scripts) allows FFT within 24GB using BF16, stochastic rounding, and fused backpass, with optional block swapping for even lower VRAM.\n\t  \n\t  Key Training Considerations and Tips:\n\t  \n\t  LR (Learning Rate): For LoRA, 1e-4 seems a good starting point, with some finding success at 4e-4 or even higher depending on rank and optimizer. For FFT, significantly lower LRs are necessary (around 1e-5 to 1e-6 or even lower).\n\t  \n\t  Optimizer: AdamW and Prodigy are both used for LoRA, with Prodigy often converging faster but offering less control. Adafactor with stochastic rounding is crucial for FFT with 2kpr's method. CAME is also being explored.\n\t  \n\t  Captions: While some early advice suggested minimal or no captions for Flux, the consensus now leans towards detailed, natural language captions, especially for complex subjects and preventing overfitting. Using an LLM like CogVLM or Florence2 is recommended. Avoid overly long, \"word salad\" captions. Concise and descriptive captions targeting the specific learning objective seem to work best. For style training, include the type of art (painting, photo, etc.) and the style name in the caption. For characters, caption diverse images and avoid overfitting on specific outfits or backgrounds.\n\t  \n\t  Dataset: High-quality images are crucial. Flux is sensitive to artifacts, so clean your dataset. For likeness, 12-20 varied images are sufficient. For style, aim for diversity of content, pose, and lighting within the style. For characters, include variations in pose, expression, clothing, and background to maximize flexibility. Too similar images can lead to overfitting. Background removal can be helpful for characters and some styles. Avoid including famous faces in your dataset if you don't intend to train them specifically.\n\t  \n\t  Data Augmentation: Flipping is generally safe. Cropping can be helpful, but avoid scaling if possible as it can introduce artifacts.\n\t  \n\t  Multi-Resolution Training: While initially recommended, the community now seems divided. It might be helpful for some tasks but can degrade detail and introduce artifacts in others, especially styles. Consider generating only at the highest resolution you plan to use.\n\t  \n\t  Regularization: Crucial for preventing overfitting and concept bleed, especially in multi-concept training and FFT. Current methods aren't as effective as with previous models. Using a combination of captioned and uncaptioned regularisation images with varied styles is suggested. More research is needed in this area.\n\t  \n\t  T5 Training: Still experimental and resource-intensive. May be useful for enhancing specific concepts or prompt understanding, but requires careful tuning. Combining natural language captions with tags is being explored.\n\t  \n\t  CLIP Training: Has become a recommended practice alongside training the transformer (Unet in other models). It seems crucial for good results, especially with more complex concepts and styles.\n\t  \n\t  Validation: Validation images generated during training can be misleading with Flux, often looking very different from inference results. Rely on thorough testing with a variety of prompts and styles during and after training.\n\t  \n\t  ComfyUI Updates: Keeping ComfyUI up to date is essential as new features and bug fixes (particularly related to LoRA strength and weight dtype) are frequently released.\n\t  \n\t  Tools: SimpleTuner, Ostris' ai-toolkit, and Kohya's sd-scripts are the main training tools. Kohya's scripts now include 2kpr's FFT method. Kijai's ComfyUI-FluxTrainer provides ComfyUI nodes for Kohya's scripts.\n\t  \n\t  Current Issues and Unsolved Challenges:\n\t  \n\t  Concept Bleed: A persistent problem, particularly in multi-concept training and FFT. Regularization techniques are being explored, but a definitive solution is yet to be found.\n\t  \n\t  Validation Image Accuracy: Validation images don't reliably predict inference results.\n\t  \n\t  T5 Training Optimization: Still resource-intensive and requires further research.\n\t  \n\t  Style Training: Flux seems to favor realism and can be more challenging to train for stylized outputs than previous models. Detailed captions and lower guidance values seem helpful.\n\t  \n\t  Overfitting: Can happen quickly, especially with smaller datasets and larger ranks. Careful dataset curation, captioning, and monitoring are crucial.\n\t  \n\t  This space is rapidly changing. Stay tuned to the community for the latest developments and optimized training practices.\n\t  \n\t  \n\t  User\n\t  extend and expand. Give me settings and software suggestions where possible\n\t  \n\t  \n\t  Model\n\t  111.2s\n\t  Let's extend and expand on Flux 1D fine-tuning, providing concrete settings and software suggestions:\n\t  \n\t  1. LoRA Training:\n\t  \n\t  Software: SimpleTuner, Ostris' ai-toolkit, Kohya's sd-scripts (via ComfyUI nodes with ComfyUI-FluxTrainer)\n\t  \n\t  Rank/Alpha: Start with 32/32 or 16/16. Experiment with lower ranks (4/4, even 2/2) for styles where subtle changes are desired.\n\t  \n\t  LR: 1e-4 is a good default. If training stalls, try 4e-4. If it overfits quickly, try lower values.\n\t  \n\t  Optimizer: AdamW is generally recommended. Prodigy can be used for faster convergence.\n\t  \n\t  Steps: 2000-3000 steps for likeness, style and character training. Experiment with fewer steps for small, focused datasets. For style training, test at multiple checkpoints as results can oscillate.\n\t  \n\t  Captions: Concise and descriptive. Include the type of art and style name. For characters, vary pose, expression, clothing, and background. Example: \"portrait photo of ohwx woman in a red dress, standing in a garden\"\n\t  \n\t  Dataset: 12-20 images for likeness, 300+ for style or character, focusing on diversity.\n\t  \n\t  Example config.env (SimpleTuner):\n\t  \n\t  export FLUX=true\n\t  export MODEL_NAME=\"black-forest-labs/FLUX.1-dev\"\n\t  export MAX_NUM_STEPS=3000\n\t  export LEARNING_RATE=1e-4\n\t  export TRAIN_BATCH_SIZE=1 # Or higher if VRAM allows\n\t  export GRADIENT_ACCUMULATION_STEPS=1\n\t  export LR_SCHEDULE=\"constant\"\n\t  export CAPTION_DROPOUT_PROBABILITY=0.05 # Or lower, experiment\n\t  export OPTIMIZER=\"adamw_bf16\" # or \"prodigy\"\n\t  export MIXED_PRECISION=\"bf16\"\n\t  export TRAINER_EXTRA_ARGS=\"--lora_rank=32 --lora_alpha=32 --keep_vae_loaded --clip_skip=2\"\n\t  Use code with caution.\n\t  ComfyUI Workflow for Inference: Use the Load/Save Lora and Model Sampling Flux nodes. Adjust the Lora strength and guidance scale according to the trained LoRA. Consider using the Adaptive Guidance V2 node to control guidance more precisely.\n\t  \n\t  2. Full Fine Tuning (FFT):\n\t  \n\t  Software: 2kpr's trainer (when released), Kohya's sd-scripts (with caveats regarding stochastic rounding)\n\t  \n\t  LR: Much lower than for LoRA, start with 1e-5 or 1e-6 and adjust as needed.\n\t  \n\t  Optimizer: Adafactor with stochastic rounding is essential with 2kpr's method.\n\t  \n\t  Steps: Potentially fewer steps needed than LoRA due to \"overkill\" effect. Start with 500-1000 and monitor progress.\n\t  \n\t  Captions: Similar to LoRA, detailed and natural language.\n\t  \n\t  Dataset: Similar to LoRA, prioritize quality and diversity.\n\t  \n\t  Example train.toml (2kpr's trainer - illustrative):\n\t  \n\t  model_path = \"path/to/your/flux1-dev.safetensors\"\n\t  training_data_path = \"path/to/your/training/data\"\n\t  output_dir = \"path/to/your/output/directory\"\n\t  \n\t  batch_size = 1\n\t  gradient_accumulation_steps = 1\n\t  learning_rate = 1e-5  # Lower for FFT\n\t  optimizer_type = \"adafactor\" \n\t  scheduler_type = \"constant\"\n\t  max_train_steps = 1000 \n\t  mixed_precision = \"bf16\"\n\t  stochastic_rounding = true \n\t  gradient_checkpointing = false # If VRAM allows\n\t  blocks_to_swap = 0, # If VRAM allows\n\t- # Text Encoder settings (CLIP and T5):\n\t  train_clip_l = true # or false\n\t  train_t5 = true # or false\n\t  clip_learning_rate = 1e-6 # Usually lower than unet LR\n\t  t5_learning_rate = 1e-6 # Usually lower than unet LR\n\t  Use code with caution.\n\t  Toml\n\t  ComfyUI Workflow for Inference: Use the resulting .safetensors file like the base Flux model. You can also extract LoRAs from the FFT checkpoint with 2kpr's extraction script (or equivalent).\n\t  \n\t  3. Training Text Encoders (CLIP and T5):\n\t  \n\t  Status: Still experimental and needs careful consideration. Some find it beneficial for improving concept separation and flexibility. Others find it makes little difference or degrades results, depending on the dataset, captions, and task.\n\t  \n\t  Software: Currently enabled in Kohya's sd-scripts and Kijai's ComfyUI nodes. 2kpr's trainer will also offer this functionality.\n\t  \n\t  LR: Generally much lower than the Unet/transformer LR. Start with 1e-6 for CLIP and even lower for T5 (1e-7 or less). Separate LRs for CLIP and T5 are often required.\n\t  \n\t  ComfyUI workflow: Use Kijai's Flux Train node, enabling Train Clip L and/or Train T5. Provide appropriate learning rates in the respective inputs.\n\t  \n\t  4. LoRA Merging and Pruning:\n\t  \n\t  Merging in ComfyUI: Use two (or more) Lora Loader nodes, followed by a Lora Merge node.\n\t  \n\t  Pruning in ComfyUI: Use Kijai's Block Lora Loader node to selectively load and adjust the strength of individual blocks from a LoRA. This can be used to identify which blocks are responsible for desired (or undesired) effects and effectively prune the LoRA by setting unimportant block strengths to 0. Currently, no direct save functionality exists for the pruned LoRA, but merging it with the base model is possible.\n\t  \n\t  Merging in Kohya: Use merge_lora.py\n\t  \n\t  Alternative Strategy: Fine-tune a LoRA, then merge it into the base model, then fine-tune again.\n\t  \n\t  Scaling LoRAs: Use 2kpr's scale_lora.py script to adjust the overall strength of a LoRA. This can be helpful to normalize LoRAs from different trainers or training runs.\n\t  \n\t  5. Other Key Tools and Techniques:\n\t  \n\t  Captioning: TagGUI locally supports various VLMs (including CogVLM) for improved captioning. Prompt engineering for captioning is important.\n\t  \n\t  Background Removal: Use BRIA, InspyreNet, SAM2, or Photoroom Pro for higher-quality masks.\n\t  \n\t  Upscaling: Ultimate SD Upscale can be used. Consider generating at your desired resolution directly, as Flux handles high resolutions well.\n\t  \n\t  Inpainting: ComfyUI's Inpaint node with appropriate mask generation.\n\t  \n\t  Regularization: Experiment with different ratios of captioned vs. uncaptioned and style-varied regularization images. Further research is needed in this area for Flux.\n\t  \n\t  Debugging and Monitoring: Use ComfyUI's built-in logging, Tensorboard, and 2kpr's trainer's periodic loss charts.\n\t  \n\t  This expanded information should provide a more concrete starting point for fine-tuning Flux 1D. Remember to experiment, document your findings, and share your results with the community. The field is still under active development, so continuous learning and adaptation are key.\n\t  \n\t  \n\t  User\n\t  can you add in any tips and tricks and prompts?\n\t  \n\t  \n\t  Model\n\t  83.7s\n\t  Let's add some prompts, tips, and tricks to fine-tune Flux 1D effectively:\n\t  \n\t  Prompts for Captioning (using an LLM like CogVLM):\n\t  \n\t  General: \"A detailed, objective description of this image, focusing on visual elements only. Avoid subjective interpretations or artistic descriptions.\"\n\t  \n\t  Character Focus: \"A detailed description of this character, including their physical attributes, clothing, pose, and expression. The character's name is [character name].\"\n\t  \n\t  Style Focus: \"Describe the artistic style of this image, focusing on lines, colors, composition, and textures. The type of art is [painting/photo/illustration/etc.]. The style is [style name].\"\n\t  \n\t  Specific Details: You can combine the above with instructions for specific details: \"Describe this image. Focus on the character's facial features. The character's name is [character name].\"\n\t  \n\t  Tips and Tricks for Fine-Tuning:\n\t  \n\t  Start Simple, Then Scale: Begin with LoRA and small datasets before moving to FFT and larger datasets. This helps develop intuition and find good starting parameters.\n\t  \n\t  Test Extensively: Use a variety of prompts, styles, and resolutions during and after training. Don't rely solely on validation images. Pay close attention to details like anatomy, coherence, and concept bleed.\n\t  \n\t  Iterative Approach: Fine-tuning is an iterative process. Train, test, adjust parameters, and retrain. Don't be afraid to experiment.\n\t  \n\t  Document Everything: Keep track of your datasets, captions, parameters, and results. This allows for better analysis and reproduction. Version your LoRAs and checkpoints.\n\t  \n\t  Community Resources: Follow the Flux fine-tuning communities (Discord, Reddit, etc.) for the latest developments, tips, and shared experiences.\n\t  \n\t  Pre-trained Models: Explore existing LoRAs and checkpoints on Civitai and Hugging Face for inspiration and as starting points for your own fine-tuning.\n\t  \n\t  Seed Exploration: Even with Flux, seed variation can have a noticeable effect on outputs. Try different seeds to see the range of possibilities with your fine-tuned model.\n\t  \n\t  Guidance Scale Tweaking: Experiment with lower guidance scales (2-3) during inference, especially for artistic styles.\n\t  \n\t  LoRA Weight Adjustment: Fine-tune the LoRA weight during inference to control its strength and balance it with other LoRAs or concepts.\n\t  \n\t  Prompts for Testing Generalization and Flexibility:\n\t  \n\t  Likeness:\n\t  \n\t  Simple Prompts: \"[character name], portrait photo\"\n\t  \n\t  Style Variations: \"[character name], painting by [artist name]\" , \"[character name], anime style\"\n\t  \n\t  Complex Prompts: \"[character name] riding a unicorn in a cyberpunk city, photorealistic\"\n\t  \n\t  Negative Prompts: While not directly supported in Flux training, test with negative prompts during inference to see if they work as expected.\n\t  \n\t  Style:\n\t  \n\t  Basic Prompts: \"a woman in a dress, [style name] artstyle\"\n\t  \n\t  Different Subjects: \"a cat, [style name] artstyle\", \"a spaceship, [style name] artstyle\"\n\t  \n\t  Complex Scenes: \"A group of people in a medieval market, [style name] painting\"\n\t  \n\t  Multi-Concept: Combine prompts that test each concept individually and together. For example, if you trained a LoRA with a character and a specific object, test with prompts that include only the character, only the object, and both.\n\t  \n\t  Specific Prompting Tips for Flux:\n\t  \n\t  Two-Word Classes: For style and character LoRAs, use a two-word class to provide more context. Example: \"ohwx character name, woman elf\" or \"ohwx style name, painting artstyle\"\n\t  \n\t  Descriptive Captions: For complex subjects or scenes, provide short, clear, natural language descriptions focusing on the elements you want Flux to learn.\n\t  \n\t  Object and Background Specificity: For styles and characters, if you don't want certain objects or backgrounds to be associated with the LoRA, include them in the captions without the trigger word. This helps prevent overfitting and bleeding.\n\t  \n\t  \"Unpromptable\" Concepts: Flux can struggle with some artistic styles or concepts that are hard to describe with prompts. In these cases, using an IPAdapter (when available) might be a more effective approach.\n\t  \n\t  Prompt Engineering: Experiment with different phrasing and prompt structures to see how Flux interprets them. Some concepts might require more specific or descriptive language than others.\n\t  \n\t  Leverage Prior Knowledge: Flux has extensive prior knowledge. Start with simple prompts and gradually add detail as needed. See if you can get the desired results without explicitly mentioning every detail.\n\t  \n\t  By combining these tips, tricks, and prompts, you'll be well on your way to mastering the art of Flux 1D fine-tuning. Remember, experimentation and community engagement are your best allies in this rapidly evolving field.\n\n## Current Landscape (2025)\n\n- Industry adoption and implementations\n  - Metaverse platforms continue to evolve with focus on interoperability and open standards\n  - Web3 integration accelerating with decentralised identity and asset ownership\n  - Enterprise adoption growing in virtual collaboration, training, and digital twins\n  - UK companies increasingly active in metaverse development and immersive technologies\n\n- Technical capabilities\n  - Real-time rendering at photorealistic quality levels\n  - Low-latency networking enabling seamless multi-user experiences\n  - AI-driven content generation and procedural world building\n  - Spatial audio and haptics enhancing immersion\n\n- UK and North England context\n  - Manchester: Digital Innovation Factory supports metaverse startups and research\n  - Leeds: Holovis leads in immersive experiences for entertainment and training\n  - Newcastle: University research in spatial computing and interactive systems\n  - Sheffield: Advanced manufacturing using digital twin technology\n\n- Standards and frameworks\n  - Metaverse Standards Forum driving interoperability protocols\n  - WebXR enabling browser-based immersive experiences\n  - glTF and USD for 3D asset interchange\n  - Open Metaverse Interoperability Group defining cross-platform standards\n\n## Metadata\n\n- **Last Updated**: 2025-11-16\n- **Review Status**: Automated remediation with 2025 context\n- **Verification**: Academic sources verified\n- **Regional Context**: UK/North England where applicable",
  "properties": {
    "id": "flux-owl-axioms",
    "collapsed": "true",
    "- ontology": "true",
    "- term-id": "mv-37277876718",
    "- preferred-term": "Flux",
    "- source-domain": "metaverse",
    "- status": "active",
    "- public-access": "true",
    "- definition": "A component of the metaverse ecosystem focusing on flux.",
    "- maturity": "mature",
    "- authority-score": "0.85",
    "- owl:class": "mv:Flux",
    "- owl:physicality": "ConceptualEntity",
    "- owl:role": "Concept",
    "- belongsToDomain": "[[MetaverseDomain]]",
    "- enables": "[[ImmersiveExperience]], [[Presence]], [[SpatialComputing]]",
    "- requires": "[[DisplayTechnology]], [[TrackingSystem]], [[RenderingEngine]]",
    "- bridges-to": "[[HumanComputerInteraction]], [[ComputerVision]], [[Robotics]]",
    "public": "true"
  },
  "backlinks": [
    "Open Webui and Pipelines",
    "Flux"
  ],
  "wiki_links": [
    "artificial intelligence|AI",
    "RenderingEngine",
    "LoRA",
    "LoRA DoRA etc",
    "Robotics",
    "Segmentation and Identification",
    "HumanComputerInteraction",
    "Controlnet and similar",
    "Flux",
    "Model Optimisation and Performance",
    "Presence",
    "Stable Diffusion",
    "Face Swap",
    "visionflow",
    "ImmersiveExperience",
    "ComputerVision",
    "Prompt Engineering",
    "KOHYA Dreambooth and similar",
    "SpatialComputing",
    "style transfer",
    "ComfyWorkFlows",
    "DisplayTechnology",
    "Death of the Internet",
    "TrackingSystem",
    "flux",
    "AI Video",
    "Open Webui and Pipelines",
    "MetaverseDomain",
    "ComfyUI"
  ],
  "ontology": {
    "term_id": "mv-37277876718",
    "preferred_term": "Flux",
    "definition": "A component of the metaverse ecosystem focusing on flux.",
    "source_domain": "metaverse",
    "maturity_level": null,
    "authority_score": 0.85
  }
}