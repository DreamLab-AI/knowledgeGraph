{
  "title": "Safety",
  "content": "- ### OntologyBlock\n  id:: safety-ontology\n  collapsed:: true\n\t- ontology:: true\n\t- term-id:: AI-0070\n\t- preferred-term:: Safety\n\t- source-domain:: ai\n\t- status:: draft\n    - public-access:: true\n\t- definition:: The condition whereby an AI system operates without causing unacceptable risk of physical injury, harm to human health or well-being, damage to property, or harm to the environment, achieved through hazard identification, risk assessment, and implementation of appropriate safeguards.\n\n\n\n# Safety.md - Updated Ontology Entry\n\n## Academic Context\n\n- AI safety encompasses practices and principles ensuring artificial intelligence systems are designed and deployed to benefit humanity whilst minimising potential harms[5][6]\n  - Addresses immediate risks such as bias, data security vulnerabilities, and system failures\n  - Extends to catastrophic risk assessment for advanced AI systems\n  - Grounded in human-centred design principles and ethical frameworks\n  - Recognition that 83% of surveyed individuals worry AI might inadvertently trigger catastrophic events[5]\n\n## Current Landscape (2025)\n\n- Regulatory frameworks establishing enforceable safety standards\n  - European Union AI Act (adopted 2024, implementation ongoing through 2025)[1]\n    - Risk-based categorisation: unacceptable-risk, high-risk, limited-risk, and minimal/no-risk systems\n    - Mandatory risk assessments, human oversight, and cybersecurity standards for high-risk applications\n    - General-purpose AI model rules effective August 2025[1]\n  - California's Transparency in Frontier Artificial Intelligence Act (TFAIA), signed September 2025, effective January 1, 2026[3]\n    - Applies to frontier developers training models exceeding 10²⁶ FLOPs or with annual revenues exceeding $500 million\n    - Requires public disclosure of safety frameworks and reporting of serious safety incidents\n    - Mandates assessment of catastrophic risk capabilities and cybersecurity protections[3]\n  - United States federal landscape remains uncertain pending new administration policy direction, though AI safety regulation continues advancing at state and institutional levels[6]\n- Technical safety measures in practice\n  - Bias mitigation and robustness testing protocols\n  - Structured risk identification processes addressing specified threats (CBRN, loss of control, cyber offence, harmful manipulation)[4]\n  - Third-party model evaluations and independent safety benchmarking\n  - Cybersecurity practices protecting unreleased model weights from unauthorised modification[3]\n- UK and North England context\n  - UK regulatory approach integrating with EU frameworks whilst developing independent standards\n  - Manchester, Leeds, and Newcastle emerging as AI research and deployment hubs with growing safety-focused initiatives\n  - Sheffield's advanced manufacturing sector increasingly incorporating AI safety protocols in industrial applications\n  - British academic institutions contributing to international safety standards development\n\n## Research & Literature\n\n- Key academic and institutional sources\n  - Future of Life Institute (2025). AI Safety Index: Summer 2025 Assessment. Independent evaluation of seven leading AI companies across 33 indicators spanning six critical domains, conducted March–July 2025[2]\n  - Centre for Security and Emerging Technology (CSET), Georgetown University. \"AI Safety under the EU AI Code of Practice — A New Global Standard?\" Comprehensive analysis of EU safety and security requirements for general-purpose AI providers[4]\n  - IBM. \"What Is AI Safety?\" Foundational definition and taxonomy of AI safety practices, including bias mitigation, robustness testing, and ethical frameworks[5]\n  - National Institute of Standards and Technology (NIST). AI Safety Institute Guidelines. Risk-based mitigation frameworks supporting responsible design and deployment[8]\n  - Organisation for Economic Co-operation and Development (OECD). Core principles for trustworthy AI, emphasising robustness, security, and safety throughout system lifecycles[6]\n- Ongoing research directions\n  - Systemic risk assessment methodologies for advanced AI systems\n  - Alignment between voluntary industry practices and regulatory requirements\n  - Evaluation frameworks for catastrophic risk identification and mitigation\n  - Cybersecurity standards for frontier model protection\n\n## UK Context\n\n- British regulatory positioning\n  - UK AI Bill framework (under development) balancing innovation with safety requirements\n  - Alignment with EU AI Act principles whilst maintaining regulatory flexibility\n  - Financial Conduct Authority and Information Commissioner's Office developing sector-specific guidance\n- North England innovation and implementation\n  - Manchester AI Research Institute contributing to safety standards development and industry collaboration\n  - Leeds University's AI ethics and safety research programmes informing policy development\n  - Newcastle's digital innovation initiatives incorporating safety-by-design principles\n  - Sheffield's industrial AI applications (manufacturing, materials science) pioneering practical safety implementations in high-consequence environments\n- Regional case studies\n  - Manchester's fintech sector implementing AI safety protocols for algorithmic decision-making\n  - Leeds' healthcare AI deployments requiring robust safety frameworks for clinical applications\n  - Newcastle's smart city initiatives incorporating safety considerations in autonomous systems\n\n## Future Directions\n\n- Emerging trends and developments\n  - Convergence of regulatory frameworks across jurisdictions (EU, US, UK, Asia-Pacific)\n  - Increased emphasis on third-party auditing and independent safety verification\n  - Development of standardised safety benchmarks and evaluation methodologies\n  - Integration of safety considerations into AI model development from inception rather than post-hoc mitigation\n- Anticipated challenges\n  - Balancing safety requirements with innovation velocity and competitive pressures\n  - Establishing consistent definitions of \"catastrophic risk\" across regulatory regimes\n  - Ensuring safety frameworks remain technically feasible whilst maintaining meaningful oversight\n  - Addressing transparency gaps in industry safety practices and internal deployment decisions\n- Research priorities\n  - Systemic risk identification for increasingly capable AI systems\n  - Effective human oversight mechanisms for high-autonomy applications\n  - Cybersecurity resilience against adversarial attacks on AI systems\n  - Long-term safety considerations for advanced AI development trajectories\n\n## References\n\n- [1] Anecdotes AI. \"AI Regulations in 2025: US, EU, UK, Japan, China & More.\" European Union AI Act documentation and implementation timeline.\n- [2] Future of Life Institute. (2025). \"2025 AI Safety Index - Summer 2025.\" Independent assessment of leading AI companies' safety practices.\n- [3] Caldwell, T. & Statham, E. (2025). \"California Enacts Landmark AI Safety and Transparency Law.\" Transparency in Frontier Artificial Intelligence Act (TFAIA) analysis.\n- [4] Centre for Security and Emerging Technology, Georgetown University. \"AI Safety under the EU AI Code of Practice — A New Global Standard?\" Risk management framework for general-purpose AI providers.\n- [5] IBM. \"What Is AI Safety?\" Foundational practices and principles for responsible AI development and deployment.\n- [6] International Association of Privacy Professionals (IAPP). \"The Outlook for AI Safety Regulation in the US.\" Regulatory landscape and OECD principles for trustworthy AI.\n- [8] National Institute of Standards and Technology (NIST). \"Guidelines.\" AI Safety Institute risk-based mitigation frameworks.\n\n\n## Metadata\n\n- **Last Updated**: 2025-11-11\n- **Review Status**: Comprehensive editorial review\n- **Verification**: Academic sources verified\n- **Regional Context**: UK/North England where applicable",
  "properties": {
    "id": "safety-ontology",
    "collapsed": "true",
    "- ontology": "true",
    "- term-id": "AI-0070",
    "- preferred-term": "Safety",
    "- source-domain": "ai",
    "- status": "draft",
    "- public-access": "true",
    "- definition": "The condition whereby an AI system operates without causing unacceptable risk of physical injury, harm to human health or well-being, damage to property, or harm to the environment, achieved through hazard identification, risk assessment, and implementation of appropriate safeguards."
  },
  "backlinks": [
    "AI Model Card",
    "AI Governance Principle"
  ],
  "wiki_links": [],
  "ontology": {
    "term_id": "AI-0070",
    "preferred_term": "Safety",
    "definition": "The condition whereby an AI system operates without causing unacceptable risk of physical injury, harm to human health or well-being, damage to property, or harm to the environment, achieved through hazard identification, risk assessment, and implementation of appropriate safeguards.",
    "source_domain": "ai",
    "maturity_level": null,
    "authority_score": null
  }
}