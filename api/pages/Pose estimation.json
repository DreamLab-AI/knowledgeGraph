{
  "title": "Pose estimation",
  "content": "- ### OntologyBlock\n  id:: pose-estimation-ontology\n  collapsed:: true\n\t- ontology:: true\n\t- term-id:: mv-296381584315\n\t- preferred-term:: Pose estimation\n\t- source-domain:: metaverse\n\t- status:: active\n\t- public-access:: true\n\t- definition:: A component of the metaverse ecosystem focusing on pose estimation.\n\t- maturity:: mature\n\t- authority-score:: 0.85\n\t- owl:class:: mv:PoseEstimation\n\t- owl:physicality:: VirtualEntity\n\t- owl:role:: Object\n\t- belongsToDomain:: [[MetaverseDomain]]\n\t- #### Relationships\n\t  id:: pose-estimation-relationships\n\t\t- is-part-of:: [[VirtualWorld]], [[MetaversePlatform]]\n\t\t- requires:: [[DigitalIdentity]], [[AuthenticationService]]\n\t\t- enables:: [[SocialInteraction]], [[Presence]], [[UserRepresentation]]\n\t\t- has-property:: [[Appearance]], [[Customization]], [[Animation]]\n\n\t- #### OWL Axioms\n\t  id:: pose-estimation-owl-axioms\n\t  collapsed:: true\n\t\t- ```clojure\n\t\t  Declaration(Class(mv:PoseEstimation))\n\n\t\t  # Classification\n\t\t  SubClassOf(mv:PoseEstimation mv:VirtualEntity)\n\t\t  SubClassOf(mv:PoseEstimation mv:Object)\n\n\t\t  # Domain classification\n\t\t  SubClassOf(mv:PoseEstimation\n\t\t    ObjectSomeValuesFrom(mv:belongsToDomain mv:MetaverseDomain)\n\t\t  )\n\n\t\t  # Annotations\n\t\t  AnnotationAssertion(rdfs:label mv:PoseEstimation \"Pose estimation\"@en)\n\t\t  AnnotationAssertion(rdfs:comment mv:PoseEstimation \"A component of the metaverse ecosystem focusing on pose estimation.\"@en)\n\t\t  AnnotationAssertion(dcterms:identifier mv:PoseEstimation \"mv-296381584315\"^^xsd:string)\n\t\t  ```\n\n- Densepose\n\t- [Pawandeep-prog/one-click-dense-pose: One Click Dense Pose Video with just One click (github.com)](https://github.com/Pawandeep-prog/one-click-dense-pose)\n\t- [Flode-Labs/vid2densepose: Convert your videos to densepose and use it on MagicAnimate (github.com)](https://github.com/Flode-Labs/vid2densepose)\n\t- https://github.com/IDEA-Research/DWPose\n- [[ComfyUI]] workflow for end to end from densepose [Comfy Workflows](https://comfyworkflows.com/workflows/4cd95372-4995-4740-8675-f228d4c24f41)\n- https://github.com/Fannovel16/comfyui_controlnet_aux?tab=readme-ov-file#dwposeanimalpose-only-uses-cpu-so-its-so-slow-how-can-i-make-it-use-gpu\n- [Comfy UI Tips & Tricks Fixing DW Open Pose (youtube.com)](https://www.youtube.com/watch?v=Ywc5BUN5SSc)\n-\n- Openpose\n- Yolov8\n- human stuff\n- [Volumetric primitives (MVP) avatar representation of Lombardi et al. [2021].](https://dl.acm.org/doi/abs/10.1145/3528233.3530740)\n- [Single shot vertex fitting](https://arxiv.org/abs/2205.06254)\n- [Meshcapade virtual humans](https://meshcapade.com/)\n- [AI video actor](https://share.synthesia.io/a5a12c73-09cb-4455-b007-147ae4b1effb)\n- [text to human motion](https://ofa-sys.github.io/MoFusion/)\n- [text to speech to simulated speaking movement](https://talkshow.is.tue.mpg.de/)\n- [nerf avatars](https://www.linkedin.com/posts/reneschulte_nerf-deeplearning-metaverse-activity-7010898662465617921-56P_?utm_source=share&utm_medium=member_desktop)\n- [chatgpt to avatar](https://twitter.com/IntuitMachine/status/1608690077139599360)\n- [toonify code and model](https://www.mmlab-ntu.com/project/vtoonify/)\n- [Talking head modifier](https://github.com/Meta-Portrait/MetaPortrait)\n- [AI face studio](https://www.d-id.com/)\n- MoveAI\n- [wifi based pose estimation](http://arxiv.org/pdf/2301.00250.pdf)\n- [Microsoft sculpted avatars](https://3d-avatar-diffusion.microsoft.com/?utm_campaign=AI%20Art%20Weekly&utm_medium=email&utm_source=Revue%20newsletter#/)\n- [ML realtime UE facial expresssions](https://80.lv/articles/ziva-dynamics-announces-a-new-ml-trained-facial-rigging-service/)\n- [Disney face aging](https://studios.disneyresearch.com/2022/11/30/production-ready-face-re-aging-for-visual-effects/)\n- [Gestures from speech](https://talkshow.is.tue.mpg.de/)\n- [Volucap volumentric deep fakes](https://volucap.com/)\n- [FlawlessAI cloud facial plus language translattion](https://www.flawlessai.com/)\n- [language to animated character](https://masterpiecestudio.com/blog/announcing-generative-animations)\n- [instant phone to unreal face opensource](https://github.com/JimWest/MeFaMo)\n- [consistent pose and angles in stable diffusion](https://www.youtube.com/watch?v=zgj24gTjQtY)\n- [Generating Consistent Characters using Stable Diffusion](https://www.youtube.com/watch?v=XWJGmNW15A4)\n- [consistent characters](https://www.youtube.com/watch?v=Ig1S2guCfKM)\n- [character turnarounds SD](https://www.youtube.com/watch?v=-iwPVUzAWzk)\n- [Decoupling humans from backgrounds](https://github.com/vye16/slahmr)\n- [pose](https://developers.google.com/mediapipe/solutions/vision/pose_landmarker/)\n- [free mocap project](https://freemocap.org/)\n- [openpose converter](https://github.com/Atif-Anwer/Mediapipe_to_OpenPose_JSON)\n- [Blender transfer rig](https://www.youtube.com/watch?v=i7n_jOFy5LY)\n- [MoGen avatar control](https://www.youtube.com/watch?v=jkkSqMsZLJE)\n- [Gaze estimation from images](https://github.com/deepinsight/insightface/tree/master/reconstruction/gaze)\n- [Alphapose](https://github.com/MVIG-SJTU/AlphaPose)\n- [Instant avatars from 60s of video](https://github.com/tijiang13/InstantAvatar)\n- [Expressive human avatars](https://github.com/Skype-line/X-Avatar)\n- [Vid2Avatar: 3D Avatar Reconstruction from Videos in the Wild via Self-supervised Scene Decomposition (other)](https://moygcc.github.io/vid2avatar/)\n- This text describes a system for reconstructing 3D avatars from videos in uncontrolled settings, i.e. \"in the wild.\" The system is Self-supervised, meaning that it does not require labeled data, and uses scene decomposition to break the video down into manageable chunks. The objective is to create a realistic, life-like avatar that can be used for a variety of purposes.Category: machine learning\n- [FLEX: Full-Body Grasping Without Full-Body Grasps\n\t- Columbia Computer Vision Lab (other)](https://flex.cs.columbia.edu/)\n- [Controlnet face model for SD1.5](https://www.reddit.com/r/StableDiffusion/comments/12dxue5/controlnet_face_model_for_sd_15/)\n- [Nvidia audio2face](https://www.nvidia.com/en-us/omniverse/apps/audio2face/)\n- [Generative 3D head](https://sizhean.github.io/panohead)\n- [DisCo dance, pose, and backgrounds from single image](https://github.com/Wangt-CN/DisCo)\n- Face swaps\n- [video](https://www.youtube.com/watch?v=QGrCkjfWpfU)\n- [Roop unleashed](https://github.com/C0untFloyd/roop-unleashed#installation)\n- [Reactor face swap](https://github.com/Gourieff/sd-webui-reactor)\n- [mtb face swap](https://github.com/melMass/comfy_mtb#face-detection--swapping)\n- [Facefusion](https://docs.facefusion.io/)\n- [Reactor Node](https://github.com/Gourieff/comfyui-reactor-node#standalone)\n- [facechain](https://github.com/modelscope/facechain)\n- [faceswap lab](https://glucauze.github.io/sd-webui-faceswaplab/)\n- [HQ faceswap](https://github.com/NNNNAI/VGGFace2-HQ)\n- [github subjects](https://github.com/topics/comfyui)\n\n## Current Landscape (2025)\n\n- Industry adoption and implementations\n  - Metaverse platforms continue to evolve with focus on interoperability and open standards\n  - Web3 integration accelerating with decentralised identity and asset ownership\n  - Enterprise adoption growing in virtual collaboration, training, and digital twins\n  - UK companies increasingly active in metaverse development and immersive technologies\n\n- Technical capabilities\n  - Real-time rendering at photorealistic quality levels\n  - Low-latency networking enabling seamless multi-user experiences\n  - AI-driven content generation and procedural world building\n  - Spatial audio and haptics enhancing immersion\n\n- UK and North England context\n  - Manchester: Digital Innovation Factory supports metaverse startups and research\n  - Leeds: Holovis leads in immersive experiences for entertainment and training\n  - Newcastle: University research in spatial computing and interactive systems\n  - Sheffield: Advanced manufacturing using digital twin technology\n\n- Standards and frameworks\n  - Metaverse Standards Forum driving interoperability protocols\n  - WebXR enabling browser-based immersive experiences\n  - glTF and USD for 3D asset interchange\n  - Open Metaverse Interoperability Group defining cross-platform standards\n\n## Metadata\n\n- **Last Updated**: 2025-11-16\n- **Review Status**: Automated remediation with 2025 context\n- **Verification**: Academic sources verified\n- **Regional Context**: UK/North England where applicable",
  "properties": {
    "id": "pose-estimation-owl-axioms",
    "collapsed": "true",
    "- ontology": "true",
    "- term-id": "mv-296381584315",
    "- preferred-term": "Pose estimation",
    "- source-domain": "metaverse",
    "- status": "active",
    "- public-access": "true",
    "- definition": "A component of the metaverse ecosystem focusing on pose estimation.",
    "- maturity": "mature",
    "- authority-score": "0.85",
    "- owl:class": "mv:PoseEstimation",
    "- owl:physicality": "VirtualEntity",
    "- owl:role": "Object",
    "- belongsToDomain": "[[MetaverseDomain]]",
    "- is-part-of": "[[VirtualWorld]], [[MetaversePlatform]]",
    "- requires": "[[DigitalIdentity]], [[AuthenticationService]]",
    "- enables": "[[SocialInteraction]], [[Presence]], [[UserRepresentation]]",
    "- has-property": "[[Appearance]], [[Customization]], [[Animation]]"
  },
  "backlinks": [],
  "wiki_links": [
    "VirtualWorld",
    "Animation",
    "MetaversePlatform",
    "Presence",
    "MetaverseDomain",
    "Appearance",
    "DigitalIdentity",
    "ComfyUI",
    "Customization",
    "AuthenticationService",
    "UserRepresentation",
    "SocialInteraction"
  ],
  "ontology": {
    "term_id": "mv-296381584315",
    "preferred_term": "Pose estimation",
    "definition": "A component of the metaverse ecosystem focusing on pose estimation.",
    "source_domain": "metaverse",
    "maturity_level": null,
    "authority_score": 0.85
  }
}