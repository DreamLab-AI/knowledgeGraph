{
  "title": "Humans, Avatars , Character",
  "content": "- ### OntologyBlock\n  id:: humans,-avatars-,-character-ontology\n  collapsed:: true\n\t- ontology:: true\n\t- term-id:: mv-187488308969\n\t- preferred-term:: Humans, Avatars , Character\n\t- source-domain:: metaverse\n\t- status:: active\n\t- public-access:: true\n\t- definition:: A component of the metaverse ecosystem focusing on humans, avatars , character.\n\t- maturity:: mature\n\t- authority-score:: 0.85\n\t- owl:class:: mv:HumansAvatarsCharacter\n\t- owl:physicality:: VirtualEntity\n\t- owl:role:: Object\n\t- belongsToDomain:: [[MetaverseDomain]]\n\t- #### Relationships\n\t  id:: humans,-avatars-,-character-relationships\n\t\t- is-part-of:: [[VirtualWorld]], [[MetaversePlatform]]\n\t\t- requires:: [[DigitalIdentity]], [[AuthenticationService]]\n\t\t- enables:: [[SocialInteraction]], [[Presence]], [[UserRepresentation]]\n\t\t- has-property:: [[Appearance]], [[Customization]], [[Animation]]\n\n\t- #### OWL Axioms\n\t  id:: humans,-avatars-,-character-owl-axioms\n\t  collapsed:: true\n\t\t- ```clojure\n\t\t  Declaration(Class(mv:HumansAvatarsCharacter))\n\n\t\t  # Classification\n\t\t  SubClassOf(mv:HumansAvatarsCharacter mv:VirtualEntity)\n\t\t  SubClassOf(mv:HumansAvatarsCharacter mv:Object)\n\n\t\t  # Domain classification\n\t\t  SubClassOf(mv:HumansAvatarsCharacter\n\t\t    ObjectSomeValuesFrom(mv:belongsToDomain mv:MetaverseDomain)\n\t\t  )\n\n\t\t  # Annotations\n\t\t  AnnotationAssertion(rdfs:label mv:HumansAvatarsCharacter \"Humans, Avatars , Character\"@en)\n\t\t  AnnotationAssertion(rdfs:comment mv:HumansAvatarsCharacter \"A component of the metaverse ecosystem focusing on humans, avatars , character.\"@en)\n\t\t  AnnotationAssertion(dcterms:identifier mv:HumansAvatarsCharacter \"mv-187488308969\"^^xsd:string)\n\t\t  ```\n\npublic:: true\n\n- ## Humans, Avatars, and Character\n\t- ### Avatar Generation: Creating Digital Beings from Scratch\n\t\t- This section focuses on platforms and research enabling the generation of complete avatars, encompassing both visual representation and underlying technologies.\n\t\t- * [REPLIKANT](https://www.replikant.com/): An AI-assisted 3D avatar and animation platform designed for creators.\n\t\t  * [Meta Research Paper](https://drive.google.com/file/d/1i4NJKAggS82wqMamCJ1OHRGgViuyoY6R/view): A research paper from Meta exploring an unspecified aspect of avatar generation.\n\t\t  * [Heygen](https://www.heygen.com/): A platform for generating and animating realistic avatars from text prompts and images.\n\t\t  * [Synthesia](https://www.synthesia.io/): A leading platform for creating AI-powered videos featuring realistic avatars.\n\t- ### Face Generation: Crafting Realistic and Expressive Visages\n\t\t- This section explores projects and techniques dedicated to the intricate art of generating digital faces, capturing nuances of expression and identity.\n\t\t- * [Media2Face](https://sites.google.com/view/media2face): A project focused on generating faces from various media inputs.\n\t\t  * [Towards a Simultaneous and Granular Identity-Expression Control in Personalized Face Generation](https://diffsfsr.github.io/): Research on personalized face generation with fine-grained control over identity and expression.\n\t\t  * [HeadStudio](https://huggingface.co/papers/2402.06149): A project utilizing 3D Gaussian Splatting to create animatable head avatars from text.\n\t\t  * **Lip Sync:**  Achieving realistic lip synchronization is crucial for believable avatars. These resources delve into this challenge:\n\t\t    * [VASA-1 (Microsoft Research)](https://www.microsoft.com/en-us/research/project/vasa-1/): A project from Microsoft Research exploring lip synchronization techniques.\n\t\t    * [wav2lips](https://becominghuman.ai/deepfake-audio-with-wav2lip-263f0f0e84bc): An article exploring the use of wav2lip for deepfake audio and lip synchronization.\n\t\t    * [Realtime lip-sync API](https://getsynchronicity.io/): An API offering real-time lip synchronization capabilities.\n\t- ### Body Generation: Building Digital Bodies with Realistic Form\n\t\t- This section examines projects that focus on generating realistic and diverse digital bodies, laying the foundation for virtual humans and characters.\n\t\t- * [Volumetric Primitives (MVP)](https://dl.acm.org/doi/abs/10.1145/3528233.3530740): A research paper exploring the use of volumetric primitives for avatar representation.\n\t\t  * [Gaussian Shell Maps](https://rameenabdal.github.io/GaussianShellMaps/): A research project dedicated to the use of Gaussian Shell Maps for efficient 3D human generation.\n\t\t  * [En3D](https://menyifang.github.io/projects/En3D/index.html): A project focused on generating 3D human models with realistic detail.\n\t\t  * [Character Creator 3 (Reallusion)](https://www.reallusion.com/character-creator/): A powerful tool for creating realistic and stylised 3D characters.\n\t- ### Hand Generation: Recreating the Complexity of Human Hands\n\t\t- The human hand, with its intricate movements and expressiveness, poses a unique challenge for digital recreation. This section highlights projects tackling this complexity.\n\t\t  \n\t\t  * [URHand](https://frozenburning.github.io/projects/urhand/): A project dedicated to the generation and animation of realistic digital hands.\n\t- ### Audio-Driven Avatar: Animating Avatars with the Power of Voice\n\t\t- This section explores projects that leverage audio input to drive avatar animation, creating a more immersive and responsive experience.\n\t\t  \n\t\t  * [audio2photoreal (Facebook Research)](https://github.com/facebookresearch/audio2photoreal): A project from Facebook Research focused on generating photorealistic avatars driven by audio.\n\t- ### Human Texture Estimation: Bringing Digital Skin to Life\n\t\t- Realistic skin texture is crucial for believable digital humans. This section explores projects and research dedicated to accurately estimating and recreating human skin textures.\n\t\t  \n\t\t  * [SMPLitex](https://dancasas.github.io/projects/SMPLitex/index.html): A generative model and dataset for estimating 3D human texture from a single image.\n\t\t  * [Human texture estimation from a single image](https://scholar.google.com/scholar?q=human+texture+estimation+from+a+single+image): A broader exploration of the field of human texture estimation from single images.\n\t- ### Miscellaneous: Exploring the Wider Landscape of Digital Human Creation\n\t\t- This section encompasses a variety of projects and resources that contribute to the broader field of digital human and avatar creation.\n\t\t  \n\t\t  * [StableIdentity](https://qinghew.github.io/StableIdentity/): A project exploring the insertion of individuals into new environments.\n\t\t  * [What You See Is What You GAN](https://research.nvidia.com/labs/nxp/wysiwyg/): An NVIDIA research paper on rendering high-fidelity geometry in 3D GANs.\n\t\t  * [Generating Consistent Characters (Twitter Thread)](https://twitter.com/manuvision/status/1745326572154863816?s=20): A Twitter thread by Manu.Vision discussing methods for generating consistent characters.\n\t\t  * [Consistent Characters with Scenario (YouTube Tutorial)](https://www.youtube.com/watch?v=aEnDEraK3I8): A YouTube tutorial on creating consistent characters within a specific scenario.\n\t\t  * [MoMask (GitHub Repository)](https://github.com/EricGuo5513/momask-codes): The official implementation of MoMask, a method for generative masked modelling of 3D human motions.\n\t\t  * [Character Asset Creator (Reddit Post)](https://www.reddit.com/r/StableDiffusion/comments/19aymjf/i_made_a_character_asset_creator_comfyui_backend/): A Reddit post discussing a Character Asset Creator for StableDiffusion.\n\t\t  * [Mikugg Emotions Script](https://rentry.org/mikugg-emotions-script): A script for creating emotions in Mikugg.\n\t\t  * [Miku.gg Bot Emotions Documentation](https://docs.miku.gg/guides/bots/create-bot-emotions/): Documentation on creating bot emotions within the Miku.gg platform.\n\t\t  * [Consistent Character Maker Workflow (Civitai)](https://civitai.com/models/287147/workflow-consistant-character-maker): A workflow for generating consistent characters using Stable Diffusion.\n\t- ### Character Consistency: Maintaining Identity Across Generations\n\t\t- Achieving consistency in character appearance across multiple generations is a key challenge. This section highlights resources and techniques addressing this issue.\n\t\t- * [Consistent Character Creation (OpenArt)](https://openart.ai/promptbook/an-attempt-at-full-character-consistency-using-sdxl-model-with-lightning-8-step-lora-comfyui-workflow): A workflow for achieving full-character consistency using the SDXL model with a Lightning 8-step LoRA.\n\t- ### A Workflow for 3D Character Creation and Animation: A Step-by-Step Guide\n\t\t- This section outlines a comprehensive workflow for creating and animating 3D characters, leveraging a variety of tools and techniques.\n\t\t  \n\t\t  * [LinkedIn Post detailing the workflow](https://www.linkedin.com/posts/lovis-odin-7a751360_how-to-create-this-3d-animation-for-free-activity-7179810196922257409-8PcO?utm_source=share&utm_medium=member_desktop): A LinkedIn post outlining the workflow in detail.\n\t\t  * Tools used in the workflow:\n\t\t    * [Midjourney](https://www.midjourney.com/): For generating initial character concepts.\n\t\t    * [Tripo3d.ai](https://tripo3d.ai/): For creating 3D models from images.\n\t\t    * [AI Digimans' PBR Texture Generation Tool](https://lnkd.in/gBvGH_PH): For generating realistic textures.\n\t\t    * [Mixamo](https://mixamo.com/): For animating the 3D models.\n\t\t    * [Spline.design](https://spline.design/): For further customisation and enhancement.\n\t- ### Animation: Breathing Life into Digital Characters\n\t\t- Bringing digital characters to life requires compelling animation. This section explores projects and resources focused on achieving realistic and expressive character movement.\n\t\t\t- * [Animatable Gaussians (GitHub Repository)](https://github.com/lizhe00/AnimatableGaussians/tree/master): Code for \"Animatable Gaussians: Learning Pose-dependent Gaussian Maps for High-fidelity Human Avatar Modeling,\" presented at CVPR 2024.\n\t\t\t  * [3D Gaussian Blendshapes](https://gapszju.github.io/GaussianBlendshape/): Exploration of the use of 3D Gaussian Blendshapes for head avatar animation.\n\t\t\t  * [Eggnog](https://www.eggnog.ai/infinite): A platform for creating infinite AI videos.\n\t\t\t  * [Efficient Portrait Animation (LivePortrait)](https://liveportrait.github.io/): A project focused on efficient portrait animation with stitching and retargeting control.\n\t\t\t    * [Note by Daniele](https://primal.net/e/note1jcmsj7ls2fnueeqmg8t6cn4a7zpg53wxwgtmv47hpvurw40y8fdq4ylfc9): A note on LivePortrait by Daniele.\n\t\t\t    * [ComfyUI Nodes for LivePortrait (GitHub Repository)](https://github.com/kijai/ComfyUI-LivePortraitKJ?tab=readme-ov-file): ComfyUI nodes designed for LivePortrait.\n\t\t\t  * [CLARA2 (GitHub Repository)](https://github.com/0xMatthew/CLARA2): A 3D-rendered AI agent designed to present PowerPoint presentations.\n\t\t\t  * [Consistent Character API (Replicate)](https://replicate.com/fofr/consistent-character): An API for running fofr's consistent character model.\n\t\t\t  * [Joystick-Controlled Character Manipulation (Twitter)](https://x.com/JungleSilicon/status/1798457212291150189): A concept for manipulating character features using a joystick.\n\t\t\t  * [Volucap Authentic Digital Avatars](https://volucap.com/): A company specialising in creating authentic digital avatars.\n\t\t\t  * [Openpose Controlnets (Civitai Article)](https://civitai.com/articles/157/openpose-controlnets-v11-using-poses-and-generating-new-ones): An article explaining how to use and generate poses with Openpose Controlnets V1.1.\n\t\t\t  * [AI Modelling Agency (Creative Bloq Article)](https://www.creativebloq.com/news/ai-model-agency): An article discussing the emergence of AI modelling agencies.\n\t\t\t  * [Free VRChat Avatars & 3D Assets (VRCMods)](https://vrcmods.com/): A collection of free VRChat avatars and 3D assets.\n\t\t\t  * [AniTalker](https://x-lance.github.io/AniTalker/): A project focused on animating talking heads.\n\t\t\t  * [VLOGGER](https://enriccorona.github.io/vlogger/): A project related to creating virtual vloggers.\n\t\t\t  * [StoryDiffusion](https://storydiffusion.github.io/): A project exploring consistent self-attention for long-range image and video generation.\n\t\t\t  * [SoccerNet Game State Reconstruction (GitHub Repository)](https://github.com/SoccerNet/sn-gamestate): A project focusing on athlete tracking and identification on a minimap.\n\t\t\t  * [Ukraine's AI Avatar for Consular Affairs (Reddit Post)](https://www.reddit.com/r/singularity/comments/1chhgd3/ukraines_ministry_of_foreign_affairs_annonced/): A discussion on Ukraine's use of an AI avatar for consular updates.\n\t\t\t  * [Animating Images with Viggle AI (YouTube Tutorial)](https://www.youtube.com/watch?v=jMVTljPXVVo&t=75s): A tutorial on animating images using Viggle AI.\n\t\t\t  * [PhysAvatar (Hugging Face Paper)](https://huggingface.co/papers/2404.04421): Research on learning the physics of dressed 3D avatars from visual observations.\n\t\t\t  * [Vid2Avatar](https://moygcc.github.io/vid2avatar/): A project focused on reconstructing 3D avatars from videos.\n\t\t\t  * [Human Tracking and SLAM Capture (YouTube Video)](https://youtu.be/u9Z8CK561_Y?t=2057): A demonstration of human tracking and SLAM capture technology.\n\t\t\t  * [ComfyUI Character Turntable with SV3D (Reddit Post)](https://www.reddit.com/r/StableDiffusion/comments/1bme62y/comfyui_creating_a_character_turntable_with_sv3d/): A discussion on creating character turntables using ComfyUI and SV3D.\n\t\t\t  * [Animating Characters for Free (LinkedIn Post)](https://www.linkedin.com/posts/jacques-alomo_animate-any-character-now-for-free-activity-7177666734957420544-N7v9/?utm_source=share&utm_medium=member_android): A post highlighting methods for animating characters for free.\n\t\t\t  * [Midjourney Character Reference Feature (Medium Article)](https://degennfts.medium.com/new-midjourney-feature-character-reference-oppenheimer-example-906750d53d3f): An article exploring Midjourney's Character Reference feature.\n\t\t\t  * [Full-Character Consistency with SDXL (Reddit Post)](https://www.reddit.com/r/StableDiffusion/comments/1azn5lq/an_attempt_at_full-character_consistancy_sdxl/): A discussion on achieving full-character consistency using SDXL.\n\t\t\t  * [Create Bot Emotions (Miku.gg Documentation)](https://docs.miku.gg/guides/bots/create-bot-emotions/): Documentation on creating bot emotions within the Miku.gg platform.\n\t\t\t  * [Consistent Emotions on a Character with ComfyUI (Reddit User)](https://www.reddit.com/user/iwantofftheride00/): A Reddit user's plans to publish a method for achieving consistent emotions on a character using ComfyUI.\n\t\t\t  * [BakedAvatar](https://buaavrcg.github.io/BakedAvatar/): A project focused on avatar creation.\n\t\t\t  * [Dreamtalk (GitHub Repository)](https://github.com/ali-vilab/dreamtalk): The official implementation of Dreamtalk, focusing on expressive talking head generation.\n\t\t\t  * [CharTurnerBeta LoRA (Civitai)](https://civitai.com/models/7252/charturnerbeta-lora-experimental): A LoRA for multi-direction consistency in Stable Diffusion character generation.\n\t\t\t  * [VividTalk](https://humanaigc.github.io/vivid-talk/): A project focused on one-shot audio-driven talking head generation.\n\t\t\t  * **Gaussian-Based Avatars (Hugging Face Papers):**\n\t\t\t    * [Relightable Gaussian Codec Avatars](https://huggingface.co/papers/2312.03704): Research on using Gaussian codecs for avatar representation.\n\t\t\t    * [Gaussian Head Avatar](https://huggingface.co/papers/2312.03029): Research on creating high-fidelity head avatars using dynamic Gaussians.\n\t\t\t  * [NLW Education Discord Projects (Discord Channel)](https://discord.com/channels/1181054284528373761/1181055410950647838): A Discord channel discussing projects related to ElevenLabs and character/avatar creation.\n\t\t\t  * [D-ID AI Video Mobile App](https://www.d-id.com/creative-reality-studio-mobile-app/): A mobile app for creating AI videos.\n\t\t\t  * [GAIA (Microsoft)](https://microsoft.github.io/GAIA/): A project from Microsoft exploring advanced avatar technologies.\n\t\t\t  \n\t\t\t  This meticulously curated collection offers a comprehensive overview of the dynamic field of digital human and avatar creation. Explore, learn, and contribute to the ongoing evolution of this exciting frontier!\n\t\t\t  \n\t\t\t  **Note:** Some links may lead to projects or resources that are still under development or experimental. Remember to review any licensing information before using code or assets from these projects.\n\n## Current Landscape (2025)\n\n- Industry adoption and implementations\n  - Metaverse platforms continue to evolve with focus on interoperability and open standards\n  - Web3 integration accelerating with decentralised identity and asset ownership\n  - Enterprise adoption growing in virtual collaboration, training, and digital twins\n  - UK companies increasingly active in metaverse development and immersive technologies\n\n- Technical capabilities\n  - Real-time rendering at photorealistic quality levels\n  - Low-latency networking enabling seamless multi-user experiences\n  - AI-driven content generation and procedural world building\n  - Spatial audio and haptics enhancing immersion\n\n- UK and North England context\n  - Manchester: Digital Innovation Factory supports metaverse startups and research\n  - Leeds: Holovis leads in immersive experiences for entertainment and training\n  - Newcastle: University research in spatial computing and interactive systems\n  - Sheffield: Advanced manufacturing using digital twin technology\n\n- Standards and frameworks\n  - Metaverse Standards Forum driving interoperability protocols\n  - WebXR enabling browser-based immersive experiences\n  - glTF and USD for 3D asset interchange\n  - Open Metaverse Interoperability Group defining cross-platform standards\n\n## Metadata\n\n- **Last Updated**: 2025-11-16\n- **Review Status**: Automated remediation with 2025 context\n- **Verification**: Academic sources verified\n- **Regional Context**: UK/North England where applicable",
  "properties": {
    "id": "humans,-avatars-,-character-owl-axioms",
    "collapsed": "true",
    "- ontology": "true",
    "- term-id": "mv-187488308969",
    "- preferred-term": "Humans, Avatars , Character",
    "- source-domain": "metaverse",
    "- status": "active",
    "- public-access": "true",
    "- definition": "A component of the metaverse ecosystem focusing on humans, avatars , character.",
    "- maturity": "mature",
    "- authority-score": "0.85",
    "- owl:class": "mv:HumansAvatarsCharacter",
    "- owl:physicality": "VirtualEntity",
    "- owl:role": "Object",
    "- belongsToDomain": "[[MetaverseDomain]]",
    "- is-part-of": "[[VirtualWorld]], [[MetaversePlatform]]",
    "- requires": "[[DigitalIdentity]], [[AuthenticationService]]",
    "- enables": "[[SocialInteraction]], [[Presence]], [[UserRepresentation]]",
    "- has-property": "[[Appearance]], [[Customization]], [[Animation]]",
    "public": "true"
  },
  "backlinks": [
    "Mixed reality",
    "Daniel AI creative technologist",
    "Social contract and jobs",
    "Louby Lou",
    "Gaussian splatting and Similar"
  ],
  "wiki_links": [
    "DigitalIdentity",
    "AuthenticationService",
    "Appearance",
    "SocialInteraction",
    "MetaversePlatform",
    "Animation",
    "UserRepresentation",
    "Customization",
    "VirtualWorld",
    "MetaverseDomain",
    "Presence"
  ],
  "ontology": {
    "term_id": "mv-187488308969",
    "preferred_term": "Humans, Avatars , Character",
    "definition": "A component of the metaverse ecosystem focusing on humans, avatars , character.",
    "source_domain": "metaverse",
    "maturity_level": null,
    "authority_score": 0.85
  }
}