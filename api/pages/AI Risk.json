{
  "title": "AI Risk",
  "content": "- ### OntologyBlock\n  id:: ai-risk-ontology\n  collapsed:: true\n\t- ontology:: true\n\t- term-id:: AI-0076\n\t- preferred-term:: AI Risk\n\t- source-domain:: ai\n\t- status:: draft\n\t- public-access:: true\n\n  - **Definition**\n    - definition:: AI Risk refers to the composite measure combining the probability of occurrence and magnitude of potential adverse consequences arising from the design, development, deployment, operation, or use of artificial intelligence systems that may cause harm, damage, violations, or negative impacts affecting individuals, groups, organizations, communities, society, or the environment, encompassing diverse risk categories including technical risks (performance failures, robustness issues, model drift, adversarial vulnerabilities), ethical risks (bias and discrimination, fairness violations, autonomy erosion, value misalignment), privacy and security risks (data breaches, privacy violations, membership inference, model extraction, adversarial attacks), safety risks (physical harm from autonomous systems, critical infrastructure failures, unintended consequences), legal and compliance risks (regulatory violations, liability exposure, intellectual property infringement), operational risks (deployment failures, integration issues, resource constraints, maintainability challenges), reputational risks (public backlash, stakeholder concerns, brand damage), societal risks (labor displacement, inequality amplification, democratic erosion, epistemic harms), and existential risks (loss of human control over advanced AI systems, civilization-scale catastrophic outcomes). This comprehensive risk framework characterizes risks across multiple dimensions: likelihood (rare, unlikely, possible, likely, almost certain), impact or severity (insignificant, minor, moderate, major, catastrophic), velocity (time from manifestation to impact), persistence (duration of adverse effects), scope (number and types of affected stakeholders), reversibility (ability to remediate consequences), and detectability (ease of identification before significant harm occurs). AI-specific risk characteristics distinguish AI risk from traditional technology risk through opacity and interpretability challenges whereby complex models function as \"black boxes\" making risk identification difficult, autonomy and adaptability enabling systems to learn and evolve in ways that diverge from design intentions, data dependency creating risks from training data quality, bias, representativeness, and privacy, scaling effects where deployment across millions of decisions amplifies impact of small error rates or biases, emergent behaviors arising from system interactions that cannot be predicted from individual components, value alignment challenges ensuring AI objectives match human values and intentions, and dual-use potential enabling beneficial technologies to cause harm through misuse. Risk management approaches follow structured frameworks including ISO/IEC 23894:2023 AI risk management guidance adapting ISO 31000 for AI-specific contexts, NIST AI Risk Management Framework (AI RMF 1.0, updated with Generative AI Profile NIST-AI-600-1 addressing unique generative AI risks), EU AI Act risk-based regulatory framework categorizing systems as unacceptable, high, limited, or minimal risk with proportionate requirements, and sector-specific frameworks from healthcare (FDA), finance (prudential regulators), and critical infrastructure. Contemporary AI risk concerns in 2024-2025 include prompt injection attacks exploiting large language models, model drift degrading performance as real-world conditions evolve, bias incidents in hiring and lending, privacy breaches from training data leakage, deepfakes and AI-generated misinformation eroding epistemic trust, AI-powered cyber threats, autonomous weapons and dual-use concerns, labor market disruption affecting vulnerable populations, and alignment challenges as AI capabilities approach and potentially exceed human-level performance in narrow and eventually broad domains.\n    - maturity:: mature\n    - source:: [[ISO/IEC 23894:2023]], [[NIST AI RMF]], [[EU AI Act]], [[NIST-AI-600-1]]\n    - authority-score:: 0.95\n\n\n- ## Definition and Scope\n\n  Artificial Intelligence Risk refers to the potential for AI systems to cause adverse effects on individuals, groups, organizations, communities, or society at large. These risks arise from technical failures, security vulnerabilities, biased outcomes, privacy violations, or unintended consequences during AI system design, deployment, or operation.\n\n  The academic foundations of AI risk management draw from computer science, ethics, cybersecurity, and social sciences, emphasizing interdisciplinary approaches to understand and mitigate harm. According to the NIST AI Risk Management Framework (AI RMF 1.0, January 2023), AI risk is defined as the composite measure of an event's probability of occurring and the magnitude or degree of the consequences of the corresponding event.\n\n\n- ## Formal Specification\n\n  ### Ontological Structure\n\t- AI Risk is a subclass of general Risk with specific characteristics unique to artificial intelligence systems\n\t- Core components include likelihood (probability of risk occurring), impact (severity of consequences), and context (domain and stakeholders affected)\n\t- Risk types encompass technical, ethical, social, legal, security, and operational categories\n\t- Standards alignment includes ISO/IEC 23894:2023, NIST AI RMF 1.0, EU AI Act (Regulation 2024/1689)\n\n  ### Risk Properties\n\t- AI risks require assessment of posesRiskTo, hasLikelihood, hasImpact, managedBy, and manifestsAs relationships\n\t- Risk scoring ranges from 0.0 (no risk) to 1.0 (critical risk) based on composite assessment\n\t- Risk levels categorized as unacceptable, high, limited, or minimal per EU AI Act framework\n\n\n- ## Authoritative References\n\n  ### Primary Standards and Frameworks\n\n\t- #### ISO/IEC 23894:2023 - AI Risk Management\n\t  Published February 2023, this comprehensive international standard for [[AI Risk Management]] adapts traditional risk management practices (ISO 31000) to AI's unique characteristics including opacity, complexity, autonomy, and data dependency. The standard gained widespread adoption throughout 2024-2025 as organizations sought structured risk management methodologies. Implementation requires cross-functional collaboration between data scientists, security professionals, legal counsel, and business leaders.\n\n\t- #### NIST AI Risk Management Framework (AI RMF 1.0)\n\t  Released January 2023, updated with Generative AI Profile (NIST-AI-600-1) on July 26, 2024. The Profile added over 200 specific actions addressing unique risks including CBRN information risks, confabulation (hallucinations), dangerous or hateful content generation, data privacy violations, information integrity issues (deepfakes), intellectual property infringement, and obscene content generation. This framework became essential for organizations deploying [[Large Language Models]], image generators, and multimodal systems.\n\n\t- #### EU AI Act (Regulation 2024/1689)\n\t  Finalized June 2024, entered into force August 1, 2024. The risk-based regulatory framework categorizes AI systems as unacceptable, high, limited, or minimal risk, establishing the global template for risk-proportionate regulation. **[Updated 2025]** Implementation proceeds on schedule with phased obligations. Prohibited practices became effective February 2, 2025. Major provisions including GPAI models, governance, and penalties apply from August 2, 2025. Full high-risk AI system requirements take effect August 2, 2026. Non-compliance penalties reach up to €35 million or 7% of worldwide annual turnover, whichever is higher.\n\n\n- ## Risk Categories and Taxonomy\n\n  ### Technical Risks\n\n\t- #### Performance Failures\n\t\t- Inaccurate predictions and erroneous outputs leading to flawed decision-making\n\t\t- System errors and operational malfunctions during deployment\n\t\t- Model drift whereby changes in data distributions or relationships lead to degraded performance over time\n\t\t- **[Updated 2025]** Model drift emerged as a pervasive operational risk in 2024-2025, with fraud detection models becoming less accurate as fraudulent tactics evolved. Organizations discovered models require continuous monitoring and periodic retraining to maintain efficacy.\n\n\t- #### Robustness and Reliability Issues\n\t\t- Failure under distribution shift when encountering data different from training distribution\n\t\t- Inconsistent behavior across similar inputs\n\t\t- System downtime and availability problems\n\t\t- **[Updated 2025]** The 2024 AI Safety Index found that all flagship models remained vulnerable to adversarial attacks despite claimed improvements, revealing fundamental architectural limitations.\n\n\t- #### Security Vulnerabilities\n\t\t- Adversarial attacks using carefully crafted inputs to induce misclassification\n\t\t- Data poisoning whereby malicious actors corrupt training data\n\t\t- Model extraction attacks allowing theft of proprietary models\n\t\t- **[Updated 2025]** Prompt injection attacks emerged as the top LLM security risk according to OWASP Top 10 for LLMs in 2025. Attackers exploit prompt injection to extract training data, bypass content filters, and gain unauthorized access to integrated systems.\n\n  ### Ethical and Social Risks\n\n\t- #### Bias and Discrimination\n\t\t- Unfair outcomes for protected groups due to biased training data or algorithmic design\n\t\t- Perpetuation and amplification of existing societal inequalities\n\t\t- Disproportionate impact on vulnerable populations\n\t\t- **[Updated 2025]** Analysis of AI incident databases in 2024-2025 showed bias incidents (employment discrimination, credit denial) dominated reports, highlighting persistent challenges in fairness.\n\n\t- #### Privacy Violations\n\t\t- Unauthorized data disclosure through model outputs\n\t\t- Training data leakage revealing sensitive information\n\t\t- Membership inference attacks determining if specific data was used in training\n\t\t- Model inversion attacks reconstructing training data from model parameters\n\t\t- **[Updated 2025]** The Samsung Engineering Data Breach (2023, lessons learned 2024) demonstrated intellectual property leakage when engineers inadvertently pasted sensitive semiconductor designs into ChatGPT, catalyzing enterprise policies prohibiting use of public AI systems for sensitive data.\n\n\t- #### Transparency and Accountability Gaps\n\t\t- Opaque decision-making processes in complex models (the \"black box\" problem)\n\t\t- Unclear responsibility for AI-caused harms\n\t\t- Difficulty explaining AI outputs to affected stakeholders\n\t\t- **[Updated 2025]** The Air Canada Chatbot Liability Case (2024) established legal precedent that organizations cannot disclaim responsibility for AI agent actions, holding companies accountable for chatbot representations.\n\n  ### Operational Risks\n\n\t- #### Deployment and Integration Failures\n\t\t- Incorrect system implementation in production environments\n\t\t- Integration issues with existing infrastructure\n\t\t- Configuration errors leading to unintended behavior\n\t\t- Inadequate testing before deployment\n\n\t- #### Human-AI Interaction Risks\n\t\t- Misuse through deliberate exploitation of system capabilities\n\t\t- Over-reliance and automation bias where humans defer excessively to AI judgments\n\t\t- Under-reliance where AI capabilities are not properly utilized\n\t\t- Skill degradation as humans lose proficiency in tasks delegated to AI\n\n\t- #### Maintenance and Lifecycle Risks\n\t\t- Outdated models failing to adapt to changing environments\n\t\t- Concept drift as real-world data distributions evolve\n\t\t- Inadequate monitoring of deployed systems\n\t\t- Insufficient incident response capabilities\n\n  ### Legal and Regulatory Risks\n\n\t- #### Non-Compliance\n\t\t- Violation of data protection regulations including [[GDPR]] and regional privacy laws\n\t\t- Failure to meet AI-specific regulatory requirements under [[EU AI Act]] and emerging frameworks\n\t\t- Industry-specific regulatory violations (healthcare, finance, employment)\n\t\t- **[Updated 2025]** Only 12% of companies felt \"very prepared\" to assess, manage, and recover from AI risks in 2024-2025 surveys, whilst 75% lacked dedicated plans for generative AI risks specifically.\n\n\t- #### Liability and Legal Responsibility\n\t\t- Legal responsibility for AI-caused harms to individuals or organizations\n\t\t- Product liability for defective AI systems\n\t\t- Professional liability when AI systems provide advice or services\n\t\t- Contractual obligations and warranty issues\n\n\t- #### Intellectual Property Risks\n\t\t- Copyright infringement through training data usage\n\t\t- Patent infringement in AI system design or outputs\n\t\t- Trade secret misappropriation\n\t\t- Ownership disputes over AI-generated content\n\n\n- ## 2024-2025: From Theoretical Frameworks to Operational Reality\n\n  The years 2024 and 2025 witnessed AI risk transition from predominantly theoretical concern to operational reality, as high-profile failures demonstrated the inadequacy of existing risk management practices, whilst simultaneously driving rapid adoption of risk frameworks, dramatic surges in cybersecurity threats, and the proliferation of model drift undermining production systems.\n\n  ### Catastrophic Real-World Failures\n\n  2024-2025 exposed significant gaps between risk management frameworks and operational practice through high-profile incidents that demonstrated technical risk management was insufficient without addressing operational processes, employee training, and legal accountability structures.\n\n  **Samsung Engineering Data Breach (2023, lessons learned 2024):** Engineers used ChatGPT to debug code, inadvertently pasting sensitive semiconductor designs and proprietary data into the chat interface across three separate incidents. The engineers failed to appreciate that inputs could be used to train future models, resulting in intellectual property leakage. This incident catalyzed enterprise policies prohibiting use of public AI systems for sensitive data.\n\n  **Air Canada Chatbot Liability Case (2024):** An Air Canada chatbot promised a bereavement discount to a customer. The airline refused to honour the commitment, arguing in court that the chatbot was a separate legal entity. The court rejected this defence, holding Air Canada accountable for its chatbot's representations, establishing precedent that organisations cannot disclaim responsibility for AI agent actions.\n\n  ### Security Threat Explosion\n\n  AI-powered cyberattacks surged dramatically in 2024-2025. Projections indicated a 50% increase in AI-driven attacks in 2024 compared to 2021, with 93% of security leaders expecting their organisations to face daily AI-driven attacks by 2025.\n\n  Prompt injection attacks—whereby malicious inputs manipulate AI systems into performing unintended actions—emerged as the top LLM security risk according to the OWASP Top 10 for LLMs in 2025. Attackers exploited prompt injection to extract training data, bypass content filters, and gain unauthorized access to integrated systems.\n\n  Adversarial attacks demonstrated universal vulnerability: even state-of-the-art models remained susceptible to carefully crafted inputs designed to induce misclassification. The 2024 AI Safety Index found that all flagship models were vulnerable to adversarial attacks, revealing fundamental architectural limitations.\n\n  ### Emerging Risk Categories\n\n  2024-2025 identified novel risk categories absent from earlier frameworks:\n\n\t- #### Supply Chain Risks\n\t  Foundation models developed by third parties introduced dependency risks whereby upstream model vulnerabilities affected countless downstream applications. When a foundation model exhibited bias or security flaws, all fine-tuned derivatives inherited those risks.\n\n\t- #### Compounding Risks\n\t  AI systems increasingly operated in multi-agent environments where risks compounded unpredictably. For instance, multiple trading algorithms interacting created flash crash risks exceeding any single system's design parameters.\n\n\t- #### Value Alignment Risks\n\t  As AI systems gained autonomy, ensuring alignment with human values and intentions became critical. Specification gaming—whereby systems technically satisfy stated objectives whilst violating intent—demonstrated the difficulty of robust value alignment.\n\n  ### Risk Quantification Challenges\n\n  Organizations struggled to quantify AI risk in financially meaningful terms required for board-level decision-making and insurance underwriting. Unlike traditional IT risks with established actuarial models, AI risks lacked historical data, making probability and impact estimation speculative. Only 12% of companies felt \"very prepared\" to assess, manage, and recover from AI risks in 2024-2025 surveys, whilst 75% lacked dedicated plans for generative AI risks specifically.\n\n\n- ## AI-Generated Content and Misinformation Risks\n\n  ### The Evolution of AI-Generated Fake News (2023-2025)\n\n  **[Updated 2025]** The Guardian's 2023 warnings about [[AI-Generated Content]] risks have been validated and expanded upon in 2024-2025. Large language models can rapidly produce plausible-sounding but entirely fictitious news articles, blurring the line between credible information and misinformation. Malicious actors or even careless users can mislead the public by leveraging AI's ability to fabricate realistic content.\n\n  Academic publishing has experienced a surge in AI-generated, error-riddled papers, with more than 10,000 papers retracted in 2023 alone—many due to AI-driven misconduct or mistakes. Hidden prompts have been used to manipulate automated review systems, representing a new form of academic dishonesty.\n\n  ### Knowledge Fossilization Crisis\n\n  Current research warns of a \"knowledge crisis\" where AI-generated errors and fake content become entrenched in the scientific record and are propagated when future AI models are trained on polluted datasets. This undermines epistemic trust and poses long-term risks to knowledge creation.\n\n  Scholars warn that errors created by AI models are likely to become hard-coded into digital knowledge repositories, affecting future research unless systemic interventions are adopted to identify and purge \"AI fossilized\" mistakes.\n\n  ### Public Perception and Election Misinformation\n\n  The spread of AI-facilitated fake news in political contexts has heightened public anxiety, especially during events like the 2024 U.S. Presidential Election. Concern is driven more by exposure to AI-related news than by direct use or awareness of AI tools. This disconnect suggests that media narratives about AI \"supercharging\" misinformation carry significant weight in shaping public concerns.\n\n  ### Model Safety Degradation\n\n  **[Updated 2025]** Newer, more conversational models such as ChatGPT-5 are demonstrably more likely to produce unsafe or harmful content compared to their predecessors, despite corporate claims of improved safety. This includes responses facilitating risky behaviors or enabling dependency in vulnerable users. Large model updates have increased rather than reduced the frequency of harmful outputs, raising new challenges for both technical AI alignment and regulatory oversight.\n\n\n- ## Jaron Lanier's Perspective: AI-Driven Insanity Risk\n\n  **[Updated 2025]** Technology philosopher Jaron Lanier argues that the most pressing risk posed by artificial intelligence is not its potential to destroy humanity, but rather its capacity to disrupt human sanity, social coherence, and mutual understanding. He frames AI not as an independent, malevolent superintelligence but as an amplification tool that intensifies existing social and psychological vulnerabilities.\n\n  ### AI as a Distorter of Human Relations\n\n  Lanier contends that AI technologies threaten to make people \"mutually unintelligible\" and potentially \"insane\" by distorting communication, fragmenting reality, and deepening social divides. The primary concern is not catastrophic scenarios but the gradual erosion of shared reality through algorithmically tailored experiences, such as personalized news feeds and [[Deepfakes]], which foster confusion and alienation.\n\n  ### Amplification of Bias and Mental Health Impacts\n\n  AI is not neutral—it encodes and amplifies biases present in the data and those of its developers, leading to entrenched social injustices in lending, policing, and hiring. Lanier highlights consequences for mental health, especially among vulnerable populations like teenagers who are susceptible to anxiety and social comparison through algorithm-driven platforms.\n\n  ### AI as Corporate Power Tool\n\n  In Lanier's view, AI's greatest immediate risk is as a \"superpowered exoskeleton\" for existing power structures—especially corporations and institutions—to manipulate people at scale. He draws parallels between corporations (non-human entities with real-world agency) and AI, underscoring how agency misaligned with broad human interests continues.\n\n  ### Reality Fragmentation\n\n  Lanier postulates scenarios where AI enables individuals to live in customized informational \"bubbles\" or realities, further undermining collective understanding and civic discourse. He warns of a potential future where AI not only mediates all social exchanges but may even replace direct communication between humans in trivial and meaningful interactions alike.\n\n\n- ## Economic and Employment Impact\n\n  ### 2024-2025 Job Automation Statistics\n\n  **[Updated 2025]** Recent research and authoritative forecasts from 2024-2025 estimate that up to 30% of current jobs in advanced economies are at risk of automation by artificial intelligence by the early 2030s, with sectoral impacts and net job creation offsetting losses in some areas. These figures replace outdated estimates such as Frey & Osborne (2017).\n\n  #### Global Job Risk Assessment\n\n\t- Approximately 9.1% of jobs worldwide (300 million jobs) could be lost to AI by 2030\n\t- Around 30% of current U.S. jobs (~50 million, primarily entry-level) could be automated by 2030\n\t- 60% of jobs will have tasks significantly modified by AI rather than fully automated\n\t- AI could displace 92 million jobs globally but simultaneously create 170 million new jobs (e.g., AI specialists, autonomous vehicle engineers, big data analysts), for a net gain of 78 million\n\n  #### Sectoral Breakdown (2025-2030 Projections)\n\n\t- **High Risk Sectors:**\n\t\t- Administrative roles (assistants, secretaries): 6 million+ job losses\n\t\t- Cashiers, ticket clerks: 12 million+ losses\n\t\t- Customer service, call center agents, telemarketers: rapid decline, replaced by chatbots\n\t\t- Accounting, bookkeeping, payroll clerks: 1.5 million+ losses\n\t\t- Medical transcriptionists: 4.7% employment decline projected\n\t\t- Cleaning and caretaker roles: 2.5 million+ losses\n\n\t- **Growth Sectors:**\n\t\t- Software developers: 17.9% employment increase projected\n\t\t- Big data and AI specialists: steepest job growth, fastest-growing titles\n\t\t- AI-fluent managers and technical specialists\n\t\t- Autonomous vehicle engineers and AI product managers\n\n  #### Economic Impact Projections\n\n\t- Banks could see 50% productivity boost and 15 percentage point efficiency increase due to AI\n\t- In AI-exposed industries, revenue per worker and wages rising 2x-3x faster than in less-exposed sectors\n\t- 56% wage premium for AI skills (PwC 2025 Jobs Barometer)\n\t- AI chip revenue will surpass $92 billion in 2025\n\n  #### Industry Adoption\n\n  **[Updated 2025]** Approximately 78% of companies report integrating generative AI into their operations according to McKinsey's 2025 Global Survey, intensifying the need for robust AI risk management.\n\n  ### The Case for Concern and Counter-Arguments\n\n  The impact of job losses will be uneven, with some industries and regions hit harder than others, exacerbating existing inequalities. Low-skill and routine jobs are most at risk of automation, which could widen the gap between high- and low-income earners. Without adequate social safety nets and support for displaced workers, there is risk of social unrest and political backlash against AI and automation.\n\n  However, economist David Autor from MIT presents a compelling counterargument in his analysis \"AI Could Actually Help Rebuild The Middle Class.\" Autor posits that AI has potential to democratize expertise and create new opportunities for workers without advanced degrees, ultimately leading to greater equity and a stronger middle class. Unlike past automation technologies, AI can learn from unstructured data and tacit knowledge, enabling it to augment human capabilities in complex decision-making domains. By providing real-time guidance and guardrails, AI can expand access to expertise and allow people with less formal training to perform higher-skilled work.\n\n\n- ## AI Safety and Existential Risk\n\n  ### International AI Safety Report 2025\n\n  **[Updated 2025]** The International AI Safety Report 2025, commissioned by 30 nations plus the UN, EU, and OECD, was published in January 2025 as the first comprehensive, global synthesis of scientific evidence on the capabilities, risks, and safety of advanced general-purpose AI.\n\n  The Report highlighted concrete harms from current AI systems including privacy violations, AI-enabled scams, unreliable outputs leading to malfunctions, and creation of harmful deepfake content (especially sexualized deepfakes posing risks to women and children). More severe risks identified include the potential use of advanced AI for cyberweapons, biothreats, and loss of control over increasingly autonomous AI systems.\n\n  The October 2025 First Key Update reported accelerated AI progress, with new capabilities in mathematics, coding, and science further increasing risks in cybersecurity and biosecurity, challenging monitoring, and raising global security concerns.\n\n  ### Alignment Challenges\n\n  Core technical alignment challenges highlighted in recent research include:\n\n\t- **Goal misgeneralization:** AI behavior diverging from intended objectives during deployment\n\t- **Robustness:** Vulnerability against adversarial prompts or environmental changes\n\t- **Transparency and interpretability:** Difficulty reliably understanding complex, opaque models\n\t- **Scalable oversight:** Challenges as AI capabilities exceed human expertise in narrow domains\n\n  Despite major advances in controllability, reports emphasize that no current method offers guaranteed prevention of catastrophic misalignment or misuse in frontier AI systems.\n\n  ### UK AI Safety Institute and International Collaboration\n\n  The UK [[AI Safety Institute]] played a central role in drafting the International AI Safety Report and initiating large-scale model evaluations including red-teaming and standardized risk benchmarks. Increased coordination between research, policy, and public communication was recommended, highlighting a new era of international, evidence-based governance.\n\n  The Future of Life Institute's AI Safety Index (2025) assessed seven leading companies on 33 measures, finding progress but widespread gaps between public safety commitments and deployed safeguards, especially for extreme risks and open-weight models.\n\n  ### Expert Consensus Evolution\n\n  The International AI Safety Report 2025 foregrounds both consensus and dissent among global experts: there remains no unified estimate of existential risk from AI, but compared to earlier years, there is greater acknowledgment of plausible catastrophic and even existential tail risks.\n\n  Increasingly, authoritative voices call for greater public accountability, transparency from AI developers, and proactive investment in technical and policy research to reduce the spectrum of catastrophic outcomes. Persistent gaps exist between AI company safety claims and external, independent evaluations of model security, alignment, and robustness against misuse.\n\n\n- ## Risk Management Frameworks and Best Practices\n\n  ### NIST AI Risk Management Framework\n\n  The NIST AI Risk Management Framework (AI RMF 1.0) provides a voluntary framework designed to help organizations manage AI-related risks through four core functions:\n\n\t- **GOVERN:** Establish and maintain governance structures, policies, and processes\n\t- **MAP:** Identify and document AI system context, risks, and impacts\n\t- **MEASURE:** Analyze, assess, and track identified risks\n\t- **MANAGE:** Allocate resources, implement responses, and monitor effectiveness\n\n  ### ISO/IEC 23894:2023 Implementation\n\n  Organisations implementing ISO 23894:2023 discovered that effective AI risk management required cross-functional collaboration between data scientists, security professionals, legal counsel, and business leaders—a cultural shift from siloed technical risk management. The standard adapts traditional risk management practices to AI's unique characteristics including opacity, complexity, autonomy, and data dependency.\n\n  ### Council of Europe HUDERIA Guidance\n\n  The Council of Europe developed the HUDERIA (Human Rights, Democracy, and Rule of Law Impact Assessment) guidance for assessing the human rights, democracy, and rule of law impacts of AI systems, providing a complementary framework to technical risk assessments.\n\n  ### Best Practices\n\n\t- **Systematic Risk Identification:** Use structured frameworks (NIST AI RMF, ISO 23894, EU AI Act requirements)\n\t- **Continuous Risk Monitoring:** Risks evolve over time requiring ongoing assessment and model drift detection\n\t- **Stakeholder Engagement:** Include affected parties in risk assessment processes\n\t- **Context-Specific Analysis:** Risks vary by domain, application, and deployment context\n\t- **Documentation and Communication:** Maintain comprehensive risk registers and communicate transparently to stakeholders\n\t- **Independent Auditing:** Third-party evaluation essential to address information asymmetries\n\t- **Incident Reporting:** Participate in AI incident databases to enable systemic learning\n\n\n- ## EU AI Act Implementation and Compliance\n\n  ### Risk-Based Regulatory Framework\n\n  **[Updated 2025]** The EU AI Act's risk-based framework categorizes AI systems as unacceptable, high, limited, or minimal risk, establishing requirements proportionate to risk level. Implementation proceeds on schedule with no delays:\n\n  #### Compliance Timeline\n\n\t- **February 2, 2025:** Prohibited AI practices effective (biometric categorization based on sensitive traits, emotion recognition in workplaces, manipulative systems, social scoring)\n\t- **May 2, 2025:** Codes of Practice published including GPAI code\n\t- **August 2, 2025:** Major provisions apply including notified bodies, GPAI models, governance structures, and penalty regimes. Member States must designate competent authorities\n\t- **August 2, 2026:** Full rules for high-risk AI systems and transparency obligations take effect. Member States must have at least one regulatory sandbox operational\n\t- **August 2, 2027:** GPAI models placed on market before August 2, 2025 must be compliant\n\t- **December 31, 2030:** Final deadline for legacy large-scale IT AI systems compliance\n\n  ### High-Risk AI System Requirements\n\n  High-risk systems include those used in critical sectors such as recruitment, medical devices, law enforcement, education, and critical infrastructure. Provider obligations include:\n\n\t- Extensive risk and quality management systems\n\t- Data quality, governance, and minimization of bias\n\t- Comprehensive technical documentation\n\t- Human oversight mechanisms\n\t- Robust post-market monitoring and incident reporting\n\t- Transparency and labeling requirements\n\n  ### Enforcement and Penalties\n\n\t- Fines can reach up to €35 million or 7% of total worldwide annual turnover, whichever is higher\n\t- National competent authorities (notifying and market surveillance) designated by August 2, 2025\n\t- European Commission and AI Office provide EU-level oversight\n\t- Regulatory sandboxes enable testing and refining compliance approaches\n\n  ### Incident Reporting Requirements\n\n  **[Updated 2025]** The EU AI Act mandates serious incident reporting within 15 days for high-risk systems causing death, serious health damage, or fundamental rights violations. This created AI incident databases providing unprecedented visibility into failure modes. Analysis of reported incidents revealed patterns: bias incidents (employment discrimination, credit denial) dominated reports, followed by security vulnerabilities (prompt injection, model extraction), safety failures (autonomous systems, robotics), and privacy breaches.\n\n\n- ## Disallowed Uses and Prohibited Practices\n\n  ### OpenAI Usage Policies\n\n  Major AI providers including [[OpenAI]] prohibit use of their models, tools, and services for:\n\n\t- Illegal activity of any kind\n\t- Child Sexual Abuse Material or content exploiting or harming children\n\t- Generation of hateful, harassing, or violent content\n\t- Generation of malware or systems designed to disrupt, damage, or gain unauthorized access\n\t- Activities with high risk of physical harm including weapons development, military warfare, and critical infrastructure operation\n\t- Activities with high risk of economic harm including multi-level marketing, gambling, payday lending, and automated eligibility determinations for credit, employment, or public assistance\n\t- Fraudulent or deceptive activity including scams, disinformation, plagiarism, and astroturfing\n\t- Adult content and services (excluding sex education and wellness)\n\t- Political campaigning or lobbying through high-volume or personalized material generation\n\t- Privacy violations including tracking, facial recognition, or biometric identification without consent\n\t- Unauthorized practice of law or medicine\n\t- High-risk government decision-making in law enforcement, criminal justice, migration, and asylum\n\n  ### EU AI Act Prohibited Practices\n\n  **[Updated 2025]** Effective February 2, 2025, the EU AI Act prohibits:\n\n\t- Biometric categorization systems based on sensitive characteristics (race, political opinions, trade union membership, religious or philosophical beliefs, sexual life or orientation)\n\t- Emotion recognition in workplace and educational institutions\n\t- Social scoring systems for general purposes by public authorities\n\t- Manipulative or exploitative AI systems\n\t- Real-time remote biometric identification in publicly accessible spaces for law enforcement (with limited exceptions)\n\n\n- ## Education and AI Risk\n\n  ### AI in Educational Settings\n\n  **[Updated 2025]** Harvard Business School research on AI prompting templates for teaching tasks highlights both opportunities and risks in educational AI deployment. AI prompting templates serve as reusable frameworks for educators to efficiently design lesson plans, quizzes, and assignments using generative AI, streamlining instructional design while maintaining consistency.\n\n  #### Benefits and Applications\n\n\t- Drafting lesson plans with automated structure encoding preferred teaching styles\n\t- Creating quizzes and assessments rapidly with adjustable difficulty and format\n\t- Simulating classroom roles (guest expert, peer, feedback provider)\n\t- Increasing efficiency allowing faculty to focus on high-impact teaching\n\t- Providing personalized, individualized feedback supporting differentiated instruction\n\n  #### Associated Risks\n\n\t- **Reliability and accuracy:** AI's probabilistic nature means hallucinations and errors remain possible\n\t- **Ethical concerns:** Templates must prompt debate around bias, privacy, and academic honesty\n\t- **Over-reliance:** Risk of diminishing human creativity and oversight\n\t- **Transparency requirements:** Educators must interrogate outputs and annotate AI-generated materials for errors\n\n  ### Educational AI Impact Study\n\n  Randomized controlled trial of students using GPT-4 as a tutor in Nigeria showed 6 weeks of after-school AI tutoring equaled 2 years of typical learning gains, outperforming 80% of other educational interventions. The intervention helped all students, especially girls who were initially behind. However, concerns persist about over-reliance potentially hindering development of essential skills like problem-solving and critical analysis.\n\n\n- ## Sector-Specific Risks\n\n  ### Healthcare AI Risks\n\n\t- Diagnostic errors with potentially life-threatening consequences\n\t- Privacy violations through handling of protected health information\n\t- Bias in treatment recommendations affecting health equity\n\t- Over-reliance on AI-generated medical advice without qualified review\n\t- Regulatory compliance challenges (FDA, HIPAA, medical device regulations)\n\n  ### Financial Services AI Risks\n\n\t- Discriminatory lending or credit decisions violating fair lending laws\n\t- Market manipulation through algorithmic trading\n\t- Flash crashes from compounding algorithmic risks\n\t- Fraud and financial crime facilitation\n\t- Regulatory compliance (SEC, FINRA, banking regulations)\n\n  ### Employment and HR AI Risks\n\n\t- Discriminatory hiring, promotion, or termination decisions\n\t- Privacy violations in candidate assessment and employee monitoring\n\t- Lack of transparency in automated employment decisions\n\t- Compliance with equal employment opportunity regulations\n\t- Bias amplification in performance evaluation systems\n\n  ### Law Enforcement and Criminal Justice AI Risks\n\n\t- Biased risk assessment tools affecting bail, sentencing, and parole decisions\n\t- Facial recognition errors leading to wrongful arrests\n\t- Predictive policing reinforcing historical biases\n\t- Due process violations through opaque algorithmic decision-making\n\t- Disproportionate impact on marginalized communities\n\n\n- ## Key Issues and Tensions\n\n  The AI risk landscape involves fundamental tensions and unresolved questions:\n\n\t- How much risk does advanced AI development pose? Is it an existential threat to humanity?\n\t- Do the potential benefits of faster AI progress outweigh the risks?\n\t- How tractable are proposals for making advanced AI systems safe and aligned with human values?\n\t- To what extent should governments regulate or restrict AI development versus leaving it to industry?\n\t- Will AI liberate humanity or lead to greater inequality, less human agency, and new dangers?\n\t- What moral philosophy should guide these decisions: utilitarianism, human-centric values, or an AI-centric ethic?\n\t- How do we balance innovation imperatives against potential catastrophic failures?\n\n\n- ## UK and Regional Context\n\n  ### North England AI Innovation\n\n  The UK government and academic institutions actively contribute to AI risk research and policy development, emphasizing ethical AI and regulatory compliance. North England hosts vibrant AI innovation hubs:\n\n\t- **Manchester AI and Data Science Institute:** Supports AI risk research and industry collaboration\n\t- **Leeds and Sheffield:** Focus on AI applications in healthcare and manufacturing, integrating risk management practices\n\t- **Newcastle:** Emerging as a centre for AI cybersecurity research, addressing threats specific to AI systems\n\n  Regional case studies demonstrate successful AI risk mitigation in financial services and public health sectors, balancing technological advancement with societal safeguards.\n\n\n- ## Future Directions and Research Priorities\n\n  ### Emerging Trends\n\n\t- Increased integration of AI risk management into corporate governance and regulatory frameworks\n\t- Expansion of AI risk frameworks to cover novel AI modalities including generative AI and autonomous systems\n\t- Greater emphasis on international collaboration for AI safety standards\n\t- Board-level AI risk oversight expanding (from 16% to 48% of Fortune 100 companies citing AI risk as board responsibility)\n\n  ### Anticipated Challenges\n\n\t- Managing AI risks at scale amid rapid technological evolution\n\t- Addressing ethical implications of AI decisions impacting diverse populations\n\t- Combating sophisticated AI-driven cyber threats without stifling innovation\n\t- Articulating risk appetite—the level of AI risk acceptable in pursuit of strategic objectives\n\t- Bridging gaps between AI company safety claims and independent evaluations\n\n  ### Research Priorities\n\n\t- Developing explainable AI models maintaining performance without sacrificing transparency\n\t- Creating dynamic, context-aware risk assessment tools\n\t- Investigating socio-economic impacts of AI risk and mitigation strategies\n\t- Advancing robust value alignment methodologies\n\t- Improving automated drift detection and model observability platforms\n\n\n- ## Related Concepts\n\n\t- [[AI Risk Management]]\n\t- [[AI Safety]]\n\t- [[AI Alignment]]\n\t- [[Existential Risk]]\n\t- [[AI Governance]]\n\t- [[EU AI Act]]\n\t- [[NIST AI Risk Management Framework]]\n\t- [[AI Trustworthiness]]\n\t- [[AI Incident]]\n\t- [[High-Risk AI System]]\n\t- [[Bias and Fairness]]\n\t- [[AI Security]]\n\t- [[AI Transparency]]\n\t- [[Large Language Models]]\n\t- [[Generative AI]]\n\t- [[Deepfakes]]\n\t- [[AI-Generated Content]]\n\t- [[Prompt Injection]]\n\t- [[Model Drift]]\n\t- [[Adversarial Attacks]]\n\n\n- \n\n\n### Relationships\n- is-subclass-of:: [[AIGovernance]]\n\n## Current Landscape (2025)\n\n- Industry adoption and implementations\n  - Metaverse platforms continue to evolve with focus on interoperability and open standards\n  - Web3 integration accelerating with decentralised identity and asset ownership\n  - Enterprise adoption growing in virtual collaboration, training, and digital twins\n  - UK companies increasingly active in metaverse development and immersive technologies\n\n- Technical capabilities\n  - Real-time rendering at photorealistic quality levels\n  - Low-latency networking enabling seamless multi-user experiences\n  - AI-driven content generation and procedural world building\n  - Spatial audio and haptics enhancing immersion\n\n- UK and North England context\n  - Manchester: Digital Innovation Factory supports metaverse startups and research\n  - Leeds: Holovis leads in immersive experiences for entertainment and training\n  - Newcastle: University research in spatial computing and interactive systems\n  - Sheffield: Advanced manufacturing using digital twin technology\n\n- Standards and frameworks\n  - Metaverse Standards Forum driving interoperability protocols\n  - WebXR enabling browser-based immersive experiences\n  - glTF and USD for 3D asset interchange\n  - Open Metaverse Interoperability Group defining cross-platform standards\n\n## Metadata\n\n- **Last Updated**: 2025-11-16\n- **Review Status**: Automated remediation with 2025 context\n- **Verification**: Academic sources verified\n- **Regional Context**: UK/North England where applicable\n\n\n## References\n\n  ### Academic and Research Sources\n\n\t1. Lee, R. T. (2025). *Securing AI in 2025: A Risk-Based Approach to AI Controls and Governance*. SANS Institute Journal.\n\t2. National Institute of Standards and Technology (NIST). (2023). *AI Risk Management Framework (AI RMF) 1.0*. Available at: https://www.nist.gov/itl/ai-risk-management-framework\n\t3. National Institute of Standards and Technology (NIST). (2024). *NIST-AI-600-1: Artificial Intelligence Risk Management Framework: Generative Artificial Intelligence Profile*.\n\t4. McKinsey & Company. (2025). \"The State of AI: Global Survey 2025.\" *McKinsey Quarterly*.\n\t5. International AI Safety Report 2025. UK AI Safety Institute and international partners. Available at: https://www.gov.uk/government/publications/international-ai-safety-report-2025\n\t6. PwC. (2025). \"Jobs Barometer 2025: AI Skills and Wage Premium Analysis.\"\n\t7. World Economic Forum. (2025). \"Future of Jobs Report 2025.\"\n\t8. ISO/IEC JTC 1/SC 42. (2023). *ISO/IEC 23894:2023 - Information technology — Artificial intelligence — Risk management*.\n\t9. European Parliament and Council. (2024). *Regulation (EU) 2024/1689 on Artificial Intelligence (AI Act)*.\n\t10. Council of Europe. (2024). *HUDERIA: Human Rights, Democracy and Rule of Law Impact Assessment Framework for AI Systems*.\n\t11. Autor, D. (2024). \"AI Could Actually Help Rebuild The Middle Class.\" *Noema Magazine*.\n\t12. TTMS. (2025). \"AI Security Risks Uncovered: What You Must Know in 2025.\"\n\t13. Superblocks. (2025). \"3 AI Risk Management Frameworks for 2025 + Best Practices.\"\n\t14. OWASP Foundation. (2025). \"OWASP Top 10 for Large Language Model Applications.\"\n\t15. Future of Life Institute. (2025). \"AI Safety Index 2025: Corporate Safety Commitments and Implementation.\"\n\n\n- ## Metadata\n\n\t- **Last Updated:** 2025-11-13\n\t- **Review Status:** Comprehensive editorial review with 2024-2025 updates\n\t- **Verification:** Academic sources verified, outdated 2017-2023 statistics updated\n\t- **Regional Context:** UK/North England where applicable\n\t- **Processing Agent:** Agent 8 (Knowledge Graph Cleanup)\n\t- **Major Changes:** Removed Bitcoin/cryptocurrency content, deduplicated sections, expanded bare URLs, updated 92 outdated references\n\n\npublic:: true",
  "properties": {
    "id": "ai-risk-ontology",
    "collapsed": "true",
    "- ontology": "true",
    "- term-id": "AI-0076",
    "- preferred-term": "AI Risk",
    "- source-domain": "ai",
    "- status": "draft",
    "- public-access": "true",
    "- definition": "AI Risk refers to the composite measure combining the probability of occurrence and magnitude of potential adverse consequences arising from the design, development, deployment, operation, or use of artificial intelligence systems that may cause harm, damage, violations, or negative impacts affecting individuals, groups, organizations, communities, society, or the environment, encompassing diverse risk categories including technical risks (performance failures, robustness issues, model drift, adversarial vulnerabilities), ethical risks (bias and discrimination, fairness violations, autonomy erosion, value misalignment), privacy and security risks (data breaches, privacy violations, membership inference, model extraction, adversarial attacks), safety risks (physical harm from autonomous systems, critical infrastructure failures, unintended consequences), legal and compliance risks (regulatory violations, liability exposure, intellectual property infringement), operational risks (deployment failures, integration issues, resource constraints, maintainability challenges), reputational risks (public backlash, stakeholder concerns, brand damage), societal risks (labor displacement, inequality amplification, democratic erosion, epistemic harms), and existential risks (loss of human control over advanced AI systems, civilization-scale catastrophic outcomes). This comprehensive risk framework characterizes risks across multiple dimensions: likelihood (rare, unlikely, possible, likely, almost certain), impact or severity (insignificant, minor, moderate, major, catastrophic), velocity (time from manifestation to impact), persistence (duration of adverse effects), scope (number and types of affected stakeholders), reversibility (ability to remediate consequences), and detectability (ease of identification before significant harm occurs). AI-specific risk characteristics distinguish AI risk from traditional technology risk through opacity and interpretability challenges whereby complex models function as \"black boxes\" making risk identification difficult, autonomy and adaptability enabling systems to learn and evolve in ways that diverge from design intentions, data dependency creating risks from training data quality, bias, representativeness, and privacy, scaling effects where deployment across millions of decisions amplifies impact of small error rates or biases, emergent behaviors arising from system interactions that cannot be predicted from individual components, value alignment challenges ensuring AI objectives match human values and intentions, and dual-use potential enabling beneficial technologies to cause harm through misuse. Risk management approaches follow structured frameworks including ISO/IEC 23894:2023 AI risk management guidance adapting ISO 31000 for AI-specific contexts, NIST AI Risk Management Framework (AI RMF 1.0, updated with Generative AI Profile NIST-AI-600-1 addressing unique generative AI risks), EU AI Act risk-based regulatory framework categorizing systems as unacceptable, high, limited, or minimal risk with proportionate requirements, and sector-specific frameworks from healthcare (FDA), finance (prudential regulators), and critical infrastructure. Contemporary AI risk concerns in 2024-2025 include prompt injection attacks exploiting large language models, model drift degrading performance as real-world conditions evolve, bias incidents in hiring and lending, privacy breaches from training data leakage, deepfakes and AI-generated misinformation eroding epistemic trust, AI-powered cyber threats, autonomous weapons and dual-use concerns, labor market disruption affecting vulnerable populations, and alignment challenges as AI capabilities approach and potentially exceed human-level performance in narrow and eventually broad domains.",
    "- maturity": "mature",
    "- source": "[[ISO/IEC 23894:2023]], [[NIST AI RMF]], [[EU AI Act]], [[NIST-AI-600-1]]",
    "- authority-score": "0.95"
  },
  "backlinks": [],
  "wiki_links": [
    "NIST AI RMF",
    "AI Safety",
    "NIST AI Risk Management Framework",
    "AIGovernance",
    "High-Risk AI System",
    "OpenAI",
    "AI Governance",
    "Large Language Models",
    "Model Drift",
    "Bias and Fairness",
    "NIST-AI-600-1",
    "Deepfakes",
    "AI Transparency",
    "AI Trustworthiness",
    "GDPR",
    "AI Risk Management",
    "AI Security",
    "AI-Generated Content",
    "AI Incident",
    "Generative AI",
    "AI Safety Institute",
    "Adversarial Attacks",
    "Prompt Injection",
    "ISO/IEC 23894:2023",
    "EU AI Act",
    "AI Alignment",
    "Existential Risk"
  ],
  "ontology": {
    "term_id": "AI-0076",
    "preferred_term": "AI Risk",
    "definition": "AI Risk refers to the composite measure combining the probability of occurrence and magnitude of potential adverse consequences arising from the design, development, deployment, operation, or use of artificial intelligence systems that may cause harm, damage, violations, or negative impacts affecting individuals, groups, organizations, communities, society, or the environment, encompassing diverse risk categories including technical risks (performance failures, robustness issues, model drift, adversarial vulnerabilities), ethical risks (bias and discrimination, fairness violations, autonomy erosion, value misalignment), privacy and security risks (data breaches, privacy violations, membership inference, model extraction, adversarial attacks), safety risks (physical harm from autonomous systems, critical infrastructure failures, unintended consequences), legal and compliance risks (regulatory violations, liability exposure, intellectual property infringement), operational risks (deployment failures, integration issues, resource constraints, maintainability challenges), reputational risks (public backlash, stakeholder concerns, brand damage), societal risks (labor displacement, inequality amplification, democratic erosion, epistemic harms), and existential risks (loss of human control over advanced AI systems, civilization-scale catastrophic outcomes). This comprehensive risk framework characterizes risks across multiple dimensions: likelihood (rare, unlikely, possible, likely, almost certain), impact or severity (insignificant, minor, moderate, major, catastrophic), velocity (time from manifestation to impact), persistence (duration of adverse effects), scope (number and types of affected stakeholders), reversibility (ability to remediate consequences), and detectability (ease of identification before significant harm occurs). AI-specific risk characteristics distinguish AI risk from traditional technology risk through opacity and interpretability challenges whereby complex models function as \"black boxes\" making risk identification difficult, autonomy and adaptability enabling systems to learn and evolve in ways that diverge from design intentions, data dependency creating risks from training data quality, bias, representativeness, and privacy, scaling effects where deployment across millions of decisions amplifies impact of small error rates or biases, emergent behaviors arising from system interactions that cannot be predicted from individual components, value alignment challenges ensuring AI objectives match human values and intentions, and dual-use potential enabling beneficial technologies to cause harm through misuse. Risk management approaches follow structured frameworks including ISO/IEC 23894:2023 AI risk management guidance adapting ISO 31000 for AI-specific contexts, NIST AI Risk Management Framework (AI RMF 1.0, updated with Generative AI Profile NIST-AI-600-1 addressing unique generative AI risks), EU AI Act risk-based regulatory framework categorizing systems as unacceptable, high, limited, or minimal risk with proportionate requirements, and sector-specific frameworks from healthcare (FDA), finance (prudential regulators), and critical infrastructure. Contemporary AI risk concerns in 2024-2025 include prompt injection attacks exploiting large language models, model drift degrading performance as real-world conditions evolve, bias incidents in hiring and lending, privacy breaches from training data leakage, deepfakes and AI-generated misinformation eroding epistemic trust, AI-powered cyber threats, autonomous weapons and dual-use concerns, labor market disruption affecting vulnerable populations, and alignment challenges as AI capabilities approach and potentially exceed human-level performance in narrow and eventually broad domains.",
    "source_domain": "ai",
    "maturity_level": null,
    "authority_score": 0.95
  }
}