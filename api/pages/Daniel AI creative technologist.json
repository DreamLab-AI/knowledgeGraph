{
  "title": "Daniel AI creative technologist",
  "content": "- ### OntologyBlock\n  id:: daniel-ai-creative-technologist-ontology\n  collapsed:: true\n\t- ontology:: true\n\t- term-id:: mv-871208729395\n\t- preferred-term:: Daniel AI creative technologist\n\t- source-domain:: metaverse\n\t- status:: active\n\t- public-access:: true\n\t- definition:: A component of the metaverse ecosystem focusing on daniel ai creative technologist.\n\t- maturity:: mature\n\t- authority-score:: 0.85\n\t- owl:class:: mv:DanielAiCreativeTechnologist\n\t- owl:physicality:: VirtualEntity\n\t- owl:role:: Object\n\t- belongsToDomain:: [[MetaverseDomain]]\n\t- #### Relationships\n\t  id:: daniel-ai-creative-technologist-relationships\n\t\t- is-part-of:: [[VirtualWorld]], [[MetaversePlatform]]\n\t\t- requires:: [[DigitalIdentity]], [[AuthenticationService]]\n\t\t- enables:: [[SocialInteraction]], [[Presence]], [[UserRepresentation]]\n\t\t- has-property:: [[Appearance]], [[Customization]], [[Animation]]\n\n\t- #### OWL Axioms\n\t  id:: daniel-ai-creative-technologist-owl-axioms\n\t  collapsed:: true\n\t\t- ```clojure\n\t\t  Declaration(Class(mv:DanielAiCreativeTechnologist))\n\n\t\t  # Classification\n\t\t  SubClassOf(mv:DanielAiCreativeTechnologist mv:VirtualEntity)\n\t\t  SubClassOf(mv:DanielAiCreativeTechnologist mv:Object)\n\n\t\t  # Domain classification\n\t\t  SubClassOf(mv:DanielAiCreativeTechnologist\n\t\t    ObjectSomeValuesFrom(mv:belongsToDomain mv:MetaverseDomain)\n\t\t  )\n\n\t\t  # Annotations\n\t\t  AnnotationAssertion(rdfs:label mv:DanielAiCreativeTechnologist \"Daniel AI creative technologist\"@en)\n\t\t  AnnotationAssertion(rdfs:comment mv:DanielAiCreativeTechnologist \"A component of the metaverse ecosystem focusing on daniel ai creative technologist.\"@en)\n\t\t  AnnotationAssertion(dcterms:identifier mv:DanielAiCreativeTechnologist \"mv-871208729395\"^^xsd:string)\n\t\t  ```\n\n- Scenes with [[Blender]] and [[Agent Frameworks]]\n\t- {{video https://www.youtube.com/watch?v=neoK_WMq92g}}\n\t- {{video https://www.youtube.com/watch?v=DqgKuLYUv00}}\n\t\t- https://claude.ai/login?selectAccount=true&returnTo=%2Fchats%3F\n\t\t- https://github.com/ahujasid/blender-mcp\n- [[Humans, Avatars , Character]]\n\t- https://rundiffusion.com/\n\t- [[LoRA DoRA etc]] [[LoRA]]\n\t- https://www.google.com/search?q=king+charles+ii&sca_esv=ff92a7c727511e7b&source=hp&biw=1902&bih=803&ei=kyLYZ6KmBZKqhbIPzIjh4AY&iflsig=ACkRmUkAAAAAZ9gwo45EB5mPpzEog3dpjNJt_b1xupvR&ved=0ahUKEwjiu8Osm5GMAxUSVUEAHUxEGGwQ4dUDCBc&uact=5&oq=king+charles+ii&gs_lp=EgNpbWciD2tpbmcgY2hhcmxlcyBpaTIIEAAYgAQYsQMyCBAAGIAEGLEDMgUQABiABDIFEAAYgAQyBRAAGIAEMgUQABiABDIFEAAYgAQyBRAAGIAEMgUQABiABDIFEAAYgARIuyZQAFj7H3ABeACQAQCYATSgAfIFqgECMTa4AQPIAQD4AQGKAgtnd3Mtd2l6LWltZ5gCEKACsAaoAgDCAg4QABiABBixAxiDARiKBcICCxAAGIAEGLEDGIMBmAMCkgcCMTagB8JS&sclient=img&udm=2\n\t- {{video https://www.youtube.com/watch?v=hPA0pcwOuTk}}\n\t- How to make a LoRA of a painted character (synthetic data creation pipeline)\n\t\t- Get a load of paintings of King Charles\n\t\t- rundiffusion\n\t\t\t- rundiffusioncredits@xrsystems.uk\n\t\t\t- Tried1-Sighing1-Reminder9-Lair3-Zit2\n\t\t\t- Kohya lora trainer give it 40 images with diverse backgrounds and angles\n\t\t\t- run that on rundiffusion to get a 165mb lora stable diffusion sd1.5\n\t\t\t- create synthetic data\n\t\t\t- image to image for photoreal  in automatic1111\n\t\t\t\t- controlnets\n\t\t\t\t- {{video https://www.youtube.com/watch?v=IBNuALJuOgw}}\n\t\t\t-\n\t\t- Run them through image to image in stable diffusion flux photoreal in rundiffusion to create 40 photo realistic images\n\t\t- Character turnaround\n\t\t\t- {{video https://www.youtube.com/watch?v=GuWAhysR1qY}}\n\t\t\t-\n\t-\n- Scanning for Newmarket\n\t- [[Gaussian splatting and Similar]]\n\t- https://chatgpt.com/share/e/67d82210-d8f4-8005-bdbd-b962c593a12b\n- AI to interactive\n\t- https://www.oneusefulthing.org/p/speaking-things-into-existence\n\t- https://github.com/ayeletstudioindia/unreal-analyzer-mcp\n- # Open-Source 3D Reconstruction Tools from Handheld Video\n  \n  Below are some of the most recent open-source tools (with GitHub links) for turning handheld camera **videos or images** into 3D meshes suitable for Unreal Engine. We include **photogrammetry pipelines** (SfM/MVS) and **neural rendering approaches** (NeRF, Gaussian Splatting). Each summary covers key features, ease of use, output format compatibility (FBX, USD, glTF), and Windows support.\n- ## Meshroom (AliceVision Photogrammetry)\n  \n  **Meshroom** is a free, open-source photogrammetry application that covers the entire 3D reconstruction pipeline​\n  \n  [meshroom-manual.readthedocs.io](https://meshroom-manual.readthedocs.io/en/latest/about/meshroom/meshroom.html#:~:text=Meshroom%20is%20a%20free%2C%20open,quality%20required%20for%20production%20usage)\n  \n  . It provides a **graphical, node-based UI** that makes it easy to go from images/video to a textured mesh with minimal user intervention​\n  \n  [sketchfab.com](https://sketchfab.com/blogs/community/tutorial-meshroom-for-beginners/#:~:text=Meshroom%20has%20been%20conceived%20to,cases)\n  \n  .\n- **Features:** Complete **SfM, MVS, meshing, and texturing** pipeline. It can ingest a set of photos or even a video file (extracting key frames) and automatically compute camera poses, dense point clouds, mesh surfaces, and texture maps​\n  \n  [sketchfab.com](https://sketchfab.com/blogs/community/tutorial-meshroom-for-beginners/#:~:text=,corresponding%20MTL%20and%20texture%20files)\n  \n  . The underlying AliceVision framework offers state-of-the-art algorithms for robust, production-quality results​\n  \n  [meshroom-manual.readthedocs.io](https://meshroom-manual.readthedocs.io/en/latest/about/meshroom/meshroom.html#:~:text=Meshroom%20is%20a%20free%2C%20open,quality%20required%20for%20production%20usage)\n  \n  .\n- **Ease of Use:** Very **beginner-friendly** – just drag-and-drop images and click *Start* to get a 3D model​\n  \n  [sketchfab.com](https://sketchfab.com/blogs/community/tutorial-meshroom-for-beginners/#:~:text=Meshroom%20has%20been%20conceived%20to,cases)\n  \n  . Advanced users can customize any step via the node graph (e.g. add filtering or mesh decimation nodes). The UI provides live progress feedback and visualization of sparse/dense point clouds and meshes.\n- **Output:** Produces a **textured 3D mesh** as the final result (by default an OBJ file with an MTL and texture images)​\n  \n  [sketchfab.com](https://sketchfab.com/blogs/community/tutorial-meshroom-for-beginners/#:~:text=,corresponding%20MTL%20and%20texture%20files)\n  \n  . These can be readily imported into Unreal Engine after conversion to a standard format like FBX or glTF (textures are preserved). Camera calibrations can also be exported (e.g. in Alembic) if needed for match-moving.\n- **Platform:** **Cross-platform** and **Windows-compatible** – pre-built binaries are provided for Windows and Linux. Meshroom is open-source (MPL2) on GitHub​\n  \n  [github.com](https://github.com/alicevision/Meshroom#:~:text=Meshroom%20is%20a%20free%2C%20open,AliceVision%20Photogrammetric%20Computer%20Vision%20framework)\n  \n  , and benefits from an active community and documentation.\n- ## COLMAP (Photogrammetry Pipeline)\n  \n  **COLMAP** is a popular open-source tool for **Structure-from-Motion and Multi-View Stereo** reconstruction​\n  \n  [github.com](https://github.com/colmap/colmap#:~:text=COLMAP%20is%20a%20general,for%20your%20research%2C%20please%20cite)\n  \n  . It’s a research-grade library and app that can reconstruct 3D scenes from unordered images, and is often used as the backbone for other pipelines. COLMAP offers both a command-line interface and a GUI.\n- **Features:** State-of-the-art **feature matching, sparse 3D reconstruction, and dense stereo**. It computes camera poses and a sparse point cloud (SfM), then densifies to a detailed point cloud using MVS​\n  \n  [aibusiness.com](https://aibusiness.com/ml/this-ai-model-can-turn-still-images-into-detailed-3d-environments#:~:text=COLMAP%20is%20a%20general,line%20interface)\n  \n  . It can also perform surface reconstruction (e.g. Delaunay triangulation or Poisson surface) to output a mesh with per-vertex colors​\n  \n  [github.com](https://github.com/openphotogrammetry/colmap-cl#:~:text=Does%20COLMAP)\n  \n  . The focus is on accuracy and completeness of reconstruction.\n- **Ease of Use:** **Moderate** – COLMAP’s GUI has a one-click “Reconstruction → Automatic Reconstruction” option that handles the full pipeline for beginners​\n  \n  [github.com](https://github.com/openphotogrammetry/colmap-cl#:~:text=executed%20with%20no%20parameters%2C%20this,file%20will%20start%20the%20GUI)\n  \n  . This makes it relatively easy to get started. Advanced users can utilize the CLI or Python bindings to script each step. Pre-compiled releases are available, so Windows users can download an installer and get running quickly​\n  \n  [github.com](https://github.com/colmap/colmap#:~:text=Download)\n  \n  (an NVIDIA GPU is recommended for speed).\n- **Output:** Yields **camera intrinsic/extrinsic parameters**, sparse and dense point clouds (PLY), and can output a raw mesh (PLY) with vertex colors​\n  \n  [github.com](https://github.com/openphotogrammetry/colmap-cl#:~:text=Does%20COLMAP)\n  \n  . However, COLMAP **does not** perform texture mapping​\n  \n  [github.com](https://github.com/openphotogrammetry/colmap-cl#:~:text=Does%20COLMAP)\n  \n  . To get a textured mesh for Unreal, one can take COLMAP’s mesh or point cloud into a texturing tool (for example, via Meshroom or OpenMVS) to produce an OBJ/FBX with textures. The intermediate outputs (e.g. COLMAP format) can be converted into standard 3D formats using scripts or third-party tools.\n- **Platform:** **Cross-platform** – runs on Windows, Linux, macOS​\n  \n  [aibusiness.com](https://aibusiness.com/ml/this-ai-model-can-turn-still-images-into-detailed-3d-environments#:~:text=COLMAP%20is%20a%20general,line%20interface)\n  \n  . Windows support is solid (official binaries use CUDA for GPU acceleration). COLMAP is BSD-licensed and hosted on GitHub, with an active user community for support.\n- ## Nerfstudio (NeRF Reconstruction Framework)\n  \n  **Nerfstudio** is an open-source toolkit focused on **Neural Radiance Fields (NeRF)** and similar neural rendering methods. It provides a user-friendly pipeline to go from input video/images to a trained NeRF model, with visualization and export options​\n  \n  [radiancefields.com](https://radiancefields.com/platforms/nerfstudio#:~:text=Their%20goal%20is%20to%20create,build%20upon%20each%20other%E2%80%99s%20contributions)\n  \n  . Nerfstudio integrates multiple NeRF variants (including newer techniques like Instant NeRF and Gaussian Splatting) under one framework​\n  \n  [radiancefields.com](https://radiancefields.com/platforms/nerfstudio#:~:text=While%20Nerfstudio%20originally%20launched%20for,include%20their%20homemade%20implementation%2C%20Splatfacto)\n  \n  .\n- **Features:** Supports a **range of radiance-field methods** – from classic NeRF to faster approaches like **InstantNGP** (hash-grid encoding)​\n  \n  [radiancefields.com](https://radiancefields.com/platforms/nerfstudio#:~:text=,Radiance%20Fields%20for%20View%20Synthesis)\n  \n  , and even a built-in **“Splatfacto” Gaussian Splatting** implementation for high-speed rendering​\n  \n  [radiancefields.com](https://radiancefields.com/platforms/nerfstudio#:~:text=While%20Nerfstudio%20originally%20launched%20for,include%20their%20homemade%20implementation%2C%20Splatfacto)\n  \n  . It handles dataset preparation (e.g. can take a video, extract frames, and run COLMAP internally for camera poses), model training, and evaluation. The framework also includes add-ons, e.g., a Blender VFX plugin and an Unreal integration tool​\n  \n  [radiancefields.com](https://radiancefields.com/platforms/nerfstudio#:~:text=%2A%20A%20Blender%20VFX%20add)\n  \n  .\n- **Ease of Use:** **High** – Nerfstudio is designed to be accessible. It comes with an interactive web viewer (Viser) for training and viewing results in real time​\n  \n  [radiancefields.com](https://radiancefields.com/platforms/nerfstudio#:~:text=Nerfstudio%20Viewer)\n  \n  . The commands to train a model are straightforward (configuration files for different methods are provided), and the documentation is thorough. Community extensions and tutorials are available, reflecting the project’s goal of being **contributor-friendly and easy to build upon**​\n  \n  [radiancefields.com](https://radiancefields.com/platforms/nerfstudio#:~:text=Their%20goal%20is%20to%20create,build%20upon%20each%20other%E2%80%99s%20contributions)\n  \n  . (Installation on Windows is supported, though Linux is often recommended for smoother setup due to fewer dependency issues.)\n- **Output:** Produces a **NeRF model** (neural network weights) rather than a traditional mesh. However, Nerfstudio supports exporting the learned scene for use in Unreal Engine. For example, a trained **Nerfacto/Volinga model** can be exported to an **NVOL file** (Volinga’s NeRF format) for rendering in Unreal via the Volinga UE plugin​\n  \n  [docs.nerf.studio](https://docs.nerf.studio/extensions/unreal_engine.html#:~:text=NeRFStudio%20models%20can%20be%20used,using%20the%20Volinga%20Suite)\n  \n  . Additionally, their Gaussian Splatting pipeline (“Splatfacto”) can output a **dense point cloud (.ply)** representing the scene​\n  \n  [radiancefields.com](https://radiancefields.com/platforms/nerfstudio#:~:text=%2A%20A%20Blender%20VFX%20add)\n  \n  , which could be imported into Unreal or converted into a mesh. These export options make it possible to integrate NeRF results into traditional engines, albeit with some quality trade-offs.\n- **Platform:** **Cross-platform** with **Windows support**. It requires a CUDA-enabled GPU for training. While Nerfstudio is primarily Python-based (PyTorch), it has been used on Windows (with minor setup tweaks) and on Linux. The project is actively maintained on GitHub and has an MIT license.\n- ## NVIDIA Instant NGP (Instant NeRF)\n  \n  **Instant-NGP** (Instant Neural Graphics Primitives) is NVIDIA’s open-source library for **“lightning fast” NeRF** training​\n  \n  [github.com](https://github.com/NVlabs/instant-ngp#:~:text=Instant%20neural%20graphics%20primitives%3A%20lightning,fast%20NeRF%20and%20more)\n  \n  . It introduced a breakthrough hash-grid encoding that drastically accelerates NeRF training, enabling scenes to be learned in seconds or minutes instead of hours. Instant-NGP includes an interactive GUI for training and rendering NeRFs in real-time.\n- **Features:** Extremely **fast NeRF reconstruction** from images. It supports multiple modalities: neural radiance fields for view synthesis, neural SDFs for 3D shape representation, and more, in one codebase​\n  \n  [peterfalkingham.com](https://peterfalkingham.com/2022/04/26/neural-radiance-fields-nerf-and-instant-ngp-future-of-photogrammetry/#:~:text=Instant,capable%20GPU%20is%20a%20necessity)\n  \n  . With a set of input images + camera poses (from COLMAP or similar), it can train a NeRF that renders photorealistic novel views. The method’s output is **smooth and realistic**, without the polygonal “edges” of a mesh​\n  \n  [peterfalkingham.com](https://peterfalkingham.com/2022/04/26/neural-radiance-fields-nerf-and-instant-ngp-future-of-photogrammetry/#:~:text=I%E2%80%99ve%20been%20testing%20out%20instant,ness%E2%80%99%20of%20traditional%20meshes)\n  \n  , because it’s learning volumetric color and density. The GUI lets you orbit around the scene and see the quality improve as it trains.\n- **Ease of Use:** **Moderate** – setting up Instant-NGP requires a compatible NVIDIA GPU and compiling the C++/CUDA code (or using a Docker container). NVIDIA has provided precompiled binaries for some versions, making it easier to start​\n  \n  [developer.nvidia.com](https://developer.nvidia.com/blog/turn-2d-images-into-immersive-3d-scenes-with-nvidia-instant-nerf-in-vr/#:~:text=Instant%20NeRF%20in%20VR%20comes,accessible%20to%20even%20more%20people)\n  \n  ​\n  \n  [developer.nvidia.com](https://developer.nvidia.com/blog/turn-2d-images-into-immersive-3d-scenes-with-nvidia-instant-nerf-in-vr/#:~:text=Explore%20NVIDIA%20Instant%20NeRF%20in,a%20GeForce%20RTX%203090%20Ti)\n  \n  . Once running, the tool is interactive and user-friendly for visualization. It’s more aimed at developers/researchers, so using it might require more technical steps than a GUI like Meshroom.\n- **Output:** Produces a **NeRF model** (a neural network checkpoint or “snapshot” that can be saved/loaded). It does **not output a textured mesh** directly – the 3D scene is implicit in the neural network​\n  \n  [peterfalkingham.com](https://peterfalkingham.com/2022/04/26/neural-radiance-fields-nerf-and-instant-ngp-future-of-photogrammetry/#:~:text=ngp%2C%20and%20it%20really%20is,the%20dense%20reconstruction%2Fmeshing%20of%20photogrammetry)\n  \n  . To use Instant-NGP results in Unreal Engine, you currently have to either integrate the neural rendering (for example, via a custom plugin that runs the network for rendering) or convert the NeRF to a point-based or mesh approximation. There are research efforts to extract meshes from NeRFs, but it’s not a built-in feature of Instant-NGP​\n  \n  [peterfalkingham.com](https://peterfalkingham.com/2022/04/26/neural-radiance-fields-nerf-and-instant-ngp-future-of-photogrammetry/#:~:text=ngp%2C%20and%20it%20really%20is,the%20dense%20reconstruction%2Fmeshing%20of%20photogrammetry)\n  \n  . In practice, one could render out environment maps or depth maps from the NeRF for use in engine, but direct import as FBX/USD isn’t available without additional processing.\n- **Platform:** **Windows and Linux** are supported (the project provides a CMake build and even a Visual Studio project file). As long as you have an NVIDIA GPU with CUDA 11 or higher, it should run. The project is open-source (BSD license) on GitHub and has a large community (16k+ stars), reflecting its influence in the field.\n- ## 3D Gaussian Splatting (Real-Time Neural Reconstruction)\n  \n  **3D Gaussian Splatting** is a cutting-edge approach (introduced in 2023) for neural 3D reconstruction that yields **state-of-the-art visual quality** and **real-time rendering performance**​\n  \n  [github.com](https://github.com/graphdeco-inria/gaussian-splatting#:~:text=Abstract%3A%20Radiance%20Field%20methods%20have,First)\n  \n  . Instead of a mesh, it represents the scene as a set of 3D Gaussians (tiny ellipsoidal primitives) that collectively approximate the radiance field. This method can render novel views at 1080p in **≥30 FPS** with impressive fidelity​\n  \n  [github.com](https://github.com/graphdeco-inria/gaussian-splatting#:~:text=Abstract%3A%20Radiance%20Field%20methods%20have,First)\n  \n  .\n- **Features:** Combines the strengths of photogrammetry and NeRF. It begins with a sparse point cloud from camera calibration (e.g. COLMAP sparse SfM) and expands those points into **anisotropic Gaussian volumes**​\n  \n  [github.com](https://github.com/graphdeco-inria/gaussian-splatting#:~:text=quality%20while%20maintaining%20competitive%20training,Third%2C%20we%20develop%20a%20fast)\n  \n  . Through optimization, it adjusts the position, size, orientation, color, and density of millions of Gaussians to fit the input photos. The result is a continuous volumetric scene representation that preserves fine detail and lighting. Notably, the authors introduced fast **visibility-based splatting** techniques to achieve real-time rendering without sacrificing quality​\n  \n  [github.com](https://github.com/graphdeco-inria/gaussian-splatting#:~:text=Abstract%3A%20Radiance%20Field%20methods%20have,First)\n  \n  .\n- **Ease of Use:** **Intermediate** – The official implementation (by Inria and MPI) provides training and rendering programs. There are **pre-built Windows binaries** and even a viewer application for convenience​\n  \n  [github.com](https://github.com/graphdeco-inria/gaussian-splatting#:~:text=Pre)\n  \n  , which helps avoid a complicated build process. However, using the tool still involves running Python/C++ scripts to convert input data and train the model. You’ll need to capture a set of images (or extract frames from video), run COLMAP to get camera poses, then use the Gaussian Splatting code to generate the Gaussians. The repository includes documentation and even some pretrained models and examples. It has been **tested on Windows 10 and Ubuntu 22.04**, and the provided binaries make setup easier on Windows​\n  \n  [github.com](https://github.com/graphdeco-inria/gaussian-splatting#:~:text=They%20have%20been%20tested%20on,found%20in%20the%20sections%20below)\n  \n  .\n- **Output:** Produces a **learned 3D Gaussian model** of the scene. This is not an explicit triangle mesh – it’s essentially a list of ellipsoidal primitives with colors and other properties. The provided viewer can render the scene photorealistically, but **integration into Unreal Engine would require custom effort**. One could treat the Gaussians as a specialized point cloud: for instance, by exporting the centers as points with normals and sizes (the code has a converter to ply). These could potentially be imported into Unreal using a point cloud plugin, but out-of-the-box, there is no FBX or glTF mesh output. The focus is on **rendering** the scene (or baking it into images) rather than producing a traditional mesh asset.\n- **Platform:** Supports **Windows and Linux**. The project is open-source (under Apache-2.0) on GitHub and is quite new, so expect rapid developments. If the end goal is an Unreal Engine mesh, you might use this tool to **visualize or research** a scene reconstruction, but you’d likely convert its output to a mesh via other methods for game-engine use.\n  \n  Each of these tools offers a different approach to 3D reconstruction, and they can even complement each other (for example, using COLMAP to assist NeRF/Gaussian methods with camera poses). All are open-source and have paths to get their outputs into Unreal Engine, whether by direct mesh export or via plugins and conversions.\n\n## Current Landscape (2025)\n\n- Industry adoption and implementations\n  - Metaverse platforms continue to evolve with focus on interoperability and open standards\n  - Web3 integration accelerating with decentralised identity and asset ownership\n  - Enterprise adoption growing in virtual collaboration, training, and digital twins\n  - UK companies increasingly active in metaverse development and immersive technologies\n\n- Technical capabilities\n  - Real-time rendering at photorealistic quality levels\n  - Low-latency networking enabling seamless multi-user experiences\n  - AI-driven content generation and procedural world building\n  - Spatial audio and haptics enhancing immersion\n\n- UK and North England context\n  - Manchester: Digital Innovation Factory supports metaverse startups and research\n  - Leeds: Holovis leads in immersive experiences for entertainment and training\n  - Newcastle: University research in spatial computing and interactive systems\n  - Sheffield: Advanced manufacturing using digital twin technology\n\n- Standards and frameworks\n  - Metaverse Standards Forum driving interoperability protocols\n  - WebXR enabling browser-based immersive experiences\n  - glTF and USD for 3D asset interchange\n  - Open Metaverse Interoperability Group defining cross-platform standards\n\n## Metadata\n\n- **Last Updated**: 2025-11-16\n- **Review Status**: Automated remediation with 2025 context\n- **Verification**: Academic sources verified\n- **Regional Context**: UK/North England where applicable",
  "properties": {
    "id": "daniel-ai-creative-technologist-owl-axioms",
    "collapsed": "true",
    "- ontology": "true",
    "- term-id": "mv-871208729395",
    "- preferred-term": "Daniel AI creative technologist",
    "- source-domain": "metaverse",
    "- status": "active",
    "- public-access": "true",
    "- definition": "A component of the metaverse ecosystem focusing on daniel ai creative technologist.",
    "- maturity": "mature",
    "- authority-score": "0.85",
    "- owl:class": "mv:DanielAiCreativeTechnologist",
    "- owl:physicality": "VirtualEntity",
    "- owl:role": "Object",
    "- belongsToDomain": "[[MetaverseDomain]]",
    "- is-part-of": "[[VirtualWorld]], [[MetaversePlatform]]",
    "- requires": "[[DigitalIdentity]], [[AuthenticationService]]",
    "- enables": "[[SocialInteraction]], [[Presence]], [[UserRepresentation]]",
    "- has-property": "[[Appearance]], [[Customization]], [[Animation]]"
  },
  "backlinks": [],
  "wiki_links": [
    "Agent Frameworks",
    "Animation",
    "Humans, Avatars , Character",
    "UserRepresentation",
    "MetaverseDomain",
    "AuthenticationService",
    "DigitalIdentity",
    "Gaussian splatting and Similar",
    "SocialInteraction",
    "Presence",
    "Customization",
    "Blender",
    "LoRA DoRA etc",
    "LoRA",
    "Appearance",
    "MetaversePlatform",
    "VirtualWorld"
  ],
  "ontology": {
    "term_id": "mv-871208729395",
    "preferred_term": "Daniel AI creative technologist",
    "definition": "A component of the metaverse ecosystem focusing on daniel ai creative technologist.",
    "source_domain": "metaverse",
    "maturity_level": null,
    "authority_score": 0.85
  }
}