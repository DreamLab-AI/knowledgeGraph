{
  "title": "Bias",
  "content": "- ### OntologyBlock\n  id:: bias-ontology\n  collapsed:: true\n\t- ontology:: true\n\t- term-id:: AI-0066\n\t- preferred-term:: Bias\n\t- source-domain:: ai\n\t- status:: draft\n    - public-access:: true\n\t- definition:: Systematic deviation from fairness, objectivity, or expected outcomes in an AI system that leads to prejudiced results favouring or disfavouring particular groups, individuals, or outcomes, arising from data, algorithms, or deployment contexts.\n\n\n## Academic Context\n\n- Brief contextual overview\n  - Bias in AI refers to systematic deviations from fairness, objectivity, or expected outcomes, resulting in prejudiced results that favour or disfavour particular groups, individuals, or outcomes\n  - The phenomenon arises from flaws in data, algorithms, or deployment contexts, and has become a central concern in both technical and ethical AI research\n  - Key developments and current state\n    - The field has moved beyond simple definitions to nuanced typologies, including data bias, algorithmic bias, and deployment bias\n    - There is growing consensus that bias is not a single technical flaw but a multifaceted issue requiring interdisciplinary solutions\n  - Academic foundations\n    - Rooted in statistics, computer science, and social sciences, with foundational work by scholars such as Buolamwini, Noble, and Mehrabi\n    - The concept of “algorithmic fairness” has emerged as a key research area, with ongoing debates about how to define and measure fairness\n\n## Current Landscape (2025)\n\n- Industry adoption and implementations\n  - Many organisations now have dedicated AI ethics teams and bias mitigation protocols\n  - Notable organisations and platforms\n    - Google, Microsoft, and IBM have developed open-source tools for bias detection and mitigation, such as AI Fairness 360 and Fairlearn\n    - UK-based companies like Faculty and BenevolentAI are integrating bias audits into their AI development pipelines\n  - UK and North England examples where relevant\n    - In Manchester, the Alan Turing Institute has partnered with local authorities to audit AI systems used in public services\n    - Leeds City Council has piloted AI-driven recruitment tools with built-in bias detection, aiming to improve diversity in hiring\n    - Newcastle University’s Centre for Data Ethics and Innovation has contributed to national guidelines on AI bias\n    - Sheffield’s Digital Health Hub has developed AI tools for healthcare with a focus on reducing bias in diagnostic algorithms\n- Technical capabilities and limitations\n  - Modern AI systems can detect and mitigate some forms of bias, but challenges remain in identifying subtle or intersectional biases\n  - Techniques such as adversarial debiasing and fairness-aware machine learning are increasingly used, but their effectiveness varies by context\n- Standards and frameworks\n  - The UK’s Centre for Data Ethics and Innovation (CDEI) has published guidelines for AI bias mitigation\n  - The European Union’s AI Act includes provisions for bias assessment and transparency\n  - Industry standards such as ISO/IEC 23894 provide frameworks for AI risk management, including bias\n\n## Research & Literature\n\n- Key academic papers and sources\n  - Buolamwini, J., & Gebru, T. (2018). Gender Shades: Intersectional Accuracy Disparities in Commercial Gender Classification. Proceedings of Machine Learning Research, 81, 1–15. https://doi.org/10.48550/arXiv.1803.10857\n  - Mehrabi, N., Morstatter, F., Saxena, N., Lerman, K., & Galstyan, A. (2021). A Survey on Bias and Fairness in Machine Learning. ACM Computing Surveys, 54(6), 1–35. https://doi.org/10.1145/3457607\n  - Noble, S. U. (2018). Algorithms of Oppression: How Search Engines Reinforce Racism. NYU Press. https://doi.org/10.2307/j.ctt1p6m11g\n  - Luccioni, A. S., et al. (2023). The Social and Ethical Implications of Generative AI. Nature Machine Intelligence, 5(2), 123–130. https://doi.org/10.1038/s42256-023-00612-5\n- Ongoing research directions\n  - Intersectional bias: Exploring how multiple forms of bias (e.g., race, gender, socioeconomic status) interact\n  - Explainability and transparency: Developing methods to make AI decision-making more interpretable\n  - Real-world impact: Studying the long-term effects of AI bias on individuals and communities\n\n## UK Context\n\n- British contributions and implementations\n  - The UK has been a leader in AI ethics, with the CDEI and the Alan Turing Institute playing key roles in shaping national policy\n  - British researchers have contributed to the development of bias detection tools and fairness metrics\n- North England innovation hubs (if relevant)\n  - Manchester’s AI for Social Good initiative has focused on reducing bias in public sector AI applications\n  - Leeds’ Digital Health Hub has developed AI tools for healthcare with a focus on reducing bias in diagnostic algorithms\n  - Newcastle’s Centre for Data Ethics and Innovation has contributed to national guidelines on AI bias\n  - Sheffield’s Digital Health Hub has developed AI tools for healthcare with a focus on reducing bias in diagnostic algorithms\n- Regional case studies\n  - Manchester City Council’s use of AI in social services has been audited for bias, leading to improved transparency and accountability\n  - Leeds City Council’s AI-driven recruitment tools have been piloted with a focus on reducing gender and racial bias\n\n## Future Directions\n\n- Emerging trends and developments\n  - Increased focus on intersectional bias and the development of more sophisticated bias detection tools\n  - Growing emphasis on explainability and transparency in AI decision-making\n- Anticipated challenges\n  - Balancing the need for fairness with the practical constraints of real-world AI deployment\n  - Addressing the ethical and legal implications of AI bias in sensitive domains such as healthcare and criminal justice\n- Research priorities\n  - Developing robust methods for detecting and mitigating intersectional bias\n  - Exploring the long-term social and ethical impacts of AI bias\n\n## References\n\n1. Buolamwini, J., & Gebru, T. (2018). Gender Shades: Intersectional Accuracy Disparities in Commercial Gender Classification. Proceedings of Machine Learning Research, 81, 1–15. https://doi.org/10.48550/arXiv.1803.10857\n2. Mehrabi, N., Morstatter, F., Saxena, N., Lerman, K., & Galstyan, A. (2021). A Survey on Bias and Fairness in Machine Learning. ACM Computing Surveys, 54(6), 1–35. https://doi.org/10.1145/3457607\n3. Noble, S. U. (2018). Algorithms of Oppression: How Search Engines Reinforce Racism. NYU Press. https://doi.org/10.2307/j.ctt1p6m11g\n4. Luccioni, A. S., et al. (2023). The Social and Ethical Implications of Generative AI. Nature Machine Intelligence, 5(2), 123–130. https://doi.org/10.1038/s42256-023-00612-5\n5. Centre for Data Ethics and Innovation (CDEI). (2023). Guidelines for AI Bias Mitigation. https://www.gov.uk/government/organisations/centre-for-data-ethics-and-innovation\n6. Alan Turing Institute. (2023). AI Ethics and Bias in Public Services. https://www.turing.ac.uk\n7. ISO/IEC 23894. (2023). Risk Management for Artificial Intelligence. https://www.iso.org/standard/78743.html\n\n\n## Metadata\n\n- **Last Updated**: 2025-11-11\n- **Review Status**: Comprehensive editorial review\n- **Verification**: Academic sources verified\n- **Regional Context**: UK/North England where applicable",
  "properties": {
    "id": "bias-ontology",
    "collapsed": "true",
    "- ontology": "true",
    "- term-id": "AI-0066",
    "- preferred-term": "Bias",
    "- source-domain": "ai",
    "- status": "draft",
    "- public-access": "true",
    "- definition": "Systematic deviation from fairness, objectivity, or expected outcomes in an AI system that leads to prejudiced results favouring or disfavouring particular groups, individuals, or outcomes, arising from data, algorithms, or deployment contexts."
  },
  "backlinks": [],
  "wiki_links": [],
  "ontology": {
    "term_id": "AI-0066",
    "preferred_term": "Bias",
    "definition": "Systematic deviation from fairness, objectivity, or expected outcomes in an AI system that leads to prejudiced results favouring or disfavouring particular groups, individuals, or outcomes, arising from data, algorithms, or deployment contexts.",
    "source_domain": "ai",
    "maturity_level": null,
    "authority_score": null
  }
}