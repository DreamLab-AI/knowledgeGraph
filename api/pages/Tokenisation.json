{
  "title": "Tokenisation",
  "content": "- ### OntologyBlock\n  id:: tokenisation-ontology\n  collapsed:: true\n\t- ontology:: true\n\t- term-id:: AI-0231\n\t- preferred-term:: Tokenisation\n\t- source-domain:: ai\n\t- status:: draft\n    - public-access:: true\n\t- definition:: The process of breaking text into discrete units (tokens) that can be processed by neural networks, ranging from characters to subwords to whole words.\n\n\n\n\n## Academic Context\n\n- Tokenisation is a foundational step in Natural Language Processing (NLP), involving the segmentation of text into smaller, meaningful units called tokens.\n  - Tokens may be characters, subwords, words, or sentences, depending on the application and language characteristics.\n  - This process enables computational models, including neural networks, to process and analyse human language effectively.\n- The academic foundation of tokenisation lies in linguistics and computer science, combining rule-based and statistical methods to handle diverse languages and scripts.\n  - Early work focused on word and sentence boundaries; modern approaches incorporate subword units to better manage out-of-vocabulary words and morphological complexity.\n\n## Current Landscape (2025)\n\n- Tokenisation remains a critical preprocessing step in NLP pipelines, essential for tasks such as text classification, sentiment analysis, machine translation, and named entity recognition.\n  - Industry adoption spans major technology companies, research institutions, and startups, utilising tokenisation methods tailored to their specific language data and model architectures.\n- Notable platforms and tools include:\n  - NLTK and SpaCy for general-purpose tokenisation in Python.\n  - Advanced tokenisers like BERT’s WordPiece and SentencePiece, which use subword tokenisation to improve handling of rare or compound words.\n- In the UK, and particularly in North England cities such as Manchester, Leeds, Newcastle, and Sheffield, NLP research and applications are growing, supported by universities and innovation hubs.\n  - For example, the University of Manchester’s NLP group contributes to tokenisation research and its application in healthcare and social media analysis.\n- Technical capabilities have advanced to handle multilingual and noisy text, but challenges remain with ambiguous token boundaries, idiomatic expressions, and domain-specific jargon.\n- Standards and frameworks for tokenisation are evolving, with open-source libraries and community-driven datasets promoting reproducibility and benchmarking.\n\n## Research & Literature\n\n- Key academic papers and sources include:\n  - Kudo, T., & Richardson, J. (2018). SentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing. *Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing: System Demonstrations*, 66–71. DOI: 10.18653/v1/D18-2012\n  - Sennrich, R., Haddow, B., & Birch, A. (2016). Neural Machine Translation of Rare Words with Subword Units. *Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (ACL)*, 1715–1725. DOI: 10.18653/v1/P16-1162\n  - Jurafsky, D., & Martin, J. H. (2023). *Speech and Language Processing* (3rd ed. draft). Available at: https://web.stanford.edu/~jurafsky/slp3/\n- Ongoing research focuses on:\n  - Improving tokenisation for low-resource and morphologically rich languages.\n  - Context-aware tokenisation that adapts dynamically to domain and task.\n  - Integration of tokenisation with end-to-end neural architectures to reduce preprocessing overhead.\n\n## UK Context\n\n- The UK has a vibrant NLP research community, with contributions to tokenisation methods and applications.\n  - Universities such as Manchester, Leeds, and Newcastle are active in developing tokenisation techniques for English dialects and multilingual corpora.\n- North England innovation hubs foster collaborations between academia and industry, applying tokenisation in sectors like healthcare, finance, and social media monitoring.\n  - For instance, Sheffield’s NLP research includes tokenisation strategies tailored to social media text, which is often informal and noisy.\n- Regional case studies highlight the adaptation of tokenisation to local linguistic phenomena, including dialectal variations and code-switching common in urban areas of North England.\n\n## Future Directions\n\n- Emerging trends include:\n  - Hybrid tokenisation approaches combining rule-based and neural methods for greater accuracy and flexibility.\n  - Tokenisation as a differentiable component within neural networks, enabling end-to-end learning.\n  - Expansion of tokenisation techniques to multimodal data, integrating text with speech and visual inputs.\n- Anticipated challenges:\n  - Handling increasingly diverse and informal language data, including social media, dialects, and mixed languages.\n  - Balancing tokenisation granularity to optimise model performance without excessive computational cost.\n- Research priorities:\n  - Developing universal tokenisation frameworks adaptable across languages and domains.\n  - Enhancing interpretability of tokenisation decisions to support transparency in NLP systems.\n\n## References\n\n1. Kudo, T., & Richardson, J. (2018). SentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing. *Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing: System Demonstrations*, 66–71. DOI: 10.18653/v1/D18-2012\n2. Sennrich, R., Haddow, B., & Birch, A. (2016). Neural Machine Translation of Rare Words with Subword Units. *Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (ACL)*, 1715–1725. DOI: 10.18653/v1/P16-1162\n3. Jurafsky, D., & Martin, J. H. (2023). *Speech and Language Processing* (3rd ed. draft). Available at: https://web.stanford.edu/~jurafsky/slp3/\n4. GeeksforGeeks. (2025). Tokenization in Natural Language Processing (NLP). Last updated July 23, 2025.\n5. Debut InfoTech. (2025). NLP Tokenization Guide: Methods, Types & Tools.\n6. Neptune.ai. (2025). Tokenization in NLP: Types, Challenges, Examples, Tools.\n7. DataCamp. (2025). What is Tokenization? Types, Use Cases, Implementation.\n8. IBM. (2025). What Is NLP (Natural Language Processing)? IBM Think Blog.\n\n\n## Metadata\n\n- **Last Updated**: 2025-11-11\n- **Review Status**: Comprehensive editorial review\n- **Verification**: Academic sources verified\n- **Regional Context**: UK/North England where applicable",
  "properties": {
    "id": "tokenisation-ontology",
    "collapsed": "true",
    "- ontology": "true",
    "- term-id": "AI-0231",
    "- preferred-term": "Tokenisation",
    "- source-domain": "ai",
    "- status": "draft",
    "- public-access": "true",
    "- definition": "The process of breaking text into discrete units (tokens) that can be processed by neural networks, ranging from characters to subwords to whole words."
  },
  "backlinks": [],
  "wiki_links": [],
  "ontology": {
    "term_id": "AI-0231",
    "preferred_term": "Tokenisation",
    "definition": "The process of breaking text into discrete units (tokens) that can be processed by neural networks, ranging from characters to subwords to whole words.",
    "source_domain": "ai",
    "maturity_level": null,
    "authority_score": null
  }
}