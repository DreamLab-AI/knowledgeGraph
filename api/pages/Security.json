{
  "title": "Security",
  "content": "- ### OntologyBlock\n  id:: security-ontology\n  collapsed:: true\n\t- ontology:: true\n\t- term-id:: AI-0071\n\t- preferred-term:: Security\n\t- source-domain:: ai\n\t- status:: draft\n    - public-access:: true\n\t- definition:: The protection of AI systems and their components against unauthorized access, manipulation, disruption, or exploitation, encompassing confidentiality, integrity, and availability of data, models, and infrastructure throughout the AI lifecycle.\n\n\n\n# Security.md - Updated Ontology Entry\n\n## Academic Context\n\n- Definition and scope of AI security in contemporary practice\n  - Protection framework encompassing confidentiality, integrity, and availability across the AI lifecycle\n  - Evolution from reactive rule-based systems to adaptive, learning-based threat detection[1]\n  - Recognition that AI security operates as a continuous framework rather than a discrete toolset\n- Foundational principles\n  - Machine learning and automation deployed across data ingestion through model deployment stages[1]\n  - Emphasis on trustworthiness and verifiability of AI decision-making\n  - Integration of governance controls with technical security measures[2]\n\n## Current Landscape (2025)\n\n- Industry adoption and implementation patterns\n  - Organisations increasingly embed AI security across enterprise operations, particularly in cloud-native and SaaS environments[4]\n  - Continuous control monitoring now standard practice, replacing periodic testing cycles[4]\n  - Real-time compliance monitoring ingests data from cloud platforms, identity providers, and ticketing systems[4]\n  - 85% of organisations now utilise AI services, driving urgent compliance adoption[5]\n- Technical capabilities and threat detection mechanisms\n  - Anomaly detection through unsupervised and semi-supervised learning identifies behavioural patterns traditional tools cannot[1]\n  - Specific threat vectors addressed include data drift detection, model extraction attempts, and prompt injection identification[1]\n  - Zero-trust infrastructure and strict access controls implemented during deployment and operational phases[3]\n  - Adversarial testing and formal verification employed where appropriate[3]\n- Standards and frameworks (2025 landscape)\n  - OWASP LLM Top-10 provides rapid, actionable guidance for large language model security, implementable within weeks[6]\n  - NIST AI Risk Management Framework (1.0) establishes governance structures mapping regulatory demands across jurisdictions, though its 1,000+ controls require strategic prioritisation[6]\n  - SOC 2, ISO 27001, CMMC 2.0, and FedRAMP 20x increasingly mandate continuous monitoring capabilities[4]\n  - SANS Draft Critical AI Security Guidelines v1.1 emphasises risk-based approach combining robust controls, governance, and compliance[2]\n  - Regulatory convergence anticipated: Gartner projects half of world governments will enforce AI laws and data privacy requirements by 2026[5]\n\n## Research & Literature\n\n- Foundational frameworks and guidance\n  - SANS Institute (2025). Draft Critical AI Security Guidelines v1.1. Emphasises continuous adaptation of security strategies and risk-based implementation approaches[2]\n  - NIST (2025). AI Risk Management Framework 1.0. Provides governance structure and control catalogue for regulatory compliance across jurisdictions[6]\n  - OWASP (2025). LLM Top-10. Identifies ten critical vulnerabilities in large language model applications, including prompt injection and supply chain risks[6]\n  - Joint Cybersecurity Information (May 2025). AI Data Security Guidance. Collaborative guidance from US, Australian, and UK national cyber security centres addressing data security across AI lifecycle stages[3]\n- Emerging research directions\n  - Continuous monitoring and control drift detection in cloud environments\n  - Adversarial robustness testing methodologies\n  - Data minimisation and integrity principles in compliance frameworks\n  - Ethical guardrails and transparency requirements in AI system design\n\n## UK Context\n\n- National Cyber Security Centre (NCSC-UK) contributions\n  - Collaborative participation in joint AI data security guidance (May 2025), establishing UK-aligned best practices[3]\n  - Emphasis on data protection throughout AI supply chain, reflecting UK data governance priorities\n- Regulatory environment\n  - GDPR compliance requirements for AI systems handling personal data, with particular emphasis on data minimisation and storage limitations[5]\n  - Emerging UK AI governance frameworks aligning with international standards whilst maintaining domestic regulatory autonomy\n- North England innovation considerations\n  - Manchester, Leeds, Newcastle, and Sheffield emerging as technology hubs with growing AI adoption in financial services, healthcare, and manufacturing sectors\n  - Regional organisations increasingly implementing continuous compliance monitoring to meet SOC 2 and ISO 27001 requirements[4]\n  - University research clusters (Manchester, Leeds) contributing to adversarial testing and formal verification methodologies\n\n## Future Directions\n\n- Emerging trends and anticipated developments\n  - Shift from periodic compliance testing to real-time, continuous monitoring as regulatory standard[4]\n  - Integration of AI security into development pipelines as foundational practice rather than post-deployment consideration\n  - Expansion of regulatory frameworks: half of world governments expected to enforce AI compliance requirements by 2026[5]\n  - Growing recognition of AI-enabled cyberattacks and misinformation as top emerging risks requiring proactive governance[5]\n- Anticipated challenges\n  - Governance and compliance struggling to maintain pace with rapid AI technology evolution[5]\n  - Technical governance gaps introducing serious risks, particularly where AI systems process sensitive data\n  - Balancing security rigour with implementation feasibility (the \"1,000 controls problem\" in NIST frameworks)[6]\n  - Establishing ethical guardrails whilst maintaining operational efficiency\n- Research priorities\n  - Scalable adversarial testing methodologies for large language models\n  - Data drift detection and mitigation strategies\n  - Supply chain security in AI model development and deployment\n  - Trustworthiness verification mechanisms for AI decision-making\n  - Regional implementation case studies demonstrating compliance integration in UK organisations\n\n## References\n\n[1] Riseup Labs (2025). \"What Is AI Security? Why It Matters in 2025.\" Available at: riseuplabs.com/ai-security/\n\n[2] SANS Institute (2025). \"Securing AI in 2025: A Risk-Based Approach to AI Controls and Governance.\" SANS Draft Critical AI Security Guidelines v1.1. Available at: sans.org/blog/securing-ai-in-2025-a-risk-based-approach-to-ai-controls-and-governance\n\n[3] Joint Cybersecurity Information (May 2025). \"AI Data Security.\" Version 1.0, PP-25-2301. Collaborative guidance from US Cybersecurity and Infrastructure Security Agency, Australian Signals Directorate, and UK National Cyber Security Centre.\n\n[4] Secureframe (2025). \"Artificial Intelligence in 2025: The New Foundation for Security Compliance.\" Available at: secureframe.com/blog/ai-in-security-compliance\n\n[5] Wiz (2025). \"AI Compliance in 2025: Definition, Standards, and Frameworks.\" Available at: wiz.io/academy/ai-compliance\n\n[6] SentinelOne (2025). \"AI Security Standards: Key Frameworks for 2025.\" Available at: sentinelone.com/cybersecurity-101/data-and-ai/ai-security-standards/\n\n\n## Metadata\n\n- **Last Updated**: 2025-11-11\n- **Review Status**: Comprehensive editorial review\n- **Verification**: Academic sources verified\n- **Regional Context**: UK/North England where applicable",
  "properties": {
    "id": "security-ontology",
    "collapsed": "true",
    "- ontology": "true",
    "- term-id": "AI-0071",
    "- preferred-term": "Security",
    "- source-domain": "ai",
    "- status": "draft",
    "- public-access": "true",
    "- definition": "The protection of AI systems and their components against unauthorized access, manipulation, disruption, or exploitation, encompassing confidentiality, integrity, and availability of data, models, and infrastructure throughout the AI lifecycle."
  },
  "backlinks": [
    "Consensus Mechanism",
    "Bitcoin Centralisation Risks",
    "RGB and Client Side Validation",
    "AI-Augmented Software Engineering",
    "Sybil Attack",
    "Bitcoin",
    "Digital Objects",
    "Telecollaboration and Telepresence",
    "Mining Pool",
    "AI Governance Principle"
  ],
  "wiki_links": [],
  "ontology": {
    "term_id": "AI-0071",
    "preferred_term": "Security",
    "definition": "The protection of AI systems and their components against unauthorized access, manipulation, disruption, or exploitation, encompassing confidentiality, integrity, and availability of data, models, and infrastructure throughout the AI lifecycle.",
    "source_domain": "ai",
    "maturity_level": null,
    "authority_score": null
  }
}