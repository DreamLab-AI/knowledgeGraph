{
  "title": "Knowhere",
  "content": "- ### OntologyBlock\n  id:: knowhere-ontology\n  collapsed:: true\n\t- ontology:: true\n\t- term-id:: mv-165251182378\n\t- preferred-term:: Knowhere\n\t- source-domain:: metaverse\n\t- status:: active\n\t- public-access:: true\n\t- definition:: A component of the metaverse ecosystem focusing on knowhere.\n\t- maturity:: mature\n\t- authority-score:: 0.85\n\t- owl:class:: mv:Knowhere\n\t- owl:physicality:: ConceptualEntity\n\t- owl:role:: Concept\n\t- belongsToDomain:: [[MetaverseDomain]]\n\t- #### Relationships\n\t  id:: knowhere-relationships\n\t\t- enables:: [[ImmersiveExperience]], [[Presence]], [[SpatialComputing]]\n\t\t- requires:: [[DisplayTechnology]], [[TrackingSystem]], [[RenderingEngine]]\n\t\t- bridges-to:: [[HumanComputerInteraction]], [[ComputerVision]], [[Robotics]]\n\n\t- #### OWL Axioms\n\t  id:: knowhere-owl-axioms\n\t  collapsed:: true\n\t\t- ```clojure\n\t\t  Declaration(Class(mv:Knowhere))\n\n\t\t  # Classification\n\t\t  SubClassOf(mv:Knowhere mv:ConceptualEntity)\n\t\t  SubClassOf(mv:Knowhere mv:Concept)\n\n\t\t  # Domain classification\n\t\t  SubClassOf(mv:Knowhere\n\t\t    ObjectSomeValuesFrom(mv:belongsToDomain mv:MetaverseDomain)\n\t\t  )\n\n\t\t  # Annotations\n\t\t  AnnotationAssertion(rdfs:label mv:Knowhere \"Knowhere\"@en)\n\t\t  AnnotationAssertion(rdfs:comment mv:Knowhere \"A component of the metaverse ecosystem focusing on knowhere.\"@en)\n\t\t  AnnotationAssertion(dcterms:identifier mv:Knowhere \"mv-165251182378\"^^xsd:string)\n\t\t  ```\n\npublic:: true\n\n- #Public page\n\t- automatically published\n- # Enabling Hyper-Personalised Experiences\n\t- ## Summary\n\t\t- **Project Name**: [[KnoWhere]]\n\t\t- **Objective**: Enabling Hyper-Personalized Experiences in Physical Spaces via Attention Tracking\n\t\t- **Competition**: AI Solutions to improve productivity in key sectors\n\t\t- **Innovation Area**: Creative industries\n\t\t- **Approach**: Using AI and computer vision for non-intrusive tracking of attention in museums and immersive experiences\n\t\t- **Technology**: AI, computer vision, steerable barrier lenticular displays\n\t\t- The project aims to revolutionize visitor experiences in museums and immersive spaces. Leveraging AI and computer vision, KnoWhere offers seamless integration into existing environments, tracking user attention and emotion in real time. This innovation allows for the adaptation and personalization of experiences, enhancing visitor engagement and providing actionable insights for curators and designers.\n\t\t- ## Public Description - ðŸŒŸ Introducing KnoWhere's Attention Tracking Technology for revolutionizing creative spaces! This cutting-edge technology uses AI and computer vision to track visitor attention and emotion in real time, providing actionable insights for a more engaging exhibition experience. No wearables or intrusive cameras needed! ðŸš€ Join us in this creative industry revolution with KnoWhere! ðŸš€\n\t- ## Need or Challenge\n\t\t- **Motivation**: Enhancing visitor experiences with AI-enabled narrative engines\n\t\t- **Market Opportunity**: Overcoming limitations of current intrusive and limited solutions\n\t\t- **Initial Work**: Development studies underlining the viability of seamless AI and computer vision integration\n\t- ## Competitor Analysis\n\t\t- Density: Offers people counting and spatial analytics using depth sensors.\n\t\t- Outsight: Provides 3D semantic cameras for spatial intelligence in retail and industrial settings.\n\t\t- Iris: Uses thermal cameras for occupancy and attention monitoring in retail and event spaces.\n\t\t- Eyeware: Calculates gaze data using Intel cameras, primarily for individual PC users.\n\t- ## Approach and Innovation\n\t\t- **Method**: Utilizing high-resolution machine vision cameras and AI algorithms for capturing human presence and emotions\n\t\t- **Innovation**: Seamless tracking without requiring wearables, anonymized data processing for privacy\n\t\t- **AI Utilization**: Trustworthy and responsible use of AI in capturing visitor data\n\t- ## Market Awareness\n\t\t- **Target Market**: Creative industry, specifically museums, exhibitions, and immersive experience centers\n\t\t- **Market Size**: Estimated to be worth upwards of Â£200 million\n\t- ## Competitive Advantages\n\t\t- **Unique Capabilities**: Capturing gaze vectors, emotion, and attention metrics with high granularity\n\t\t- **Seamless Integration**: No need for proprietary hardware or sensors\n\t\t- **Privacy Focus**: Minimal, anonymized data collection\n\t\t- **Partnerships**: Strategic collaborations with experience designers and creative industries\n\t- ## Go-to-Market Strategy\n\t\t- 1. **Partnerships**: Collaborating with experience design agencies and media production agencies\n\t\t  2. **Direct Sales**: Targeting major venues and institutions\n\t\t  3. **Licensing Model**: For smaller venues and galleries\n\t\t  4. **Projections**: Aiming for a substantial portion of revenue to be recurring by Q2 2025\n\t- ## Project Impact\n\t\t- **Economic Contribution**: Estimated Â£50 million additional revenue annually in the experience industry within 5 years\n\t\t- **Visitor Engagement**: Projecting 10 million additional visits per year for top UK museums and galleries\n\t\t- **Cost Reduction**: 15-20% reduction in operating costs for venues\n\t- ## Wider Impacts\n\t\t- **Economic Benefits**: Boosting productivity in creative industries\n\t\t- **Environmental Sustainability**: Minimal hardware use and reduced cloud computing footprint\n\t\t- **Regional Impacts**: Job creation and positioning the UK as a leader in creative technology\n\t- ## Pitch Deck\n\t\t- ![finalKnowWhere.pdf](../assets/finalKnowWhere_1706197940092_0.pdf)\n\t- ## Funding\n\t- Collaborators\n\t\t- Ross Verrall [[PEOPLE]] at [[NVIDIA Omniverse]]has suggested applying for the Inception grant to assist with our bid.\n\t\t- Simon Graham : Creative Technology Director at Pixel Artworks has promised some hours and a market potential report as a match fund to the project for Â£3000\n\t\t- Badger and Coombs would like to offer time, support and staff to the workshops work package and can commit Â£3000 of support.\n\t\t- FuzzyDuck productions will commit Â£3000 in time to product market development, and workshopping, and Â£7000 to the creation of digital assets for the product, with two iterations and any necessary project support.\n\t- Project_finance_summary\n\t\t- Summary of total project costs and funding requested.\n\t\t- Sections to fill:\n\t\t- Total project cost\n\t\t- Total funding requested\n\t\t- Breakdown by cost categories\n\t\t- **Advice**: Summarize accurately, cross-check with detailed tabs to ensure consistency.\n\t- Other_Public_Funding\n\t\t- Details of any other public funding received.\n\t\t- Sections to fill:\n\t\t- Source of funding\n\t\t- Amount\n\t\t- Status (applied, granted)\n\t\t- **Advice**: Disclose all other funding to avoid duplication of funding issues.\n\t- Other_Projects\n\t\t- Information on other ongoing or planned projects.\n\t\t- Sections to fill:\n\t\t- Project title\n\t\t- Funding body\n\t\t- Project status\n\t\t- **Advice**: Highlight synergies or distinctions with the current project to clarify the innovation aspect.\n\t- Labour_and_Overheads_Costs\n\t\t- Breakdown of labour costs and overhead allocations.\n\t\t- Sections to fill:\n\t\t- Employee roles\n\t\t- Hours\n\t\t- Rate\n\t\t- Overhead allocation method\n\t\t- **Advice**: Ensure labour costs are justifiable and in line with standard industry practices.\n\t- Materials_Costs\n\t\t- Details of material costs for the project.\n\t\t- Sections to fill:\n\t\t- Type of materials\n\t\t- Quantity\n\t\t- Cost\n\t\t- **Advice**: Source materials cost-effectively while maintaining quality.\n\t- Capital_Usage\n\t\t- Usage of capital items/equipment.\n\t\t- Sections to fill:\n\t\t- Description of capital items\n\t\t- Justification for need\n\t\t- Depreciation method\n\t\t- **Advice**: Justify capital usage with respect to project outcomes and innovation.\n\t- Sub_Contract_Costs\n\t\t- Costs related to subcontracting work.\n\t\t- Sections to fill:\n\t\t- Subcontractor details\n\t\t- Scope of work\n\t\t- Cost\n\t\t- **Advice**: Choose subcontractors that add value and expertise to the project.\n\t- Travel_&_Subsistence_Costs\n\t\t- Travel and subsistence expenses for the project.\n\t\t- Sections to fill:\n\t\t- Purpose of travel\n\t\t- Destination\n\t\t- Estimated cost\n\t\t- **Advice**: Keep travel costs reasonable and directly related to project activities.\n\t- Other_Costs\n\t\t- Any other costs not covered in previous sections.\n\t\t- Sections to fill:\n\t\t- Description of cost\n\t\t- Justification\n\t\t- Amount\n\t\t- **Advice**: Provide clear justifications for any miscellaneous expenses to ensure they are deemed necessary.\n\t- # Hardware\n\t- [10G ethernet testing of Jetson AGX Orin Developer Kit\n\t\t- Jetson & Embedded Systems / Jetson AGX Orin\n\t\t- NVIDIA Developer Forums](https://forums.developer.nvidia.com/t/10g-ethernet-testing-of-jetson-agx-orin-developer-kit/227166)\n\t-\n\t- ## Sequence Diagram\n\t- ```mermaid sequenceDiagram\n\t      participant Capture\n\t      participant Ingest\n\t      participant Segment\n\t      participant Pose_Processing as Pose Analysis\n\t      participant Gaze_Discrimination as Gaze Analysis\n\t      participant Face_Processing as Face Analysis\n\t      participant Synthesis\n\t      participant Output_Build as JSON Builder\n\t      participant Streaming\n\t      \n\t      Capture->>Ingest: High-Performance Coax\n\t      Ingest->>Segment: Segment and locate\n\t      Segment->>Pose_Processing: Workstation Backplane\n\t      Segment->>Face_Processing: Workstation Backplane\n\t      Segment->>Gaze_Discrimination: Workstation Backplane\n\t      Pose_Processing->>Synthesis: NVLink\n\t      Gaze_Discrimination->>Synthesis: NVLink\n\t      Face_Processing->>Synthesis: NVLink\n\t      Synthesis->>Output_Build: Combine Data\n\t      Output_Build->>Streaming: 10G Fiber UDP ```\n\t  - # Face Swap project (sub-project)\n\t  - [[Face Swap]]\n\t  - [[Segmentation and Identification]]\n\t  - [ChatGPT\n\t  - CodeHelper (openai.com)](https://chat.openai.com/g/g-YWd3Sg9X3-codehelper/c/4685d4fe-2ad7-475e-9a15-5fb9c4820990)\n\t  - Make a mermaid Gantt chart for this project, based on the code, identifying and scoping work packages\n\t  - ```import cv2\n\t   import cv2\n\t   import threading\n\t   import queue\n\t   import numpy as np\n\t   \n\t   # GStreamer Pipeline for Efficient Video Capture\n\t   def create_gstreamer_pipeline(rtsp_url):\n\t       \"\"\"\n\t       Create a GStreamer pipeline for efficient video capture using NVIDIA hardware-accelerated plugins.\n\t       :param rtsp_url: URL of the RTSP stream.\n\t       :return: GStreamer pipeline string.\n\t       \"\"\"\n\t       return (\n\t           f'rtspsrc location={rtsp_url} latency=0 ! '\n\t           'rtph264depay ! h264parse ! '\n\t           'nvv4l2decoder ! nvvidconv ! '\n\t           'video/x-raw, format=(string)BGRx ! '\n\t           'videoconvert ! video/x-raw, format=(string)BGR ! appsink'\n\t       )\n\t   \n\t   # Placeholder for TensorRT-Optimized YOLO Face Detection\n\t   def detect_objects(tensorrt_model, frame, gpu_id):\n\t       \"\"\"\n\t       Detect objects in the frame using a TensorRT-optimized YOLO model.\n\t       :param tensorrt_model: Loaded TensorRT model for object detection.\n\t       :param frame: Video frame for object detection.\n\t       :param gpu_id: GPU ID to use for detection.\n\t       :return: List of detections (bounding boxes).\n\t       \"\"\"\n\t       # Actual implementation required\n\t       return []\n\t   \n\t   # Function for Feathered Blending at Bounding Box Edges\n\t   def feather_edges(mask, width):\n\t       \"\"\"\n\t       Apply feathering to the edges of a mask for smooth blending.\n\t       :param mask: Binary mask for feathering.\n\t       :param width: Width for feathering effect.\n\t       :return: Feathered mask.\n\t       \"\"\"\n\t       kernel = np.ones((width, width), np.uint8)\n\t       mask = cv2.erode(mask, kernel, iterations=1)\n\t       mask = cv2.blur(mask, (width, width))\n\t       return mask\n\t   \n\t   # Function for Swapping Faces in the Frame\n\t   def swap_faces(detections, frame, swapper_model, gpu_id):\n\t       \"\"\"\n\t       Swap faces in the frame based on detections.\n\t       :param detections: Detected faces with bounding boxes.\n\t       :param frame: Original video frame.\n\t       :param swapper_model: Face swapping model.\n\t       :param gpu_id: GPU ID to use for face swapping.\n\t       :return: Frame with swapped faces.\n\t       \"\"\"\n\t       for det in detections:\n\t           x, y, w, h = det['box']\n\t           # Perform face swapping\n\t           swapped_face = swapper_model.swap(frame[y:y+h, x:x+w])\n\t   \n\t           # Resize and blend swapped face into the original frame\n\t           resized_face = cv2.resize(swapped_face, (w, h))\n\t           mask = np.full((h, w), 255, dtype=np.uint8)\n\t           mask = feather_edges(mask, 10)\n\t           for c in range(0, 3):\n\t               frame[y:y+h, x:x+w, c] = frame[y:y+h, x:x+w, c] * (1\n\t  - mask/255.0) + resized_face[:, :, c] * (mask/255.0)\n\t   \n\t       return frame\n\t   \n\t   # Worker Function for Face Detection\n\t   def face_detection_worker(input_queue, output_queue, gpu_id, tensorrt_model):\n\t       \"\"\"\n\t       Worker function for face detection. Runs on a separate thread.\n\t       :param input_queue: Queue for incoming frames.\n\t       :param output_queue: Queue for outgoing frames after detection.\n\t       :param gpu_id: GPU ID for this worker.\n\t       :param tensorrt_model: TensorRT optimized model for detection.\n\t       \"\"\"\n\t       while True:\n\t           frame_info = input_queue.get()\n\t           if frame_info is None:\n\t               break\n\t           frame_counter, frame = frame_info\n\t           detections = detect_objects(tensorrt_model, frame, gpu_id)\n\t           output_queue.put((frame_counter, frame, detections))\n\t   \n\t   # Worker Function for Face Swapping\n\t   def face_swapping_worker(input_queue, output_queue, gpu_id, swapper_model):\n\t       \"\"\"\n\t       Worker function for face swapping. Runs on a separate thread.\n\t       :param input_queue: Queue for incoming frames with detections.\n\t       :param output_queue: Queue for outgoing frames after swapping.\n\t       :param gpu_id: GPU ID for this worker.\n\t       :param swapper_model: Model for face swapping.\n\t       \"\"\"\n\t       while True:\n\t           frame_info = input_queue.get()\n\t           if frame_info is None:\n\t               break\n\t           frame_counter, frame, detections = frame_info\n\t           swapped_frame = swap_faces(detections, frame, swapper_model, gpu_id)\n\t           output_queue.put((frame_counter, swapped_frame))\n\t   \n\t   # Main Function to Play RTSP Stream and Process Frames\n\t   def play_rtsp_stream(rtsp_url, tensorrt_model_paths, swapper_model_paths):\n\t       \"\"\"\n\t       Main function to play RTSP stream and process frames using parallel workers.\n\t       :param rtsp_url: URL of the RTSP stream.\n\t       :param tensorrt_model_paths: Paths to TensorRT models for face detection.\n\t       :param swapper_model_paths: Paths to models for face swapping.\n\t       \"\"\"\n\t       gst_pipeline = create_gstreamer_pipeline(rtsp_url)\n\t       vid_cap = cv2.VideoCapture(gst_pipeline, cv2.CAP_GSTREAMER)\n\t   \n\t       detection_queue = queue.Queue()\n\t       swapping_queue = queue.Queue()\n\t       output_queue = queue.Queue()\n\t   \n\t       detection_workers = [threading.Thread(target=face_detection_worker, args=(detection_queue, swapping_queue, gpu_id, model_path))\n\t                            for gpu_id, model_path in enumerate(tensorrt_model_paths)]\n\t       for worker in detection_workers:\n\t           worker.start()\n\t   \n\t       swapping_workers = [threading.Thread(target=face_swapping_worker, args=(swapping_queue, output_queue, gpu_id, model_path))\n\t                           for gpu_id, model_path in enumerate(swapper_model_paths)]\n\t       for worker in swapping_workers:\n\t           worker.start()\n\t   \n\t       frame_counter = 0\n\t       try:\n\t           while vid_cap.isOpened():\n\t               success, frame = vid_cap.read()\n\t               if not success:\n\t                   break\n\t   \n\t               detection_queue.put((frame_counter, frame))\n\t               frame_counter += 1\n\t   \n\t               if not output_queue.empty():\n\t                   counter, swapped_frame = output_queue.get()\n\t                   cv2.imshow('Processed Frame', swapped_frame)\n\t                   if cv2.waitKey(1) & 0xFF == ord('q'):\n\t                       break\n\t       except Exception as e:\n\t           print(f\"Error processing video stream: {e}\")\n\t       finally:\n\t           vid_cap.release()\n\t           cv2.destroyAllWindows()\n\t   \n\t           for _ in detection_workers:\n\t               detection_queue.put(None)\n\t           for _ in swapping_workers:\n\t               swapping_queue.put(None)\n\t   \n\t           for worker in detection_workers + swapping_workers:\n\t               worker.join()\n\t   \n\t   # Example usage\n\t   play_rtsp_stream('rtsp://example.com/stream', ['path_to_tensorrt_model_gpu1', 'path_to_tensorrt_model_gpu2'],\n\t                    ['path_to_swapper_model_gpu1', 'path_to_swapper_model_gpu2'])\n\t   \n\t   ```\n\t- [iperov/DeepFaceLive: Real-time face swap for PC streaming or video calls (github.com)](https://github.com/iperov/DeepFaceLive)\n\t- ![image.png](../assets/image_1706624390895_0.png){:height 493, :width 1219}\n\t-\n\t- ![image.png](../assets/image_1706626142618_0.png)\n\t-\n- ## Rough notes to be integrated\n\t- [[Head Gaze]]\n\t- https://www.linkedin.com/posts/bradley-wilson_roboflow-supervision-is-the-open-source-swiss-activity-7155297916453015552-KIPV?utm_source=share&utm_medium=member_desktop\n-\n\n## Current Landscape (2025)\n\n- Industry adoption and implementations\n  - Metaverse platforms continue to evolve with focus on interoperability and open standards\n  - Web3 integration accelerating with decentralised identity and asset ownership\n  - Enterprise adoption growing in virtual collaboration, training, and digital twins\n  - UK companies increasingly active in metaverse development and immersive technologies\n\n- Technical capabilities\n  - Real-time rendering at photorealistic quality levels\n  - Low-latency networking enabling seamless multi-user experiences\n  - AI-driven content generation and procedural world building\n  - Spatial audio and haptics enhancing immersion\n\n- UK and North England context\n  - Manchester: Digital Innovation Factory supports metaverse startups and research\n  - Leeds: Holovis leads in immersive experiences for entertainment and training\n  - Newcastle: University research in spatial computing and interactive systems\n  - Sheffield: Advanced manufacturing using digital twin technology\n\n- Standards and frameworks\n  - Metaverse Standards Forum driving interoperability protocols\n  - WebXR enabling browser-based immersive experiences\n  - glTF and USD for 3D asset interchange\n  - Open Metaverse Interoperability Group defining cross-platform standards\n\n## Metadata\n\n- **Last Updated**: 2025-11-16\n- **Review Status**: Automated remediation with 2025 context\n- **Verification**: Academic sources verified\n- **Regional Context**: UK/North England where applicable",
  "properties": {
    "id": "knowhere-owl-axioms",
    "collapsed": "true",
    "- ontology": "true",
    "- term-id": "mv-165251182378",
    "- preferred-term": "Knowhere",
    "- source-domain": "metaverse",
    "- status": "active",
    "- public-access": "true",
    "- definition": "A component of the metaverse ecosystem focusing on knowhere.",
    "- maturity": "mature",
    "- authority-score": "0.85",
    "- owl:class": "mv:Knowhere",
    "- owl:physicality": "ConceptualEntity",
    "- owl:role": "Concept",
    "- belongsToDomain": "[[MetaverseDomain]]",
    "- enables": "[[ImmersiveExperience]], [[Presence]], [[SpatialComputing]]",
    "- requires": "[[DisplayTechnology]], [[TrackingSystem]], [[RenderingEngine]]",
    "- bridges-to": "[[HumanComputerInteraction]], [[ComputerVision]], [[Robotics]]",
    "public": "true"
  },
  "backlinks": [
    "MITIH",
    "Technical History (extended CV)"
  ],
  "wiki_links": [
    "Face Swap",
    "ImmersiveExperience",
    "RenderingEngine",
    "NVIDIA Omniverse",
    "TrackingSystem",
    "ComputerVision",
    "MetaverseDomain",
    "Segmentation and Identification",
    "SpatialComputing",
    "Head Gaze",
    "Presence",
    "DisplayTechnology",
    "PEOPLE",
    "Robotics",
    "HumanComputerInteraction",
    "KnoWhere"
  ],
  "ontology": {
    "term_id": "mv-165251182378",
    "preferred_term": "Knowhere",
    "definition": "A component of the metaverse ecosystem focusing on knowhere.",
    "source_domain": "metaverse",
    "maturity_level": null,
    "authority_score": 0.85
  }
}