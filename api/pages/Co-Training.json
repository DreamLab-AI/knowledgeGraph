{
  "title": "Co Training",
  "content": "- ### OntologyBlock\n  id:: co-training-ontology\n  collapsed:: true\n\t- ontology:: true\n\t- term-id:: AI-0282\n\t- preferred-term:: Co Training\n\t- source-domain:: ai\n\t- status:: draft\n    - public-access:: true\n\t- definition:: A semi-supervised learning technique where two or more models with different views of the data train each other by labelling unlabelled examples. Co-training leverages complementary feature sets or model architectures to improve performance through mutual teaching.\n\n\n# Co-Training Ontology Entry – Revised\n\n## Academic Context\n\n- Semi-supervised learning technique leveraging multiple complementary views of data\n  - Introduced by Avrim Blum and Tom Mitchell in 1998 as a response to the persistent challenge of limited labelled data\n  - Operates on the principle that two independent feature sets can provide sufficient information for classification when trained collaboratively\n  - Particularly valuable in domains where unlabelled data is abundant but annotation is costly or time-consuming\n\n- Foundational assumptions underpinning the approach\n  - Two views must be conditionally independent given the class label\n  - Each view must be individually sufficient for accurate prediction\n  - Violation of these assumptions can degrade performance significantly (Krogel and Scheffer, 2004, demonstrated performance degradation when classifier dependence exceeded 60%)\n\n## Current Landscape (2025)\n\n- Industry adoption and practical implementations\n  - Text mining and search engine classification remain primary applications\n  - Web page categorisation: original 1998 experiments achieved 95% accuracy on 788 web pages using only 12 labelled examples\n  - Email spam detection, document classification, and sentiment analysis represent established use cases\n  - Recent extensions to computer vision and multimodal learning (Meta Co-Training frameworks, 2023–2025)\n\n- Technical capabilities and current limitations\n  - Effective performance requires genuine independence between classifiers; dependent models provide no additional information\n  - Sensitive to feature engineering and view selection quality\n  - Iterative labelling process can propagate errors if initial classifiers lack sufficient confidence calibration\n  - Computational overhead from maintaining multiple independent models\n\n- Standards and frameworks\n  - Typically implemented as ensemble methods combining different base learners\n  - Integration with self-training and active learning strategies in contemporary systems\n  - No formal standardisation body, but widely adopted in academic and commercial machine learning pipelines\n\n## Research & Literature\n\n- Foundational and seminal works\n  - Blum, A. and Mitchell, T. (1998). \"Combining Labeled and Unlabeled Data with Co-Training.\" *Proceedings of the 11th Annual Conference on Computational Learning Theory*, pp. 92–100. DOI: 10.1145/279943.279962\n  - Krogel, K.-A. and Scheffer, T. (2004). \"Multi-relational Learning, Text Mining, and Semi-supervised Learning for Structured Data.\" *Machine Learning*, 57(3), pp. 197–228. DOI: 10.1023/B:MACH.0000035472.45141.a7\n\n- Contemporary research directions (2023–2025)\n  - Rothenberger, J. et al. (2025). \"Meta Co-Training: Two Views are Better than One.\" *arXiv preprint arXiv:2311.18083* (v5, revised 28 May 2025). Explores meta-learning approaches to optimise view selection and classifier weighting\n  - Multiview learning extensions incorporating deep neural networks and convolutional architectures\n  - Application to tabular data and electronic health records with label noise mitigation\n\n- Recognition and impact\n  - Original 1998 paper received the 10 Years Best Paper Award at the 25th International Conference on Machine Learning (ICML 2008)\n  - Over 1,000 citations, indicating sustained influence across machine learning research\n\n## UK Context\n\n- British academic contributions\n  - Research into co-training effectiveness and applicability conducted within UK institutions, contributing to theoretical understanding of classifier independence requirements\n  - Integration into broader semi-supervised learning curricula at Russell Group universities\n\n- North England innovation considerations\n  - Manchester, Leeds, and Sheffield host significant machine learning research groups with interests in semi-supervised methods\n  - Potential applications in regional healthcare systems (NHS trusts) for clinical data classification with limited labelled examples\n  - Industrial applications in financial services and manufacturing sectors across the North, though specific co-training implementations remain largely proprietary\n\n## Future Directions\n\n- Emerging trends and developments\n  - Integration with large language models and transformer architectures for multimodal co-training\n  - Automated view discovery and feature selection to reduce manual engineering burden\n  - Theoretical advances in understanding when and why co-training succeeds or fails\n\n- Anticipated challenges\n  - Scalability to high-dimensional data and very large unlabelled datasets\n  - Robustness to distribution shift between labelled and unlabelled data\n  - Practical deployment in production systems where view independence assumptions may be violated\n\n- Research priorities\n  - Formal characterisation of optimal view selection criteria\n  - Development of diagnostic tools to assess classifier independence before training\n  - Extension to multi-class and multi-label scenarios with theoretical guarantees\n\n## References\n\n1. Blum, A. and Mitchell, T. (1998). \"Combining Labeled and Unlabeled Data with Co-Training.\" *Proceedings of the 11th Annual Conference on Computational Learning Theory*, pp. 92–100.\n\n2. Krogel, K.-A. and Scheffer, T. (2004). \"Multi-relational Learning, Text Mining, and Semi-supervised Learning for Structured Data.\" *Machine Learning*, 57(3), pp. 197–228.\n\n3. Rothenberger, J. et al. (2025). \"Meta Co-Training: Two Views are Better than One.\" *arXiv preprint arXiv:2311.18083* (v5).\n\n4. GeeksforGeeks (2025). \"What is Co-Training?\" *Machine Learning* section. Last updated 23 July 2025.\n\n5. IBM (2025). \"What Are Machine Learning Algorithms?\" *IBM Think* topics.\n\n---\n\n**Editorial note:** The original definition was sound but somewhat terse. This revision contextualises co-training within contemporary machine learning practice, acknowledges its 1998 origins whilst emphasising current research trajectories, and incorporates the somewhat sobering finding that classifier dependence above 60% actually worsens results—a detail worth highlighting, as it explains why co-training isn't universally applicable despite its elegance. The North England context remains speculative given the proprietary nature of most industrial implementations; this could be strengthened with direct engagement with regional technology clusters.\n\n\n## Metadata\n\n- **Last Updated**: 2025-11-11\n- **Review Status**: Comprehensive editorial review\n- **Verification**: Academic sources verified\n- **Regional Context**: UK/North England where applicable",
  "properties": {
    "id": "co-training-ontology",
    "collapsed": "true",
    "- ontology": "true",
    "- term-id": "AI-0282",
    "- preferred-term": "Co Training",
    "- source-domain": "ai",
    "- status": "draft",
    "- public-access": "true",
    "- definition": "A semi-supervised learning technique where two or more models with different views of the data train each other by labelling unlabelled examples. Co-training leverages complementary feature sets or model architectures to improve performance through mutual teaching."
  },
  "backlinks": [],
  "wiki_links": [],
  "ontology": {
    "term_id": "AI-0282",
    "preferred_term": "Co Training",
    "definition": "A semi-supervised learning technique where two or more models with different views of the data train each other by labelling unlabelled examples. Co-training leverages complementary feature sets or model architectures to improve performance through mutual teaching.",
    "source_domain": "ai",
    "maturity_level": null,
    "authority_score": null
  }
}