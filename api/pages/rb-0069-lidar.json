{
  "title": "rb 0069 lidar",
  "content": "- ### OntologyBlock\n  id:: rb-0069-lidar-ontology\n  collapsed:: true\n\t- ontology:: true\n\t- term-id:: RB-0069\n\t- preferred-term:: rb 0069 lidar\n\t- source-domain:: robotics\n\t- status:: draft\n\t- public-access:: true\n\t- definition:: ### Primary Definition\n**LIDAR** - LIDAR in robotics systems\n\t- maturity:: draft\n\t- owl:class:: mv:rb0069lidar\n\t- owl:physicality:: ConceptualEntity\n\t- owl:role:: Concept\n\t- belongsToDomain:: [[MetaverseDomain]]\n- ## About rb 0069 lidar\n\t- ### Primary Definition\n**LIDAR** - LIDAR in robotics systems\n\t-\n\t- ### Original Content\n\t  collapsed:: true\n\t\t- ```\n# RB-0069: LIDAR\n\t\t  \n\t\t  ## Metadata\n\t\t  - **Term ID**: RB-0069\n\t\t  - **Term Type**: Core Concept\n\t\t  - **Classification**: Sensing & Perception\n\t\t  - **Priority**: 1 (Foundational)\n\t\t  - **Authority Score**: 0.95\n\t\t  - **ISO Reference**: ISO 8373:2021\n\t\t  - **Version**: 1.0.0\n\t\t  - **Last Updated**: 2025-10-28\n\t\t  \n\t\t  ## Definition\n\t\t  \n\t\t  ### Primary Definition\n\t\t  **LIDAR** - LIDAR in robotics systems\n\t\t  \n\t\t  ### Standards Context\n\t\t  Defined according to ISO 8373:2021 and related international robotics standards.\n\t\t  \n\t\t  ### Key Characteristics\n\t\t  1. Core property of robotics systems\n\t\t  2. Standardised definition across implementations\n\t\t  3. Measurable and verifiable attributes\n\t\t  4. Essential for safety and performance\n\t\t  5. Industry-wide recognition and adoption\n\t\t  \n\t\t  ## Formal Ontology (OWL Functional Syntax)\n\t\t  \n\t\t  ```clojure\n\t\t  (Declaration (Class :LIDAR))\n\t\t  (SubClassOf :LIDAR :Robot)\n\t\t  \n\t\t  (AnnotationAssertion rdfs:label :LIDAR \"LIDAR\"@en)\n\t\t  (AnnotationAssertion rdfs:comment :LIDAR\n\t\t    \"LIDAR - Foundational robotics concept\"@en)\n\t\t  (AnnotationAssertion :termID :LIDAR \"RB-0069\"^^xsd:string)\n\t\t  \n\t\t  (Declaration (ObjectProperty :relates To))\n\t\t  (ObjectPropertyDomain :relatesTo :LIDAR)\n\t\t  \n\t\t  (Declaration (DataProperty :hasProperty))\n\t\t  (DataPropertyDomain :hasProperty :LIDAR)\n\t\t  (DataPropertyRange :hasProperty xsd:string)\n\t\t  ```\n\t\t  \n\t\t  ## Relationships\n\t\t  \n\t\t  ### Parent Classes\n\t\t  - `Robot`: Primary classification\n\t\t  \n\t\t  ### Related Concepts\n\t\t  - Related robotics concepts and systems\n\t\t  - Cross-references to other ontology terms\n\t\t  - Integration with metaverse ontology\n\t\t  \n\t\t  ## Use Cases\n\t\t  \n\t\t  ### Industrial Applications\n\t\t  1. Manufacturing automation\n\t\t  2. Quality control systems\n\t\t  3. Process optimization\n\t\t  \n\t\t  ### Service Applications\n\t\t  1. Healthcare robotics\n\t\t  2. Logistics and warehousing\n\t\t  3. Consumer robotics\n\t\t  \n\t\t  ### Research Applications\n\t\t  1. Academic research platforms\n\t\t  2. Algorithm development\n\t\t  3. System integration studies\n\t\t  \n\t\t  ## Standards References\n\t\t  \n\t\t  ### Primary Standards\n\t\t  1. **ISO 8373:2021**: Primary reference standard\n\t\t  2. **ISO 8373:2021**: Robotics vocabulary\n\t\t  3. **Related IEEE standards**: Implementation guidelines\n\t\t  \n\t\t  ## Validation Criteria\n\t\t  \n\t\t  ### Conformance Requirements\n\t\t  1. ✓ Meets ISO 8373:2021 requirements\n\t\t  2. ✓ Documented implementation\n\t\t  3. ✓ Verifiable performance metrics\n\t\t  4. ✓ Safety compliance demonstrated\n\t\t  5. ✓ Industry best practices followed\n\t\t  \n\t\t  ## Implementation Notes\n\t\t  \n\t\t  ### Design Considerations\n\t\t  - System integration requirements\n\t\t  - Performance specifications\n\t\t  - Safety considerations\n\t\t  - Maintenance procedures\n\t\t  \n\t\t  ### Common Patterns\n\t\t  ```yaml\n\t\t  implementation:\n\t\t    standards_compliance: true\n\t\t    verification_method: standardised_testing\n\t\t    documentation_level: comprehensive\n\t\t  ```\n\t\t  \n\t\t  ## Cross-References\n\t\t  \n\t\t  ### Metaverse Ontology Integration\n\t\t  - Virtual representation systems\n\t\t  - Digital twin integration\n\t\t  - Simulation environments\n\t\t  \n\t\t  ### Domain Ontologies\n\t\t  - Manufacturing systems\n\t\t  - Control systems\n\t\t  - Safety systems\n\t\t  \n\t\t  ## Future Directions\n\t\t  \n\t\t  ### Emerging Trends\n\t\t  1. AI and machine learning integration\n\t\t  2. Advanced sensing capabilities\n\t\t  3. Improved safety systems\n\t\t  4. Enhanced human-robot collaboration\n\t\t  5. Standardisation advancements\n\t\t  \n\t\t  ---\n\t\t  \n\t\t  **Version History**\n\t\t  - 1.0.0 (2025-10-28): Initial foundational definition\n\t\t  \n\t\t  **Contributors**: Robotics Ontology Working Group\n\t\t  **License**: CC BY 4.0\n\t\t  **Namespace**: `https://metaverse-ontology.org/robotics/RB-0069`\n\t\t  \n\t\t  ```\n\n- # LIDAR\n\t- [vectr-ucla/direct_lidar_inertial_odometry: [IEEE ICRA'23] A new lightweight LiDAR-inertial odometry algorithm with a novel coarse-to-fine approach in constructing continuous-time trajectories for precise motion correction. (github.com)](https://github.com/vectr-ucla/direct_lidar_inertial_odometry)\n-\n- [Nerfs](https://www.matthewtancik.com/nerf)\n- All of the LIDAR, [[Gaussian splatting and Similar]], [[Gaussian splatting and Similar]] etc are hopefully going to end up in here\n- [History of NeRFs](https://neuralradiancefields.io/history-of-neural-radiance-fields/)\n- waiting on capture\n- use polycam\n\t- try the BTS cam?\n- [viewier](https://github.com/sxyu/volrend)\n- Windows NeRF environment to WebGL\n- [install windows NeRF](https://github.com/bycloudai/instant-ngp-Windows)\n- check out mip nerf 360s\n\t- [Record3D](https://github.com/marek-simonik/record3d_unity_streaming)\n- [github of links](https://github.com/yenchenlin/awesome-NeRF)\n- [nerfs with polycam](https://www.linkedin.com/posts/robcsloan_nerfstudio-nerfstudio-polycam-activity-6999169160379297792-SN4F?utm_source=share&utm_medium=member_desktop)\n- [Polycam developer mode instructions](https://docs.nerf.studio/en/latest/quickstart/custom_dataset.html#polycam-capture)\n- [Nerf to animated people oneshot](https://elicit3d.github.io/)\n- [4K ultra high res nerfs with code](https://paperswithcode.com/paper/4k-nerf-high-fidelity-neural-radiance-fields)\n- [code](https://github.com/frozoul/4K-NeRF)\n- [city modelling](https://www.reddit.com/r/deeplearning/comments/zowgqn/neural_rendering_reconstruct_your_city_in_3d/)\n- [more city modelling](https://waymo.com/research/block-nerf/)\n- [field guide](https://github.com/3a1b2c3/seeingSpace/wiki/Hands-on:-Getting-started-and-Nerf-frameworks)\n- [NeRF SLAM](https://github.com/ToniRV/NeRF-SLAM)\n- [NeuralUDF surface capture](https://www.xxlong.site/NeuralUDF/)\n- [stablisation paper](https://arxiv.org/abs/2102.06205)\n- [nerfs without neural nets](https://alexyu.net/plenoxels/)\n- [NeuS2: Fast Learning of Neural Implicit Surfaces\n  for Multi-view Reconstruction](https://vcai.mpi-inf.mpg.de/projects/NeuS2/)\n- [Original 2020 nerf paper](https://www.matthewtancik.com/nerf)\n- [Recolour NeRF](https://sites.google.com/view/recolornerf?pli=1)\n- [Volinga Nerf into Unreal](https://volinga.ai/)\n- [Text2Nerf4D](https://make-a-video3d.github.io/)\n- [Robust nerfs which deal with occlusion](https://robustnerf.github.io/public/)\n- [Blender integration](https://github.com/JamesPerlman/NeRFRenderCore/blob/main/src/integrations/blender.cuh)\n- [Rapidnerf VR integration with erase](https://github.com/NVlabs/instant-ngp#vr-controls)\n- [Nerf to large scale geom](https://bakedsdf.github.io/)\n- [ELICIT,ELICIT creates free-viewpoint motion videos from a single image by constructing an animatable NeRF representation in one-shot learning. Offcial website of 'One-shot Implicit Animatable Avatars with Model-based Priors'](https://elicit3d.github.io/)\n- [GitHub frozoul/4K-NeRF: Official implementation of arxiv paper   4K-NeRF: High Fidelity Neural Radiance Fields at Ultra High Resolutions   , Official implementation of arxiv paper   4K-NeRF: High Fidelity Neural Radiance Fields at Ultra High Resolutions   - GitHub frozoul/4K-NeRF: Official implementation of arxiv paper   4K-NeRF: High Fidelity Neural Radiance Fields at Ultra High Resolutions](https://github.com/frozoul/4k-nerf)\n- [ClimateNeRF,-](https://climatenerf.github.io/)\n- [GitHub ToniRV/NeRF-SLAM: NeRF-SLAM: Real-Time Dense Monocular SLAM with Neural Radiance Fields.](https://github.com/tonirv/nerf-slam)\n- [HumanNeRF: Free-viewpoint Rendering of Moving People from Monocular Video,HumanNeRF: Free-viewpoint Rendering of Moving People from Monocular Video](https://grail.cs.washington.edu/projects/humannerf/)\n- [editing nerfs with instructions](https://instruct-nerf2nerf.github.io/)\n- [instruct2nerf twitter thread](https://mobile.twitter.com/bilawalsidhu/status/1638919452392583169)\n- [Render without cuda using just pytorch](https://github.com/taichi-dev/taichi-nerfs)\n- [Nerf with free camera trajectory](https://totoro97.github.io/projects/f2-nerf/)\n- [Language embedded nerfs (LERFS)](https://www.lerf.io/)\n- [Splatting paper, go where you like](https://repo-sam.inria.fr/fungraph/3d-gaussian-splatting/)\n- [nerf RPN](https://github.com/lyclyc52/NeRF_RPN)\n- [google indoor reconstruction from nerfs](https://ai.googleblog.com/2023/06/reconstructing-indoor-spaces-with-nerf.html)\n- [focal length for capture](https://neuralradiancefields.io/whats-the-best-focal-length-to-take-a-nerf/)\n- The paper [Zip-NeRF](https://jonbarron.info/zipnerf/): Anti-Aliased Grid-Based Neural Radiance Fields\" proposes a technique that combines ideas from rendering and signal processing to combat aliasing in grid-based representations of neural radiance fields (NeRF). NeRF's learned mapping from spatial coordinates to colors and volumetric density can be accelerated through the use of grid-based representations, but they lack an explicit understanding of scale and often introduce aliasing. The proposed technique combines mip-NeRF 360 and Instant NGP to yield error rates that are 8%-77% lower than either prior technique and trains 24x faster than mip-NeRF 360. The technique uses multisampling to approximate the average NGP feature over a conical frustum, and the method produces prefiltered renderings that do not flicker or shimmer, even as the camera moves laterally. Moreover, their improvements to proposal network supervision result in a prefiltered proposal output that preserves the foreground object for all frames, preventing an artifact called z-aliasing where foreground content alternately appears and disappears as the camera moves towards or away from the scene content. The proposed method shows promising results for accelerating NeRF training while combating aliasing in grid-based representations.\n- [baked nerf mesh paper](https://bakedsdf.github.io/)\n- [Facebook VR nerf](https://neuralradiancefields.io/venturing-beyond-reality-vr-nerf/)\n- RP-Lidar + Raspberry pi + ROS RTAB-MAP\n- [RTAB-Map](http://introlab.github.io/rtabmap/)\n- [Reality Scan](https://www.unrealengine.com/en-US/blog/realityscan-is-now-free-to-download-on-ios)\n- [Drone SLAM](https://www.youtube.com/watch?v=CEC5UwPV9gY)\n- [Adobe substance3d](https://www.substance3d.com/)\n- [3DPresso](https://3dpresso.ai/viewer?seq=mr3.yg5isic8KGJZ1DAjW5VMc)\n- [Apple point cloud rendering](https://machinelearning.apple.com/research/pointersect)\n- [Nvidia NeuralAngelo](https://research.nvidia.com/labs/dir/neuralangelo/)\n- [OmniMotion track all pixels](https://huggingface.co/papers/2306.05422)\n- [Leica handheld scanner](https://leica-geosystems.com/products/laser-scanners/autonomous-reality-capture/leica-blk2go-handheld-imaging-laser-scanner)\n- [Meshroom open source photogrammetry](https://alicevision.org/#meshroom)\n- [Nira.app](https://nira.app/)\n- [Houdini mesh from google earth](https://github.com/xjorma/EarthMeshHoudini?)\n- [DiffusionLight: Light Probes for Free by Painting a Chrome Ball](https://diffusionlight.github.io/index.html)\n- [MocapEvery (jiyewise.github.io)](https://jiyewise.github.io/projects/MocapEvery/)\n\n- # LIDAR\n\t- [vectr-ucla/direct_lidar_inertial_odometry: [IEEE ICRA'23] A new lightweight LiDAR-inertial odometry algorithm with a novel coarse-to-fine approach in constructing continuous-time trajectories for precise motion correction. (github.com)](https://github.com/vectr-ucla/direct_lidar_inertial_odometry)\n-\n- [Nerfs](https://www.matthewtancik.com/nerf)\n- All of the LIDAR, [[Gaussian splatting and Similar]], [[Gaussian splatting and Similar]] etc are hopefully going to end up in here\n- [History of NeRFs](https://neuralradiancefields.io/history-of-neural-radiance-fields/)\n- waiting on capture\n- use polycam\n\t- try the BTS cam?\n- [viewier](https://github.com/sxyu/volrend)\n- Windows NeRF environment to WebGL\n- [install windows NeRF](https://github.com/bycloudai/instant-ngp-Windows)\n- check out mip nerf 360s\n\t- [Record3D](https://github.com/marek-simonik/record3d_unity_streaming)\n- [github of links](https://github.com/yenchenlin/awesome-NeRF)\n- [nerfs with polycam](https://www.linkedin.com/posts/robcsloan_nerfstudio-nerfstudio-polycam-activity-6999169160379297792-SN4F?utm_source=share&utm_medium=member_desktop)\n- [Polycam developer mode instructions](https://docs.nerf.studio/en/latest/quickstart/custom_dataset.html#polycam-capture)\n- [Nerf to animated people oneshot](https://elicit3d.github.io/)\n- [4K ultra high res nerfs with code](https://paperswithcode.com/paper/4k-nerf-high-fidelity-neural-radiance-fields)\n- [code](https://github.com/frozoul/4K-NeRF)\n- [city modelling](https://www.reddit.com/r/deeplearning/comments/zowgqn/neural_rendering_reconstruct_your_city_in_3d/)\n- [more city modelling](https://waymo.com/research/block-nerf/)\n- [field guide](https://github.com/3a1b2c3/seeingSpace/wiki/Hands-on:-Getting-started-and-Nerf-frameworks)\n- [NeRF SLAM](https://github.com/ToniRV/NeRF-SLAM)\n- [NeuralUDF surface capture](https://www.xxlong.site/NeuralUDF/)\n- [stablisation paper](https://arxiv.org/abs/2102.06205)\n- [nerfs without neural nets](https://alexyu.net/plenoxels/)\n- [NeuS2: Fast Learning of Neural Implicit Surfaces\n  for Multi-view Reconstruction](https://vcai.mpi-inf.mpg.de/projects/NeuS2/)\n- [Original 2020 nerf paper](https://www.matthewtancik.com/nerf)\n- [Recolour NeRF](https://sites.google.com/view/recolornerf?pli=1)\n- [Volinga Nerf into Unreal](https://volinga.ai/)\n- [Text2Nerf4D](https://make-a-video3d.github.io/)\n- [Robust nerfs which deal with occlusion](https://robustnerf.github.io/public/)\n- [Blender integration](https://github.com/JamesPerlman/NeRFRenderCore/blob/main/src/integrations/blender.cuh)\n- [Rapidnerf VR integration with erase](https://github.com/NVlabs/instant-ngp#vr-controls)\n- [Nerf to large scale geom](https://bakedsdf.github.io/)\n- [ELICIT,ELICIT creates free-viewpoint motion videos from a single image by constructing an animatable NeRF representation in one-shot learning. Offcial website of 'One-shot Implicit Animatable Avatars with Model-based Priors'](https://elicit3d.github.io/)\n- [GitHub frozoul/4K-NeRF: Official implementation of arxiv paper   4K-NeRF: High Fidelity Neural Radiance Fields at Ultra High Resolutions   , Official implementation of arxiv paper   4K-NeRF: High Fidelity Neural Radiance Fields at Ultra High Resolutions   - GitHub frozoul/4K-NeRF: Official implementation of arxiv paper   4K-NeRF: High Fidelity Neural Radiance Fields at Ultra High Resolutions](https://github.com/frozoul/4k-nerf)\n- [ClimateNeRF,-](https://climatenerf.github.io/)\n- [GitHub ToniRV/NeRF-SLAM: NeRF-SLAM: Real-Time Dense Monocular SLAM with Neural Radiance Fields.](https://github.com/tonirv/nerf-slam)\n- [HumanNeRF: Free-viewpoint Rendering of Moving People from Monocular Video,HumanNeRF: Free-viewpoint Rendering of Moving People from Monocular Video](https://grail.cs.washington.edu/projects/humannerf/)\n- [editing nerfs with instructions](https://instruct-nerf2nerf.github.io/)\n- [instruct2nerf twitter thread](https://mobile.twitter.com/bilawalsidhu/status/1638919452392583169)\n- [Render without cuda using just pytorch](https://github.com/taichi-dev/taichi-nerfs)\n- [Nerf with free camera trajectory](https://totoro97.github.io/projects/f2-nerf/)\n- [Language embedded nerfs (LERFS)](https://www.lerf.io/)\n- [Splatting paper, go where you like](https://repo-sam.inria.fr/fungraph/3d-gaussian-splatting/)\n- [nerf RPN](https://github.com/lyclyc52/NeRF_RPN)\n- [google indoor reconstruction from nerfs](https://ai.googleblog.com/2023/06/reconstructing-indoor-spaces-with-nerf.html)\n- [focal length for capture](https://neuralradiancefields.io/whats-the-best-focal-length-to-take-a-nerf/)\n- The paper [Zip-NeRF](https://jonbarron.info/zipnerf/): Anti-Aliased Grid-Based Neural Radiance Fields\" proposes a technique that combines ideas from rendering and signal processing to combat aliasing in grid-based representations of neural radiance fields (NeRF). NeRF's learned mapping from spatial coordinates to colors and volumetric density can be accelerated through the use of grid-based representations, but they lack an explicit understanding of scale and often introduce aliasing. The proposed technique combines mip-NeRF 360 and Instant NGP to yield error rates that are 8%-77% lower than either prior technique and trains 24x faster than mip-NeRF 360. The technique uses multisampling to approximate the average NGP feature over a conical frustum, and the method produces prefiltered renderings that do not flicker or shimmer, even as the camera moves laterally. Moreover, their improvements to proposal network supervision result in a prefiltered proposal output that preserves the foreground object for all frames, preventing an artifact called z-aliasing where foreground content alternately appears and disappears as the camera moves towards or away from the scene content. The proposed method shows promising results for accelerating NeRF training while combating aliasing in grid-based representations.\n- [baked nerf mesh paper](https://bakedsdf.github.io/)\n- [Facebook VR nerf](https://neuralradiancefields.io/venturing-beyond-reality-vr-nerf/)\n- RP-Lidar + Raspberry pi + ROS RTAB-MAP\n- [RTAB-Map](http://introlab.github.io/rtabmap/)\n- [Reality Scan](https://www.unrealengine.com/en-US/blog/realityscan-is-now-free-to-download-on-ios)\n- [Drone SLAM](https://www.youtube.com/watch?v=CEC5UwPV9gY)\n- [Adobe substance3d](https://www.substance3d.com/)\n- [3DPresso](https://3dpresso.ai/viewer?seq=mr3.yg5isic8KGJZ1DAjW5VMc)\n- [Apple point cloud rendering](https://machinelearning.apple.com/research/pointersect)\n- [Nvidia NeuralAngelo](https://research.nvidia.com/labs/dir/neuralangelo/)\n- [OmniMotion track all pixels](https://huggingface.co/papers/2306.05422)\n- [Leica handheld scanner](https://leica-geosystems.com/products/laser-scanners/autonomous-reality-capture/leica-blk2go-handheld-imaging-laser-scanner)\n- [Meshroom open source photogrammetry](https://alicevision.org/#meshroom)\n- [Nira.app](https://nira.app/)\n- [Houdini mesh from google earth](https://github.com/xjorma/EarthMeshHoudini?)\n- [DiffusionLight: Light Probes for Free by Painting a Chrome Ball](https://diffusionlight.github.io/index.html)\n- [MocapEvery (jiyewise.github.io)](https://jiyewise.github.io/projects/MocapEvery/)\n\n\n## Current Landscape (2025)\n\n- Industry adoption and implementations\n  - Metaverse platforms continue to evolve with focus on interoperability and open standards\n  - Web3 integration accelerating with decentralised identity and asset ownership\n  - Enterprise adoption growing in virtual collaboration, training, and digital twins\n  - UK companies increasingly active in metaverse development and immersive technologies\n\n- Technical capabilities\n  - Real-time rendering at photorealistic quality levels\n  - Low-latency networking enabling seamless multi-user experiences\n  - AI-driven content generation and procedural world building\n  - Spatial audio and haptics enhancing immersion\n\n- UK and North England context\n  - Manchester: Digital Innovation Factory supports metaverse startups and research\n  - Leeds: Holovis leads in immersive experiences for entertainment and training\n  - Newcastle: University research in spatial computing and interactive systems\n  - Sheffield: Advanced manufacturing using digital twin technology\n\n- Standards and frameworks\n  - Metaverse Standards Forum driving interoperability protocols\n  - WebXR enabling browser-based immersive experiences\n  - glTF and USD for 3D asset interchange\n  - Open Metaverse Interoperability Group defining cross-platform standards\n\n## Metadata\n\n- **Last Updated**: 2025-11-16\n- **Review Status**: Automated remediation with 2025 context\n- **Verification**: Academic sources verified\n- **Regional Context**: UK/North England where applicable",
  "properties": {
    "id": "rb-0069-lidar-ontology",
    "collapsed": "true",
    "- ontology": "true",
    "- term-id": "RB-0069",
    "- preferred-term": "rb 0069 lidar",
    "- source-domain": "robotics",
    "- status": "draft",
    "- public-access": "true",
    "- definition": "### Primary Definition",
    "- maturity": "draft",
    "- owl:class": "mv:rb0069lidar",
    "- owl:physicality": "ConceptualEntity",
    "- owl:role": "Concept",
    "- belongsToDomain": "[[MetaverseDomain]]"
  },
  "backlinks": [],
  "wiki_links": [
    "MetaverseDomain",
    "Gaussian splatting and Similar"
  ],
  "ontology": {
    "term_id": "RB-0069",
    "preferred_term": "rb 0069 lidar",
    "definition": "### Primary Definition",
    "source_domain": "robotics",
    "maturity_level": null,
    "authority_score": null
  }
}