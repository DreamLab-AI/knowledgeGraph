{
  "title": "Proprietary Video",
  "content": "- ### OntologyBlock\n  id:: proprietary-video-ontology\n  collapsed:: true\n\t- ontology:: true\n\t- term-id:: mv-192862651986\n\t- preferred-term:: Proprietary Video\n\t- source-domain:: metaverse\n\t- status:: active\n\t- public-access:: true\n\t- definition:: A component of the metaverse ecosystem focusing on proprietary video.\n\t- maturity:: mature\n\t- authority-score:: 0.85\n\t- owl:class:: mv:ProprietaryVideo\n\t- owl:physicality:: ConceptualEntity\n\t- owl:role:: Concept\n\t- belongsToDomain:: [[MetaverseDomain]]\n\t- #### Relationships\n\t  id:: proprietary-video-relationships\n\t\t- enables:: [[ImmersiveExperience]], [[Presence]], [[SpatialComputing]]\n\t\t- requires:: [[DisplayTechnology]], [[TrackingSystem]], [[RenderingEngine]]\n\t\t- bridges-to:: [[HumanComputerInteraction]], [[ComputerVision]], [[Robotics]]\n\n\t- #### OWL Axioms\n\t  id:: proprietary-video-owl-axioms\n\t  collapsed:: true\n\t\t- ```clojure\n\t\t  Declaration(Class(mv:ProprietaryVideo))\n\n\t\t  # Classification\n\t\t  SubClassOf(mv:ProprietaryVideo mv:ConceptualEntity)\n\t\t  SubClassOf(mv:ProprietaryVideo mv:Concept)\n\n\t\t  # Domain classification\n\t\t  SubClassOf(mv:ProprietaryVideo\n\t\t    ObjectSomeValuesFrom(mv:belongsToDomain mv:MetaverseDomain)\n\t\t  )\n\n\t\t  # Annotations\n\t\t  AnnotationAssertion(rdfs:label mv:ProprietaryVideo \"Proprietary Video\"@en)\n\t\t  AnnotationAssertion(rdfs:comment mv:ProprietaryVideo \"A component of the metaverse ecosystem focusing on proprietary video.\"@en)\n\t\t  AnnotationAssertion(dcterms:identifier mv:ProprietaryVideo \"mv-192862651986\"^^xsd:string)\n\t\t  ```\n\npublic:: true\n\n\t- #Public page automatically published\n- Seems that everyone expects this to break through this year.\n- [Justine Moore from A16Z](https://www.linkedin.com/in/justinemoore94/) has compiled 2023 on Twitter.\n\t- {{tweet (https://twitter.com/venturetwins/status/1737526316948496651}}\n\t- [AI Video 2023 Google Sheets](https://docs.google.com/spreadsheets/d/1FZqG6ESvUzfkZB7AOW5YE9pOYOUfjzw7h6XLREvOJo8/edit?pli=1#gid=514547156)\n- ![image.png](../assets/image_1704284304612_0.png)\n- [AI Video papers from the majors](https://docs.google.com/spreadsheets/d/1FZqG6ESvUzfkZB7AOW5YE9pOYOUfjzw7h6XLREvOJo8/edit?pli=1#gid=0)\n- ![photo_2024-01-03_12-19-29.jpg](../assets/photo_2024-01-03_12-19-29_1704284666431_0.jpg)\n- ## Closed Source Video id:: 659a922a-1d6b-4ae8-82ad-8d7c2814f25f\n\t- ### Pika Labs\n\t\t- **Current leader**:\n\t\t  {{tweet https://twitter.com/martial_artwork/status/1742138390517014918}}\n\t\t- **Prompt Creativity & Flexibility**: Excels in this area, enabling users to directly influence the animation with their prompts.\n\t\t- **Human Motion Animation**: Attempts adventurous animations but may result in distortions.\n\t\t- **Camera Motion Options**: Offers accurate, straightforward camera motions but lacks the dynamic range of Runway ML.\n\t\t- **Pros**: Free version (recently reduced quality), supports multiple aspect ratios, provides tutorials for prompt writing.\n\t\t- **Cons**: Creations are visible to other users, potential for idea theft, and traffic issues on Discord server‚Äã[](https://www.toolify.ai/ai-news/revolutionary-ai-animation-tools-pika-vs-runway-78636)‚Äã‚Äã[](https://dragganaitool.uk/pika-labs-vs-runwayml-gen2/)‚Äã. Expensive to use through [Pika Art website](https://pika.art/login) $60pcm,\n\t- ### Runway ML\n\t\t- [twitter link to the render loading below](https://twitter.com/bennash/status/1746188870679400543)\n\t\t  {{twitter https://twitter.com/bennash/status/1746188870679400543}}\n\t\t- **Basic Animation**: Offers cinematic camera movements and more convincing human motion, but faces issues with brightness and image integrity.\n\t\t- **Prompt Creativity & Flexibility**: Less flexible in prompt creativity, occasionally disregarding user prompts.\n\t\t- **Human Motion Animation**: Produces high-quality animations but sometimes distorts the original image.\n\t\t- **Camera Motion Options**: Provides dynamic camera shots, including zooming, panning, and rotating, but may lead to distortion.\n\t\t- **Pros**: Web-based platform ensuring privacy, offers 120 free credits, advanced features, and the option to extend video length.\n\t\t- **Cons**: Limited to 16:9 aspect ratio, may not be as flexible as Pika Labs in prompt generation‚Äã[](https://www.toolify.ai/ai-news/revolutionary-ai-animation-tools-pika-vs-runway-78636)‚Äã‚Äã[](https://dragganaitool.uk/pika-labs-vs-runwayml-gen2/)‚Äã.\n\t- ### Mid Journey have said:\n\t\t- **Midjourney Video** \"will not be like any other AI video products that are currently available out there and will be 10X better.\"\n\t\t\t- **David Holz**: ‚Äú*MidJourney video may not be consistently making what you want, but the quality will be consistently good by default.*‚Äù\n\t\t- **Video Training**: The Midjourney team will start to train the video/animation model, which will come before 3D.\n\t\t\t- Already have all the data needed to train the model.\n\t\t- **3D:** needs more data to train, so it‚Äôs a bit slower than expected.\n\t\t- {{video https://youtu.be/LY3B0d623wA}}\n\t- ### [VideoPoet ‚Äì Google Research](https://sites.research.google/videopoet/)\n\t\t- **Overview:** Google's text to video, linked to Bard, but not yet available.\n\t- ### [HeyGen](https://www.heygen.com/) for video avatars\n\t\t- **Overview**: HeyGen emphasizes security and ethics in its AI video platform, being SOC 2 compliant and focusing on data protection.\n\t\t- **Notable Features**: Known for its user-friendly interface and effectiveness in creating short, engaging videos useful for various departments like HR and training.\n\t\t- **Target Audience**: Targets SMEs, offering a range of applications from casual to professional use‚Äã‚Äã.\n\t- ## Virtual production\n\t\t- ### Simulon (Virtual Production)\n\t\t\t- **Cloud rendered magic**: Still early, and I'm not QUITE sure how it works.\n\t\t\t- {{tweet https://twitter.com/diveshnaidoo/status/1735006300386336919}}\n\t\t\t- [Automotive example](https://www.linkedin.com/posts/divesh-naidoo-48809934_vfx-cgi-virtualproduction-activity-7186786217445711875-7ByY?)\n\t\t\t- https://www.instagram.com/reel/C6fQz81oDMS/\n\t\t\t-\n\t\t- ### My flossverse stuff from 2022\n\t\t\t- {{tweet https://twitter.com/flossverse/status/1629601804521537537}}\n\t\t- ### Skyglass\n\t\t\t- Straight up virtual production on iPhone\n\t\t\t- {{tweet https://twitter.com/skyglassapp/status/1712599252575412474}}\n\t\t- ### Adobe integrates everything to Premier\n\t\t\t- {{video https://www.youtube.com/watch?v=6de4akFiNYM&t=1s}}\n\t\t- ### Other Notable Research\n\t\t\t- ByteDance [MagicVideo-V2: Multi-Stage High-Aesthetic Video Generation (magicvideov2.github.io)](https://magicvideov2.github.io/)\n\t- ## What's next: 3D world creation\n\t  id:: 659a9247-f51f-4b45-9673-df22ec0476dd\n\t\t- Again, midjourney are working on a model. - üü¢ Best I can find is [Sudo AII](https://www.sudo.ai/)\n\t\t- <iframe src=\"https://www.sudo.ai\" style=\"width: 100%; height: 600px\"></iframe>\n\t\t- <iframe src=\"https://yueyang1996.github.io/holodeck/\" style=\"width: 100%; height: 600px\"></iframe>\n\t\t- https://research.nvidia.com/labs/toronto-ai/AlignYourGaussians/\n\t\t- [Mosaic-SDF for 3D Generative Models (connectedpapers.com)](https://www.connectedpapers.com/main/a7d6d07fdb631ce263ec2ddad72df269587fd3c9/Mosaic%20SDF-for-3D-Generative-Models/graph)\n\t\t- https://lioryariv.github.io/msdf/\n\t\t- ## Voice to CAD like Tony Stark is [obviously coming](https://www.linkedin.com/posts/bengeskin_i-feel-like-tony-stark-and-doctor-strange-activity-7152044309213519872-YUnm/?)\n\t\t\t- ![xrCAD.mp4](../assets/xrCAD_1705345928224_0.mp4){:height 44, :width 66}\n\t\t\t- # [[Metaverse and Telecollaboration]]\n\t\t\t- üü¢ I could go on all day about this, goods and bads. I literally wrote a book on it.\n\t\t\t- üü¢ A lot (for me) hinges on [[OpenUSD]] the universal scene language. It's been SO long since we have had something useful.\n\t\t\t- Nvidia have a text to 3D pipeline for [[Omniverse]]. Will be interesting to see what the use cases are. This is their new Cesium [geo tile integration](https://cesium.com/blog/2024/01/16/now-available-[[NVIDIA Omniverse]]-aeco-demo-pack/) giving global instant models.\n\t\t\t-\n\t\t\t- ![1705423306024.mp4](../assets/1705423306024_1705437842029_0.mp4)\n\t-\n\t-\n\t- {{tweet https://twitter.com/BlockadeLabs/status/1719818562917761094}}\n- This is a [[presentation]] slide and the next slide is [[Open Generative AI tools]]\n\n## Current Landscape (2025)\n\n- Industry adoption and implementations\n  - Metaverse platforms continue to evolve with focus on interoperability and open standards\n  - Web3 integration accelerating with decentralised identity and asset ownership\n  - Enterprise adoption growing in virtual collaboration, training, and digital twins\n  - UK companies increasingly active in metaverse development and immersive technologies\n\n- Technical capabilities\n  - Real-time rendering at photorealistic quality levels\n  - Low-latency networking enabling seamless multi-user experiences\n  - AI-driven content generation and procedural world building\n  - Spatial audio and haptics enhancing immersion\n\n- UK and North England context\n  - Manchester: Digital Innovation Factory supports metaverse startups and research\n  - Leeds: Holovis leads in immersive experiences for entertainment and training\n  - Newcastle: University research in spatial computing and interactive systems\n  - Sheffield: Advanced manufacturing using digital twin technology\n\n- Standards and frameworks\n  - Metaverse Standards Forum driving interoperability protocols\n  - WebXR enabling browser-based immersive experiences\n  - glTF and USD for 3D asset interchange\n  - Open Metaverse Interoperability Group defining cross-platform standards\n\n## Metadata\n\n- **Last Updated**: 2025-11-16\n- **Review Status**: Automated remediation with 2025 context\n- **Verification**: Academic sources verified\n- **Regional Context**: UK/North England where applicable",
  "properties": {
    "id": "659a9247-f51f-4b45-9673-df22ec0476dd",
    "collapsed": "true",
    "- ontology": "true",
    "- term-id": "mv-192862651986",
    "- preferred-term": "Proprietary Video",
    "- source-domain": "metaverse",
    "- status": "active",
    "- public-access": "true",
    "- definition": "A component of the metaverse ecosystem focusing on proprietary video.",
    "- maturity": "mature",
    "- authority-score": "0.85",
    "- owl:class": "mv:ProprietaryVideo",
    "- owl:physicality": "ConceptualEntity",
    "- owl:role": "Concept",
    "- belongsToDomain": "[[MetaverseDomain]]",
    "- enables": "[[ImmersiveExperience]], [[Presence]], [[SpatialComputing]]",
    "- requires": "[[DisplayTechnology]], [[TrackingSystem]], [[RenderingEngine]]",
    "- bridges-to": "[[HumanComputerInteraction]], [[ComputerVision]], [[Robotics]]",
    "public": "true",
    "- ## Closed Source Video id": "659a922a-1d6b-4ae8-82ad-8d7c2814f25f"
  },
  "backlinks": [
    "Suggested Reading Order",
    "Proprietary Image Generation"
  ],
  "wiki_links": [
    "Omniverse",
    "Metaverse and Telecollaboration",
    "presentation",
    "ImmersiveExperience",
    "TrackingSystem",
    "Presence",
    "Robotics",
    "SpatialComputing",
    "Open Generative AI tools",
    "DisplayTechnology",
    "MetaverseDomain",
    "ComputerVision",
    "OpenUSD",
    "HumanComputerInteraction",
    "NVIDIA Omniverse",
    "RenderingEngine"
  ],
  "ontology": {
    "term_id": "mv-192862651986",
    "preferred_term": "Proprietary Video",
    "definition": "A component of the metaverse ecosystem focusing on proprietary video.",
    "source_domain": "metaverse",
    "maturity_level": null,
    "authority_score": 0.85
  }
}