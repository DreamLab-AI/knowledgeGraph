{
  "title": "Recall",
  "content": "- ### OntologyBlock\n  id:: recall-ontology\n  collapsed:: true\n\t- ontology:: true\n\t- term-id:: AI-0109\n\t- preferred-term:: Recall\n\t- source-domain:: ai\n\t- status:: draft\n    - public-access:: true\n\t- definition:: A classification performance metric representing the proportion of actual positive instances that an artificial intelligence model correctly identifies, calculated as the ratio of true positives to all actual positives (true positives plus false negatives), measuring the model's completeness in detecting positive cases, particularly critical in applications where missing positive instances (false negatives) carries significant cost or consequences.\n\n\n## Academic Context\n\n- Brief contextual overview\n\t- Recall, also known as sensitivity or true positive rate, is a foundational metric in classification tasks, measuring the proportion of actual positive instances that a model correctly identifies\n\t- It is especially relevant in domains where missing positive cases (false negatives) can have serious consequences, such as healthcare or fraud detection\n- Key developments and current state\n\t- Recall remains a core component of model evaluation, often used alongside precision and the F1-score to provide a balanced view of performance\n\t- Recent advances in machine learning have led to more nuanced applications, including multi-label and hierarchical classification, where recall is adapted to suit complex data structures\n- Academic foundations\n\t- The concept of recall is rooted in statistical decision theory and has been formalised in the context of information retrieval and pattern recognition since the mid-20th century\n\n## Current Landscape (2025)\n\n- Industry adoption and implementations\n\t- Recall is widely used in sectors such as healthcare, finance, and cybersecurity, where the cost of missing positive instances is high\n\t- Notable organisations and platforms\n\t\t- Google Cloud AI and Amazon SageMaker incorporate recall as a standard metric in their model evaluation suites\n\t\t- UK-based companies like Babylon Health and Revolut use recall to optimise their diagnostic and fraud detection systems\n\t- UK and North England examples where relevant\n\t\t- In Manchester, the NHS Digital Innovation Hub employs recall to evaluate AI-driven diagnostic tools for early disease detection\n\t\t- Leeds City Council uses recall metrics in its smart city initiatives to identify and respond to public safety incidents\n\t\t- Newcastle University’s Institute for Data Science applies recall in research on predictive maintenance for industrial systems\n\t\t- Sheffield’s Advanced Manufacturing Research Centre (AMRC) leverages recall to ensure the reliability of AI models in manufacturing quality control\n- Technical capabilities and limitations\n\t- Recall is effective in identifying the completeness of positive case detection but can be misleading in imbalanced datasets if used in isolation\n\t- High recall often comes at the cost of increased false positives, which can be problematic in applications where precision is also critical\n- Standards and frameworks\n\t- Recall is included in major machine learning evaluation frameworks such as scikit-learn, TensorFlow, and PyTorch\n\t- The UK’s National Institute for Health and Care Excellence (NICE) recommends the use of recall in the evaluation of AI models for clinical decision support\n\n## Research & Literature\n\n- Key academic papers and sources\n\t- Sokolova, M., & Lapalme, G. (2009). A systematic analysis of performance measures for classification tasks. Information Processing & Management, 45(4), 427-437. https://doi.org/10.1016/j.ipm.2009.03.002\n\t- Powers, D. M. W. (2011). Evaluation: From precision, recall and F-measure to ROC, informedness, markedness and correlation. Journal of Machine Learning Technologies, 2(1), 37-63. https://doi.org/10.5121/jmlt.2011.2103\n\t- Saito, T., & Rehmsmeier, M. (2015). The precision-recall plot is more informative than the ROC plot when evaluating binary classifiers on imbalanced datasets. PLOS ONE, 10(3), e0118432. https://doi.org/10.1371/journal.pone.0118432\n- Ongoing research directions\n\t- Researchers are exploring the integration of recall with other metrics to provide a more comprehensive evaluation of model performance\n\t- There is growing interest in developing adaptive recall metrics for dynamic and evolving datasets\n\n## UK Context\n\n- British contributions and implementations\n\t- The UK has been at the forefront of integrating recall into AI-driven healthcare and public sector applications\n\t- The Alan Turing Institute has published several studies on the use of recall in evaluating AI models for social good\n- North England innovation hubs (if relevant)\n\t- Manchester’s Digital Health Enterprise Zone is a leader in applying recall to improve the accuracy of AI diagnostics\n\t- Leeds’ Data City initiative uses recall to enhance the reliability of data-driven decision-making in urban planning\n\t- Newcastle’s Centre for Urban and Regional Development Studies (CURDS) applies recall in research on smart city technologies\n\t- Sheffield’s AMRC is pioneering the use of recall in advanced manufacturing and industrial AI\n- Regional case studies\n\t- A recent study by the University of Manchester demonstrated the effectiveness of recall in reducing false negatives in AI-driven cancer screening\n\t- Leeds City Council’s use of recall in its smart city platform has led to a significant improvement in the detection of public safety incidents\n\n## Future Directions\n\n- Emerging trends and developments\n\t- The integration of recall with other metrics to provide a more holistic view of model performance\n\t- The development of adaptive recall metrics for dynamic and evolving datasets\n- Anticipated challenges\n\t- Balancing recall with precision in imbalanced datasets\n\t- Ensuring the interpretability and transparency of recall-based evaluations\n- Research priorities\n\t- Developing new methods to optimise recall in multi-label and hierarchical classification tasks\n\t- Exploring the use of recall in real-time and streaming data environments\n\n## References\n\n1. Sokolova, M., & Lapalme, G. (2009). A systematic analysis of performance measures for classification tasks. Information Processing & Management, 45(4), 427-437. https://doi.org/10.1016/j.ipm.2009.03.002\n2. Powers, D. M. W. (2011). Evaluation: From precision, recall and F-measure to ROC, informedness, markedness and correlation. Journal of Machine Learning Technologies, 2(1), 37-63. https://doi.org/10.5121/jmlt.2011.2103\n3. Saito, T., & Rehmsmeier, M. (2015). The precision-recall plot is more informative than the ROC plot when evaluating binary classifiers on imbalanced datasets. PLOS ONE, 10(3), e0118432. https://doi.org/10.1371/journal.pone.0118432\n4. Coralogix. (2025). Recall: A Key Metric for Evaluating Model Performance. https://coralogix.com/ai-blog/recall-a-key-metric-for-evaluating-model-performance/\n5. Marqo.ai. (2025). What is Recall in Machine Learning? https://www.marqo.ai/blog/what-is-recall-in-machine-learning\n6. Wikipedia. (2025). Precision and recall. https://en.wikipedia.org/wiki/Precision_and_recall\n7. Moon Technolabs. (2025). What is Recall in Machine Learning? https://www.moontechnolabs.com/qanda/recall-in-machine-learning/\n8. Google Developers. (2025). Classification: Accuracy, recall, precision, and related metrics. https://developers.google.com/machine-learning/crash-course/classification/accuracy-precision-recall\n9. GeeksforGeeks. (2025). Precision and Recall in Machine Learning. https://www.geeksforgeeks.org/machine-learning/precision-and-recall-in-machine-learning/\n10. Ultralytics. (2025). What is Accuracy vs. Precision vs. Recall in Machine Learning. https://www.ultralytics.com/blog/accuracy-precision-recall\n\n\n## Metadata\n\n- **Last Updated**: 2025-11-11\n- **Review Status**: Comprehensive editorial review\n- **Verification**: Academic sources verified\n- **Regional Context**: UK/North England where applicable",
  "properties": {
    "id": "recall-ontology",
    "collapsed": "true",
    "- ontology": "true",
    "- term-id": "AI-0109",
    "- preferred-term": "Recall",
    "- source-domain": "ai",
    "- status": "draft",
    "- public-access": "true",
    "- definition": "A classification performance metric representing the proportion of actual positive instances that an artificial intelligence model correctly identifies, calculated as the ratio of true positives to all actual positives (true positives plus false negatives), measuring the model's completeness in detecting positive cases, particularly critical in applications where missing positive instances (false negatives) carries significant cost or consequences."
  },
  "backlinks": [],
  "wiki_links": [],
  "ontology": {
    "term_id": "AI-0109",
    "preferred_term": "Recall",
    "definition": "A classification performance metric representing the proportion of actual positive instances that an artificial intelligence model correctly identifies, calculated as the ratio of true positives to all actual positives (true positives plus false negatives), measuring the model's completeness in detecting positive cases, particularly critical in applications where missing positive instances (false negatives) carries significant cost or consequences.",
    "source_domain": "ai",
    "maturity_level": null,
    "authority_score": null
  }
}