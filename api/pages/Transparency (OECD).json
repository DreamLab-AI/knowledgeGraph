{
  "title": "Transparency (OECD)",
  "content": "- ### OntologyBlock\n  id:: transparency-(oecd)-ontology\n  collapsed:: true\n\t- ontology:: true\n\t- term-id:: AI-0161\n\t- preferred-term:: Transparency (OECD)\n\t- source-domain:: ai\n\t- status:: draft\n    - public-access:: true\n\t- definition:: AI actors should commit to transparency and responsible disclosure regarding AI systems, providing sufficient information to enable people to understand AI outcomes, challenge decisions and participate meaningfully in AI-influenced processes.\n\n\n\n\n## Academic Context\n\n- The OECD AI Principles, first adopted in 2019 and updated through 2023 and 2024, represent the pioneering intergovernmental standard for trustworthy AI governance.\n  - These principles are grounded in human rights, democratic values, and a rights-based approach to AI development and deployment.\n  - Transparency and explainability are core tenets, requiring AI actors to provide meaningful, context-appropriate information about AI systems’ data, logic, and decision-making processes.\n- Academically, these principles draw on interdisciplinary research spanning computer science, ethics, law, and social sciences, emphasising the importance of explainability to enable user understanding, contestability, and meaningful participation in AI-influenced decisions.\n\n## Current Landscape (2025)\n\n- Industry adoption of transparency practices is growing, with many organisations voluntarily reporting on AI risk management under the OECD’s new reporting framework launched in early 2025.\n  - This framework encourages disclosure of governance measures without constituting formal certification, promoting a culture of openness rather than bureaucratic box-ticking.\n  - Leading AI developers worldwide are enhancing system robustness, security, and transparency, as highlighted in recent OECD reports.\n- In the UK, transparency is increasingly embedded in AI governance strategies, aligning with the OECD principles and anticipating forthcoming regulatory frameworks.\n- Technical capabilities have advanced to support explainability, though challenges remain in balancing transparency with proprietary concerns and preventing information overload for users.\n- Standards and frameworks continue to evolve, with the OECD’s principles influencing the EU AI Act, UK regulatory approaches, and global interoperability efforts.\n\n## Research & Literature\n\n- Key academic sources include:\n  - Floridi, L., Cowls, J., Beltrametti, M., et al. (2018). \"AI4People—An Ethical Framework for a Good AI Society: Opportunities, Risks, Principles, and Recommendations.\" *Minds and Machines*, 28(4), 689–707. DOI: 10.1007/s11023-018-9482-5\n  - Mittelstadt, B. D., Russell, C., & Wachter, S. (2019). \"Explaining explanations in AI.\" *Proceedings of the Conference on Fairness, Accountability, and Transparency*, 279–288. DOI: 10.1145/3287560.3287574\n  - OECD (2024). *Recommendation of the Council on Artificial Intelligence*. OECD Publishing. DOI: 10.1787/eedfee77-en\n- Ongoing research focuses on improving explainability methods, user-centric transparency, and balancing transparency with privacy and security concerns.\n\n## UK Context\n\n- The UK government and regulatory bodies actively endorse the OECD AI Principles, integrating transparency into national AI strategies.\n- North England, including innovation hubs in Manchester, Leeds, Newcastle, and Sheffield, hosts several AI research centres and startups prioritising transparent AI systems.\n  - For example, Manchester’s AI research institutes collaborate with industry to develop explainable AI tools tailored for healthcare and public services.\n  - Leeds and Sheffield contribute through interdisciplinary projects combining AI ethics and technical transparency.\n- Regional case studies demonstrate practical applications of transparency principles in public sector AI deployments, enhancing citizen trust and engagement.\n\n## Future Directions\n\n- Emerging trends include:\n  - Enhanced voluntary reporting mechanisms with richer, standardised transparency disclosures.\n  - Development of AI transparency tools that adapt explanations to diverse user needs and contexts.\n  - Integration of transparency with AI safety and accountability frameworks to form comprehensive governance.\n- Anticipated challenges:\n  - Avoiding transparency fatigue among users bombarded with complex information.\n  - Reconciling transparency with commercial confidentiality and security imperatives.\n  - Ensuring transparency efforts do not become mere performative gestures but lead to genuine user empowerment.\n- Research priorities:\n  - User experience studies on effective transparency communication.\n  - Technical innovations in explainability for complex, evolving AI systems.\n  - Cross-jurisdictional harmonisation of transparency standards.\n\n## References\n\n1. OECD (2024). *Recommendation of the Council on Artificial Intelligence*. OECD Publishing. DOI: 10.1787/eedfee77-en  \n2. Floridi, L., Cowls, J., Beltrametti, M., et al. (2018). AI4People—An Ethical Framework for a Good AI Society: Opportunities, Risks, Principles, and Recommendations. *Minds and Machines*, 28(4), 689–707. DOI: 10.1007/s11023-018-9482-5  \n3. Mittelstadt, B. D., Russell, C., & Wachter, S. (2019). Explaining explanations in AI. *Proceedings of the Conference on Fairness, Accountability, and Transparency*, 279–288. DOI: 10.1145/3287560.3287574  \n4. OECD (2025). *Governing with Artificial Intelligence*. OECD Publishing.  \n5. OECD (2025). OECD finds growing transparency efforts among leading AI developers. OECD Press Release, September 2025.\n\n\n## Metadata\n\n- **Last Updated**: 2025-11-11\n- **Review Status**: Comprehensive editorial review\n- **Verification**: Academic sources verified\n- **Regional Context**: UK/North England where applicable",
  "properties": {
    "id": "transparency-(oecd)-ontology",
    "collapsed": "true",
    "- ontology": "true",
    "- term-id": "AI-0161",
    "- preferred-term": "Transparency (OECD)",
    "- source-domain": "ai",
    "- status": "draft",
    "- public-access": "true",
    "- definition": "AI actors should commit to transparency and responsible disclosure regarding AI systems, providing sufficient information to enable people to understand AI outcomes, challenge decisions and participate meaningfully in AI-influenced processes."
  },
  "backlinks": [],
  "wiki_links": [],
  "ontology": {
    "term_id": "AI-0161",
    "preferred_term": "Transparency (OECD)",
    "definition": "AI actors should commit to transparency and responsible disclosure regarding AI systems, providing sufficient information to enable people to understand AI outcomes, challenge decisions and participate meaningfully in AI-influenced processes.",
    "source_domain": "ai",
    "maturity_level": null,
    "authority_score": null
  }
}