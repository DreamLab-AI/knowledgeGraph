{
  "title": "k-Anonymity in Datasets",
  "content": "- ### OntologyBlock\n  id:: k-anonymity-in-datasets-ontology\n  collapsed:: true\n\n  - **Identification**\n    - public-access:: true\n    - ontology:: true\n    - term-id:: AI-0421\n    - preferred-term:: k-Anonymity in Datasets\n    - source-domain:: ai\n    - status:: in-progress\n    - version:: 1.0\n    - last-updated:: 2025-10-29\n\n  - **Definition**\n    - definition:: k-Anonymity in Datasets is a privacy-preserving property ensuring that each record in a dataset is indistinguishable from at least k-1 other records with respect to quasi-identifiers (attributes that could potentially identify individuals when combined, such as age, gender, zip code), preventing re-identification attacks by guaranteeing anonymity sets of at least size k. This technique achieves anonymization through generalization (replacing specific values with broader categories, such as exact age → age range [30-40], 5-digit zip code → 3-digit prefix) and suppression (removing or masking particularly identifying attribute values when generalization insufficient), producing equivalence classes where all records within a class share identical quasi-identifier values. The privacy guarantee states that for any record in the dataset, an adversary with knowledge of quasi-identifiers cannot distinguish the target individual from k-1 others, formalized as minimum group size ≥ k for all equivalence classes partitioned by quasi-identifiers. However, limitations include vulnerability to homogeneity attacks when sensitive attributes lack diversity within equivalence classes (all k individuals having same disease diagnosis still leaks information), background knowledge attacks leveraging external information to narrow anonymity sets, and composition attacks combining multiple published datasets to re-identify individuals despite each satisfying k-anonymity independently. Extensions addressing these limitations include l-diversity requiring each equivalence class to contain at least l distinct sensitive attribute values ensuring diversity beyond quasi-identifier indistinguishability, t-closeness requiring sensitive attribute distribution within each equivalence class to be close (within threshold t) to distribution in overall dataset preventing attribute disclosure through distribution differences, and δ-presence ensuring individuals' presence or absence in dataset cannot be determined with confidence exceeding δ. Implementation algorithms include Mondrian recursive partitioning of feature space creating balanced anonymization groups, Incognito bottom-up lattice traversal exploring generalization strategies, and μ-Argus specialized tool for statistical disclosure control in survey data, with typical parameter selections including k ≥ 5 minimum for meaningful privacy, k ≥ 10 recommended for sensitive data, l ≥ 2-3 for l-diversity, and t ≤ 0.2 for t-closeness applications.\n    - maturity:: mature\n    - source:: [[Sweeney (2002)]], [[Machanavajjhala et al. (2007)]], [[Li et al. (2007)]]\n    - authority-score:: 0.95\n\n  - **Semantic Classification**\n    - owl:class:: aigo:KAnonymityInDatasets\n    - owl:physicality:: VirtualEntity\n    - owl:role:: Process\n    - owl:inferred-class:: aigo:VirtualProcess\n    - belongsToDomain:: [[AIEthicsDomain]]\n    - implementedInLayer:: [[ConceptualLayer]]\n\n  - #### Relationships\n    id:: k-anonymity-in-datasets-relationships\n\n  - #### OWL Axioms\n    id:: k-anonymity-in-datasets-owl-axioms\n    collapsed:: true\n    - ```clojure\n      (Declaration (Class :KAnonymityInDatasets))\n(AnnotationAssertion rdfs:label :KAnonymityInDatasets \"k-Anonymity in Datasets\"@en)\n(SubClassOf :KAnonymityInDatasets :AIGovernancePrinciple)\n      ```\n\n- ## About k-Anonymity in Datasets\n  id:: k-anonymity-in-datasets-about\n\n  - \n  -\n    - ### Use Cases\n  - ### Healthcare Databases\n  -\n    **HIPAA Safe Harbor** de-identification:\n    - Remove 18 identifier types\n    - Can use k-anonymity for additional protection\n  -\n    **Example**:\n    ```python\n    # HIPAA + k-anonymity\n    qi_cols = ['Age', 'ZIP', 'AdmissionDate']\n    data_anonymised = mondrian(data_deidentified, qi_cols, k=5)\n    ```\n    -\n  - ### Best Practices\n    **1. QI Identification**:\n    - Include all potentially identifying attributes\n    - Consider combinations\n  -\n    **2. Parameter Selection**:\n    - **k ≥ 5**: Minimum for meaningful privacy\n    - **k ≥ 10**: Recommended for sensitive data\n    - Consider l-diversity (l ≥ 2-3) and t-closeness (t ≤ 0.2)\n  -\n    **3. Utility Testing**:\n    - Validate analytical queries pre/post anonymisation\n    - Measure information loss\n  -\n    **4. Threat Modeling**:\n    - Assess adversary knowledge\n    - Simulate attacks\n\n\n## Current Landscape (2025)\n\n- Industry adoption and implementations\n  - Metaverse platforms continue to evolve with focus on interoperability and open standards\n  - Web3 integration accelerating with decentralised identity and asset ownership\n  - Enterprise adoption growing in virtual collaboration, training, and digital twins\n  - UK companies increasingly active in metaverse development and immersive technologies\n\n- Technical capabilities\n  - Real-time rendering at photorealistic quality levels\n  - Low-latency networking enabling seamless multi-user experiences\n  - AI-driven content generation and procedural world building\n  - Spatial audio and haptics enhancing immersion\n\n- UK and North England context\n  - Manchester: Digital Innovation Factory supports metaverse startups and research\n  - Leeds: Holovis leads in immersive experiences for entertainment and training\n  - Newcastle: University research in spatial computing and interactive systems\n  - Sheffield: Advanced manufacturing using digital twin technology\n\n- Standards and frameworks\n  - Metaverse Standards Forum driving interoperability protocols\n  - WebXR enabling browser-based immersive experiences\n  - glTF and USD for 3D asset interchange\n  - Open Metaverse Interoperability Group defining cross-platform standards\n\n## Metadata\n\n- **Last Updated**: 2025-11-16\n- **Review Status**: Automated remediation with 2025 context\n- **Verification**: Academic sources verified\n- **Regional Context**: UK/North England where applicable",
  "properties": {
    "id": "k-anonymity-in-datasets-about",
    "collapsed": "true",
    "- public-access": "true",
    "- ontology": "true",
    "- term-id": "AI-0421",
    "- preferred-term": "k-Anonymity in Datasets",
    "- source-domain": "ai",
    "- status": "in-progress",
    "- version": "1.0",
    "- last-updated": "2025-10-29",
    "- definition": "k-Anonymity in Datasets is a privacy-preserving property ensuring that each record in a dataset is indistinguishable from at least k-1 other records with respect to quasi-identifiers (attributes that could potentially identify individuals when combined, such as age, gender, zip code), preventing re-identification attacks by guaranteeing anonymity sets of at least size k. This technique achieves anonymization through generalization (replacing specific values with broader categories, such as exact age → age range [30-40], 5-digit zip code → 3-digit prefix) and suppression (removing or masking particularly identifying attribute values when generalization insufficient), producing equivalence classes where all records within a class share identical quasi-identifier values. The privacy guarantee states that for any record in the dataset, an adversary with knowledge of quasi-identifiers cannot distinguish the target individual from k-1 others, formalized as minimum group size ≥ k for all equivalence classes partitioned by quasi-identifiers. However, limitations include vulnerability to homogeneity attacks when sensitive attributes lack diversity within equivalence classes (all k individuals having same disease diagnosis still leaks information), background knowledge attacks leveraging external information to narrow anonymity sets, and composition attacks combining multiple published datasets to re-identify individuals despite each satisfying k-anonymity independently. Extensions addressing these limitations include l-diversity requiring each equivalence class to contain at least l distinct sensitive attribute values ensuring diversity beyond quasi-identifier indistinguishability, t-closeness requiring sensitive attribute distribution within each equivalence class to be close (within threshold t) to distribution in overall dataset preventing attribute disclosure through distribution differences, and δ-presence ensuring individuals' presence or absence in dataset cannot be determined with confidence exceeding δ. Implementation algorithms include Mondrian recursive partitioning of feature space creating balanced anonymization groups, Incognito bottom-up lattice traversal exploring generalization strategies, and μ-Argus specialized tool for statistical disclosure control in survey data, with typical parameter selections including k ≥ 5 minimum for meaningful privacy, k ≥ 10 recommended for sensitive data, l ≥ 2-3 for l-diversity, and t ≤ 0.2 for t-closeness applications.",
    "- maturity": "mature",
    "- source": "[[Sweeney (2002)]], [[Machanavajjhala et al. (2007)]], [[Li et al. (2007)]]",
    "- authority-score": "0.95",
    "- owl:class": "aigo:KAnonymityInDatasets",
    "- owl:physicality": "VirtualEntity",
    "- owl:role": "Process",
    "- owl:inferred-class": "aigo:VirtualProcess",
    "- belongsToDomain": "[[AIEthicsDomain]]",
    "- implementedInLayer": "[[ConceptualLayer]]"
  },
  "backlinks": [],
  "wiki_links": [
    "AIEthicsDomain",
    "Sweeney (2002)",
    "Machanavajjhala et al. (2007)",
    "Li et al. (2007)",
    "ConceptualLayer"
  ],
  "ontology": {
    "term_id": "AI-0421",
    "preferred_term": "k-Anonymity in Datasets",
    "definition": "k-Anonymity in Datasets is a privacy-preserving property ensuring that each record in a dataset is indistinguishable from at least k-1 other records with respect to quasi-identifiers (attributes that could potentially identify individuals when combined, such as age, gender, zip code), preventing re-identification attacks by guaranteeing anonymity sets of at least size k. This technique achieves anonymization through generalization (replacing specific values with broader categories, such as exact age → age range [30-40], 5-digit zip code → 3-digit prefix) and suppression (removing or masking particularly identifying attribute values when generalization insufficient), producing equivalence classes where all records within a class share identical quasi-identifier values. The privacy guarantee states that for any record in the dataset, an adversary with knowledge of quasi-identifiers cannot distinguish the target individual from k-1 others, formalized as minimum group size ≥ k for all equivalence classes partitioned by quasi-identifiers. However, limitations include vulnerability to homogeneity attacks when sensitive attributes lack diversity within equivalence classes (all k individuals having same disease diagnosis still leaks information), background knowledge attacks leveraging external information to narrow anonymity sets, and composition attacks combining multiple published datasets to re-identify individuals despite each satisfying k-anonymity independently. Extensions addressing these limitations include l-diversity requiring each equivalence class to contain at least l distinct sensitive attribute values ensuring diversity beyond quasi-identifier indistinguishability, t-closeness requiring sensitive attribute distribution within each equivalence class to be close (within threshold t) to distribution in overall dataset preventing attribute disclosure through distribution differences, and δ-presence ensuring individuals' presence or absence in dataset cannot be determined with confidence exceeding δ. Implementation algorithms include Mondrian recursive partitioning of feature space creating balanced anonymization groups, Incognito bottom-up lattice traversal exploring generalization strategies, and μ-Argus specialized tool for statistical disclosure control in survey data, with typical parameter selections including k ≥ 5 minimum for meaningful privacy, k ≥ 10 recommended for sensitive data, l ≥ 2-3 for l-diversity, and t ≤ 0.2 for t-closeness applications.",
    "source_domain": "ai",
    "maturity_level": null,
    "authority_score": 0.95
  }
}