{
  "title": "Hyperparameter",
  "content": "- ### OntologyBlock\n  id:: hyperparameter-ontology\n  collapsed:: true\n\t- ontology:: true\n\t- term-id:: AI-0048\n\t- preferred-term:: Hyperparameter\n\t- source-domain:: ai\n\t- status:: active\n    - public-access:: true\n\t- definition:: A configuration variable set before training that controls the learning process but is not learned from data. Examples include learning rate, batch size, and number of layers.\n\t- maturity:: established\n\t- owl:class:: ml:Hyperparameter\n\t- owl:physicality:: ConceptualEntity\n\t- owl:role:: Concept\n\t- belongsToDomain:: [[MachineLearningDomain]]\n\n\n## Academic Context\n\n- Brief contextual overview\n  - Hyperparameters are foundational to machine learning, controlling how models learn without being learned themselves from training data\n  - Distinguished from model parameters (weights and biases learned during training)\n  - Critical for model performance and generalisation\n- Academic foundations\n  - Formal definition established in ISO/IEC 22989:2022 for AI terminology\n  - Extensively studied in optimisation theory and statistical learning frameworks\n  - Research spans automated hyperparameter tuning, neural architecture search, and meta-learning\n\n## Current Landscape (2025)\n\n- Industry adoption and implementations\n  - Universal across all machine learning frameworks including TensorFlow, PyTorch, scikit-learn, and JAX\n  - Automated tuning tools widely adopted: Optuna, Ray Tune, Weights & Biases Sweeps\n  - UK companies like DeepMind and Faculty AI pioneering efficient hyperparameter optimisation\n- Technical capabilities and limitations\n  - Common hyperparameters include learning rate, batch size, number of layers, dropout rate, regularisation strength\n  - Manual tuning remains time-consuming and computationally expensive\n  - AutoML platforms increasingly automate selection but may lack domain-specific insights\n- Standards and frameworks\n  - ISO/IEC 22989:2022 provides standardised AI terminology including hyperparameters\n  - NIST AI Risk Management Framework addresses documentation requirements\n  - EU AI Act Article 11 mandates disclosure of hyperparameter choices for high-risk systems\n\n## Research & Literature\n\n- Key academic papers and sources\n  - Bergstra, J., & Bengio, Y. (2012). \"Random Search for Hyper-Parameter Optimization.\" Journal of Machine Learning Research, 13, 281-305. [http://jmlr.org/papers/v13/bergstra12a.html](http://jmlr.org/papers/v13/bergstra12a.html)\n  - Feurer, M., & Hutter, F. (2019). \"Hyperparameter Optimization.\" In Automated Machine Learning (pp. 3-33). Springer. [https://doi.org/10.1007/978-3-030-05318-5_1](https://doi.org/10.1007/978-3-030-05318-5_1)\n  - Elsken, T., et al. (2019). \"Neural Architecture Search: A Survey.\" Journal of Machine Learning Research, 20(55), 1-21. [http://jmlr.org/papers/v20/18-598.html](http://jmlr.org/papers/v20/18-598.html)\n- Ongoing research directions\n  - Transfer learning for hyperparameter tuning across related tasks\n  - Bayesian optimisation and Gaussian processes for efficient search\n  - Meta-learning approaches to learn optimal hyperparameter configurations\n\n## UK Context\n\n- British contributions and implementations\n  - DeepMind's research on automated hyperparameter tuning for reinforcement learning systems\n  - University of Oxford's work on Bayesian optimisation for hyperparameter search\n  - UK-based Faculty AI develops enterprise AutoML platforms with sophisticated tuning\n- North England innovation hubs\n  - University of Manchester's machine learning research group explores efficient tuning methods\n  - Leeds Institute for Data Analytics investigates hyperparameter optimisation for healthcare AI\n- Regional case studies\n  - Manchester-based Peak AI applies automated hyperparameter tuning to retail forecasting models\n  - Newcastle University's digital health projects optimise medical imaging models via principled hyperparameter search\n\n## Future Directions\n\n- Emerging trends and developments\n  - Neural architecture search (NAS) automating both architecture and hyperparameter selection\n  - Meta-learning frameworks predicting optimal hyperparameters from dataset characteristics\n  - Green AI initiatives balancing performance with energy efficiency in hyperparameter tuning\n- Anticipated challenges\n  - Computational cost of exhaustive search remains prohibitive for large-scale models\n  - Generalisation across domains still requires domain expertise\n  - Explainability of automated tuning decisions for regulatory compliance\n- Research priorities\n  - Efficient search algorithms reducing computational burden\n  - Transfer learning for hyperparameter configurations\n  - Integration with model interpretability and fairness constraints\n\n## Hyperparameter Tuning Methods\n\n- Grid search\n  - Exhaustive search over specified parameter grid\n  - Guarantees finding best combination within grid but computationally expensive\n  - scikit-learn GridSearchCV: [https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html)\n- Random search\n  - Samples random combinations from parameter distributions\n  - Often more efficient than grid search for high-dimensional spaces\n  - scikit-learn RandomizedSearchCV: [https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html)\n- Bayesian optimisation\n  - Uses probabilistic model to guide search towards promising regions\n  - Popular tools include Optuna, Hyperopt, and Google Vizier\n- Neural architecture search\n  - Automates both architecture and hyperparameter selection\n  - Computationally intensive but yields state-of-the-art results\n\n## References\n\n1. ISO/IEC 22989:2022. Information technology — Artificial intelligence — Artificial intelligence concepts and terminology.\n2. Bergstra, J., & Bengio, Y. (2012). Random Search for Hyper-Parameter Optimization. Journal of Machine Learning Research, 13, 281-305. http://jmlr.org/papers/v13/bergstra12a.html\n3. Feurer, M., & Hutter, F. (2019). Hyperparameter Optimization. In Automated Machine Learning (pp. 3-33). Springer. https://doi.org/10.1007/978-3-030-05318-5_1\n4. Elsken, T., et al. (2019). Neural Architecture Search: A Survey. Journal of Machine Learning Research, 20(55), 1-21. http://jmlr.org/papers/v20/18-598.html\n5. NIST AI 100-3. U.S. Leadership in AI: A Plan for Federal Engagement in Developing Technical Standards and Related Tools. https://www.nist.gov/publications/us-leadership-ai-plan-federal-engagement-developing-technical-standards-and-related\n\n\n## Metadata\n\n- Last Updated: 2025-11-11\n- Review Status: Comprehensive editorial review\n- Verification: Academic sources verified\n- Regional Context: UK context included",
  "properties": {
    "id": "hyperparameter-ontology",
    "collapsed": "true",
    "- ontology": "true",
    "- term-id": "AI-0048",
    "- preferred-term": "Hyperparameter",
    "- source-domain": "ai",
    "- status": "active",
    "- public-access": "true",
    "- definition": "A configuration variable set before training that controls the learning process but is not learned from data. Examples include learning rate, batch size, and number of layers.",
    "- maturity": "established",
    "- owl:class": "ml:Hyperparameter",
    "- owl:physicality": "ConceptualEntity",
    "- owl:role": "Concept",
    "- belongsToDomain": "[[MachineLearningDomain]]"
  },
  "backlinks": [],
  "wiki_links": [
    "MachineLearningDomain"
  ],
  "ontology": {
    "term_id": "AI-0048",
    "preferred_term": "Hyperparameter",
    "definition": "A configuration variable set before training that controls the learning process but is not learned from data. Examples include learning rate, batch size, and number of layers.",
    "source_domain": "ai",
    "maturity_level": null,
    "authority_score": null
  }
}