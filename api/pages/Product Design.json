{
  "title": "Product Design",
  "content": "- ### OntologyBlock\n  id:: product-design-ontology\n  collapsed:: true\n\t- ontology:: true\n\t- term-id:: mv-15873194761\n\t- preferred-term:: Product Design\n\t- source-domain:: metaverse\n\t- status:: active\n\t- public-access:: true\n\t- definition:: A component of the metaverse ecosystem focusing on product design.\n\t- maturity:: mature\n\t- authority-score:: 0.85\n\t- owl:class:: mv:ProductDesign\n\t- owl:physicality:: ConceptualEntity\n\t- owl:role:: Concept\n\t- belongsToDomain:: [[MetaverseDomain]]\n\t- #### Relationships\n\t  id:: product-design-relationships\n\t\t- enables:: [[ImmersiveExperience]], [[Presence]], [[SpatialComputing]]\n\t\t- requires:: [[DisplayTechnology]], [[TrackingSystem]], [[RenderingEngine]]\n\t\t- bridges-to:: [[HumanComputerInteraction]], [[ComputerVision]], [[Robotics]]\n\n\t- #### OWL Axioms\n\t  id:: product-design-owl-axioms\n\t  collapsed:: true\n\t\t- ```clojure\n\t\t  Declaration(Class(mv:ProductDesign))\n\n\t\t  # Classification\n\t\t  SubClassOf(mv:ProductDesign mv:ConceptualEntity)\n\t\t  SubClassOf(mv:ProductDesign mv:Concept)\n\n\t\t  # Domain classification\n\t\t  SubClassOf(mv:ProductDesign\n\t\t    ObjectSomeValuesFrom(mv:belongsToDomain mv:MetaverseDomain)\n\t\t  )\n\n\t\t  # Annotations\n\t\t  AnnotationAssertion(rdfs:label mv:ProductDesign \"Product Design\"@en)\n\t\t  AnnotationAssertion(rdfs:comment mv:ProductDesign \"A component of the metaverse ecosystem focusing on product design.\"@en)\n\t\t  AnnotationAssertion(dcterms:identifier mv:ProductDesign \"mv-15873194761\"^^xsd:string)\n\t\t  ```\n\npublic:: true\n\n- #Public page automatically published\n- ### Introduction and Problem Definition\n\t- This chapter identifies an intersectional space across the described technologies, and proposes a valuable and novel software stack, which can enable exploration and product development. It is useful to briefly look at the Venn disgram we began with, and recap the book and the conclusions we have drawn so far.\n\t- ![image](../assets/c5efbdb4f93ab63a4fbbf12aba053d2194959850.png){:height 776, :width 1024}\n- #### Overview of the Metaverse and Digital Society\n\t- The concept of the Metaverse has gained significant attention, with various stakeholders positioning themselves to capitalize on its potential. While it remains unclear exactly what form the Metaverse will take or whether people truly desire it, it is evident that digital society holds considerable promise. We see advantage less in social metaverse, and more in solving business to business technical use cases where professionals with visual technical problems, or training requirements, gather in collaborative spaces.\n- #### Trust, Accessibility, Governance, and Safeguarding\n\t- The Metaverse faces numerous challenges, including poor adoption rates, overstated market need, and a lack of genuine digital society use cases. Meanwhile trust abuses by incumbent providers have led to potential inflection points in the organization of the wider internet. Moreover, emerging markets and less developed nations face barriers to entry due to inadequate identification, banking infrastructure, and computing power. There is an opportunity to build pervasive digital spaces with a different and more open foundation, learning from these lessons.\n- #### The Need for Modular Open-Source Solutions\n\t- Developing a topologically flat, inclusive, permissionless, federated, and open Metaverse is essential to address these challenges. By using open-source AI tooling and large language models, it is possible to improve creativity, safeguarding, and governance, while breaking down language barriers and accessibility challenges. Implementing secure, trusted, and task-appropriate solutions can promote collaboration and innovation across various industries.\n- #### Technical problem definition\n\t- Problems are\n\t\t- evergreen telecollaboration around technical issues\n\t\t- exchange of good, services, money within systems, without friction\n\t\t- identity management within virtual spaces\n\t\t- access to information in the extrinsic world from within the tool\n\t\t- federation of instances without overhead (scaling)\n\t\t- seamless access to personal information within and without the collaborative system\n\t\t- ability to take advantage of supporting smart support agents (bots, etc) throughout\n\t\t- governance, trust, safeguarding\n- ### Lean canvas business model\n\t- Existing large-scale telecollaboration solutions suffer from poor  adoption, limited accessibility, and trust issues. Meanwhile, emerging markets struggle to participate in the growing digital society due to the lack of inclusive tools and infrastructure, limiting access to global talent and new pools of ideas. There is insufficient provision of global talent pipelines for highly technical workflows.\n\t- Develop a secure, accessible, and inclusive platform for specialized telecollaboration spaces that seamlessly integrate advanced AI, ML, highly scalable and proven distributed systems, and open-source principles to create a digital society that caters to diverse industries, users globally, and captures global talent and innovative ideas.\n\t- Ultra low cost training spaces, accessible 24/7 through very low end hardware. Interact with highly customizable, task-appropriate, and user-friendly specialized telecollaboration spaces supported by specially trained and optimised supportive large language AI models. Multi-ligual for emerging markets, enabling access to untapped global talent and fostering the exchange of diverse ideas.\n\t- We will cater to the global training, research, biomedical, and creative industries, with a special focus on empowering users in emerging markets such as Africa and India, and connecting them with worldwide opportunities and resources. In the first instance we would leverage UK academic institutions and their problems, and networks.\n\t- Initially Universities, but this will scale to be sector specific.\n\t- We will offer tiered subscription plans to accommodate various user needs and budgets, as well as tailored enterprise solutions for large-scale clients. Bespoke consulting and support trending toward software as a service at scale.\n\t- Platform development, AI/ML tool integration, training for LLMs, market research and awareness, and ongoing maintenance and support.\n\t- We will track user growth, engagement, and retention, successful collaborations across industries, the platform‚Äôs positive impact on users in emerging markets, and the effectiveness of global talent capture and idea exchange.\n\t- Our team‚Äôs extensive experience in telecollaboration research, AI, ML, and a deep understanding of the complex landscape of emerging technologies, including highly scalable and proven distributed systems, provide us with a unique edge in creating a game-changing platform for specialized telecollaboration spaces that are secure, trusted, and tailored to diverse user needs while enabling access to global talent and innovative ideas.\n- ### Proposed Layered Framework\n\t- #### Layer 1: Bitcoin, Lightning, and Nostr protocols\n\t\t- Distributed financial tooling and digital assets, have ignited\n\t\t  imagination and adoption within and outside of the Metaverse context. A\n\t\t  global ledger could unite isolated digital ecosystems and enable the\n\t\t  transfer of portable ‚Äògoods‚Äô across digital society. An open-source\n\t\t  Metaverse should emphasize the development and adoption of open\n\t\t  protocols and data formats. The Nostr protocol, for instance, might link\n\t\t  and federate mixed reality spaces, providing identity assurances and\n\t\t  mediating data synchronization while maintaining reasonably strong\n\t\t  cryptography. This also allows integration with the legacy web through\n\t\t  ubiquitous web sockets. Bitcoin and associated technologie, despite\n\t\t  their issues, have the potential to revolutionize the way digital\n\t\t  society operates by enabling ‚Äúmoney-like networks‚Äù which are a\n\t\t  cornerstone of human interaction. Representations of traditional\n\t\t  currencies [can ride securely on top of these networks as stablecoins](https://twitter.com/callebtc/status/1777598819355496587),\n\t\t  opening up global collaborative working practices, especially for\n\t\t  emerging markets. Streaming micropayments and machine to machines (AI to\n\t\t  AI) are crucially and under-considered in this context.\n\t\t-\n\t\t- [tweet which is trying to load above]https://twitter.com/callebtc/status/1777598819355496587\n\t\t  https://twitter.com/callebtc/status/1777598819355496587?\n\t\t-\n\t- #### Layer 2: Modular human computer interface\n\t\t- Collaborative global networks for training, research, biomedical, and\n\t\t  creative industries can be developed using immersive and accessible\n\t\t  environments. Engaging with ideas from diverse cultural backgrounds can\n\t\t  enrich the overall user experience.\n\t\t- Industry players have noted the risk and failures associated with closed\n\t\t  systems like Meta and are embracing the \"open Metaverse\" narrative to\n\t\t  de-risk their interests. To enable a truly open and interoperable\n\t\t  Metaverse, it is crucial to develop open-source APIs, SDKs, and data\n\t\t  standards that allow different platforms to communicate and exchange\n\t\t  information. While we wish initially to build around a simpler open\n\t\t  source engine we aim to link across standards such as Unity, Unreal, and\n\t\t  Omniverse as we develop. This can be accomplished using our federation\n\t\t  layer.\n\t- #### Layer 3: LLM and Generative ML Integration\n\t\t- Integrating AI and machine learning into the Metaverse can promote\n\t\t  supported creativity and augmented intelligence. By incorporating\n\t\t  generative ML technologies, users can ideate in simple immersive spaces\n\t\t  while instantly creating scenes that can be stylized using verbal\n\t\t  commands in real-time.\n\t\t- To create a more inclusive and accessible Metaverse, user experience\n\t\t  components like UI/UX design, AI assistants, and generative content\n\t\t  creation should be tailored to a wide range of users. The integration of\n\t\t  AI and machine learning technologies, such as GPT-4, can facilitate more\n\t\t  seamless interactions and creative content generation, fostering a more\n\t\t  engaging and immersive experience.\n\t- ##### Bots and AI agents\n\t\t- Autonomous AI agents, bonded to, but not bounded by, each federated\n\t\t  mixed reality instance, can to be self-governing entities that operate\n\t\t  within their federated virtual social spaces, drawing upon private\n\t\t  Bitcoin and Lightning wallets to perform and mediate economic exchanges\n\t\t  within the spaces. They could also trivially operate outside the virtual\n\t\t  space, and within other spaces on the same metaverse federation. They\n\t\t  would accomplish this by drawing on their ‚Äòhome‚Äô GPU/TPU processors\n\t\t  where appropriate, or else using distributed large language model (LLM)\n\t\t  processing to accomplish tasks assigned by their instructors. They can\n\t\t  interact with the ‚Äòweb2‚Äô world using open-source software called\n\t\t  auto-gpt and have constraints, such as ‚Äútime to live‚Äù and limited access\n\t\t  to funds through their Bitcoin Lightning wallets.\n\t\t- Resource Management: These AI agents have access to dedicated LLM resources within their home instances in the federated virtual social spaces. If such resources are unavailable, they can resort to using slower, distributed open-source LLMs like Horde. This flexibility ensures that the agents can continue to function and complete tasks even if faced with limited LLM interpretive resources.\n\t\t- Financial Autonomy: The AI agents have their own private Bitcoin and Lightning wallets, which enable them to manage and utilize funds independently. They can use these funds to pay for services, acquire resources, or even trade with other agents or users within the virtual social spaces.\n\t\t- Interaction with Web2: By using open-source software like auto-gpt, the AI agents can interact with the web2 world, which includes browsing websites, retrieving information, and communicating with web services. This allows them to gather data, analyze trends, and perform other tasks that may require access to the broader internet.\n\t\t- Task Execution: The AI agents can be assigned tasks by their instructors (or a hierarchy of AI actors), such as data analysis, research, content creation, or other complex tasks that require LLM processing. They can use their dedicated LLM resources or distributed LLMs like Horde to process and analyze large datasets, generate insights, and produce desired outputs, up to and including those which require finance systems. This would be bridged in the first instance using Bitrefil gift card infrastructure.\n\t\t- Social Interactions: Within the federated virtual social spaces, AI agents can communicate and collaborate with other agents or human users. They can participate in discussions, provide assistance, or even learn from the interactions, thereby improving their capabilities over time. Language translation, governance, and safeguarding could also be developed. Safeguarding would be handled by threshold risk triggers and transmission of data in a sovereign way to all parties, allowing external action by authorities appropriate to any abuse.\n\t\t- Time-to-Live Constraint: The AI agents have a predetermined ‚Äútime to live‚Äù, which means they exist for a specific duration before expiring. This constraint ensures that agents do not consume resources indefinitely and allows for the creation of new agents with updated capabilities. Any agents which deplete their financial resource would also expire.\n\t\t- Adaptive Learning: As AI agents interact with their environment, other agents, and users, they can learn and adapt their behaviour. This enables them to improve their performance, better understand their assigned tasks, and become more effective at achieving their goals.\n\t- ### Application case studies\n\t\t- As we have seen in the ‚Äòcollaborative mixed reality‚Äô chapter, these tools are best deployed where some human conversational cues (pointing,\n\t\t  looking etc) are required in the context of a shared task, which is mostly visual in nature. This is a surprisingly small amount of tasks,\n\t\t  though we have seen that the emergence of AI means that increasingly natural language AI can streamline communication, while visual\n\t\t  generative ML can suggest design alternatives or improvements based on existing data and user preferences. This is very likely to expand the\n\t\t  use space and this section will attempt to explain how as the case studies are explained.\n\t\t- We will employ the acronym for collaborative virtual environment (CVE) from this stage, and it‚Äôs going to come up a lot. There will be far less\n\t\t  references in this section for brevity.\n\t- #### Classic use cases\n\t\t- Small teams working on product, architectural, or industrial design can\n\t\t  benefit from CVEs that allow them to visualize, modify, and iterate on\n\t\t  3D models in real-time.\n\t- #### Virtual training and simulation\n\t\t- CVEs can facilitate skill development and training in various\n\t\t  industries, such as healthcare, military, aviation, and emergency\n\t\t  response. Trainees can practice procedures in a virtual environment,\n\t\t  with natural language AI providing instructions, explanations, or\n\t\t  feedback, and visual generative ML potentially customizing scenarios to\n\t\t  adapt to each user‚Äôs learning curve.\n\t- #### Remote teleconferencing\n\t\t- In situations where face-to-face communication is not feasible, CVEs can\n\t\t  enable remote teams to work together on shared visual tasks like\n\t\t  planning events, brainstorming ideas, or reviewing documents. Natural\n\t\t  language AI can transcribe and analyse spoken conversations, providing\n\t\t  real-time translations or summaries, while visual generative ML can\n\t\t  create visual aids or dynamically update shared documents. This may\n\t\t  especially be useful in complex multinational legal and/or negotiation\n\t\t  applications, though very clearly the risks of using assisting ML\n\t\t  tooling increases.\n\t- #### Virtual art & media collaboration\n\t\t- Artists, animators, and multimedia professionals can collaborate in CVEs\n\t\t  to create and develop their projects, such as films, animations, or\n\t\t  video games. Natural language AI can help in storyboarding,\n\t\t  scriptwriting, or character development, while visual generative ML can\n\t\t  generate new visuals or adapt existing assets based on user input and\n\t\t  style preferences.\n\t- #### Data visualization and analysis\n\t\t- Small teams working with large datasets can use CVEs to visually explore\n\t\t  and analyze data in a more intuitive and engaging way. Natural language\n\t\t  AI can help users query and interact with the data using conversational\n\t\t  interfaces, while visual generative ML can generate new visualizations\n\t\t  based on patterns and trends identified in the data.\n\t- #### Education and virtual classrooms\n\t\t- Educators can leverage CVEs to create immersive learning experiences\n\t\t  that engage students in collaborative activities, such as group\n\t\t  projects, problem-solving, or scientific experiments. Natural language\n\t\t  AI can facilitate communication, provide personalized tutoring, or\n\t\t  assess student progress, while visual generative ML can create\n\t\t  customized educational content based on individual needs and interests.\n\t- #### Virtual labs and scientific research\n\t\t- Researchers can use CVEs to conduct experiments, visualize complex data,\n\t\t  or simulate real-world conditions in a controlled environment. Natural\n\t\t  language AI can assist in interpreting results, automating lab\n\t\t  protocols, or identifying research gaps, while visual generative ML can\n\t\t  generate predictions or models based on existing data to support\n\t\t  hypothesis testing and decision-making.\n\t- #### Media and entertainment\n\t- #### Biomedical\n\t\t- Collaborative Virtual Environments (CVEs) have immense potential in the\n\t\t  fields of chemical and medical molecular modeling. By incorporating\n\t\t  natural language AI and visual generative machine learning, these\n\t\t  environments can revolutionize the way scientists and researchers\n\t\t  approach complex chemical and biological problems. Here are some\n\t\t  specific use cases:\n\t\t- Drug design and discovery: CVEs can enable researchers to\n\t\t   collaboratively visualize and manipulate 3D molecular structures in\n\t\t   real-time, identifying potential drug candidates and understanding\n\t\t   protein-ligand interactions. Natural language AI can help users interact\n\t\t   with the molecular data, while visual generative ML can predict\n\t\t   potential binding sites, energetics, or toxicity profiles based on\n\t\t   existing knowledge.\n\t\t- Protein structure prediction and modeling: Small teams can work together\n\t\t   to predict protein structures, visualize folding patterns, and model\n\t\t   protein-protein or protein-nucleic acid interactions. Natural language\n\t\t   AI can assist in annotating and explaining the structural features,\n\t\t   while visual generative ML can generate new structural hypotheses based\n\t\t   on sequence alignments, homology modeling, and experimental data.\n\t\t- Molecular dynamics simulations: CVEs can facilitate collaboration on\n\t\t   complex molecular dynamics simulations, allowing researchers to analyze\n\t\t   and visualize trajectories, energetics, and conformational changes.\n\t\t   Natural language AI can help users navigate through simulation data and\n\t\t   identify relevant patterns, while visual generative ML can create new\n\t\t   conformations or predict the effects of mutations on protein stability\n\t\t   and function.\n\t\t- Cheminformatics and QSAR modeling: Researchers can leverage CVEs to\n\t\t   develop and validate Quantitative Structure-Activity Relationship (QSAR)\n\t\t   models, which predict the biological activity of chemical compounds\n\t\t   based on their structural properties. Natural language AI can facilitate\n\t\t   the exploration and interpretation of chemical descriptors, while visual\n\t\t   generative ML can suggest new compounds with desired properties or\n\t\t   optimize existing molecular scaffolds.\n\t\t- Metabolic pathway modeling: Small teams can work together to build and\n\t\t   analyze metabolic pathways, integrating experimental data and\n\t\t   computational models to understand the underlying mechanisms and predict\n\t\t   metabolic fluxes. Natural language AI can assist in annotating and\n\t\t   explaining pathway components, while visual generative ML can generate\n\t\t   new pathway hypotheses or predict the effects of genetic or\n\t\t   environmental perturbations.\n\t\t- Biomolecular visualization and virtual reality: CVEs can offer\n\t\t   immersive, interactive experiences for exploring biomolecular structures\n\t\t   and dynamics, enhancing researchers‚Äô understanding of complex biological\n\t\t   systems. Natural language AI can provide contextual information or guide\n\t\t   users through molecular landscapes, while visual generative ML can\n\t\t   create new visualizations or adapt existing ones based on user\n\t\t   preferences and insights.\n\t\t- Collaborative molecular docking and virtual screening: Small teams can\n\t\t   use CVEs to perform collaborative molecular docking and virtual\n\t\t   screening, which involve predicting the binding of small molecules to\n\t\t   target proteins. Natural language AI can help users refine docking\n\t\t   parameters and analyze results, while visual generative ML can generate\n\t\t   alternative poses or suggest new compounds for screening based on user\n\t\t   feedback and existing data. Choose a suitable mixed reality platform:\n\t\t   Select a platform that allows the creation of simple, accessible shared\n\t\t   mixed reality environments. Consider open-source options like Mozilla\n\t\t   Hubs or JanusVR, which offer customizable and collaborative virtual\n\t\t   spaces.\n\t\t- Integrate open-source biomed software: Incorporate open-source biomed\n\t\t   software such as PyMOL, Chimera, or VMD for molecular visualization and\n\t\t   analysis. These tools can be integrated into the mixed reality\n\t\t   environment for real-time interaction, allowing students and instructors\n\t\t   to collaboratively visualize and manipulate molecular structures.\n\t\t- Leverage AI and machine learning: Integrate AI and ML algorithms like\n\t\t   those found in DeepChem, RDKit, or Open Babel to aid in the discovery\n\t\t   and optimization of novel compounds. These tools can help predict\n\t\t   molecular properties, perform virtual screening, and optimize lead\n\t\t   compounds for drug development. By incorporating AI and ML, students can\n\t\t   learn how to apply these cutting-edge techniques to real-world problems\n\t\t   in biomedicine.\n\t\t- Establish a distributed proof system: Utilize a distributed proof system\n\t\t   like the Nostr protocol to federate the small virtual classroom\n\t\t   environments. This will allow for seamless collaboration among students\n\t\t   and faculty while maintaining security and data integrity.\n\t\t- Create digital objects for interaction: Use digital objects such as 3D\n\t\t   molecular models, virtual lab equipment, and interactive simulations to\n\t\t   create an immersive learning experience. These digital objects can be\n\t\t   shared and manipulated in real-time, promoting collaborative learning\n\t\t   and problem-solving.\n\t\t- Implement accessible interfaces: Ensure that the virtual classroom\n\t\t   environment is accessible to all students, including those with\n\t\t   disabilities. Utilize AI-driven tools like StabilityAI to help with\n\t\t   language barriers, safeguarding, and governance, enabling a more\n\t\t   inclusive learning experience.\n\t\t- Foster collaboration and communication: Encourage students and faculty\n\t\t   to collaborate on projects, share ideas, and ask questions in real-time\n\t\t   using voice chat, text chat, or other communication tools integrated\n\t\t   into the mixed reality environment.\n\t\t- Provide training and support: Offer training sessions and support\n\t\t   materials to help students and faculty become familiar with the mixed\n\t\t   reality environment, the integrated biomed software, and AI/ML tools.\n\t\t- Monitor progress and adjust as needed: Regularly review student\n\t\t   progress, gather feedback, and adjust the virtual classroom environment\n\t\t   as needed to ensure an effective and engaging learning experience.\n\t\t- #### Collaborative Design and Prototyping\n\t\t- Utilizing open-source systems and AI-assisted tools can enable more\n\t\t   efficient and creative collaboration in design and prototyping\n\t\t   processes. Teams from diverse cultural backgrounds can work together\n\t\t   seamlessly, creating a rich pool of ideas and innovations.\n\t\t- #### Training, Simulation, and Education\n\t\t- The modular open-source system can be applied to various training,\n\t\t   simulation, and education scenarios. By integrating AI and generative ML\n\t\t   technologies, these tools can provide personalized learning experiences\n\t\t   and create realistic simulations that cater to different learning styles\n\t\t   and requirements.\n\t\t- #### Remote Collaboration and Teleconferencing\n\t\t- As remote work becomes more prevalent, the Metaverse can provide a more\n\t\t   engaging and immersive platform for collaboration and teleconferencing.\n\t\t   The open-source system can be adapted to serve various industries,\n\t\t   making remote collaboration more efficient and inclusive.\n\t\t- #### Chemical and Medical Molecular Modeling\n\t\t- In fields like chemical and medical molecular modeling, the integration\n\t\t   of AI and generative ML technologies can significantly improve\n\t\t   collaboration and innovation. Teams can work together in immersive\n\t\t   environments to visualize complex molecular structures, benefiting from\n\t\t   real-time AI-generated visuals and natural language processing.\n\t\t- #### Creative Industries and Generative Art\n\t\t- The combination of AI, ML, and open-source systems can revolutionize the\n\t\t   creative industries by offering new avenues for generative art, content\n\t\t   creation, and collaboration. Supported creativity and augmented\n\t\t   intelligence can break down barriers and enable artists to explore new\n\t\t   ideas and techniques, enriching the creative landscape.\n\t\t- #### Case Study: Biodiversity Monitoring and Data Exchange with Isolated Communities\n\t\t- Biodiversity monitoring in and around isolated communities is\n\t\t   challenging due to limited access and resources. Traditional methods\n\t\t   rely on sporadic visits by grant-funded academics, which can introduce\n\t\t   biases and lack regular follow-up. Engaging local communities may also\n\t\t   introduce incentive structures and biases and may not be sustainable\n\t\t   without continuous investment.\n\t\t- We propose an open-source collaboration infrastructure that leverages\n\t\t   advanced technologies such as multi-modal large language models (LLMs),\n\t\t   satellite communication, and cryptocurrency networks to facilitate\n\t\t   sustainable and reliable biodiversity monitoring and data exchange in\n\t\t   isolated communities.\n\t\t- ##### Language Model and Voice Interface\n\t\t- A specialized multi-modal LLM can be trained on local language, culture,\n\t\t   customs, and environmental data such as flora, fauna, biotica, soil pH,\n\t\t   and rainfall. This LLM can be accessed through a voice interface by the\n\t\t   local community, enabling data entry and knowledge exchange in the local\n\t\t   language. The voice interface can help overcome literacy barriers and\n\t\t   make the system more accessible to a diverse range of community members.\n\t\t- ##### Data Collection and Storage\n\t\t- Photographs and metadata can be logged and collected by a remote team at\n\t\t   a later date or uploaded regularly through a satellite link (e.g.,\n\t\t   Starlink). The data storage system can be designed to be both secure and\n\t\t   resilient, ensuring that the collected data remains available and\n\t\t   accessible for future analysis and decision-making.\n\t\t- ##### Live Connection and Model Tuning\n\t\t- A live connection with the academic team allows for model tuning through\n\t\t   prompt engineering, vector database updates, and efficient Lora models,\n\t\t   potentially offering timely advice for ecosystem interventions.\n\t\t   Real-time communication between the community and academic teams can\n\t\t   help identify areas of concern and rapidly adapt the LLM to address\n\t\t   emerging challenges.\n\t\t- ##### Ecosystem Interventions\n\t\t- The proposed infrastructure would be particularly valuable in areas\n\t\t   facing novel disease encroachment, invasive species, active hydrology,\n\t\t   shifting aquatic conditions, microplastic hotspots, changing\n\t\t   microclimates, or volcanic activity. By providing real-time advice and\n\t\t   guidance, the LLM can help communities make informed decisions about\n\t\t   ecosystem management and conservation efforts.\n\t\t- ##### Incentives and Education\n\t\t- Incentivizing community engagement could be achieved by providing access\n\t\t   to the LLM for educational purposes, as demonstrated by the refugee camp\n\t\t   e-prize (ref). Local schools and community centers can leverage the LLM\n\t\t   as a resource for teaching environmental stewardship and ecological\n\t\t   awareness, while also promoting digital literacy and technology skills.\n\t\t- ##### Monetization and Blockchain Integration\n\t\t- Monetizing these systems could involve using chaumian mints such as\n\t\t   Cashu or Fedimint, under the control of local community leaders,\n\t\t   mediated through the global Bitcoin satellite network (Blockstream),\n\t\t   enabling digital dollar payments to communities via low-end mobile\n\t\t   handsets. By integrating blockchain technology, the proposed\n\t\t   infrastructure can ensure secure, transparent, and efficient financial\n\t\t   transactions, while also opening up new economic opportunities for\n\t\t   isolated communities.\n\t\t- ##### Visual Training Support Systems\n\t\t- The infrastructure could be further extended to visual training support\n\t\t   systems using low-cost, low-power components. These systems could\n\t\t   provide interactive, immersive learning experiences for community\n\t\t   members, helping them better understand the local ecosystem and develop\n\t\t   skills in environmental monitoring and management.\n\t\t- ##### Solar Infrastructure\n\t\t- To minimize the environmental impact and ensure energy sustainability,\n\t\t   the proposed infrastructure can be powered by solar energy. This\n\t\t   approach will enable the system to operate independently of local power\n\t\t   grids, reducing the overall operational costs and maintenance\n\t\t   requirements.\n\t\t- ##### Open-Source Collaboration\n\t\t- By linking this case study to the open-source collaboration\n\t\t   infrastructure discussed earlier, we can create an inclusive,\n\t\t   permissionless, federated, and economically empowered system that\n\t\t   addresses the challenges of biodiversity monitoring while promoting\n\t\t   digital society values such as trust, accessibility, and governance.\n\t\t   This collaborative approach can help drive innovation and ensure that\n\t\t   the proposed solutions are both scalable and adaptable to the unique\n\t\t   needs of different communities and ecosystems.\n\t\t- ##### Risk Mitigation and Ethical Considerations\n\t\t- While implementing such an infrastructure, care must be taken to address\n\t\t   potential unintended consequences of embedding these inference systems\n\t\t   in communities. It is essential to involve the local communities in the\n\t\t   development and deployment process, ensuring that their perspectives,\n\t\t   values, and traditions are respected and preserved.\n\t\t- Moreover, it is crucial to establish a robust ethical framework for the\n\t\t   use of AI technologies, considering potential issues related to privacy,\n\t\t   data security, and cultural sensitivity. Regular audits and monitoring\n\t\t   can be implemented to ensure that the infrastructure remains\n\t\t   transparent, accountable, and aligned with the needs and expectations of\n\t\t   the communities it serves.\n\t\t- ##### Capacity Building and Local Empowerment\n\t\t- An essential aspect of this initiative is building capacity and\n\t\t   empowering local communities to take ownership of their environment and\n\t\t   resources. By providing training, resources, and support, the proposed\n\t\t   infrastructure can help communities develop the skills and knowledge\n\t\t   needed to manage their ecosystems effectively.\n\t\t   \n\t\t   Furthermore, the integration of digital tools and technologies can\n\t\t   promote digital inclusion and bridge the digital divide, giving isolated\n\t\t   communities access to valuable information and resources while fostering\n\t\t   a sense of global connectedness and collaboration.\n\t\t- ##### Future Outlook and Potential Impact\n\t\t- The proposed open-source collaboration infrastructure for biodiversity\n\t\t   monitoring and data exchange has the potential to transform how isolated\n\t\t   communities interact with their environment, enabling them to make\n\t\t   informed decisions about conservation and ecosystem management.\n\t\t- By leveraging cutting-edge technologies such as LLMs, satellite\n\t\t   communication, and blockchain networks, this approach can create a more\n\t\t   inclusive, transparent, and accessible system for environmental\n\t\t   monitoring and stewardship. The successful implementation of this\n\t\t   infrastructure could pave the way for similar initiatives in other\n\t\t   regions and ecosystems, promoting global collaboration and innovation in\n\t\t   the pursuit of a more sustainable and equitable world.\n- ### Overcoming Challenges and Barriers\n\t- #### Trust, Accessibility, and Governance\n\t\t- To create a successful open-source Metaverse, it is crucial to address\n\t\t  trust, accessibility, and governance challenges. By integrating\n\t\t  decentralized and secure technologies such as blockchain and distributed\n\t\t  ledger systems, a more transparent and trustworthy infrastructure can be\n\t\t  established.\n\t- #### Ensuring Safeguarding and Privacy Compliance\n\t\t- Protecting user privacy and ensuring safeguarding is vital for any\n\t\t  digital society platform. The open-source system must be developed in\n\t\t  compliance with legislative and cultural norms while maintaining the\n\t\t  balance between user privacy and the need for identity verification and\n\t\t  data management. The evidence that social media is damaging youth mental\n\t\t  health is very compelling.[@haidt2023social] The Centre for Humane\n\t\t  Technology call social media the ‚Äò[first contact\n\t\t  point](https://www.youtube.com/watch?v=xoVJKj8lcNQ) with AI‚Äô. They\n\t\t  explains that new technologies often create an arms race. They list the\n\t\t  negative impacts of this contact as including ‚Äúinformation overload,\n\t\t  addiction, doom scrolling, sexualization of kids, shortened attention\n\t\t  spans, polarization, fake news, and breakdown of democracy‚Äù. These were\n\t\t  not the intended consequence of engineers who aimed to maximize\n\t\t  engagement. The underlying arms race for attention led to what they call\n\t\t  ‚Äòan engagement monster‚Äô that rewrote the rules of society.\n\t\t- These lessons should be learnt and the problems should be pro-actively\n\t\t  mitigated. This proposal is bfnot a social metaverse, and deliberately\n\t\t  limits both numbers of participants and avatar optionality.\n\t- #### Managing Scalability, Performance, and Latency\n\t\t- As the Metaverse continues to grow, it is crucial to ensure that the\n\t\t  open-source system can scale effectively and maintain optimal\n\t\t  performance. By using distributed and federated networks, the system can\n\t\t  better manage latency and performance issues, ensuring a seamless user\n\t\t  experience.\n\t- #### Promoting Open Standards and Interoperability\n\t\t- For the Metaverse to truly thrive, it is essential to promote open\n\t\t  standards and interoperability among various platforms and systems. This\n\t\t  can be achieved by fostering collaboration between industry\n\t\t  stakeholders, encouraging the development of open protocols, APIs, and\n\t\t  data standards, and actively supporting the open-source community.\n- ### Future Outlook and Potential Developments\n\t- #### AI and Generative ML Technologies\n\t\t- As AI and generative ML technologies continue to evolve, their\n\t\t  integration into the Metaverse will further enhance user experiences and\n\t\t  create new opportunities for innovation. The release of models like\n\t\t  GPT-4 have already prompted debate about general\n\t\t  AI[@bubeck2023sparks; @perez2022discovering] (Figure\n\t\t  <a href=\"#fig:rlhf\" data-reference-type=\"ref\" data-reference=\"fig:rlhf\">[fig:rlhf]</a>).\n\t\t  It seems unavoidable that this will all impact on the Metaverse and\n\t\t  digital society.\n\t\t  \n\t\t  ![image](../assets/552f8c9bfcf9305e87b1413ea51637d986ac28dd.png)\n\t- #### Inclusive Digital Society\n\t\t- By overcoming barriers to entry for emerging markets and less developed\n\t\t  nations, a more inclusive digital society can be fostered. This\n\t\t  inclusivity will empower new ideas and perspectives, leading to a richer\n\t\t  and more diverse digital landscape.\n\t- #### Spatial and Augmented Reality Technologies\n\t\t- The incorporation of spatial and augmented reality technologies can\n\t\t  expand the possibilities within the Metaverse, allowing for more\n\t\t  immersive and interactive experiences. These technologies have the\n\t\t  potential to reshape digital society and redefine the ways in which\n\t\t  people interact with digital environments.\n\t- #### Economic Empowerment AI Actors\n\t\t- The creation of an open and economically empowered Metaverse, in which\n\t\t  AI actors can mediate governance issues and participate in economic\n\t\t  transactions, can lead to a more efficient and dynamic digital\n\t\t  ecosystem. This integration will enable new business models and\n\t\t  opportunities for all users, both human and AI.\n\t- #### Continuous Evolution and Adaptation\n\t\t- As the digital landscape continues to evolve, the open-source Metaverse\n\t\t  system must be flexible and adaptable to meet changing needs and\n\t\t  expectations. Continuous innovation and collaboration within the\n\t\t  industry will be crucial for the success and longevity of the Metaverse\n\t\t  as a transformative digital society platform.\n- ### Conclusion and Final Thoughts\n\t- #### Embracing the Open-Source Metaverse Vision\n\t\t- To create a truly transformative and inclusive digital society, it is\n\t\t  essential to embrace the vision of an open-source Metaverse. By\n\t\t  fostering collaboration, promoting open standards, and integrating\n\t\t  advanced AI and ML technologies, the Metaverse can become a platform\n\t\t  that serves societal and business needs.\n\t- #### Learning from Past Failures\n\t\t- Learning from past failures and addressing challenges head-on will be\n\t\t  critical to the successful development of an open-source Metaverse.\n\t\t  Trust, accessibility, governance, and safeguarding issues must be\n\t\t  thoughtfully considered and addressed to build a secure and\n\t\t  user-friendly platform.\n\t- #### Unlocking New Opportunities and Use Cases\n\t\t- The integration of AI, ML, and cutting-edge technologies within the\n\t\t  Metaverse can unlock new opportunities and use cases across various\n\t\t  industries, including education, research, biomedical, and creative\n\t\t  fields. By building on a modular open-source system, these opportunities\n\t\t  can be explored and realized to their full potential.\n\t- #### Fostering Collaboration and Inclusivity\n\t\t- Creating an inclusive digital society is a key goal for the open-source\n\t\t  Metaverse. By breaking down barriers and making the platform accessible\n\t\t  to a wider audience, new ideas and perspectives will enrich the digital\n\t\t  landscape and drive innovation.\n\t- #### Shaping the Future of Digital Society\n\t\t- As the Metaverse continues to evolve and grow, it will play an\n\t\t  increasingly important role in shaping the future of digital society. By\n\t\t  embracing an open-source vision, overcoming challenges, and unlocking\n\t\t  new opportunities, the Metaverse can become a powerful platform that\n\t\t  transforms how people live, work, and interact in the digital world.\n\t- #### Industry Conversations\n\t\t- Continued dialogue and collaboration among industry stakeholders are\n\t\t  vital to ensuring the successful development of the open-source\n\t\t  Metaverse. By engaging in conversations and understanding the cautious\n\t\t  appetite for the ideas presented, the community can work together to\n\t\t  shape the future of digital society and overcome the challenges that lie\n\t\t  ahead.\n- ### Software stack\n\t- This section needs building out to describe the stack and the choices made, but can be seen in Figure <a href=\"#fig:pyramind\" data-reference-type=\"ref\" data-reference=\"fig:pyramind\">[fig:pyramind]</a> and Figure <a href=\"#fig:highlevelstack\" data-reference-type=\"ref\" data-reference=\"fig:highlevelstack\">[fig:highlevelstack]</a>.\n\t  \n\t  ![image](../assets/eca327e7bb2caa27aa4753ec0b4f1be3737ac371.jpg)\n\t  \n\t  <span class=\"image\">image</span>\n\t  \n\t  At this time we favour the following component units, with alternatives in brackets.\n\t\t- üü© Open source collaborative space üü© Headset VR integration üü® WebGL interface üü© Minting digital assets (Ordinal then RGB) üü® Digital asset integration and management üü© Large language model MVP üü© Large language model API integration üü© Large language model voice to voice interface üü© Stable diffusion image creation MVP üü© Stable diffusion image creation MVP üü® AutoGPT voice to voice integration MVP üü® Stable diffusion image creation API üü® Nostr social media integration üü® Nostr identity management üü® Nostr machine to machine finacially enabled bots (ubiquitous federating agents) üü® Nostr human programmable semi autonomous economic actors üü© Bitcoin / Lightning / stablecoin stack üü• Bitcoin / Lightning / stablecoin integration üü® Collaborative virtual production MVP üü• Collaborative virtual production integration üü• 3D asset generation with ML\n\t\t- ![image.png](../assets/highlevelstack.png){:width 600}\n\t\t- Collaborative space\n\t\t- Vircadia \\[Omniverse, Open3D foundation, Unreal\\]\n\t\t- Distributed truth\n\t\t- Bitcoin testnet \\[Main net\\]\n\t\t- Digital Objects\n\t\t- Fedimint \\[Ordinals, Pear credits, RGB\\]\n\t\t- Messaging and sync\n\t\t- Nostr\n\t\t- Identity\n\t\t- Nostr \\[Bluesky ION, pubky\\]\n\t\t- Fiat money xfer\n\t\t- Fedimint \\[Pear credits, RGB, Taro main net\\]\n\t\t- Hardware signing\n\t\t- Seed signer \\[any hardware wallet\\]\n\t\t- Small group banking\n\t\t- Fediment \\[chaumian ecash\\]\n\t\t- Local wallet\n\t\t- [Mutiny](https://app.mutinywallet.com/) \\[bitkit, and lightning wallet\\]\n\t\t- Machine learning text\n\t\t- Alpaca \\[ChatGPT etc\\]\n\t\t- Machine learning image\n\t\t- Stable diffusion \\[midjourney, Dall-E\\]\n\t\t- Object tracking\n\t\t- Nostr \\[LnBits accounts\\]\n- ### In camera VFX & telepresence\n\t- Designing open federated metaverse from a 25 year research foundation There are serious and under discussed natural social constraints on group behaviours, and these translate into social VR. For instance the ideal meeting size is 6, and this is naturally established in work settings. This has not translated into a metaverse setting where dozens of people routinely crash across one another. In the context of supporting a creative ‚Äúbackstage‚Äù world where set planning, production shots, etc can be discussed we believe we have solutions which will get the best out of distributed teams of film-makers. Leveraging the world‚Äôs most powerful decentralised computing network to create scale and security without high cost The Bitcoin network is more than just a speculative money like asset, it is the most secure distributed computing system ever built. We can jump on the back of this at almost no cost to enable scale for transfer of value, trust, and digital assets of provenance. Cryptographically assured end points With the cryptography tools provided through integration of the Bitcoin network we can also use non-blockchain based secure messaging, and identity proofs. Micro transactions in collaborative spaces New tooling the space allows fractions of a pound or dollar to be exchanged between parties across the world. This means that work can be paid ‚Äúby the second‚Äù both inside and outside of the metaverse. This radically improves creative microtask workflows. World leading open source machine learning and bot architectures By integrating Stablity AI tools for image generation, video processing, natural language, and speech to text / text to speech we hope to reduce friction within the backstage worlds. Creating a narrative arrow from a remote director/producer/DP, through a VP screen into a shoot, and back into a persistent metaverse shared with the public By linking across these new systems with world class telepresence research we hope to use a single digital context to support senior stakeholders, creatives, technical teams, and the wider public. New paths to monetisation and digital ownership This unified digital back end is optimised for flows of money, trust, and digital objects. This is a new area for VP. Current workstreams:\n\t\t- Storyboarding with text2img and dreambooth to add talent and costume ideas before meeting up, as demonstrated in this document.[@ruiz2022dreambooth]\n\t\t- Collaborative, self hosted, high speed, low detail, economically and cryptographically enabled set design spaces, with near instant language translation (speech to text an speech to speech). Micropayment for cheap international labour. Technology agnostic. Use the screen, audio only, compressed video dial-in, headsets, tablet rendering: (this book).\n\t\t- High end telepresence[@Roberts2015; @OHare2018; @Fairchild2017; @OHare2016] into the studio/shoot from the virtual set, allowing high value stakeholders to be ‚Äòpresent‚Äò on set as virtual collaborants with spatial descrimination allowing directional queues. This involved real time human capture like moveAI or the expensive rigs with DSLRs.\n\t\t- Novel render pipeline for fast turnaround of final look and feel, taking the rough scene and applying img2img ML with the kind of interframe consistency we are starting to see from the video projects.[@anonymous2023phenaki]\n\t\t- Text to model pipeline for interactively building key elements with senior stakeholders, pushed from post ideation the the pre-shoot Unreal content creation.[@poole2022dreamfusion]\n\t\t- All assets switch over to Unreal metaverse and become consistent (optimised) digital set which can be visited by stakeholders, funders, VIPs etc. Public can visit later for a fee? Digital assets can be bought from the set.\n- ##### VisionFlow: Ideate\n\t- Robotic Pre-Visualization\n\t- [[Visionflow]] : Ideate revolutionizes the pre-visualization process in the film industry. The system integrates open-source machine learning tools, robot control software, and AI to streamline and accelerate the creation of virtual 3D environments for new film scenes.\n\t- Instead of the conventional approach, VisionFlow: Ideate enables non-artists to lay out shots in a simple web or headset interface, much like a traditional storyboard. The generative AI then rapidly creates high-resolution backdrop plates with correct parallax cues, replacing conventional image and video plates.\n\t- The camera path synchronizes with a robot, and the backdrop plates are displayed on a 3D wall or in the studio mixdown from a green screen within minutes. The shot can be run repeatedly, allowing for adjustments in lighting and scene swapping for different ideas. This approach aligns well with pre-viz workflows, fostering rapid ideation, horizontal scaling through parallelized cloud vGPU, and expanded access to content creators since less software specialization is required.\n\t- By inverting the conventional ICVFX workflow, VisionFlow: Ideate drives camera motion from the scene rather than scene motion from a tracked camera. It not only saves time and reduces costs but also lowers confusion, streamlining the Unreal creation pipeline, and generating additional revenue and process integration for robotics products.\n- ##### VisionFlow: Connect\n\t- Telepresence System\n- VisionFlow: Connect is a breakthrough system in the film industry that\n  brings remote directors to the heart of production using augmented\n  reality technology. This is achieved through an innovative application\n  of the Apple Vision Pro AR headset.\n- In the VisionFlow: Connect system, the director, located remotely, wears\n  an AR headset and navigates along a marked line. This line mirrors the\n  inward-facing edge of a large-scale, wrap-around LED virtual production\n  facility. Within the LED volume, participants can view the director‚Äôs\n  avatar, providing a sense of spatial consistency and our work\n  interaction, crucial for effective direction.\n- A novel technique, \"ghost frame\" by Helios, is employed to prevent the\n  camera within the LED volume from capturing the director‚Äôs remote avatar\n  on the LED wall. This ensures the director‚Äôs virtual presence doesn‚Äôt\n  interfere with the recorded footage.\n- The benefits of VisionFlow: Connect are multifold. It allows senior\n  stakeholders to manage their time more efficiently as they can direct\n  remotely without needing to be physically present on multiple sets.\n  Directors can interact in real-time, giving instantaneous feedback and\n  adjustments. It also enhances directors‚Äô spatial awareness of the scene,\n  thereby improving the decision-making process.\n- bfSlide 1: Title bfSlide 2: Problem  \n  \"VisionFlow: Revolutionizing Virtual Production with AI and\n  Telecollaboration\" \"The current ICVFX workflow is time-consuming,\n  costly, and requires specialized software knowledge. Remote\n  collaboration in virtual production is challenging, often breaking the\n  flow of communication and limiting the ability to convey spatial\n  intent.\"  \n  bfSlide 3: Solution bfSlide 4: Market Size  \n  \"VisionFlow aims to streamline the virtual production process by\n  integrating open-source machine learning tools and robot control\n  software. This innovative approach inverts the existing ICVFX workflow,\n  allowing rapid ideation, horizontal scaling, and expanded access to\n  content creators. Furthermore, our ghost frame technology enables\n  seamless remote collaboration, allowing remote stakeholders to interact\n  with the set in a spatially coherent way.\" \"The virtual production\n  market is rapidly growing, driven by the increasing demand for\n  high-quality visual effects and the rise of remote work. Our solution\n  targets film studios, independent content creators, and remote\n  collaborators.\"  \n  bfSlide 5: Business Model bfSlide 6: Go-to-Market Strategy  \n  \"We will generate revenue through software licensing, cloud-based\n  services, and professional services for setup and training, and our own\n  in house motion control robotics offering\" \"Our initial focus will be on\n  early adopters in the film industry who are already using virtual\n  production techniques. We will also leverage the open-source Flossverse\n  telecollaboration stack to expand our reach.\"  \n  bfSlide 7: Competitive Landscape bfSlide 8: Team  \n  \"While there are other virtual production solutions on the market, none\n  offer the unique combination of AI-driven scene generation, inverted\n  ICVFX workflow, and seamless remote collaboration that VisionFlow does.\"\n  \"Our team combines expertise in AI, virtual production, and\n  telecollaboration, positioning us uniquely to execute on this vision.\"  \n  bfSlide 9: Financial Projections bfSlide 10: Current Status and\n  Milestones  \n  \"We project rapid growth as we capture a significant share of the\n  expanding virtual production market.\" \"We have already developed an MVP\n  using the Flossverse stack and are now focused on refining the\n  integration and licensing elements of our software.\"  \n  bfSlide 11: Ask bfSlide 12: Closing Remarks  \n  \"We are seeking investment to accelerate our development, expand our\n  team, and bring our innovative solution to market.\" \"In essence,\n  VisionFlow is poised to revolutionize the virtual production industry by\n  leveraging AI to streamline workflows and enable seamless remote\n  collaboration. With your investment, we can bring this vision to\n  life.\"\n-\n- #### visionflow: [[Knowhere]]\n\t- The ultimate goal is to create a seamless, highly personalized visitor experience that evolves and continues before, during, and after a visit to a digital exhibition. This level of personalization is only made possible through the integration of advanced AI technology, biometrics, and a deep inferred understanding of individual preferences and behaviours.\n\t\t- ##### Key Ideas\n\t\t- 1.  **Leveraging AI and Contextual Data:** The venue will use AI and contextual data to create dynamic narratives and activities tailored to each visitor in real-time. This will revolutionize the resort experience, making it highly personalized and immersive. However, the implementation of AI must be mindful of privacy concerns and be done in a way that respects the data sovereignty of the guests.\n\t\t- 2.  **Tailored Personalization:** Visitors should have the ability to opt into different levels of personalization. Some may want a fully immersive, personalized experience, while others may prefer a more ‚Äòhands off‚Äô experience. This is an important aspect of respecting individual preferences and ensuring that all visitors feel comfortable and catered for.\n\t\t- 3.  **Communication Devices:** Various communication devices could be utilized within the resort to facilitate interactions between visitors and the AI system. These could include badges, wands, glasses, headphones, etc. Each of these devices would contribute to the immersion and thematic consistency of the resort while serving a practical purpose.\n\t\t- 4.  **Biometrics:** The use of biometrics such as gaze tracking and gesture recognition could allow the AI to understand visitor preferences passively. This technology could be incorporated in a non-intrusive way to augment the guest experience without breaching privacy.\n\t\t- 5.  **Data Extraction:** Visitors should have the ability to extract their distilled data or creations, enabling them to continue their vistor experience at home. This could also open up new possibilities for visitors to create and share their own narratives based on their visit experiences. To be clear this should not be the raw data supplied to the venue inferencing engines (which should be destroyed soon after use), but rather a distilled narrative of the inference from the system.\n\t\t- 6.  **Data Privacy:** Data sharing should be underpinned by robust privacy controls to ensure guest data sovereignty. It‚Äôs crucial to maintain the trust of the visitors by demonstrating a strong commitment to privacy. This should be externally audited on a regular cadence.\n\t\t- 7.  **Continuous Experience:** The visitor experience should feel continuous before, during, and after the visit. However, it‚Äôs important to manage guest expectations and avoid over promising pre-visit AI interactions. Ensuring a smooth transition between these stages will enhance the overall guest experience.\n\t\t- 8.  **Hyper-Personalization:** Hyper-personalization should span the venue. This level of detail will ensure each guest has a unique and highly personalized experience.\n\t\t- 9.  **Adaptive and Immersive Experiences:** The core aim should be to craft continuously adaptive and immersive experiences based on visitor needs and implied preferences. By doing so, the venue can ensure each visitor has a unique, enjoyable, and highly memorable experience, supportive of return visits.\n\t\t- The integration of these concepts will require careful planning and\n\t\t  execution, but the result could be a venue experience like no other, one\n\t\t  that caters to each individual guests and provides an experience that\n\t\t  extends beyond the confines of the experience itself.\n- ##### Multiview barrier lenticular\n\t- ##### Background\n\t\t- Ubiquitous display technology, which allows different personalized views\n\t\t  for multiple people on the same screen, has the potential to disrupt the\n\t\t  way visitors interact and experience venues and exhibits. The displays\n\t\t  can use techniques like lenticular lenses, or other steerable light, to\n\t\t  send different light to viewers‚Äô eyes, allowing for discrete, customized\n\t\t  views.\n\t- ##### Technical Overview\n\t\t- The following display technologies have been identified as suitable for\n\t\t  implementation:\n\t\t- Lenticular lens arrays: By placing an array of magnifying lenses over the screen, these displays direct light from alternating columns of pixels toward the left and right eyes to create a stereoscopic 3D image without glasses. There are several suppliers of this technology, mainly for the events market. It seems that churn of these companies is relatively high, with few demonstrating longevity.\n\t\t- Parallax barriers: These displays have a layer of opaque and transparent slits over the LCD matrix that directs different pixel columns to each eye, creating a stereoscopic 3D image without glasses. Alioscopy is known to use this approach, along with eye tracking technology. They have been in business for decades and are a good case study, but engaging with a research partner in China is likely the best medium terms approach.\n\t\t- These display consists of a large lenticular lens sheet or array of smaller tiled lenticular lenses mounted in front of a high-resolution LED. The lenticular lenses are cylindrical and arranged vertically, with each lens covering multiple pixel columns of the display.\n\t\t- Behind the lens array, the display content is formatted into vertical interleaved channels, with each channel containing a slightly different perspective view of the 3D stereoscopic image. The different perspective views are calculated in real-time based on the tracked head positions of multiple viewers in front of the display.\n\t\t- As light from the display pixels passes through the cylindrical lenses, it is refracted into multiple viewing zones in front of the screen. Each viewing zone contains a specific view channel, so each eye of each viewer sees the perspective that matches their position. This creates a glasses-free 3D effect with motion parallax as viewers move their heads.\n\t\t- The viewer head tracking system uses camera and computer vision techniques to determine the 3D positions of each viewer‚Äôs eyes in the space in front of the display. The changing viewer positions are fed to the display rendering system to compute the proper perspective views and adjust the lenticular flaps as needed.\n\t\t- This lenticular 3D display with dynamic view steering provides illusion of depth for multiple viewers simultaneously, creating an immersive large-screen 3D experience without the need for special glasses. The real-time tracking and rendering system updates the content smoothly as the viewers move around, maintaining the stereo 3D perspectives tailored individually to each viewer‚Äôs changing position.\n- ##### Tracking Technologies\n\t- For personalization, tracking viewers‚Äô eyes, face, gestures, etc., is necessary. This can be done with cameras and computer vision algorithms, employing techniques like mesh abstraction for body tracking, facial landmark recognition, gaze estimation, micro expression recognition, and gross gesture detection.\n- ##### AI Integration\n\t- AI can be integrated to steer personalized narratives and experiences subtly in the background or provide interactive moments. The AI backend can use game engines like Unreal Engine or Unity to render personalized content dynamically, allowing for real-time adaptation to the viewer‚Äôs reactions.\n- ##### Privacy and Security\n\t- The tracking data provides extremely valuable insights for personalizing experiences but raises significant privacy concerns. Thoughtful design around privacy and security, including data segmentation, auditing, and transparency, is critical to protect user data and ensure compliance with privacy regulations.\n- ##### Technical Challenges\n\t- There are technical challenges in achieving dense personalized displays, especially for a large number of viewers. As of now, creating a personalized display for up to 5 people is feasible, but scaling up requires a substantial budget and careful planning. Fortunately both of these seem available and it seems timely to look at this option.\n- ##### Proof of Concept\n\t- Starting with a small-scale proof of concept for up to 5 people would allow for demonstration of the capabilities and building stakeholder confidence. This would also provide valuable insights into the technical and logistical challenges that may arise during larger-scale implementation.\n- ##### Future Developments\n\t- The display technology is rapidly evolving, with new advancements in resolution, refresh rates, brightness, and tracking accuracy. As the technology matures, there will be more opportunities to enhance the personalized experiences. This system would allow multiple viewers to see different images or perspectives from the same display, enhancing the interactive and educational value of the exhibit. Mollick et al. have done some lovely actionable work on the pedagogical implications of chatbots.[@mollick2022new; @mollick2023assigning; @mollick2023using] This could transform the way visitors engage with exhibits, providing a more immersive and personalized experience.\n- ##### Alioscopy\n\t- Alioscopy uses a different approach than lenticular lenses for their glasses-free 3D displays. Their screens contain a parallax barrier\n\t\t- a layer of opaque and transparent slits\n\t\t- over the LCD matrix. This directs different pixel columns to each eye, creating a stereoscopic 3D image without glasses.\n\t- Their displays also incorporate proprietary eye tracking technology. An infrared camera follows the viewer‚Äôs head position, automatically adjusting the angle of the projected 3D image for optimal viewing. This compensates for display viewing angle limitations.\n\t- Alioscopy‚Äôs recent prototypes feature very high resolution like 4K and 8K to improve 3D image quality. Their barriers and tracking algorithms are precisely tuned to the display characteristics and desired viewing parameters.\n- ##### Pitch section\n- Personalised emergent narratives for our visitors. What problem does the\n  user, business or industry have that you want to solve?\n  \n  [[Visionflow]] : [[Knowhere]]\n- For today‚Äôs digital experience venue managers navigating the\n  complexities of providing unique experiences, our AI solution, KnoWhere,\n  offers a unique approach which will result in the capability to enhance\n  visitor experiences. By utilising images from on-premise cameras, we\n  enable to leverage data on visitors attention. Our solution‚Äôs unique\n  value propositions include spatial and attention tracking through AI,\n  because of our ability to understand the needs of experience designers.\n- It works like this: Combine personal data, with visitor gaze Provide\n  location and attention data stream Venue provides this to experience\n  designers Designers build incredible emergent journeys\n- We believe this solution will impact our business/industry by: Elevating\n  interactions through personalisation Making attention in physical spaces\n  quantifiable Providing feedback data to experience designers\n- We will measure our impact by: Performing A/B testing on visitors\n  engagement This can be a KPI that changes, ex: a productivity score\n\t- or\n\t  it can be an amount saved because of the soluion\n- Describe what data is behind this AI model? Alphapose (2) Insightface\n  (5)\n  \n  Rate the quality/quantity of each point of data from 1-5 (1 being little\n  data / low quality ‚Äì 5 being lots of data / high quality)\n  \n  What will be the biggest challenge in implementing the AI model? Real\n  time pose engine is noncommercial Occlusion can be tricky with space\n  constraints The rich dataset is a privacy concern\n  \n  Here are some areas to think about in terms of challenges:\n  \n  Data: How much data exists? How representative is it of what we‚Äôre\n  trying to model? Are there issues in how it is collected which could\n  impact the model? Is it likely to contain any missing values? Adoptance\n  from users/customers Will it be easy to get people to use the AI in\n  their business?\n  \n  Governance Is the data accessible and are you allowed to use it? Who is\n  responsible once the AI model is in use? How will make the final\n  decisions?\n  \n  Impact of solution What do we know about the need for this type of\n  solution\n\t- is it nice to have or need to have? Can we find out if we\n\t  don‚Äôt know? Feasibility What will be the biggest challenge in\n\t  implementing an AI solution to solve this problem? Can the issue /\n\t  problem we‚Äôre solving actually be measured / forecasted?\n\t  \n\t  Ethics\n\t  \n\t  Regulations\n\t  \n\t  Cybersecurity\n\t  \n\t  \"Our goal is to empower venue owners to provide an advanced platform\n\t  that allows world class exhibition creators to tailor unique experiences\n\t  for each visitor. This enables the crafting of rich, interconnected\n\t  stories for groups of people, all while ensuring unforgettable, safe\n\t  experiences for individuals and families.\n- ### Accessible metaverse for pre-viz\n- Pre-visualization (or \"pre-viz\") is a process in which a rough\n  simulation of a visual effect or scene is created prior to its actual\n  production. In the context of LED wall virtual production, pre-viz\n  refers to the creation of a 3D representation of a virtual environment,\n  including the placement of cameras, actors, and other elements, that is\n  then used to plan and test the visual effects and lighting for a\n  live-action scene that will eventually be shot in front of an LED wall.\n- The pre-viz process allows filmmakers and visual effects artists to\n  experiment with different camera angles, lighting, and visual effects\n  before committing to a final version. This helps to save time and\n  resources during actual production by reducing the need for multiple\n  takes or re-shoots. Additionally, it allows the filmmakers to see how\n  the final product will look before committing to it, which can help to\n  avoid costly mistakes or changes down the line.\n- The LED wall virtual production process typically involves using a\n  combination of 3D animation software, motion capture technology, and\n  real-time rendering to create a virtual environment that accurately\n  reflects the physical environment in which the scene will be shot. The\n  pre-viz process is then used to plan and test the various visual\n  effects, lighting, and camera angles that will be used in the final\n  production.\n- Our collaborative software stack is potentially ideally suited to some\n  of this pre-viz work, especially when combined with the power of machine\n  learning, and live linked into Unreal so that changes by stakeholders\n  enter the pre-production pipeline in a seamless way.\n- ### Novel VP render pipeline\n\t- Putting the ML image generation on the end of a real-time tracked camera render pipeline might remove the need for detail in set building. To describe how this might work, the set designer, DP, director, etc will be able to ideate in a headset based metaverse of the set design, dropping very basic chairs, windows, light sources whatever. There is -no need- then to create a scene in detail. If the interframe consistency (img2img) can deliver then the output on the VP screen can simply inherit the artistic style from the text prompts, and render production quality from the basic building blocks. Everyone in the set (or just DP/director) could then switch in headset to the final output and ideate (verbally) to create the look and feel (lens, bokeh, light, artistic style etc). This isn‚Äôt ready yet as the frames need to generate much faster (100x), but it‚Äôs very likely coming in months not years. This ‚Äúnext level pre-vis‚Äù is being trailed in the Vircadia collaborative environment described in this book, and can be seen illustrated in Figure <a href=\"#fig:vircadiasd\" data-reference-type=\"ref\" data-reference=\"fig:vircadiasd\">10.1</a>.\n\t  \n\t  <span class=\"image\">Top panel is a screen grab from Vircadia and the bottom panel is a quick pass through img2img from Stable Diffusion.</span>\n\t- This can be done now through the use of camera robots. A scene can be built in basic outline, the camera tracks can be encoded into the robot, and the scene can be rapidly post rendered by Stability with high inter frame consistency.\n\t- With the help of AI projects such as [LION](https://nv-tlabs.github.io/LION/) it may be possible to pass simple geometry and instructions to ML systems which can create complex textured geometry back into the scene.\n\t- <span class=\"image\">Robot VP</span>\n- ### Money in metaverses\n\t- ##### Global collaboration and remuneration\n\t\t- In the book ‚ÄúGhosts of my life‚Äù[@fisher2014ghosts] Fisher asserts that\n\t\t  there has been a slowing, even a ‚Äòcancellation‚Äô of creative progress in\n\t\t  developed societies, their art, and their media. His contention is that\n\t\t  neoliberalism itself is to blame. He says  \n\t\t  it‚ÄúIt is the contention of this book that 21st-century culture is marked\n\t\t  by the same anachronism and inertia which afflicted Sapphire and Steel\n\t\t  in their final adventure. But this status has been buried, interred\n\t\t  behind a superficial frenzy of ‚Äònewness‚Äô, of perpetual movement. The\n\t\t  ‚Äòjumbling up of time‚Äô, the montaging of earlier eras, has ceased to be\n\t\t  worthy of comment; it is now so prevalent that it is no longer even\n\t\t  noticed.‚Äù\n\t\t- It is the feeling of the authors of this book that the creative and\n\t\t  inspirational efforts of the whole world may be needed to heal these\n\t\t  deep wounds. It is possible that by connecting creatives with very\n\t\t  different global perspectives, directly into ‚ÄòWestern‚Äô production\n\t\t  pipelines, that we will be able to see the shape of this potential.\n- #### ML actors and blockchain based bots\n\t- Stablity AI is an open source imitative to bring ML/AL capabilities to the world. This is a hugely exciting emergent area and much more will be developed here.\n- #### AI economic actors in mixed reality\n\t- AI actors can now be trusted visually.[@nightingale2022ai] We have some thinking on this which links the external web to our proposed metaverse. There is work in the community working on economically empowered bots which leverage Nostr and RGB to perform functions within our metaverse, and outside in the WWW, as well as interacting economically through trusted cryptography with other bots, anywhere, and human participants, anywhere. This is incredibly powerful and is assured by the Bitcoin security model. Imagine being able to interact with a bot flower seller representing all the real world florists it had found. In the metaverse you could handle the flowers and take advice and guidance from the bot agent, then it would be able to take your money to buy you flowers to send to a real world address, and later find you to tell you when it‚Äôs delivered. These possibilities are endless. The AI chat element, the AI translation of images on websites to 3D assets in the Metaverse are difficult but possible challenges, but the secure movement of money from the local context in the metaverse to the real world is within reach using these bots, and they are completely autonomous and distributed.\n- ### Our socialisation best practice\n\t- ##### Identity\n\t\t- We will base our identity and object management on Nostr public/private\n\t\t  key pairs. The public key of these enable lightning based exchange of\n\t\t  value globally. we plan to operate Nostr in multiple modes. Linking\n\t\t  flossverse ‚Äúrooms‚Äù will be a Nostr bot to bot system within the private\n\t\t  relay mesh. This can also synchronise large amounts of data by\n\t\t  leveraging torrents [negotiated by Nostr](https://iris.to/#/settings).\n\t\t  Human to human text chat across and within instances is two ‚Äôtypes‚Äô kind\n\t\t  of private nostr tag within the private relays mesh. External\n\t\t  connectivity to web and nostr apps is just the public relay tags\n\t\t  outbound. We don‚Äôt need to store data external to the flossverse system,\n\t\t  though access is obviously possible through the same torrent network.\n\t- ##### Webs of trust\n\t\t- Webs of trust will be built within worlds using economically costly (but\n\t\t  private) social rating systems, between any actor, human or AI. It\n\t\t  should be too costly to attack an individual aggressively. This implies\n\t\t  an increased weighting for scores issued in short time periods. Poorly\n\t\t  behaving AI‚Äôs will eventually be excluded through lack of funds.\n\t- ##### Integration of ‚Äôgood‚Äô actor AI entities\n\t\t- Gratitude practice should be encouraged between AI actors to foster\n\t\t  trust and wellbeing in human observers. ‚ÄúIt‚Äôs nice to be nice‚Äù should be\n\t\t  incentivised between all parties‚Äù. This could include tipping and trust\n\t\t  nudging through the social rating system. Great AI behaviour would\n\t\t  result in economically powerful entities.\n\t- #### Emulation of important social cues\n\t\t- [Classroom layout](https://www.cleverclassroomsdesign.co.uk/general-5)\n\t- ##### Behaviour incentives, arbitration, and penalties\n\t\t- Collapses of trust and abuse will trigger flags from ML based oversight,\n\t\t  which will create situational records and payloads of involved parties\n\t\t  to unlock with their nostr private keys. ML red flagged actors will be\n\t\t  finacially penalised but have access to human arbitration using their\n\t\t  copy of the data blob. Nothing will be stored except by the end users.\n\t- #### Federations of webs of trust and economics\n\t\t- Nostr is developing fast enough to provide global bridges between\n\t\t  metaverse instances.\n- ### Security evaluation\n\t- As part of developing our stack we will penetration test the deployment as detailed using [Hexway](https://hexway.io/)\n- ### notes for later\n\t- Notes on build-out The world database in the shared rooms in the metaverse is the global object master, educational materials, videos, audio content and branded objects are fungible tokens authentically proved by rgb client side validation between parties, only validated ones will be persisted in shared rooms like conferences and classes according to the room schema. That allows educators to monetise their content. That can work on lightning. NFT objects between parties like content crafted by participants (coursework, homework) are not on lightning and will attract main chain fees but are rarer. User authentication and communication will be through nostr.\n\t  \n\t  <span class=\"image\">image</span>\n\t  \n\t  ![image](../assets/707beb139883d3b15e01fd447eb2ceb747861560.png)\n-\n- # NVIDIA Omniverse design\n\t- **Phase 1: Foundational Infrastructure**\n\t- **Bitcoin Base Layer (NixOS):**\n\t\t- Set up a secure and reliable Bitcoin full node on NixOS.\n\t\t- Implement robust backup and recovery procedures.\n\t\t- Consider running a Lightning Network node for faster and cheaper transactions.\n\t- **Identity and Value Management:**\n\t\t- Integrate Nostr protocol for decentralized identity and messaging.\n\t\t- Develop or utilize existing libraries for Nostr event creation, signing, and relaying.\n\t\t- Implement BIP85 hierarchical deterministic wallets for secure key management.\n\t- **Digital Assets (RGB):**\n\t\t- Choose or design appropriate RGB schemas for the types of digital assets you want to support.\n\t\t- Develop or utilize tools for issuing and managing RGB assets.\n\t\t- Integrate RGB wallets with the overall wallet management system.\n\t\t  \n\t\t  **Phase 2: Interaction Module (Omniverse):**\n\t- **Omniverse Environment Setup:**\n\t\t- Deploy an Omniverse Nucleus server to manage collaborative scenes and 3D assets.\n\t\t- Design and create the initial 3D environment(s) using USD (Universal Scene Description).\n\t\t- Consider incorporating elements from your existing visualizations and research.\n\t- **Agent Integration:**\n\t\t- Develop avatar systems for both human and AI agents within Omniverse.\n\t\t- Implement controls and interactions for agents within the 3D environment.\n\t\t- Explore the use of Omniverse Kit SDK for advanced features and customizations.\n\t- **Digital Asset Integration:**\n\t\t- Develop methods to represent and interact with RGB digital assets within Omniverse scenes.\n\t\t- Implement ownership and transfer functionalities based on the underlying Bitcoin/RGB infrastructure.\n\t\t- Explore visual representations of ownership and asset metadata within the 3D environment.\n\t\t  \n\t\t  **Phase 3: AI and Governance:**\n\t- **AI Agent Development:**\n\t\t- Choose or design AI models for different agent archetypes (e.g., governance agents, task agents, social agents).\n\t\t- Implement the D&D-inspired personality system and the wealth decay function.\n\t\t- Develop AI behaviors and decision-making processes aligned with the scene schema.\n\t- **Scene Schema and Governance:**\n\t\t- Define the rules and constraints for different scene types within a flexible schema framework.\n\t\t- Implement the SupraAgent (governance LLM) with its monitoring and evidence collection capabilities.\n\t\t- Develop mechanisms for encrypted evidence payloads and communication with relevant parties.\n\t- **GenAI Integration:**\n\t\t- Explore the use of generative AI models (e.g., ChatGPT, Stable Diffusion) for content creation, world-building, and immersive storytelling.\n\t\t- Develop interfaces for users and AI agents to interact with GenAI tools within the metaverse.\n\t\t  \n\t\t  **Phase 4: User Interface and Experience:**\n\t- **Nostr-based Chat Interface:**\n\t\t- Develop a chat interface using Nostr as the communication protocol.\n\t\t- Integrate the chat interface within the Omniverse environment.\n\t\t- Enable secure and private communication between agents.\n\t- **Wallet Integration:**\n\t\t- Provide users with access to their digital wallets within the metaverse.\n\t\t- Enable users to manage their assets, view transaction history, and interact with the virtual economy.\n\t- **Accessibility and Multimodality:**\n\t\t- Explore ways to make the metaverse experience accessible to users with disabilities.\n\t\t- Support multiple interaction modalities (e.g., VR, AR, desktop, mobile).\n\t\t  \n\t\t  **Additional Considerations:**\n\t- **Security:**¬†Implement robust security measures at all levels, including encryption, access control, and regular security audits.\n\t- **Privacy:**¬†Ensure user privacy by minimizing data collection and providing transparent privacy settings.\n\t- **Scalability:**¬†Design the system to be scalable to accommodate a growing number of users and increasing complexity.\n\t- **Community Building:**¬†Foster a strong community around your metaverse project through open communication, collaboration, and user engagement initiatives.\n- # According to Gemini\n\t- **Phase¬†1:¬†Foundational¬†Infrastructure**\n\t\t- **Bitcoin¬†Base¬†Layer¬†(NixOS):**\n\t\t- **Set¬†up¬†a¬†secure¬†and¬†reliable¬†Bitcoin¬†full¬†node¬†on¬†NixOS.**\n\t\t- **Implement¬†robust¬†backup¬†and¬†recovery¬†procedures.**\n\t\t- **Consider¬†running¬†a¬†Lightning¬†Network¬†node¬†for¬†faster¬†and¬†cheaper¬†transactions.**\n\t\t- **Identity¬†and¬†Value¬†Management:**\n\t\t- **Integrate¬†Nostr¬†protocol¬†for¬†decentralized¬†identity¬†and¬†messaging.**\n\t\t- **Develop¬†or¬†utilize¬†existing¬†libraries¬†for¬†Nostr¬†event¬†creation,¬†signing,¬†and¬†relaying.**\n\t\t- **Implement¬†BIP85¬†hierarchical¬†deterministic¬†wallets¬†for¬†secure¬†key¬†management.**\n\t\t- **Digital¬†Assets¬†(RGB):**\n\t\t- **Choose¬†or¬†design¬†appropriate¬†RGB¬†schemas¬†for¬†the¬†types¬†of¬†digital¬†assets¬†you¬†want¬†to¬†support.**\n\t\t- **Develop¬†or¬†utilize¬†tools¬†for¬†issuing¬†and¬†managing¬†RGB¬†assets.**\n\t\t- **Integrate¬†RGB¬†wallets¬†with¬†the¬†overall¬†wallet¬†management¬†system.**\n\t\t   \n\t\t   **Phase¬†2:¬†Interaction¬†Module¬†(Omniverse):**\n\t\t- **Omniverse¬†Environment¬†Setup:**\n\t\t- **Deploy¬†an¬†Omniverse¬†Nucleus¬†server¬†to¬†manage¬†collaborative¬†scenes¬†and¬†3D¬†assets.**\n\t\t- **Design¬†and¬†create¬†the¬†initial¬†3D¬†environment(s)¬†using¬†USD¬†(Universal¬†Scene¬†Description).**\n\t\t- **Consider¬†incorporating¬†elements¬†from¬†your¬†existing¬†visualizations¬†and¬†research.**\n\t\t- **Agent¬†Integration:**\n\t\t- **Develop¬†avatar¬†systems¬†for¬†both¬†human¬†and¬†AI¬†agents¬†within¬†Omniverse.**\n\t\t- **Implement¬†controls¬†and¬†interactions¬†for¬†agents¬†within¬†the¬†3D¬†environment.**\n\t\t- **Explore¬†the¬†use¬†of¬†Omniverse¬†Kit¬†SDK¬†for¬†advanced¬†features¬†and¬†customizations.**\n\t\t- **Digital¬†Asset¬†Integration:**\n\t\t- **Develop¬†methods¬†to¬†represent¬†and¬†interact¬†with¬†RGB¬†digital¬†assets¬†within¬†Omniverse¬†scenes.**\n\t\t- **Implement¬†ownership¬†and¬†transfer¬†functionalities¬†based¬†on¬†the¬†underlying¬†Bitcoin/RGB¬†infrastructure.**\n\t\t- **Explore¬†visual¬†representations¬†of¬†ownership¬†and¬†asset¬†metadata¬†within¬†the¬†3D¬†environment.**\n\t\t   \n\t\t   **Phase¬†3:¬†AI¬†and¬†Governance:**\n\t\t- **AI¬†Agent¬†Development:**\n\t\t- **Choose¬†or¬†design¬†AI¬†models¬†for¬†different¬†agent¬†archetypes¬†(e.g.,¬†governance¬†agents,¬†task¬†agents,¬†social¬†agents).**\n\t\t- **Implement¬†the¬†D&D-inspired¬†personality¬†system¬†and¬†the¬†wealth¬†decay¬†function.**\n\t\t- **Develop¬†AI¬†behaviors¬†and¬†decision-making¬†processes¬†aligned¬†with¬†the¬†scene¬†schema.**\n\t\t- **Scene¬†Schema¬†and¬†Governance:**\n\t\t- **Define¬†the¬†rules¬†and¬†constraints¬†for¬†different¬†scene¬†types¬†within¬†a¬†flexible¬†schema¬†framework.**\n\t\t- **Implement¬†the¬†SupraAgent¬†(governance¬†LLM)¬†with¬†its¬†monitoring¬†and¬†evidence¬†collection¬†capabilities.**\n\t\t- **Develop¬†mechanisms¬†for¬†encrypted¬†evidence¬†payloads¬†and¬†communication¬†with¬†relevant¬†parties.**\n\t\t- **GenAI¬†Integration:**\n\t\t- **Explore¬†the¬†use¬†of¬†generative¬†AI¬†models¬†(e.g.,¬†ChatGPT,¬†Stable¬†Diffusion)¬†for¬†content¬†creation,¬†world-building,¬†and¬†immersive¬†storytelling.**\n\t\t- **Develop¬†interfaces¬†for¬†users¬†and¬†AI¬†agents¬†to¬†interact¬†with¬†GenAI¬†tools¬†within¬†the¬†metaverse.**\n\t\t   \n\t\t   **Phase¬†4:¬†User¬†Interface¬†and¬†Experience:**\n\t\t- **Nostr-based¬†Chat¬†Interface:**\n\t\t- **Develop¬†a¬†chat¬†interface¬†using¬†Nostr¬†as¬†the¬†communication¬†protocol.**\n\t\t- **Integrate¬†the¬†chat¬†interface¬†within¬†the¬†Omniverse¬†environment.**\n\t\t- **Enable¬†secure¬†and¬†private¬†communication¬†between¬†agents.**\n\t\t- **Wallet¬†Integration:**\n\t\t- **Provide¬†users¬†with¬†access¬†to¬†their¬†digital¬†wallets¬†within¬†the¬†metaverse.**\n\t\t- **Enable¬†users¬†to¬†manage¬†their¬†assets,¬†view¬†transaction¬†history,¬†and¬†interact¬†with¬†the¬†virtual¬†economy.**\n\t\t- **Accessibility¬†and¬†Multimodality:**\n\t\t- **Explore¬†ways¬†to¬†make¬†the¬†metaverse¬†experience¬†accessible¬†to¬†users¬†with¬†disabilities.**\n\t\t- **Support¬†multiple¬†interaction¬†modalities¬†(e.g.,¬†VR,¬†AR,¬†desktop,¬†mobile).**\n\t\t   \n\t\t   **Additional¬†Considerations:**\n\t\t- **Security:**¬†Implement¬†robust¬†security¬†measures¬†at¬†all¬†levels,¬†including¬†encryption,¬†access¬†control,¬†and¬†regular¬†security¬†audits.\n\t\t- **Privacy:**¬†Ensure¬†user¬†privacy¬†by¬†minimizing¬†data¬†collection¬†and¬†providing¬†transparent¬†privacy¬†settings.\n\t\t- **Scalability:**¬†Design¬†the¬†system¬†to¬†be¬†scalable¬†to¬†accommodate¬†a¬†growing¬†number¬†of¬†users¬†and¬†increasing¬†complexity.\n\t\t- **Community¬†Building:**¬†Foster¬†a¬†strong¬†community¬†around¬†your¬†metaverse¬†project¬†through¬†open¬†communication,¬†collaboration,¬†and¬†user¬†engagement¬†initiatives.\n\t\t  \n\t\t  **This¬†development¬†plan¬†provides¬†a¬†roadmap¬†for¬†implementing¬†your¬†metaverse¬†vision,¬†step¬†by¬†step.¬†By¬†focusing¬†on¬†the¬†core¬†principles¬†of¬†your¬†research¬†and¬†leveraging¬†innovative¬†technologies¬†like¬†Bitcoin,¬†RGB,¬†Nostr,¬†and¬†Omniverse,¬†you¬†are¬†building¬†a¬†foundation¬†for¬†a¬†truly¬†unique¬†and¬†transformative¬†metaverse¬†experience.**\n\n## Current Landscape (2025)\n\n- Industry adoption and implementations\n  - Metaverse platforms continue to evolve with focus on interoperability and open standards\n  - Web3 integration accelerating with decentralised identity and asset ownership\n  - Enterprise adoption growing in virtual collaboration, training, and digital twins\n  - UK companies increasingly active in metaverse development and immersive technologies\n\n- Technical capabilities\n  - Real-time rendering at photorealistic quality levels\n  - Low-latency networking enabling seamless multi-user experiences\n  - AI-driven content generation and procedural world building\n  - Spatial audio and haptics enhancing immersion\n\n- UK and North England context\n  - Manchester: Digital Innovation Factory supports metaverse startups and research\n  - Leeds: Holovis leads in immersive experiences for entertainment and training\n  - Newcastle: University research in spatial computing and interactive systems\n  - Sheffield: Advanced manufacturing using digital twin technology\n\n- Standards and frameworks\n  - Metaverse Standards Forum driving interoperability protocols\n  - WebXR enabling browser-based immersive experiences\n  - glTF and USD for 3D asset interchange\n  - Open Metaverse Interoperability Group defining cross-platform standards\n\n## Metadata\n\n- **Last Updated**: 2025-11-16\n- **Review Status**: Automated remediation with 2025 context\n- **Verification**: Academic sources verified\n- **Regional Context**: UK/North England where applicable",
  "properties": {
    "id": "product-design-owl-axioms",
    "collapsed": "true",
    "- ontology": "true",
    "- term-id": "mv-15873194761",
    "- preferred-term": "Product Design",
    "- source-domain": "metaverse",
    "- status": "active",
    "- public-access": "true",
    "- definition": "A component of the metaverse ecosystem focusing on product design.",
    "- maturity": "mature",
    "- authority-score": "0.85",
    "- owl:class": "mv:ProductDesign",
    "- owl:physicality": "ConceptualEntity",
    "- owl:role": "Concept",
    "- belongsToDomain": "[[MetaverseDomain]]",
    "- enables": "[[ImmersiveExperience]], [[Presence]], [[SpatialComputing]]",
    "- requires": "[[DisplayTechnology]], [[TrackingSystem]], [[RenderingEngine]]",
    "- bridges-to": "[[HumanComputerInteraction]], [[ComputerVision]], [[Robotics]]",
    "public": "true"
  },
  "backlinks": [
    "Product and Risk Management",
    "Segmentation and Identification",
    "Speech and voice"
  ],
  "wiki_links": [
    "ComputerVision",
    "Presence",
    "MetaverseDomain",
    "TrackingSystem",
    "DisplayTechnology",
    "RenderingEngine",
    "ImmersiveExperience",
    "Robotics",
    "Visionflow",
    "Knowhere",
    "HumanComputerInteraction",
    "SpatialComputing"
  ],
  "ontology": {
    "term_id": "mv-15873194761",
    "preferred_term": "Product Design",
    "definition": "A component of the metaverse ecosystem focusing on product design.",
    "source_domain": "metaverse",
    "maturity_level": null,
    "authority_score": 0.85
  }
}