{
  "title": "Fairness Constraints",
  "content": "- ### OntologyBlock\n  id:: 0382-fairness-constraints-ontology\n  collapsed:: true\n\n  - **Identification**\n    - public-access:: true\n    - ontology:: true\n    - term-id:: AI-0382\n    - preferred-term:: Fairness Constraints\n    - source-domain:: ai\n    - status:: in-progress\n    - version:: 1.0\n    - last-updated:: 2025-10-29\n\n  - **Definition**\n    - definition:: Fairness Constraints are mathematical formalizations of equitable treatment in AI systems, expressed as conditions that predictions must satisfy relative to protected attributes. These constraints are categorized into three fundamental types based on independence criteria: Independence (demographic parity) requires predictions to be independent of protected attributes (Ŷ ⊥ A), meaning P(Ŷ|A=0) = P(Ŷ|A=1); Separation (equalized odds) requires predictions to be independent of protected attributes conditional on true labels (Ŷ ⊥ A | Y), ensuring equal true positive and false positive rates across groups; and Sufficiency (predictive parity) requires true labels to be independent of protected attributes conditional on predictions (Y ⊥ A | Ŷ), ensuring equal precision and calibration across groups. These constraints formalize fairness concepts like demographic parity, equalized odds, equal opportunity (separation for positive class only), and calibration into optimization problems during model training. However, impossibility theorems (Chouldechova 2017, Kleinberg et al. 2017) prove that when base rates differ between groups, certain combinations of fairness constraints cannot be simultaneously satisfied, necessitating context-dependent tradeoffs. Implementation typically involves constrained optimization with Lagrange multipliers, where accuracy loss is balanced against fairness violations through tunable regularization parameters, as formalized in foundational research by Hardt et al. (2016) and Barocas et al. (2019).\n    - maturity:: mature\n    - source:: [[Hardt et al. (2016)]], [[Barocas et al. (2019)]], [[Chouldechova (2017)]]\n    - authority-score:: 0.95\n\n  - **Semantic Classification**\n    - owl:class:: aigo:FairnessConstraints\n    - owl:physicality:: VirtualEntity\n    - owl:role:: Process\n    - owl:inferred-class:: aigo:VirtualProcess\n    - belongsToDomain:: [[AIEthicsDomain]]\n    - implementedInLayer:: [[ConceptualLayer]]\n\n  - #### Relationships\n    id:: 0382-fairness-constraints-relationships\n\n  - #### OWL Axioms\n    id:: 0382-fairness-constraints-owl-axioms\n    collapsed:: true\n    - ```clojure\n      (Declaration (Class :FairnessConstraint))\n(SubClassOf :FairnessConstraint :MathematicalConstraint)\n(SubClassOf :FairnessConstraint :EthicalRequirement)\n\n(AnnotationAssertion rdfs:label :FairnessConstraint\n  \"Fairness Constraint\"@en)\n(AnnotationAssertion rdfs:comment :FairnessConstraint\n  \"Mathematical definitions of algorithmic fairness including independence (demographic parity), separation (equalized odds), and sufficiency (predictive parity) constraints.\"@en)\n(AnnotationAssertion :dcterms:source :FairnessConstraint\n  \"Barocas et al. (2019), Hardt et al. (2016), Chouldechova (2017)\")\n\n;; Object Properties\n(Declaration (ObjectProperty :constrainsModel))\n(ObjectPropertyDomain :constrainsModel :FairnessConstraint)\n(ObjectPropertyRange :constrainsModel :MLModel)\n\n(Declaration (ObjectProperty :requiresIndependence))\n(ObjectPropertyDomain :requiresIndependence :FairnessConstraint)\n(ObjectPropertyRange :requiresIndependence :RandomVariable)\n\n(Declaration (ObjectProperty :enforcesSeparation))\n(ObjectPropertyDomain :enforcesSeparation :FairnessConstraint)\n(ObjectPropertyRange :enforcesSeparation :ConditionalDistribution)\n\n;; Data Properties\n(Declaration (DataProperty :hasMathematicalFormulation))\n(DataPropertyDomain :hasMathematicalFormulation :FairnessConstraint)\n(DataPropertyRange :hasMathematicalFormulation xsd:string)\n\n(Declaration (DataProperty :isRelaxable))\n(DataPropertyDomain :isRelaxable :FairnessConstraint)\n(DataPropertyRange :isRelaxable xsd:boolean)\n\n(Declaration (DataProperty :hasAccuracyTradeoff))\n(DataPropertyDomain :hasAccuracyTradeoff :FairnessConstraint)\n(DataPropertyRange :hasAccuracyTradeoff xsd:boolean)\n\n;; Subclass Definitions\n(Declaration (Class :IndependenceConstraint))\n(SubClassOf :IndependenceConstraint :FairnessConstraint)\n(DataPropertyAssertion :hasMathematicalFormulation :IndependenceConstraint\n  \"Ŷ ⊥ A | X\"^^xsd:string)\n(AnnotationAssertion rdfs:comment :IndependenceConstraint\n  \"Predictions independent of protected attribute: P(Ŷ|A=0) = P(Ŷ|A=1)\"@en)\n\n(Declaration (Class :SeparationConstraint))\n(SubClassOf :SeparationConstraint :FairnessConstraint)\n(DataPropertyAssertion :hasMathematicalFormulation :SeparationConstraint\n  \"Ŷ ⊥ A | Y\"^^xsd:string)\n(AnnotationAssertion rdfs:comment :SeparationConstraint\n  \"Predictions independent of protected attribute given true label: P(Ŷ|Y,A=0) = P(Ŷ|Y,A=1)\"@en)\n\n(Declaration (Class :SufficiencyConstraint))\n(SubClassOf :SufficiencyConstraint :FairnessConstraint)\n(DataPropertyAssertion :hasMathematicalFormulation :SufficiencyConstraint\n  \"Y ⊥ A | Ŷ\"^^xsd:string)\n(AnnotationAssertion rdfs:comment :SufficiencyConstraint\n  \"True labels independent of protected attribute given predictions: P(Y|Ŷ,A=0) = P(Y|Ŷ,A=1)\"@en)\n\n;; Specific Metrics as Constraints\n(Declaration (Class :DemographicParityConstraint))\n(SubClassOf :DemographicParityConstraint :IndependenceConstraint)\n\n(Declaration (Class :EqualizedOddsConstraint))\n(SubClassOf :EqualizedOddsConstraint :SeparationConstraint)\n\n(Declaration (Class :EqualOpportunityConstraint))\n(SubClassOf :EqualOpportunityConstraint :SeparationConstraint)\n\n(Declaration (Class :PredictiveParityConstraint))\n(SubClassOf :PredictiveParityConstraint :SufficiencyConstraint)\n\n(Declaration (Class :CalibrationConstraint))\n(SubClassOf :CalibrationConstraint :SufficiencyConstraint)\n\n;; Impossibility Theorems\n(Declaration (Class :ImpossibilityTheorem))\n(AnnotationAssertion rdfs:comment :ImpossibilityTheorem\n  \"Theorems showing certain fairness constraints cannot be simultaneously satisfied\"@en)\n\n(Declaration (Class :ChouldechovaImpossibility))\n(SubClassOf :ChouldechovaImpossibility :ImpossibilityTheorem)\n(AnnotationAssertion rdfs:comment :ChouldechovaImpossibility\n  \"Cannot satisfy calibration, balance for positive class, and balance for negative class simultaneously when base rates differ\"@en)\n\n;; Constraints\n(SubClassOf :FairnessConstraint\n  (DataSomeValuesFrom :hasMathematicalFormulation xsd:string))\n      ```\n\n- ## About Fairness Constraints\n  id:: 0382-fairness-constraints-about\n\n  - \n  -\n  \n\n\n## Current Landscape (2025)\n\n- Industry adoption and implementations\n  - Metaverse platforms continue to evolve with focus on interoperability and open standards\n  - Web3 integration accelerating with decentralised identity and asset ownership\n  - Enterprise adoption growing in virtual collaboration, training, and digital twins\n  - UK companies increasingly active in metaverse development and immersive technologies\n\n- Technical capabilities\n  - Real-time rendering at photorealistic quality levels\n  - Low-latency networking enabling seamless multi-user experiences\n  - AI-driven content generation and procedural world building\n  - Spatial audio and haptics enhancing immersion\n\n- UK and North England context\n  - Manchester: Digital Innovation Factory supports metaverse startups and research\n  - Leeds: Holovis leads in immersive experiences for entertainment and training\n  - Newcastle: University research in spatial computing and interactive systems\n  - Sheffield: Advanced manufacturing using digital twin technology\n\n- Standards and frameworks\n  - Metaverse Standards Forum driving interoperability protocols\n  - WebXR enabling browser-based immersive experiences\n  - glTF and USD for 3D asset interchange\n  - Open Metaverse Interoperability Group defining cross-platform standards\n\n## Metadata\n\n- **Last Updated**: 2025-11-16\n- **Review Status**: Automated remediation with 2025 context\n- **Verification**: Academic sources verified\n- **Regional Context**: UK/North England where applicable",
  "properties": {
    "id": "0382-fairness-constraints-about",
    "collapsed": "true",
    "- public-access": "true",
    "- ontology": "true",
    "- term-id": "AI-0382",
    "- preferred-term": "Fairness Constraints",
    "- source-domain": "ai",
    "- status": "in-progress",
    "- version": "1.0",
    "- last-updated": "2025-10-29",
    "- definition": "Fairness Constraints are mathematical formalizations of equitable treatment in AI systems, expressed as conditions that predictions must satisfy relative to protected attributes. These constraints are categorized into three fundamental types based on independence criteria: Independence (demographic parity) requires predictions to be independent of protected attributes (Ŷ ⊥ A), meaning P(Ŷ|A=0) = P(Ŷ|A=1); Separation (equalized odds) requires predictions to be independent of protected attributes conditional on true labels (Ŷ ⊥ A | Y), ensuring equal true positive and false positive rates across groups; and Sufficiency (predictive parity) requires true labels to be independent of protected attributes conditional on predictions (Y ⊥ A | Ŷ), ensuring equal precision and calibration across groups. These constraints formalize fairness concepts like demographic parity, equalized odds, equal opportunity (separation for positive class only), and calibration into optimization problems during model training. However, impossibility theorems (Chouldechova 2017, Kleinberg et al. 2017) prove that when base rates differ between groups, certain combinations of fairness constraints cannot be simultaneously satisfied, necessitating context-dependent tradeoffs. Implementation typically involves constrained optimization with Lagrange multipliers, where accuracy loss is balanced against fairness violations through tunable regularization parameters, as formalized in foundational research by Hardt et al. (2016) and Barocas et al. (2019).",
    "- maturity": "mature",
    "- source": "[[Hardt et al. (2016)]], [[Barocas et al. (2019)]], [[Chouldechova (2017)]]",
    "- authority-score": "0.95",
    "- owl:class": "aigo:FairnessConstraints",
    "- owl:physicality": "VirtualEntity",
    "- owl:role": "Process",
    "- owl:inferred-class": "aigo:VirtualProcess",
    "- belongsToDomain": "[[AIEthicsDomain]]",
    "- implementedInLayer": "[[ConceptualLayer]]"
  },
  "backlinks": [],
  "wiki_links": [
    "ConceptualLayer",
    "AIEthicsDomain",
    "Barocas et al. (2019)",
    "Hardt et al. (2016)",
    "Chouldechova (2017)"
  ],
  "ontology": {
    "term_id": "AI-0382",
    "preferred_term": "Fairness Constraints",
    "definition": "Fairness Constraints are mathematical formalizations of equitable treatment in AI systems, expressed as conditions that predictions must satisfy relative to protected attributes. These constraints are categorized into three fundamental types based on independence criteria: Independence (demographic parity) requires predictions to be independent of protected attributes (Ŷ ⊥ A), meaning P(Ŷ|A=0) = P(Ŷ|A=1); Separation (equalized odds) requires predictions to be independent of protected attributes conditional on true labels (Ŷ ⊥ A | Y), ensuring equal true positive and false positive rates across groups; and Sufficiency (predictive parity) requires true labels to be independent of protected attributes conditional on predictions (Y ⊥ A | Ŷ), ensuring equal precision and calibration across groups. These constraints formalize fairness concepts like demographic parity, equalized odds, equal opportunity (separation for positive class only), and calibration into optimization problems during model training. However, impossibility theorems (Chouldechova 2017, Kleinberg et al. 2017) prove that when base rates differ between groups, certain combinations of fairness constraints cannot be simultaneously satisfied, necessitating context-dependent tradeoffs. Implementation typically involves constrained optimization with Lagrange multipliers, where accuracy loss is balanced against fairness violations through tunable regularization parameters, as formalized in foundational research by Hardt et al. (2016) and Barocas et al. (2019).",
    "source_domain": "ai",
    "maturity_level": null,
    "authority_score": 0.95
  }
}