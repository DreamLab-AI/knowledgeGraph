{
  "title": "Hyper personalisation",
  "content": "- ### OntologyBlock\n  id:: hyper-personalisation-ontology\n  collapsed:: true\n\t- ontology:: true\n\t- term-id:: mv-327814559706\n\t- preferred-term:: Hyper personalisation\n\t- source-domain:: metaverse\n\t- status:: active\n\t- public-access:: true\n\t- definition:: A component of the metaverse ecosystem focusing on hyper personalisation.\n\t- maturity:: mature\n\t- authority-score:: 0.85\n\t- owl:class:: mv:HyperPersonalisation\n\t- owl:physicality:: ConceptualEntity\n\t- owl:role:: Concept\n\t- belongsToDomain:: [[MetaverseDomain]]\n\t- #### Relationships\n\t  id:: hyper-personalisation-relationships\n\t\t- enables:: [[ImmersiveExperience]], [[Presence]], [[SpatialComputing]]\n\t\t- requires:: [[DisplayTechnology]], [[TrackingSystem]], [[RenderingEngine]]\n\t\t- bridges-to:: [[HumanComputerInteraction]], [[ComputerVision]], [[Robotics]]\n\n\t- #### OWL Axioms\n\t  id:: hyper-personalisation-owl-axioms\n\t  collapsed:: true\n\t\t- ```clojure\n\t\t  Declaration(Class(mv:HyperPersonalisation))\n\n\t\t  # Classification\n\t\t  SubClassOf(mv:HyperPersonalisation mv:ConceptualEntity)\n\t\t  SubClassOf(mv:HyperPersonalisation mv:Concept)\n\n\t\t  # Domain classification\n\t\t  SubClassOf(mv:HyperPersonalisation\n\t\t    ObjectSomeValuesFrom(mv:belongsToDomain mv:MetaverseDomain)\n\t\t  )\n\n\t\t  # Annotations\n\t\t  AnnotationAssertion(rdfs:label mv:HyperPersonalisation \"Hyper personalisation\"@en)\n\t\t  AnnotationAssertion(rdfs:comment mv:HyperPersonalisation \"A component of the metaverse ecosystem focusing on hyper personalisation.\"@en)\n\t\t  AnnotationAssertion(dcterms:identifier mv:HyperPersonalisation \"mv-327814559706\"^^xsd:string)\n\t\t  ```\n\npublic:: true\n\n- #Public page\n\t- automatically published\n-\n- ((656c7f3d-55af-42f3-989c-19f7d92f579f))\n- # The Golden Key: AI-Guided Emergent Narratives for Hyper-Personalized Location-Based Events\n- ## Overview\n- [The Golden Key](https://schedule.sxsw.com/2024/films/2196919) is an immersive, interactive art installation that generates never-ending, location-specific fairy tales using AI technologies. It combines large language models, generative AI, and user input to create unique, personalized narratives that evolve over time.\n- ## Key Features\n\t- Three large projection screens display AI-generated visuals that illustrate the unfolding story\n\t- Kiosks allow participants to input their own story elements, which are integrated into the narrative within minutes\n\t- The AI system draws from a database of story motifs, archetypes, and folktales to create a cohesive, ever-evolving narrative\n\t- The installation adapts to each location, incorporating local history, artists, and cultural elements into the generated stories\n- ## Technical Implementation\n\t- Backend infrastructure includes multiple high-performance computers running stable diffusion servers for image generation\n\t- ChatGPT 3.5 and 4 models are used for text generation, with the ability to request specific models for different parts of the system\n\t- Text-to-speech services are run locally to ensure reliability and reduce dependence on cloud services\n\t- TouchDesigner is used to create the three-panel projection mural, with slight parallax movement for a mesmerizing effect\n- ## Artistic Vision\n\t- The Golden Key explores the concept of myth-making and the role of AI in shaping cultural narratives. It invites participants to consider the implications of living in a world where artificially generated stories are ubiquitous. The installation aims to encourage critical thinking about the impact of AI on creativity, diversity, and representation in media.\n- ## Participant Experience\n\t- Participants approach kiosks where they are prompted to answer questions or provide story elements within a 60-second time limit\n\t- The user-generated content is then integrated into the ongoing narrative, appearing on the projection screens within minutes\n\t- Participants are encouraged to input multiple story elements and then sit back and watch how their contributions shape the unfolding tale\n\t- The AI-generated visuals and audio narration create an immersive, mesmerizing experience that adapts to each participant's input\n- ## Future Developments\n\t- The creators of The Golden Key are exploring new possibilities for interactive, AI-driven experiences, including:\n\t- Incorporating video and 3D elements to enhance the liveliness and immersion of the generated content\n\t- Developing mobile app touchpoints that allow participants to interact with the installation beyond the physical space\n\t- Collaborating with other artists and researchers to address issues of bias, diversity, and representation in AI-generated media\n\t- Expanding the installation to include a wider range of cultural motifs, archetypes, and folktales from around the world\n\n## Current Landscape (2025)\n\n- Industry adoption and implementations\n  - Metaverse platforms continue to evolve with focus on interoperability and open standards\n  - Web3 integration accelerating with decentralised identity and asset ownership\n  - Enterprise adoption growing in virtual collaboration, training, and digital twins\n  - UK companies increasingly active in metaverse development and immersive technologies\n\n- Technical capabilities\n  - Real-time rendering at photorealistic quality levels\n  - Low-latency networking enabling seamless multi-user experiences\n  - AI-driven content generation and procedural world building\n  - Spatial audio and haptics enhancing immersion\n\n- UK and North England context\n  - Manchester: Digital Innovation Factory supports metaverse startups and research\n  - Leeds: Holovis leads in immersive experiences for entertainment and training\n  - Newcastle: University research in spatial computing and interactive systems\n  - Sheffield: Advanced manufacturing using digital twin technology\n\n- Standards and frameworks\n  - Metaverse Standards Forum driving interoperability protocols\n  - WebXR enabling browser-based immersive experiences\n  - glTF and USD for 3D asset interchange\n  - Open Metaverse Interoperability Group defining cross-platform standards\n\n## Metadata\n\n- **Last Updated**: 2025-11-16\n- **Review Status**: Automated remediation with 2025 context\n- **Verification**: Academic sources verified\n- **Regional Context**: UK/North England where applicable",
  "properties": {
    "id": "hyper-personalisation-owl-axioms",
    "collapsed": "true",
    "- ontology": "true",
    "- term-id": "mv-327814559706",
    "- preferred-term": "Hyper personalisation",
    "- source-domain": "metaverse",
    "- status": "active",
    "- public-access": "true",
    "- definition": "A component of the metaverse ecosystem focusing on hyper personalisation.",
    "- maturity": "mature",
    "- authority-score": "0.85",
    "- owl:class": "mv:HyperPersonalisation",
    "- owl:physicality": "ConceptualEntity",
    "- owl:role": "Concept",
    "- belongsToDomain": "[[MetaverseDomain]]",
    "- enables": "[[ImmersiveExperience]], [[Presence]], [[SpatialComputing]]",
    "- requires": "[[DisplayTechnology]], [[TrackingSystem]], [[RenderingEngine]]",
    "- bridges-to": "[[HumanComputerInteraction]], [[ComputerVision]], [[Robotics]]",
    "public": "true"
  },
  "backlinks": [
    "Client side DCO",
    "Social contract and jobs",
    "Vision System",
    "rb-0068-vision-system",
    "Technical History (extended CV)"
  ],
  "wiki_links": [
    "ComputerVision",
    "Presence",
    "MetaverseDomain",
    "TrackingSystem",
    "DisplayTechnology",
    "RenderingEngine",
    "ImmersiveExperience",
    "Robotics",
    "HumanComputerInteraction",
    "SpatialComputing"
  ],
  "ontology": {
    "term_id": "mv-327814559706",
    "preferred_term": "Hyper personalisation",
    "definition": "A component of the metaverse ecosystem focusing on hyper personalisation.",
    "source_domain": "metaverse",
    "maturity_level": null,
    "authority_score": 0.85
  }
}