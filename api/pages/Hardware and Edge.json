{
  "title": "Hardware and Edge",
  "content": "- ### OntologyBlock\n  id:: hardware-and-edge-ontology\n  collapsed:: true\n\t- ontology:: true\n\t- term-id:: mv-702620567296\n\t- preferred-term:: Hardware and Edge\n\t- source-domain:: metaverse\n\t- status:: active\n\t- public-access:: true\n\t- definition:: A component of the metaverse ecosystem focusing on hardware and edge.\n\t- maturity:: mature\n\t- authority-score:: 0.85\n\t- owl:class:: mv:HardwareAndEdge\n\t- owl:physicality:: ConceptualEntity\n\t- owl:role:: Concept\n\t- belongsToDomain:: [[MetaverseDomain]]\n\t- #### Relationships\n\t  id:: hardware-and-edge-relationships\n\t\t- enables:: [[ImmersiveExperience]], [[Presence]], [[SpatialComputing]]\n\t\t- requires:: [[DisplayTechnology]], [[TrackingSystem]], [[RenderingEngine]]\n\t\t- bridges-to:: [[HumanComputerInteraction]], [[ComputerVision]], [[Robotics]]\n\n\t- #### OWL Axioms\n\t  id:: hardware-and-edge-owl-axioms\n\t  collapsed:: true\n\t\t- ```clojure\n\t\t  Declaration(Class(mv:HardwareAndEdge))\n\n\t\t  # Classification\n\t\t  SubClassOf(mv:HardwareAndEdge mv:ConceptualEntity)\n\t\t  SubClassOf(mv:HardwareAndEdge mv:Concept)\n\n\t\t  # Domain classification\n\t\t  SubClassOf(mv:HardwareAndEdge\n\t\t    ObjectSomeValuesFrom(mv:belongsToDomain mv:MetaverseDomain)\n\t\t  )\n\n\t\t  # Annotations\n\t\t  AnnotationAssertion(rdfs:label mv:HardwareAndEdge \"Hardware and Edge\"@en)\n\t\t  AnnotationAssertion(rdfs:comment mv:HardwareAndEdge \"A component of the metaverse ecosystem focusing on hardware and edge.\"@en)\n\t\t  AnnotationAssertion(dcterms:identifier mv:HardwareAndEdge \"mv-702620567296\"^^xsd:string)\n\t\t  ```\n\npublic:: true\n\n\t- #Public page automatically published\n- # Wearables\n\t- ### Orion\n\t\t- [Zero to One: How Our Custom Silicon & Chips Are Revolutionizing AR | Meta Quest Blog | Meta Store](https://www.meta.com/en-gb/blog/orion-custom-silicon-chips-ip-blocks-accelerators-ar-algorithms-energy-efficiency-reality-labs/)\n\t- ### Meta Ray Bans\n\t\t- ![rbm-plp-hero-d-data.mp4](../assets/rbm-plp-hero-d-data_1718993826789_0.mp4)\n\t\t- The Meta Ray-Ban smart glasses, developed in partnership with EssilorLuxottica, offer a range of advanced features in a [stylish form factor](https://about.fb.com/news/2023/09/new-ray-ban-meta-smart-glasses/).¬†Here's a technical overview:\n\t\t- #### Hardware\n\t\t\t- Camera: 12 MP ultra-wide camera for photos and 1080p video recording\n\t\t\t- Processor: Qualcomm Snapdragon AR1 Gen1 Platform\n\t\t\t- Audio: Custom-designed speakers with improved bass and directional audio\n\t\t\t- Microphones: Five-microphone array for immersive audio recording\n\t\t\t- Battery: Redesigned charging case with up to 36 hours of total use\n\t\t- #### Key Features\n\t\t\t- Hands-free photo and video capture\n\t\t\t- Live streaming to Facebook and Instagram\n\t\t\t- Voice commands for various functions\n\t\t\t- Open-ear audio for music and calls\n\t\t\t- Water-resistant (IPX4 rating)\n\t\t\t- Integration with Meta AI assistant\n\t\t- #### Design\n\t\t\t- Available in Wayfarer and new Headliner styles\n\t\t\t- Multiple color options, including transparent frames\n\t\t\t- Prescription-lens compatible\n\t\t\t- Reduced weight and slimmer profile for improved comfort\n\t- The Meta Ray-Ban smart glasses have been well-received, with reports indicating that around 1 million units have been shipped since their launch. To be clear, this modality of vision AR and AI is the future of **all** of this stuff. VR is still very poorly adopted and unloved, given the investment.\n\t- To reach this Meta is developing full AR glasses codenamed Orion, [slated for release in 2027](https://www.theverge.com/2023/2/28/23619730/meta-vr-oculus-ar-glasses-smartwatch-plans).¬†These glasses are designed to work in tandem with a neural interface wristband, allowing for gesture-based control and potentially revolutionizing how we interact with digital content in the physical world.\n\t- ### [Rabbit](https://www.rabbit.tech/)\n\t  id:: 659e5979-c2de-4138-b2df-ede79790ee6d - üü¢ **Description:** A pocket companion that mediates between the user and all their other web services\n\t\t- **Features:**\n\t\t\t- Designed by Teenage Engineering Co\n\t\t\t- Agentic (connect it to your data and it does stuff)\n\t\t\t\t- Needs to be connected to a lot of different user data sources\n\t\t\t- I'm sceptical, but it's a nice effort.\n\t\t\t- It's **cheap**, I would totally give this to a kid over a mobile phone. ¬£200 all in?!?\n\t\t\t- It sold out immediately.\n\t\t\t- {{tweet https://twitter.com/rabbit_hmi/status/1744781083831574824}}\n\t\t\t-\n- # Neural Interfaces\n\t- ## EMG Wristbands\n\t\t- Meta's EMG wristbands use electromyography to interpret electrical signals from the brain that control hand movements[1](https://blogs.expandreality.io/meta-are-enhancing-vr-experiences-with-neural-wristbands).¬†This technology allows for seamless and precise interactions with digital objects in virtual and augmented reality environments without the need for external cameras or sensors[1](https://blogs.expandreality.io/meta-are-enhancing-vr-experiences-with-neural-wristbands).The wristbands contain embedded sensors that capture subtle electrical signals transmitted from the brain to the hands. These signals are then translated into precise commands, enabling real-time interaction with virtual environments[1](https://blogs.expandreality.io/meta-are-enhancing-vr-experiences-with-neural-wristbands).\n\t\t- [Meta are Enhancing VR Experiences with Neural Wristbands](https://blogs.expandreality.io/meta-are-enhancing-vr-experiences-with-neural-wristbands)\n\t\t- ### Key Features and Benefits\n\t\t\t- **Precision and Intuitiveness**: The EMG technology can detect even the tiniest muscle movements, allowing for fluid and responsive interactions in VR and AR[1](https://blogs.expandreality.io/meta-are-enhancing-vr-experiences-with-neural-wristbands).\n\t\t\t- **Non-Invasive**: The wristbands offer a non-invasive way to capture neural signals, making the technology more accessible and user-friendly[1](https://blogs.expandreality.io/meta-are-enhancing-vr-experiences-with-neural-wristbands).\n\t\t\t- **Versatility**: The wristbands can be used for various applications, from simple object recognition to fully immersive environments[1](https://blogs.expandreality.io/meta-are-enhancing-vr-experiences-with-neural-wristbands).\n\t\t\t- **Adaptive Learning**: The neural interface continuously improves its understanding of each user's unique movements over time, enhancing the overall experience5\n\t\t\t- At present, the EMG wristbands can provide basic input commands, such as:\n\t\t\t\t- Finger taps (index and middle finger\n\t\t\t\t- D-pad-like gestures\n\t\t\t\t- Simple hand movements[2](https://mixed-news.com/en/bosworth-on-emg-wristband-as-quest-controller/)\n\t\t\t- While these inputs are currently limited, Meta's CTO Andrew Bosworth suggests that the technology could evolve to become an increasingly universal interface over time[2](https://mixed-news.com/en/bosworth-on-emg-wristband-as-quest-controller/).\n- # Smart Rings - sticky sensor tech\n\t- [We tested six smart rings, and there‚Äôs a clear winner - The Verge](https://www.theverge.com/2024/10/1/24259284/oura-ring-samsung-galaxy-ring-ultrahuman-ring-air-ringconn-circular-ring-evie-ring-review-wearables)\n\t-\n\t- ### Oura Smart Ring\n\t\t- **Description**: A ring that tracks overall health, focusing on sleep quality, activity levels, and readiness. Finland, not new, fairly mature.\n\t\t- **Features**:\n\t\t\t- Sleep tracking with detailed stages\n\t\t\t- Body temperature sensor\n\t\t\t- Heart rate and HRV tracking\n\t\t\t- 7-day battery life\n\t\t\t- Waterproof and sleek design\n\t\t- **AI Aspect**: Employs AI to analyze data and provide personalized health insights and recommendations.\n\t- ### Amazfit Smart Ring\n\t\t- {{tweet https://twitter.com/AmazfitGlobal/status/1745524225589547012}}\n\t- ### Rewind Pendant\n\t\t- **Description**: A wearable device designed to aid memory by passively capturing audio throughout the day.\n\t\t- **Features**:\n\t\t\t- Auto-records ambient sound\n\t\t\t- Privacy-focused with user-controlled storage\n\t\t\t- Lightweight and can be worn as a necklace\n\t\t\t- Integrates with an app for audio playback\n\t\t- **AI Aspect**: Uses AI to intelligently capture and categorize important sound bites.\n\t- ### Humane Ai Pin\n\t\t- [twitter link to the render loading below](https://twitter.com/jaredostdiek/status/1768674773389713645)\n\t\t  {{twitter https://twitter.com/jaredostdiek/status/1768674773389713645}}\n\t\t- **Description**: A wearable device focused on interaction and connectivity, emphasizing humane and natural tech usage.\n\t\t- **Features**:\n\t\t\t- Connects with various apps and services\n\t\t\t- Aims for minimal and non-intrusive notifications\n\t\t\t- Includes a camera and interactive display\n\t\t\t- Designed to promote ethical AI and user-friendly interfaces\n\t\t- **AI Aspect**: Integrates AI for intuitive interactions and ambient computing.\n\t\t- This attempt at a wearable has been roundly panned by early adopters and reviewers.\n\t\t\t- {{video https://www.youtube.com/watch?v=TitZV6k8zfA}}\n\t- ### Apollo Neuro\n\t\t- **Description**: A wearable wellness device that uses touch therapy to help the body adapt to stress.\n\t\t- **Features**:\n\t\t\t- Delivers gentle vibrations to improve stress resilience\n\t\t\t- Modes for sleep, focus, relaxation, and more\n\t\t\t- Wearable on the wrist or ankle\n\t\t\t- App-controlled\n\t\t- **AI Aspect**: Utilizes AI to personalize and adapt therapy based on user feedback and usage patterns.\n\t- ### Pavlok 3\n\t\t- **Description**: A behavior modification wearable that uses mild electric shock to help break bad habits. (weird)\n\t\t- **Features**:\n\t\t\t- Delivers a mild shock to discourage bad habits\n\t\t\t- Tracks sleep, steps, and hand motions\n\t\t\t- Programmable via an app\n\t\t\t- Community challenges and support\n\t\t- **AI Aspect**: Utilizes AI to learn patterns and optimize intervention timing.\n\t- ### Yopi\n\t\t- **Description**: A voice-based wellness coach focused on improving breathing and reducing stress.\n\t\t- **Features**:\n\t\t\t- Monitors breathing patterns\n\t\t\t- Provides real-time feedback and coaching\n\t\t\t- Portable and pairs with smartphones\n\t\t\t- Focus on breathing exercises and meditation\n\t\t- **AI Aspect**: Uses AI to customize breathing exercises and track progress.\n\t- ### ChatGPT (and whatever Siri becomes) is coming to watches\n\t\t- **Description**: A concept of smartwatches integrated with AI for interactive and adaptive communication.\n\t\t- **Features**:\n\t\t\t- Voice and text-based interaction with AI\n\t\t\t- Integration with various apps and services\n\t\t\t- Notifications, reminders, and AI-driven suggestions\n\t\t\t- Continuous updates and learning capabilities\n\t\t- **AI Aspect**: Incorporates a conversational AI model for real-time communication and assistance.\n\t\t- [Translation earbuds |Translator | language translation device ‚Äì Mymanu¬Æ](https://mymanu.com/) -\n- # Open Source\n\t- [ADeus](https://github.com/adamcohenhillel/ADeus?tab=readme-ov-file#setup-hardware---coral-ai-device)\n\t\t- **Description**: An open source AI wearable device that captures what you say and hear in the real world and then transcribes and stores it on your own server. You can then chat with Adeus using the app, and it will have all the right context about what you want to talk about\n\t\t- a truly personalized, personal AI.\n\t- [OwlAIProject/Owl: A personal wearable AI that runs locally (github.com)](https://github.com/OwlAIProject/Owl)\n\t\t- **Owl**¬†is an experiment in human-computer interaction using wearable devices to observe our lives and extract information and insights from them using AI. Presently, only audio and location are captured, but we plan to incorporate vision and other modalities as well. The objectives of the project are, broadly speaking:\n\t- [Hey Ollama](https://www.reddit.com/r/LocalLLaMA/comments/1b9hwwt/hey_ollama_home_assistant_ollama/)\n\t\t- For HA<->Ollama:¬†[https://github.com/jekalmin/extended_openai_conversation](https://github.com/jekalmin/extended_openai_conversation)\n\t\t- Hardware:¬†[https://www.espressif.com/en/news/ESP32-S3-BOX-3](https://www.espressif.com/en/news/ESP32-S3-BOX-3)\n\t\t- [https://github.com/ollama/ollama/blob/main/docs/openai.md](https://github.com/ollama/ollama/blob/main/docs/openai.md)\n\t\t- [https://github.com/kahrendt/microWakeWord/issues/2](https://github.com/kahrendt/microWakeWord/issues/2)\n\t\t- [https://github.com/jaymunro/esphome_firmware/blob/main/wake-word-voice-assistant/esp32-s3-box-3.yaml](https://github.com/jaymunro/esphome_firmware/blob/main/wake-word-voice-assistant/esp32-s3-box-3.yaml)\n\t\t- Actually you can do full two way conversations! Here's a PR someone has in progress to officially add it to esphome -¬†[https://github.com/esphome/firmware/pull/173](https://github.com/esphome/firmware/pull/173)\n\t- [AI in a Box (crowdsupply.com)](https://www.crowdsupply.com/useful-sensors/ai-in-a-box)\n\t\t- ```Your very own private AI that you can ask questions and get answers, all in a tiny box! The first AI that you can talk to, and that talks back,¬†**running locally with no internet connection**¬†so your conversations and data are completely secure. No account, setup, or subscription are needed, just plug in the box and start chatting. Need closed captions for a live event, or just to help in situations where you have trouble hearing a conversation? We‚Äôre using the latest in AI technology to display subtitles based on the audio input, which are output on the built-in display and through an HDMI connector for external monitors or screens.```\n\t\t  id:: 65d5d2b5-36a9-4cac-8efe-18bb9e2559d4\n\t\t- [usefulsensors/useful-transformers: Efficient Inference of Transformer models (github.com)](https://github.com/usefulsensors/useful-transformers)\n\t\t- ![](https://www.crowdsupply.com/img/6605/75c87a10-e0cc-4acf-9f3b-6c498c986605/useful-sensors-ai-box-ready_jpg_md-xl.jpg) -\n- # Phones\n\t- ## Samsung Galaxy S24 Series Local AI Inferencing Features\n\t\t- Live Translate for real-time voice and text translations directly within the phone app, functioning without the need for an internet connection.\n\t\t- Chat Assist in Samsung Keyboard for real-time translation in 13 languages, enabling on-device translation for messages.\n\t\t- Android Auto's capability to summarize messages and suggest relevant replies, powered by on-device AI, for safer driving experiences.\n\t\t- Note Assist for generating AI-powered summaries of notes taken within Samsung Notes, improving organization and retrieval of information.\n\t\t- Transcript Assist uses on-device AI for transcribing and summarizing voice recordings, identifying different speakers and translating content.\n\t\t- Edit Suggestion feature that uses on-device AI to suggest photo edits, enhancing the photography experience without the need for server processing.\n\t\t- Generative Edit for intelligently filling in parts of an image background, providing users with AI-powered content creation tools.\n\t- ## Google sub second inferencing on a phone.\n\t\t- [Paper page\n\t\t- MobileDiffusion: Subsecond Text-to-Image Generation on Mobile Devices (huggingface.co)](https://huggingface.co/papers/2311.16567)\n-\n- # Training and inferencing hardware\n\t- Chips and [[Hardware and Edge]] technologies are rapidly advancing to support the growing demands of artificial intelligence applications.\n\t\t- For more insights on this topic, visit [Etched](http://etched.com).\n\t\t- [Cristiano Amon: generative AI is ‚Äòevolving very, very fast‚Äô into mobile devices (FT.com)](https://www.ft.com/content/dbc0984b-4801-4aeb-bcab-480704c34161) highlights the necessity for dedicated computing resources in mobile devices.\n\t\t\t- As Amon explains, while tasks can be executed on a CPU or GPU, these processors are often preoccupied with other functions. This multitasking can hinder performance when running complex AI algorithms.\n\t\t\t- To address this challenge, there is a push towards implementing dedicated accelerated computing solutions. This includes the integration of Neural Processing Units (NPUs), which are specifically designed to handle AI workloads efficiently.\n\t\t\t- NPUs enable faster processing speeds and lower power consumption compared to traditional CPUs and GPUs, making them ideal for real-time applications such as image recognition, natural language processing, and augmented reality experiences.\n\t\t\t- The evolution of these technologies signifies a pivotal shift in how we approach AI deployment across various platforms, particularly in mobile environments where resource constraints are prevalent.\n\t\t- You‚Äôre going to see devices launch in early 2024 with a number of [[Proprietary Large Language Models]] use cases. It has the potential to create a new upgrade cycle on smartphones. And what we want is that eventually you‚Äôre going to say, you know, ‚ÄúI‚Äôve been keeping my phone for the past four years‚Äâ.‚Äâ.‚Äâ.‚ÄâNow I need to buy a new phone because I really want this gen AI capability\n\t\t- VR and AI\n\t\t\t- We are seeing gen AI coming into VR. We see an incredible potential for augmented reality and mixed reality glasses, especially as you use audio and large language models as an interface. We have been very bullish about spatial computing being the new computing platform, and we see a lot of promising developments coming: we see what Meta is doing, we see what is happening on the Android ecosystem with Google and Samsung. We are just at the beginning.\n\t\t\t- Bloomberg reports Sam Altman is in talks to raise money for a ‚Äòglobal‚Äô network of fabricators building hardware for AI. Sam Altman's plan to establish a global network of AI chip factories could revolutionize the tech industry, reducing dependence on existing semiconductor giants and ensuring a steady supply of AI advancements.\n\t\t\t- [The Groq LPU‚Ñ¢ Inference Engine - Groq](https://wow.groq.com/lpu-inference-engine/) Asic\n\t\t\t- [Chat with RTX Now Free to Download | NVIDIA Blog](https://blogs.nvidia.com/blog/chat-with-rtx-available-now/)\n\t\t- [AMD Instinct‚Ñ¢ MI300 Series Accelerators](https://www.amd.com/en/products/accelerators/instinct/mi300.html)\n\t\t- [IBM custom board](https://www.marktechpost.com/2022/10/27/ibm-research-introduces-artificial-intelligence-unit-aiu-its-first-complete-system-on-chip-designed-to-run-and-train-deep-learning-models-faster-and-more-efficiently-than-a-general-purpose-cpu/)\n\t\t- [Nvidia jetson AI](https://www.okdo.com/p/nvidia-jetson-agx-orin-64gb-developer-kit/)\n\t\t- [install cuda](https://dev.to/ajeetraina/install-cuda-on-jetson-nano-2b06)\n\t\t- [Qualcomm phone SD](https://www.theverge.com/2023/2/23/23611668/ai-image-stable-diffusion-mobile-android-qualcomm-fastest)\n\t\t- [Esperanto RISC V](https://www.esperanto.ai/)\n\t\t- [The MetaVRain asic claims 900x speed increases} on general GPU problems](https://hdh4797.wixsite.com/dhan/project-1)\n\t\t- [Google android etc](https://developers.google.com/learn/topics/on-device-ml)\n\t\t- [Intel meteor lake?](https://www.pocket-lint.com/what-is-meteor-lake-and-how-will-intel-leverage-ai-in-future/)\n\t\t- [Shopify handy](https://github.com/Shopify/handy)\n\t\t- [Comparison of GPUs](https://timdettmers.com/2023/01/16/which-gpu-for-deep-learning/)\n\t\t- [LLM on Intel XEON optmised](https://community.intel.com/t5/Blogs/Tech-Innovation/Artificial-Intelligence-AI/Numenta-and-Intel-Accelerate-Inference-20x-on-Large-Language/post/1471636)\n\t\t- [TPU v4 matrix multiplier](https://cloud.google.com/blog/topics/systems/tpu-v4-enables-performance-energy-and-co2e-efficiency-gains)\n\t\t- [Tinygrad tinybox](https://github.com/geohot/tinygrad/blob/master/docs/showcase.md#stable-diffusion)\n\t\t- [Snapdragon 8 inference](https://www.phonescoop.com/articles/article.php?a=22911) Qualcomm has launched its latest flagship chip, the Snapdragon 8 Gen 2, which is expected to power most Android smartphones next year. The new chip is more power-efficient and powerful compared to its predecessor Gen 1, and features significant improvements and new features in AI. The new Qualcomm AI engine offers an AI performance improvement of up to 4.35x and up to 60% better power efficiency. There is a new direct link between the Hexagon AI cores and the Spectra imaging cores, providing real-time semantic segmentation, and the Sensing Hub has two AI processors and more memory for smarter always-on features. The Kryo CPU cores have also been updated, with four performance cores and three efficiency cores, optimising them for legacy 32-bit apps. On the GPU side, the new Adreno cores support hardware-accelerated raytracing and Unreal Engine 5's Metahumans technology. The X70 modem supports 4-carrier aggregation for downlink speeds of up to 10 Gbps, while the onboard FastConnect 7800 system is the first to support Wi-Fi 7 with High Band Simultaneous Multi-Link. https://www.phonescoop.com/articles/article.php?a=22911\n- # Consumer Hardware\n\t- [Sideloaded app stores are coming to iOS in the EU (thenextweb.com)](https://thenextweb.com/news/sideloaded-app-stores-ios-eu-how-work)\n\t- [AI HoloBox: ChatGPT-Powered Holographic Desktop Companion by AI HoloBox ‚Äî Kickstarter](https://www.kickstarter.com/projects/752925986/ai-holobox-chatgpt-powered-holographic-desktop-companion?)\n\t-\n- **I don't personally think any of these wearables and gadgets \"break through\" vs watches, but I can see the next generation of watching inferring a LOT more and containing MUCH more functionality. People will wear watches. Sometimes.**\n- # HCI and interfaces\n\t- [Doublepoint launches gesture-touch tech for wearable devices | VentureBeat](https://venturebeat.com/games/doublepoint-launches-gesture-touch-tech-for-wearable-devices/)\n\t- [Meta's wrist reader](https://www.from-the-interface.com/wrist-interfaces/)\n- # Edge\n\t- Edge compute refers to the practice of processing and analyzing data as close to the source as possible, which is typically at the edge of a network. This approach aims to reduce latency and network congestion by performing computations and running applications on devices such as sensors, gateways, or edge servers located near the data source. By moving computational tasks closer to where the data is generated, edge compute enables real-time and low-latency decision-making.\n\t- Edge AI, also known as AI at the edge or on-device AI, refers to the deployment of artificial intelligence and machine learning algorithms on edge devices. By bringing AI capabilities closer to the data source, Edge AI eliminates the need to transmit large volumes of data to the cloud for processing. This approach enables real-time inference and decision-making directly on devices with limited computing resources, such as smartphones, drones, or IoT devices. Edge AI has several advantages, including reduced latency, improved privacy and security, offline functionality, and the ability to operate in disconnected or bandwidth-constrained environments.\n\t- Fog compute, on the other hand, extends the concept of edge compute by introducing a hierarchical architecture. It involves distributing computing resources, storage, and applications between the cloud and edge devices. In the fog computing model, intermediate fog nodes are deployed between edge devices and the cloud, enabling them to process and store data. This approach reduces the need for data to be transmitted to traditional data centers or the cloud, allowing for faster response times, increased security, and better bandwidth utilization.\n\t- Overall, the combination of edge compute, fog compute, and edge AI introduces a distributed computing paradigm that brings processing, storage, and intelligence closer to the data source. This not only improves performance and efficiency but also enables new use cases and applications in various domains, including IoT, smart cities, autonomous vehicles, and industrial automation.\n\t- These systems will drive the compute to less ‚Äòconstrained‚Äô but somewhat less capable AI systems, distributing the access but increasing risks. [[Update Cycle]]\n\t\t- [Andrej Karpathy's Baby Llama Runs on Samsung Galaxy Watch 4Baby Llama Runs on Samsung Galaxy Watch 4 (analyticsindiamag.com)](https://analyticsindiamag.com/andrej-karpathys-baby-llama-runs-on-samsung-galaxy-watch-4/)\n\t\t- Baby llama [[Large language models]] with Llama.c is 700 lines of C code!  [karpathy/llama2.c: Inference Llama 2 in one file of pure C (github.com)](https://github.com/karpathy/llama2.c)\n\t\t- ###### Edge AI compute and APUs\n\t\t- [Qualcomm phone chip](https://www.theverge.com/2023/2/23/23611668/ai-image-stable-diffusion-mobile-android-qualcomm-fastest)¬†offers low power and high speed Stable Diffusion on mobiles\n\t\t- IBM have introduced the¬†[concept of the AIU](https://research.ibm.com/blog/ibm-artificial-intelligence-unit-aiu), for high speed and low power training\n\t\t- Nvidia‚Äôs¬†[latest in the Jetson](https://www.okdo.com/p/nvidia-jetson-agx-orin-64gb-developer-kit/)¬†Edge AGX line is a high performance general AI unit for industrial applications\n\t\t- Esperanto Risc V chip¬†[claims incredible performance](https://www.esperanto.ai/News/risc-v-startup-esperanto-technologies-samples-first-ai-silicon/)¬†gains\n\t\t- The MetaVRain asic¬†[claims 900x speed increases](https://hdh4797.wixsite.com/dhan/project-1)¬†on general GPU problems\n\t\t- Microsoft are rumoured to be looking to mitigate the staggering costs of running ChatGPT ($1M/day) using forthcoming¬†[hardware of their own design](https://www.theinformation.com/articles/microsoft-readies-ai-chip-as-machine-learning-costs-surge?)\n\t\t- [Cerebras systems](https://www.cerebras.net/)¬†have built an AI architecture from the ground up and claim incredible numbers.\n\t\t- [Ushering in the Thermodynamic Future\n\t\t- Litepaper (extropic.ai)](https://www.extropic.ai/future)\n\t\t- Tenstorrent Grayskull [[Hardware and Edge]]  [Cards\n\t\t- Tenstorrent](https://tenstorrent.com/cards/) -\n- # Displays\n\t- ### Displays & Headset Hardware\n\t\t- Awaiting a bit more market stability for this section. Of note is thatMicrosoft seems to be [abandoningHololens](https://www.windowscentral.com/microsoft/microsoft-has-laid-off-entire-teams-behind-virtual-mixed-reality-and-hololens),and Apple seem to have postponed their commodity AR headset.\n\t\t- Microsoft think that creating the Perfect Illusion, that of alife-likeness in VR will require a field of view of 210 horizontal and135 vertical, 60 pixels per degree subtended, and a refresh rate of 1800Mhz according to Microsoft. They expect this by as soon as2028.[[cuervo2018creating]]\n\t\t- With the advent of[WebGPU](https://developer.chrome.com/docs/web-platform/webgpu/)alongside WebGL everything is likely to converge on the browserexperience.\n\t\t- {{renderer :linkpreview,https://skarredghost.com/2024/04/01/valve-deckard-leaked/}}\n\t- #### The Apple in the Room\n\t\t- Following the announcement of The Apple Vision Pro we start to see theconvergence of spatial computing, mixed reality, locally appliedtransformer based AI, and business. They have perhaps removed ‚Äúgorillaarm syndrome‚Äù[[boring2009scroll]] where hands in the sky interfaces arepotentially uncomfortable over long periods.[[hansberger2017dispelling]]Nathan Gitter and Amy DeDonato from the Apple Design team [introducespatial design for thedevice](https://developer.apple.com/videos/play/wwdc2023/10072/).\n\t- ##### Spatial operating systems\n\t\t- - Enabling users to design experiences not previously possible.\n\t\t- - The presentation outlines how to keep apps familiar, be human-centered, take advantage of space, enhance immersion, and make apps authentic to the platform.\n\t\t- - The world serves as an infinite canvas for new apps and games.\n\t\t- - Existing app elements should be kept familiar with common elements like sidebars, tabs, and search fields.\n\t\t- - In a spatial platform, interfaces are placed within windows to make them easily accessible and part of the user‚Äôs surroundings.\n\t- ##### Windows in Spatial Design\n\t\t- - Windows are designed with a new visual language, made of a glass material that provides contrast with the world, awareness of surroundings, and adapts to different lighting conditions.\n\t\t- - Windows can be moved, closed, and resized by users, with windows facing the user during movement.\n\t\t- - Windows are flexible and can be resized to fit comfortably within the user‚Äôs view.\n\t\t- - Choosing Window Size and Layout Windows are designed to be flexible, adapting to content, and the window size should be chosen based on this. Windows can change size dynamically based on context.\n\t\t- - Apps can use multiple windows to display content side by side or show distinct actions, but should ideally stick to a single window to avoid user overwhelm.\n\t- ##### Designing with Points\n\t\t- - Interfaces are designed with points to ensure they scale well and remain legible at different distances.\n\t\t- - Points allow designers to set the size of interface elements with familiar units. Human-Centered Design\n\t\t- - Good spatial design places the user at the center, accounting for their field of view and movement.\n\t\t- - The most important content should be placed in the center of the field of view and use landscape layouts.\n\t\t- - Ergonomics should also be considered, placing content along a natural line of sight for comfort.\n\t\t- - Designers should avoid placing content behind users or anchoring content to their view as it can be disorienting.\n\t\t- - Spatial design should aim to create stationary experiences that require minimal movement from users.\n\t\t- ##### User Mobility\n\t\t- The presentation emphasizes the importance of designing applicationsthat require minimal movement from users. It recommends usingsystem-level recentering methods to adjust the app‚Äôs view when a usermoves.\n\t\t- ##### Space Utilization\n\t\t- The importance of optimizing an app‚Äôs usage of space is discussed, asthe available physical space for users can vary. It advises againstconstraining your app based on the physical space available and insteadcreating an app that can function in any amount of space.\n\t\t- ##### Dimensionality\n\t\t- The use of depth and scale in designing the user experience isemphasized. Depth can help with hierarchy and focus, and scale can beused to emphasize content. The text warns against overusing depth,especially with text, and encourages developers to experiment with scaleto achieve the desired user experience.\n\t\t- ##### Immersiveness\n\t\t- The passage introduces the concept of an immersion spectrum, where anapp can transition between various states of immersion based on theuser‚Äôs experience. The importance of smooth transitions, designing withconsideration to user focus, thoughtful blending with reality, andkeeping the user comfortable are emphasized.\n\t\t- ##### Sound Design\n\t\t- It also highlights the importance of using spatial audio to enhance theimmersive experience of an app, which includes attaching sound toobjects and creating soundscapes.\n\t\t- ##### User Comfort\n\t\t- Recommendations for moving an immersive app, focusing on avoidingdisorienting fast movements and instead recommending fade out and fadein techniques to keep the user comfortable during motion.\n\t\t- ##### Transitions\n\t\t- It highlights the importance of clear, intuitive methods for enteringand exiting immersive experiences. It suggests using easily recognizablesymbols, such as arrows for expanding or collapsing views.\n\t\t- ##### Authenticity\n\t\t- Emphasizes creating an authentic experience that takes full advantage ofthe platform‚Äôs capabilities. An example given is Freeform, which uses alarge creative space allowing users to view all their content at once.\n\t\t- ##### Key Moments\n\t\t- Focusing on a ‚Äúkey moment‚Äù that provides a unique spatial or immersiveexperience is recommended. This could involve enhancing a moment withdepth and scale or transforming the user‚Äôs space to create a unique andmemorable experience.\n\t- Lightfield\n\t\t- [Light Field Lab](https://www.lightfieldlab.com/#tech)\n\t- [Infitec shows holographic projection screens (installation-international.com)](https://www.installation-international.com/ise-daily/infitec-shows-holographic-projection-screens)\n\t- [HYPERVSN is a 3D Integrated Holographic System for advertising, digital signage, events.](https://hypervsn.com/)\n\t- {{video https://www.youtube.com/watch?v=DxkIo-2Jzzo&}}\n\t-\n\t-\n- # Brain\n\t- Apple has submitted a patent application that raises some serious privacy and ethical concerns.\n\t- [From this post](https://www.linkedin.com/feed/update/urn:li:activity:7196781454519877632/)\n\t- ![1715807554519.jpeg](../assets/1715807554519_1716117593773_0.jpeg)\n\t- The US Patent and Trademark Office lists application 2023/0225659 as a ‚Äúbiosensing device‚Äù built into Apple‚Äôs earbuds to measure ‚Äúbiological signal parameters from a user.‚Äù\n\t\t- üëâ Electroencephalography (EEG).¬†In other words, the aim is to directly record the user‚Äôs brain waves from tiny sensors positioned within the ear canal.\n\t\t- üëâ Electromyography (EMG). This records muscle movements and the information can be used to help understand facial expressions and jaw movements related to emotion.\n\t\t- üëâ Electrooculography (EOG) tracks eye movements, particularly side-to-side.\n\t\t- üëâ Electrocardiogram (ECG) typically measures the electrical activity of the heart.\n\t\t- üëâ Galvanic skin response (GSR), which provides an indirect measure of emotional arousal ‚Äì that is, the strength of an emotional response.\n\t\t- üëâ Blood volume pulse (BVP). This is measured using photoplethysmography and provides information about heart rate (HR) and heart rate variability (HRV).\n\t- In other words, the aim is to collect a very comprehensive set of neurological and biometric data from the user.¬†Creepy, right?!\n\t- It‚Äôs unclear to me how you could even record meaningful data from within the ear.\n\t- If this kind of interface goes ahead it should be\n\t\t- 1. Voluntary. Participants should not be forced or deceived into providing physiological or neurological data. Volunteers at liberty to stop at any time.\n\t\t- 2. Limited. Personal data may only be collected for a specific, explicit and legitimate purpose. This purpose must be clearly stated, and only stored as long as needed to complete that purpose.\n\t\t- 3. Transparent. Requires informed consent including being aware the data are being collected and knowing the risks involved, including whether the information will be shared with other organizations.\n\t\t- 4. Autonomy. Free from manipulation. Participants should not be forced or deceived into making decisions they would not otherwise make.\n\t\t- 5. Valid. Must be based on valid science and led by scientifically trained staff.\n\t- To my mind, this application potentially violates 4 out of 5 of these principles (I don‚Äôt see any evidence of manipulation) and this makes me deeply uneasy!\n\n## Current Landscape (2025)\n\n- Industry adoption and implementations\n  - Metaverse platforms continue to evolve with focus on interoperability and open standards\n  - Web3 integration accelerating with decentralised identity and asset ownership\n  - Enterprise adoption growing in virtual collaboration, training, and digital twins\n  - UK companies increasingly active in metaverse development and immersive technologies\n\n- Technical capabilities\n  - Real-time rendering at photorealistic quality levels\n  - Low-latency networking enabling seamless multi-user experiences\n  - AI-driven content generation and procedural world building\n  - Spatial audio and haptics enhancing immersion\n\n- UK and North England context\n  - Manchester: Digital Innovation Factory supports metaverse startups and research\n  - Leeds: Holovis leads in immersive experiences for entertainment and training\n  - Newcastle: University research in spatial computing and interactive systems\n  - Sheffield: Advanced manufacturing using digital twin technology\n\n- Standards and frameworks\n  - Metaverse Standards Forum driving interoperability protocols\n  - WebXR enabling browser-based immersive experiences\n  - glTF and USD for 3D asset interchange\n  - Open Metaverse Interoperability Group defining cross-platform standards\n\n## Metadata\n\n- **Last Updated**: 2025-11-16\n- **Review Status**: Automated remediation with 2025 context\n- **Verification**: Academic sources verified\n- **Regional Context**: UK/North England where applicable",
  "properties": {
    "id": "65d5d2b5-36a9-4cac-8efe-18bb9e2559d4",
    "collapsed": "true",
    "- ontology": "true",
    "- term-id": "mv-702620567296",
    "- preferred-term": "Hardware and Edge",
    "- source-domain": "metaverse",
    "- status": "active",
    "- public-access": "true",
    "- definition": "A component of the metaverse ecosystem focusing on hardware and edge.",
    "- maturity": "mature",
    "- authority-score": "0.85",
    "- owl:class": "mv:HardwareAndEdge",
    "- owl:physicality": "ConceptualEntity",
    "- owl:role": "Concept",
    "- belongsToDomain": "[[MetaverseDomain]]",
    "- enables": "[[ImmersiveExperience]], [[Presence]], [[SpatialComputing]]",
    "- requires": "[[DisplayTechnology]], [[TrackingSystem]], [[RenderingEngine]]",
    "- bridges-to": "[[HumanComputerInteraction]], [[ComputerVision]], [[Robotics]]",
    "public": "true"
  },
  "backlinks": [
    "Client side DCO",
    "Octave Multi Model Laboratory",
    "BC-0072-node",
    "Leopold Aschenbrenner",
    "Segmentation and Identification",
    "Interfaces",
    "State of the art in AI",
    "Hardware and Edge",
    "BC-0014-block-time"
  ],
  "wiki_links": [
    "TrackingSystem",
    "Hardware and Edge",
    "MetaverseDomain",
    "HumanComputerInteraction",
    "cuervo2018creating",
    "boring2009scroll",
    "ComputerVision",
    "RenderingEngine",
    "Large language models",
    "Proprietary Large Language Models",
    "hansberger2017dispelling",
    "SpatialComputing",
    "DisplayTechnology",
    "Robotics",
    "Update Cycle",
    "Presence",
    "ImmersiveExperience"
  ],
  "ontology": {
    "term_id": "mv-702620567296",
    "preferred_term": "Hardware and Edge",
    "definition": "A component of the metaverse ecosystem focusing on hardware and edge.",
    "source_domain": "metaverse",
    "maturity_level": null,
    "authority_score": 0.85
  }
}