{
  "title": "Predictions",
  "content": "- ### OntologyBlock\n  id:: predictions-ontology\n  collapsed:: true\n\t- ontology:: true\n\t- term-id:: mv-178553823927\n\t- preferred-term:: Predictions\n\t- source-domain:: metaverse\n\t- status:: active\n\t- public-access:: true\n\t- definition:: A component of the metaverse ecosystem focusing on predictions.\n\t- maturity:: mature\n\t- authority-score:: 0.85\n\t- owl:class:: mv:Predictions\n\t- owl:physicality:: ConceptualEntity\n\t- owl:role:: Concept\n\t- belongsToDomain:: [[MetaverseDomain]]\n\t- #### Relationships\n\t  id:: predictions-relationships\n\t\t- enables:: [[ImmersiveExperience]], [[Presence]], [[SpatialComputing]]\n\t\t- requires:: [[DisplayTechnology]], [[TrackingSystem]], [[RenderingEngine]]\n\t\t- bridges-to:: [[HumanComputerInteraction]], [[ComputerVision]], [[Robotics]]\n\n\t- #### OWL Axioms\n\t  id:: predictions-owl-axioms\n\t  collapsed:: true\n\t\t- ```clojure\n\t\t  Declaration(Class(mv:Predictions))\n\n\t\t  # Classification\n\t\t  SubClassOf(mv:Predictions mv:ConceptualEntity)\n\t\t  SubClassOf(mv:Predictions mv:Concept)\n\n\t\t  # Domain classification\n\t\t  SubClassOf(mv:Predictions\n\t\t    ObjectSomeValuesFrom(mv:belongsToDomain mv:MetaverseDomain)\n\t\t  )\n\n\t\t  # Annotations\n\t\t  AnnotationAssertion(rdfs:label mv:Predictions \"Predictions\"@en)\n\t\t  AnnotationAssertion(rdfs:comment mv:Predictions \"A component of the metaverse ecosystem focusing on predictions.\"@en)\n\t\t  AnnotationAssertion(dcterms:identifier mv:Predictions \"mv-178553823927\"^^xsd:string)\n\t\t  ```\n\n- Applications\n  - 1/ Model aggregators vs. single-model apps\n    - Model aggregators will take on R&D companies who only serve their own models.\n    - The depth and quality of the user interface will determine the winners and losers.\n    - Mainstream creators will prefer all-in-one apps.\n  - 2/ Story-level tools\n    - More innovation will happen to support the creative process end-to-end, including brainstorming, storyboard, clip generation, editing, and sound production.\n  - 3/ Template ecosystems\n    - The blank canvas problem will be solved: best AI artists will start creating templates that other AI creators can use.\n    - Fully AI-generated video stocks will become widely available and compete against traditional stocks.\n  - 4/ Real-time trends\n    - GenAI apps will get connected to real-time trends (music, sounds, memes, characters).\n    - This will allow users to create content that’s most relevant for the moment.\n\n- Adoption\n  - 5/ 10x growth in daily AI video creators\n    - Millions more people will start making AI art for self-entertainment and self-expression.\n    - Consumer-side adoption will far outpace B2B adoption in 2025.\n  - 6/ Superstar AI creators\n    - The “Mr Beast” or “Charlie D’Amelio” moment of AI creativity will happen in 2025.\n    - Several AI creators will get to 1M+ followers.\n    - This success will inspire millions more to start their AI creator journey.\n  - 7/ New genres\n    - We will see the explosion of crossovers (”Harry Potter + Star Wars”), adaptations, translations, and remakes of centuries of human culture.\n    - New genres will emerge, including shortform series (1-5min episodes), sketch comedy, mini-podcasts, and AI music videos.\n  - 8/ AI content factories\n    - There will be continuous growth of small companies running systems of social media accounts with high volumes of content created with the help of AI.\n  - 9/ “Me content”\n    - There will be more and better ways to create content based on a user selfie or self-description.\n    - Everyday people will be able to see full-length stories with themselves as protagonists.\n  - 10/ Adult content creation\n    - Some apps will embrace an adult-friendly approach and allow users to create NSFW content.\n    - At least one adult-friendly GenAI app will develop an effective marketing strategy without explicit demos on traditional social media.\n\n- Technology\n  - 11/ Open source will close the gap on video\n    - Open models like [Hunyuan], [CogVideoX], and [Mochi] will get even closer in quality to commercial models ([Sora](https://en.wikipedia.org/wiki/OpenAI#Sora), [Luma](https://venturebeat.com/ai/luma-ai-debuts-dream-machine-for-realistic-video-generation-heating-up-ai-media-race/), [Kling](https://venturebeat.com/ai/what-you-need-to-know-about-kling-the-ai-video-generator-rival-to-sora-thats-wowing-creators/), [Minimax](https://finance.yahoo.com/news/chinese-ai-tiger-minimax-launches-093000322.html)).\n    - At least one open model will catalyze an ecosystem of fine-tunes and plugins.\n  - 12/ Inference clouds\n    - Specialist cloud providers will run models faster and cheaper.\n    - Generation costs will fall 10x, $0.5 -> $0.05 per clip.\n    - Custom training costs will also fall by 5-10x.\n    - Lower costs will open AI video creation to a wider audience.\n  - 13/ New inference architectures\n    - More complex architectures will arise, combining multiple diffusion models, rule-based systems, test-time compute, character controls, community-trained nodes, and more.\n  - 14/ Specialized models\n    - Research focus will shift from foundational models to specialized tools: upscaling, regional editing, special effects, lip sync, and storyboarding.\n    - A new category, \"creative co-pilots,\" will emerge to assist creators through their entire creative process.\n\n- Challenges\n  - 15/ Copyright battles\n    - People will continue creating using copyrighted characters.\n    - Some IP holders will fight it, others will try to monetize it.\n    - A new licensing standard will emerge; it will NOT be blockchain-based despite efforts to make it so.\n  - 16/ On-device inference will not be impactful (yet)\n    - It will take 2-3 years before substantial parts of the creative process move from the cloud to user devices.\n  - 17/ No Hollywood breakthrough\n    - GenAI will only play a supporting role in TV and film production, helping with individual scenes, special effects, and pre-production.\n    - AI-native indie studios will take longer to succeed, with first mainstream releases in 2026 and beyond.\n  - 18/ No killer AI game\n    - There will be more pilots and demos for AI-generated games and worlds.\n    - No AI-native game will be consistently played for its quality rather than the novelty factor.\n  - 19/ No AI influencer\n    - The original idea of an AI influencer will not live up to its hype.\n    - Photorealistic human characters will not achieve celebrity-level influence and brand deal economics.\n    - The most successful AI accounts will not be human simulators.\n\n\n## Current Landscape (2025)\n\n- Industry adoption and implementations\n  - Metaverse platforms continue to evolve with focus on interoperability and open standards\n  - Web3 integration accelerating with decentralised identity and asset ownership\n  - Enterprise adoption growing in virtual collaboration, training, and digital twins\n  - UK companies increasingly active in metaverse development and immersive technologies\n\n- Technical capabilities\n  - Real-time rendering at photorealistic quality levels\n  - Low-latency networking enabling seamless multi-user experiences\n  - AI-driven content generation and procedural world building\n  - Spatial audio and haptics enhancing immersion\n\n- UK and North England context\n  - Manchester: Digital Innovation Factory supports metaverse startups and research\n  - Leeds: Holovis leads in immersive experiences for entertainment and training\n  - Newcastle: University research in spatial computing and interactive systems\n  - Sheffield: Advanced manufacturing using digital twin technology\n\n- Standards and frameworks\n  - Metaverse Standards Forum driving interoperability protocols\n  - WebXR enabling browser-based immersive experiences\n  - glTF and USD for 3D asset interchange\n  - Open Metaverse Interoperability Group defining cross-platform standards\n\n## Metadata\n\n- **Last Updated**: 2025-11-16\n- **Review Status**: Automated remediation with 2025 context\n- **Verification**: Academic sources verified\n- **Regional Context**: UK/North England where applicable",
  "properties": {
    "id": "predictions-owl-axioms",
    "collapsed": "true",
    "- ontology": "true",
    "- term-id": "mv-178553823927",
    "- preferred-term": "Predictions",
    "- source-domain": "metaverse",
    "- status": "active",
    "- public-access": "true",
    "- definition": "A component of the metaverse ecosystem focusing on predictions.",
    "- maturity": "mature",
    "- authority-score": "0.85",
    "- owl:class": "mv:Predictions",
    "- owl:physicality": "ConceptualEntity",
    "- owl:role": "Concept",
    "- belongsToDomain": "[[MetaverseDomain]]",
    "- enables": "[[ImmersiveExperience]], [[Presence]], [[SpatialComputing]]",
    "- requires": "[[DisplayTechnology]], [[TrackingSystem]], [[RenderingEngine]]",
    "- bridges-to": "[[HumanComputerInteraction]], [[ComputerVision]], [[Robotics]]"
  },
  "backlinks": [],
  "wiki_links": [
    "ComputerVision",
    "Presence",
    "MetaverseDomain",
    "TrackingSystem",
    "DisplayTechnology",
    "RenderingEngine",
    "ImmersiveExperience",
    "Robotics",
    "HumanComputerInteraction",
    "SpatialComputing"
  ],
  "ontology": {
    "term_id": "mv-178553823927",
    "preferred_term": "Predictions",
    "definition": "A component of the metaverse ecosystem focusing on predictions.",
    "source_domain": "metaverse",
    "maturity_level": null,
    "authority_score": 0.85
  }
}