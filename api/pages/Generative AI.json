{
  "title": "Generative AI",
  "content": "- ### OntologyBlock\n  id:: generative-ai-ontology\n  collapsed:: true\n\n  - **Identification**\n    - ontology:: true\n    - term-id:: AI-0860\n    - preferred-term:: Generative AI\n    - source-domain:: ai\n    - status:: complete\n    - public-access:: true\n    - version:: 2.0.0\n    - last-updated:: 2025-01-15\n    - quality-score:: 0.90\n\n  - **Definition**\n    - definition:: [[Generative AI]] encompasses [[Machine Learning]] systems capable of creating new content across modalities including text, images, audio, video, and code through [[Neural Networks]] trained on large datasets. These systems use [[Deep Learning]] architectures like [[Transformer]]s, [[Diffusion Models]], [[GANs]], and [[VAEs]] to learn data distributions and generate novel, coherent outputs. Generative AI represents a paradigm shift from discriminative models, enabling creative applications in [[Content Creation]], [[Design]], [[Art]], [[Music Generation]], and [[Code Synthesis]].\n    - maturity:: mature\n    - source:: [[OpenAI]], [[Stability AI]], [[Midjourney]], [[Anthropic]], [[Google DeepMind]], [[NIST AI Standards]]\n    - authority-score:: 0.93\n\n  - **Semantic Classification**\n    - owl:class:: ai:GenerativeAI\n    - owl:physicality:: VirtualEntity\n    - owl:role:: Process\n    - owl:inferred-class:: ai:VirtualProcess\n    - belongsToDomain:: [[AI-GroundedDomain]], [[CreativeMediaDomain]], [[ComputationAndIntelligenceDomain]]\n\n  - #### OWL Restrictions\n    - is-subclass-of some MachineLearning\n\n  - #### CrossDomainBridges\n    - bridges-to:: [[MachineLearning]] via is-subclass-of\n    - bridges-from:: [[BehaviouralFeedbackLoop]] via has-part\n\n  - #### OWL Restrictions\n    - is-subclass-of some MachineLearning\n    - implementedInLayer:: [[ComputeLayer]], [[DataLayer]], [[AlgorithmicLayer]]\n\n  - #### Relationships\n    id:: generative-ai-relationships\n    - is-subclass-of:: [[Artificial Intelligence]], [[Machine Learning]], [[Deep Learning]]\n    - has-part:: [[Large Language Models]], [[Diffusion Models]], [[GAN]], [[VAE]], [[Neural Network]], [[Training Data]]\n    - implements:: [[Text Generation]], [[Image Generation]], [[Audio Generation]], [[Video Generation]], [[Code Generation]]\n    - requires:: [[Neural Network Training]], [[Large-Scale Datasets]], [[GPU Compute]], [[Model Architecture]]\n    - enables:: [[Content Creation]], [[Creative AI]], [[Automated Design]], [[Synthetic Media]], [[AI Art]]\n    - related-to:: [[Foundation Models]], [[Prompt Engineering]], [[Fine-Tuning]], [[Model Training]], [[Transfer Learning]]\n    - bridges-to::\n      - [[NFT]] (domain: blockchain - for AI-generated art ownership)\n      - [[Smart Contract]] (domain: blockchain - for AI content licensing)\n      - [[Decentralized Storage]] (domain: blockchain - for training data)\n      - [[Lightning Network]] (domain: bitcoin - for micropayments to AI services)\n\n  - #### OWL Axioms\n    id:: generative-ai-owl-axioms\n    collapsed:: true\n    - ```clojure\n      Prefix(ai:=<http://purl.org/ai-ontology#>)\n      Prefix(bc:=<http://purl.org/blockchain-ontology#>)\n      Prefix(dt:=<http://purl.org/disruptive-tech/bridges#>)\n      Prefix(owl:=<http://www.w3.org/2002/07/owl#>)\n      Prefix(xsd:=<http://www.w3.org/2001/XMLSchema#>)\n      Prefix(rdfs:=<http://www.w3.org/2000/01/rdf-schema#>)\n\n      Ontology(<http://purl.org/ai-ontology/AI-0860>\n\n        Declaration(Class(ai:GenerativeAI))\n        AnnotationAssertion(rdfs:label ai:GenerativeAI \"Generative AI\"@en)\n        AnnotationAssertion(rdfs:comment ai:GenerativeAI\n          \"AI systems capable of creating new content across multiple modalities\"@en)\n\n        SubClassOf(ai:GenerativeAI ai:MachineLearning)\n        SubClassOf(ai:GenerativeAI ai:DeepLearning)\n\n        # Generative AI can create content\n        SubClassOf(ai:GenerativeAI\n          ObjectSomeValuesFrom(ai:canGenerate ai:Content))\n\n        # Bridge to Blockchain: Generated content can be minted as NFT\n        SubClassOf(ai:GenerativeAI\n          ObjectSomeValuesFrom(dt:canMintAs bc:NFT))\n\n        # Bridge to Bitcoin: Can accept Lightning payments\n        SubClassOf(ai:GenerativeAI\n          ObjectSomeValuesFrom(dt:canAcceptPayment btc:LightningNetwork))\n      )\n      ```\n\n- ## About Generative AI\n\n- ### Intelligence and AI Definitions (2025)\n- To set the tone, let's have [[OpenAI]]'s [[ChatGPT]] give us a definition:\n\t- *Intelligence is the ability to acquire and apply knowledge and skills in order to solve problems and adapt to new situations. It can involve a range of [[Cognitive Abilities]], such as [[Perception]], [[Learning]], [[Memory]], [[Reasoning]], and [[Decision-Making]]. Intelligence is a complex and multifaceted concept that has been studied by psychologists, philosophers, and scientists for centuries.*\n- The Oxford English Dictionary defines [[Artificial Intelligence]] as:\n\t- \"The capacity of computers or other machines to exhibit or simulate intelligent behaviour\".\n- This is murky territory. The boundary line between very capable trained systems and something that feels like intelligence is subjective, varying for each person and context.\n- The terminology in the field is both somewhat blurred and highly 'nested', encompassing [[Machine Learning]], [[Deep Learning]], [[Neural Networks]], and [[Generative AI]].\n- We will use AI and ML interchangeably in this text, but is so doing wehope to draw attention to the moment we find ourselves in. It feels likethere is an inflection point in human history happening right now,though to somewhat burst the bubble on this hyperbole it’s worth readingthe legendary Stephen Wolframs [explanation of these currentsystems](https://writings.stephenwolfram.com/2023/02/what-is-chatgpt-doing-and-why-does-it-work/)as glorified autocompletes.\n- Irrespective of the gap between the perception and truth around thesesystems there is now a feedback loop where the data that these systemsare trained on will be learning from both human **and** outputs fromsuch systems. Todays young children will never know a world in which theinformation they encounter is verifiable as of purely human origin. Theimplications of this are unclear but exciting. In writing this book itbecame obvious to add this chapter in, and change the direction on theresearch and product development, because nothing in human history willremain untouched by this. As we will see ‘metaverse’ is likely to changeat an incredible rate as a function of some parts of this technology.\n  ![]./assets/5823b4852c910b4a37b5a73f739551a66eda08b4.png\n  Major stands of generative AI and their associated models at the time of print.\n- <a href=\"#fig:llmlandscape\" data-reference-type=\"ref\" data-reference=\"fig:llmlandscape\">[fig:llmlandscape]</a>).\n  ![]./assets/ff177772944d3f8035ac7b75559cecc8b65176cd.jpg\n  Major stands of large language models from Yang et al<span class=\"citation\" data-cites=\"yang2023harnessing\"><a href=\"#fn1\" class=\"footnote-ref\" id=\"fnref1\" role=\"doc-noteref\"><sup>1</sup></a></span><section class=\"footnotes\" role=\"doc-endnotes\"><hr /><ol><li id=\"fn1\" role=\"doc-endnote\"><p>Yang et al., “Harnessing the Power of Llms in Practice.”<a href=\"#fnref1\" class=\"footnote-back\" role=\"doc-backlink\">↩︎</a></p></li></ol></section>\n- ### Generative Art Systems (2025)\n- [[Generative Art]] refers to art that is generated algorithmically rather than manually created using [[Artificial Intelligence]], [[Neural Networks]], and [[Computational Creativity]]. This encompasses [[AI Art]], [[Procedural Generation]], and [[Algorithmic Art]].\n- Generative art emerged in the 1960s alongside early [[Computer Art]]. Artists like Frieder Nake and Georg Nees used algorithms to create abstract visual art. In the 1970s and 80s, [[Fractal Geometry]] allowed for complex recursive patterns. More recently, [[Neural Networks]] and [[Deep Learning]] have enabled revolutionary forms of [[Image Generation]], which are the focus of current research and applications.\n- #### Modern Models and Systems\n- #### Image Generation (2025)\n-\n\t- **Stable Diffusion**\n\t- An [[Open Source]] [[Diffusion Model]] capable of creating realistic images from [[Text Prompts]] using [[Latent Diffusion]]. Widely used by artists and developers due to:\n-\n\t- Flexible and intuitive [[Prompt Engineering]] capabilities\n-\n\t- Many interfaces and extensions for control, most notably [[ControlNet]] for precise spatial guidance\n-\n\t- Ability to [[Fine-Tuning]] on custom datasets using [[DreamBooth]] and [[LoRA]]\n\t- Supports [[Inpainting]], [[Outpainting]], and [[Image-to-Image]] translation\n\t- Can integrate with [[Bitcoin]] wallets for [[Decentralized AI]] art marketplaces\n-\n\t- **DALL-E 3**\n\t- A proprietary [[Generative AI]] system from [[OpenAI]] that creates images from text captions using advanced [[Diffusion Models]]. Key features (2025):\n-\n\t- Stunning contextual comprehension and [[Semantic Understanding]]\n-\n\t- Diverse creative capabilities from natural language [[Prompts]]\n-\n\t- Works with [[OpenAI API]] and paid credits, enabling [[Pay-Per-Use]] via [[L402 Protocol]]\n-\n\t- Integrated with Microsoft Bing and [[ChatGPT]], with limited free tier\n-\n\t- **Midjourney**\n\t- Web-based [[Generative Art]] tool with social community aspects. Notable for (2025):\n-\n\t- Easy to start generating images quickly through [[Discord]] interface\n-\n\t- Built-in sharing and voting on generations via [[Community Curation]]\n-\n\t- Best-in-class aesthetic \"vibes\" and [[Artistic Style]]\n-\n\t- Subscription tiers with commercial licensing options\n\t- Supports [[NFT]] minting for AI-generated artwork\n-\n\t- Privacy is questionable\n-\n\t- Subject to change, making consistency of approach impossible.\n-\n\t- **Imagen**\n\t- AI system from Google producing images from text. Characteristics:\n-\n\t- Very high-resolution outputs\n-\n\t- Control over lighting and detail\n-\n\t- Currently restricted to limited partners\n- #### How they work\n- There’s a good detailed and in depth blog post by Weng[here](https://lilianweng.github.io/posts/2021-07-11-diffusion-models/#classifier-free-guidance).\n- The four main types are over-viewed [byDeshwal](https://www.linkedin.com/feed/update/urn:li:activity:7088752096853803008/)as below:\n-\n\t- \"Adversarial\" as the name suggests are two opposite networks. One learns to create images out of noise (Artist) which is actually very hard process and the other says \"umm okay! This isn’t good\" (Critic) which is a relatively easy process. So because of the fact that being an artist is very hard than being a critic, these networks are not stable and Critic learns faster than the Artist.\n-\n\t- VAE, U-Nets etc belong to this category where same network breaks down image in deeper level features using CNN and then rebuild it again. That’s like a child learning by breaking things. VAE and U-Nets are almost same with minor differences and serve as a base model in Diffusion process so that think of them ans analogues to BERT in LLMs.\n-\n\t- : Well here it becomes complex. You apply function X to an image and then you re-create the original image by applying the Exact inverse of that function. For example, a very basic function is to add 5 and then subtract 5 to get original stuff.\n-\n\t- VAE, U-Nets are used as base models. You insert some pseudo random (because you know what you added based on a timestamp \"T\") Gaussian noise to an image and instead of asking the model to predict original image, you ask the model to predict the Noise that you inserted. Since Gaussian is an additive noise independent of original signal, you subtract that from image and get original image. Another piece is that instead of predicting whole noise at once, you predict the noise for previous (T-1) step.\n- Intuitive interfaces provide easy access to these models.[Automatic1111’s WebUI](https://github.com/AUTOMATIC1111/stable-diffusion-webui) offers afull-featured frontend to Stable Diffusion.[Midjourney](https://www.midjourney.com) provides a social platform tocreate, share and discuss AI art. [Runway](https://www.runwayml.com)delivers generative models through a subscription service.\n- Fine-tuning techniques like[DreamBooth](https://arxiv.org/abs/2208.12242) allow customizing StableDiffusion by training on datasets of specific concepts.[styleGAN-NADA](https://github.com/yfszzx/stylegan-nada) improves imagequality through noise optimization.[styleGAN3](https://github.com/NVlabs/stylegan3) introduces a generatorarchitecture that achieves state-of-the-art results. Extensions like[Gaugan](https://github.com/mit-han-lab/gaugan) bring control overseasons, weather, lighting and more.\n- Vibrant communities continually push boundaries of generative artthrough platforms like the [deepdreamsubreddit](https://www.reddit.com/r/deepdream/), the [ML Art Colabsrepository](https://github.com/dvschultz), and the [Stability AIblog](https://stability.ai/blog). Events like [ButterflyDream](https://www.elfman.ai/) showcase creativity.\n- Beyond generation, image processing techniques can manipulate existingvisuals. [GFPGAN](https://github.com/TencentARC/GFPGAN) restores blurredfaces using facial landmarks and semantic segmentation.[BRDNet](https://github.com/dazhizhong/BRDNet) removes unwanted objectsthrough edge-aware propagation and diffusion.[Real-ESRGAN](https://github.com/xinntao/Real-ESRGAN) super-resolvesimages up to 16x resolution.[DeOldify](https://github.com/jantic/DeOldify) colorizes black and whitephotos through deep learning. Such techniques enable restoring,retouching and enhancing images.\n- ##### Stable Diffusion\n- ##### Stable Diffusion Ecosystem\n- As an open-source diffusion model, [StableDiffusion](https://arxiv.org/abs/2105.05233) has given rise to anextensive ecosystem of models, interfaces, extensions, and communities.\n- ###### Models\n- The core Stable Diffusion repository provides strong baselines like[sd-v1-4](https://github.com/CompVis/stable-diffusion) optimized forspeed and [sd-v2-1k](https://github.com/Stability-AI/stablediffusion)for 1024x1024 resolution. Models exist for specific domains like anime,manga, and hentai.\n- ###### Interfaces\n- Many open-source frontends provide access to Stable Diffusion:\n-\n\t- [Automatic1111’s Web UI](https://github.com/AUTOMATIC1111/stable-diffusion-webui)\n\t- full-featured frontend with extensions\n-\n\t- [Disco Diffusion](https://github.com/alembics/disco-diffusion)\n\t- focused on creative exploration\n-\n\t- [Stable Diffusion GUI](https://github.com/hlky/stable-diffusion)\n\t- cross-platform interface supporting Google Colab\n-\n\t- [A1111-SD-webui-colab](https://github.com/invincible-sam/A1111-SD-webui-colab)\n\t- run Stable Diffusion entirely in Google Colab\n- ###### Extensions\n- Additional modules provide enhanced control:\n-\n\t- [ControlNet](https://github.com/lllyasviel/ControlNet)\n\t- mask-based image editing\n-\n\t- [Vedaso](https://github.com/alembics/vedaso)\n\t- creative effect brushes\n-\n\t- [Stable Diffusion Tuner](https://github.com/camenduru/stable-diffusion-webui-tuner)\n\t- fine-tune model inside Web UI\n-\n\t- [InvokeAI](https://github.com/invoke-ai/InvokeAI)\n\t- optimized inference and rendering\n- ###### Training & Tuning\n- Stable Diffusion can be customized:\n-\n\t- [DreamBooth](https://github.com/Stability-AI/dreambooth)\n\t- fine-tune on specific concepts\n-\n\t- [Stable Diffusion Tuning](https://github.com/yfszzx/stable-diffusion-stability-ai)\n\t- improve image quality\n-\n\t- [SD highres fix](https://github.com/d8ahazard/sd_hires_face_fix)\n\t- enhance face quality\n- ###### Community\n- Vibrant communities continually advance Stable Diffusion:\n-\n\t- [StableDiffusion subreddit](https://www.reddit.com/r/StableDiffusion/)\n\t- sharing creations and discoveries\n-\n\t- [Stability AI Discord](https://discord.gg/stabilityai)\n\t- dedicated channels on SD topics\n-\n\t- [Civitai](https://civitai.com)\n\t- central model hub with versioning\n- ##### ComfyUI\n- [ComfyUI](https://github.com/comfyanonymous) is a feature-rich set oftools and libraries for building interactive web applications using the[React](https://reactjs.org) framework. It makes creating beautiful,functional UIs easy through:\n-\n\t- **React-Based**\n\t- Built on React for modular, reusable components\n-\n\t- **Declarative**\n\t- Describe desired UI state without implementation details\n-\n\t- **Extensible**\n\t- Easily add custom components and functionality\n-\n\t- **Testable**\n\t- Designed for confident testing of UI behavior\n-\n\t- **Documented**\n\t- Well-documented for easy learning\n-\n\t- **Community**\n\t- Large active community for help and support\n- Extensions provide additional capabilities:\n-\n\t- **Components**\n\t- Pre-built React components for common UI elements like buttons, menus, and forms\n-\n\t- **Animations**\n\t- Animated React components for engaging UIs\n-\n\t- **State Management**\n\t- Tools for managing UI state\n-\n\t- **Testing**\n\t- Utilities for testing ComfyUI applications\n- Other notable features include:\n-\n\t- **Responsive Design**\n\t- Components auto-adjust layouts for any device size\n-\n\t- **Internationalization**\n\t- Support for global applications in different languages\n-\n\t- **Accessibility**\n\t- Interface remains usable by people with disabilities\n- The ComfyUI ecosystem is constantly evolving with new extensions createdby the vibrant community. With its versatility, extensibility andhelpful userbase, ComfyUI empowers developers to create beautiful,functional UIs for diverse web applications. The declarative programmingstyle and reusable components help quickly build interfaces that areresponsive, accessible, and internationalized.\n- #### Video generation\n- This is incredibly fast moving area and I have many many links in myMindmap. This section is a placeholder really, I wouldn’t act on it.\n-\n\t- **DALL-E 3D**\n\t- 3D model generation by Anthropic using principles from DALL-E 2. Allows:\n-\n\t- Text-to-shape generation\n-\n\t- Control over materials and lighting\n-\n\t- Interaction with object geometry\n-\n\t- **Xformation**\n\t- Proprietary 3D generation system capable of modifying shape from images.\n-\n\t- Deforms template 3D objects to match 2D images\n-\n\t- Controllable 3D effects from image edits\n-\n\t- **Text2Mesh**\n\t- Leveraging Stable Diffusion for text-based 3D model generation.\n-\n\t- 3D stylization based on natural language input\n-\n\t- Control mesh topology and appearance\n-\n\t- **Gaussian Splatting**\n\t- A development from the NeRF technology research, and likely the main contender for all future tech right now..\n-\n\t- Fast and efficient models\n-\n\t- Simple capture\n- Extending image synthesis techniques, models like [StableDiffusion](https://arxiv.org/abs/2105.05233) are being adapted togenerate artificial video content. Dedicated systems like[Phenaki](https://www.anthropic.com/research/phenaki) and[Runway](https://runwayml.com) enable text-to-video generation withcontrol over length, resolution and scene contents.\n- Creating smooth, consistent video requires modeling inter-framecoherence. Techniques like [EBSynth](https://ebsynth.com) achieve thisthrough interpolation and style transfer between frames.[FastVid2Vid](https://www.fastvideoai.com) matches latent vectorsbetween frames to improve consistency. [Instant Neural GraphicsPrimitives](https://nvlabs.github.io/instant-ngp) (Instant-NGP) learns atemporal model over sequences of frames.\n- Existing videos can also be enhanced through diffusion models.Techniques enable automatically increasing resolution, translatingscenes to different styles, editing objects seamlessly, and more.However, concerns exist around deepfake videos and synthetic media.Moderation systems like [Anthropic’s Claude](https://www.anthropic.com)may provide remedies.\n- Overall, rapid progress in generative video hints at possibilities ofcreating immersive films, VR experiences, lifelike avatars and more. Butthoughtful governance frameworks are necessary to manage risks.\n- #### Audio generation\n- Recent breakthroughs have also extended AI synthesis to the audiodomain, enabling applications like text-to-speech, music composition,and editing podcasts.\n- Models like [Jukebox](https://jukebox.openai.com/) and [Facebook’sJukebox](https://github.com/ facebookresearch/jukebox) generate novelmusic conditioned on genres, artists, and lyrics through a hierarchicalVQ-VAE framework. Meanwhile, [CoquiTTS](https://github.com/coqui-ai/TTS) and [Tacotron2](https://github.com/NVIDIA/tacotron2) convert text to human-likespeech using end-to-end neural architectures.\n- For editing audio, tools like [Riptide](https://riptide.ai/) removevocals from songs, while [Descript](https://www.descript.com/) insertsmusic and trims silences in podcasts. However, bad actors could exploitsuch capabilities for impersonation fraud and fake media. Stronggovernance models are critical.\n- Looking ahead, advances in generative audio may enable creatinginteractive AI companions, realistic speech synthesis, and personalizedmusic experiences. But maintaining public trust through transparency andaccountability will be essential.\n- #### 3D generation\n- 3D shape generation has also made strides through AI, transitioningtext-to-image breakthroughs to the 3D domain. Methods like[GA-Fusion](https://nv-tlabs.github.io/GA-fusion) combine GANs withgradient-based optimization for high quality results.[CLIP-Forge](https://github.com/autodeskailab/clip-forge) matchesrendered images with CLIP embeddings to guide optimization.[3D-Highlighter](https://threedle.github.io/3dhighlighter) localizestext prompts on shapes by comparing CLIP similarities.\n- Other approaches focus on reconstructing shapes from images.[XFormation](https://x-a-i.github.io/xformation) deforms template 3Dshapes to match target views. [Sketch-Guided VisionModels](https://sketching-the-future.github.io) optimize an SDF to matchinput sketches. [Dream Fields](https://ajayj.com/dreamfields) uses aNeRF parameterized by FiLM.\n- Such techniques could enable creators to manifest their ideas into 3Dworlds. However, thoughtful governance is critical to reduce risksassociated with impersonation, toxic content, and intellectual property.Community building, education, and responsible deployment will helprealize the positive potential of AI.\n- #### Conclusion\n- Rapid progress in AI has unlocked breakthrough capabilities forsynthesizing realistic content across images, video, audio, and 3Dgeometry. However, concerns around biases, misinformation, and toxiccontent necessitate responsible development and deployment of thesetechnologies. Maintaining public trust through transparency,accountability and inclusivity will be key to ensuring that the benefitsof AI progress outweigh the risks. If harnessed judiciously andethically, generative AI could augment human creativity in unprecedentedways. But it should empower rather than replace us. Ongoing advances inAI safety and governance will help achieve this vision\n- #### Major trends in AI\n- ##### The concentration of AI power\n- In recent times, the arena of artificial intelligence (AI) has seen theemergence of colossal entities that have taken the helm of AI researchand development. Prominent players such as Google, Microsoft, Meta, andOpenAI have plunged billions into the cultivation of potent AIarchitectures, with a special emphasis on large language models (LLMs)like GPT-3 and ChatGPT.\n- Originating in 2015 as a non-profit entity dedicated to the openexploration of AI for the collective good, OpenAI transitioned from itsfoundational ethos following a pivotal investment of $1 billion fromMicrosoft in 2019. This infusion of capital marked a turn towards a moreproprietary and competitive orientation, with the endgame of reachingthe pinnacle of artificial general intelligence (AGI). In this paradigmshift, OpenAI’s formidable 175 billion parameter behemoth, GPT-4, becamean enigmatic entity, shielded from public scrutiny. The rationaleprovided for this clandestine stance revolved around safety andcompetitive considerations.\n- Contrastingly, Meta adopted a path of openness, fully disclosing its 65billion parameter LLaMA 2 LLM, inclusive of the model weights, to thepublic domain. This ethos is rooted in the belief that a culture ofopenness propels rapid advancement by paving the way for widespreadexperimentation and communal contributions. However, it is pertinent tonote that Meta’s LLaMA 2 does carry stipulations on commercialexploitation.\n- Initially, Google was at the forefront of AI innovation with itsTensorFlow framework, but has seen its leading position eroded by Meta’sPyTorch. Post the commercial success of its products, Google’s AIendeavors have veered towards a more proprietary model, with novelmodels and academic publications seeing the light of day postcommercialization.\n- ##### Concentration of Power and Control\n- The centralization of AI evolution within the confines of a select fewprivate behemoths such as Microsoft-backed OpenAI or Google engenders anucleus of power and control over AI advancements. Contrary Research hasan [excellentreport](https://research.contrary.com/reports/the-openness-of-ai) onthis. As AI melds deeper into the fabric of products and services, thesetitans stand to wield extensive sway over the modalities of humancommunication, thought processes, and information accessibility.\n- The dependency on a sparse set of centralized LLMs harbors risks such asa widespread dissemination of confidential data in the face of asecurity breach. Moreover, the consolidated control furnishes thesecorporations with the means to potentially curtail information or moldnarratives in alignment with their vested interests. For instance,OpenAI exercises centralized control over the narrative frameworks ofits influential models like ChatGPT.\n- ##### Lack of Transparency and Innovation\n- Centralized LLMs exhibit a veil of opacity regarding their operationalmechanics and training methodologies. The elusive nature of OpenAI’sGPT-4 renders it a veritable black box, impervious to audits aimed atuncovering issues such as bias within the 175 billion parameter model’straining data. This shroud of mystery precludes informed discourse onthe ethical employment of this technology.\n- On the flip side, Meta’s open-sourced LLaMA facilitates a level ofpublic oversight that could preemptively address inherent issues. Theclandestine nature of closed models stifles innovation as it bars asignificant portion of the research community from building atop thesemodels. Open ideologies foster a milieu of collaboration, propellingprogress forward.\n- In summation, the monopolization of AI progression and influence withina handful of private juggernauts engenders risks spanning lack oftransparency, potential censorship, stifled innovation, and singlepoints of vulnerability. A paradigm shift towards more open anddecentralized methodologies is imperative to mitigate these loomingthreats.\n- ##### Some ways of thinking\n- Poulter [CEO](https://twitter.com/jamespoulter) of Vixen Labs has comeup with a somewhat strained analogy he calls “The Central IntelligenceAgent”. I’m going to include it until I find something better because Ithink he’s struck on something by dividing up the company needs with histaxonomy (Figure<a href=\"#fig:vixenAnthro\" data-reference-type=\"ref\" data-reference=\"fig:vixenAnthro\">8.4</a>).\n- <span class=\"image\">Vixen Labs anthropomorphic taxonomy of businessfunctions</span>\n- [“Renowned Yale Professor Jeffrey Sonnenfeld Discusses CEOs’ Fear andConfusion ofA.I”](https://observer.com/2023/06/ceo-ai-survey-yale-professor/) in arecent survey and presentation, and this is worth a quick look.\n- He sees five groups (per the article):\n-\n\t- “Curious creators” argue everything you can do, you should do. (Venture capitalist Marc Andreessen recently expressed a similar view in a blog post about A.I.)\n-\n\t- “Euphoric true believers” only see the good in technology.\n-\n\t- **“Commercial profiteers” don’t necessarily understand the new technology but are enthusiastically seeking to cash in on the hype.**\n-\n\t- “Alarmist activists” advocate for restricting A.I.\n-\n\t- “Global governance advocates” support regulation and necessary crackdown.\n- This seems a pretty simplistic set of buckets, but he’s got a decentdataset, and he’s -very- eminent so perhaps we should realistically seeourselves in the emboldened commercial profiteer category. I think thiskind of self reflection is important when dealing with things thispotentially transformational. Sonnenfeld said: it“As Robert Oppenheimerwarned us, it can be very dangerous to think that technology only takesus to the best of the world.”\n- ##### Trusted enterprise AI\n- Enterprise AI, specifically designed or retrofitted for professional usecases, is becoming a significant theme. This trend is driven by leadingcompanies such as Google, Microsoft, and latterly, curiously,Salesforce. Trust has become a primary consideration. At this earlystage in the technology it is important that corporate and private usersalike bear in mind that the LLMs are ‘leaky’ and are using the data fedinto them to train themselves. They are [explicit aboutthis](https://help.openai.com/en/articles/6783457-chatgpt-general-faq).Anything that goes into ChatGPT can resurface, as Samsung have found out[to their cost](https://cybernews.com/news/chatgpt-samsung-data-leak/).At this time the following companies have responded by banning the useof the technology internally.\n-\n\t- Apple\n-\n\t- Samsung\n-\n\t- Verizon\n-\n\t- Bank of America Corp.\n-\n\t- Citigroup Inc.\n-\n\t- Deutsche Bank AG\n-\n\t- Goldman Sachs Group Inc.\n-\n\t- Wells Fargo & Co.\n-\n\t- JPMorgan Chase & Co.\n-\n\t- Amazon’s lawyers recommended caution, though a recently leaked document suggests that managers are recommending it’s use.\n- There are likely itmany more who have done this less publicly, andindeed I am aware of examples. In response to this [OpenAIannounced](https://openai.com/blog/new-ways-to-manage-your-data-in-chatgpt)a business version of its tool, ChatGPT Business. Clearly this is apremium subscription service designed to be private by default. Theservice manages multiple users and does not train its future models onany conversations that flow through the business. This approach is asignificant step towards establishing trust in AI tools, as it ensuresthat sensitive business conversations are not used to train AI models.\n- As mentioned, Salesforce has been partially AI-powered for years. Theyrecently announced a series of AI-related developments, including thepilot of ‘Einstein GPT’, dubbed “the world’s first generative AI forCRM”. This tool builds on an existing underlying intelligence layercalled Einstein, which has been running in Salesforce since 2016. Thenew generative Einstein GPT is more content-oriented, helping businessesauto-generate text, pictures, and code. This tool is designed to helpsales teams find the most likely next customer to buy. Moreinterestingly they are leveraging their expertise in ‘SalesforceVentures’ a $500 million fund focused on funding generative AI startups.They have already invested in major projects like Humane, Triple,Anthropic, and Cohere.\n- They have italso announced an AI Cloud suite: ‘Salesforce’s AI Cloud’.It includes nine GPT-powered applications designed to automate andenhance various business processes. These applications include SalesGPT, Service GPT, Marketing GPT, Commerce GPT, **Slack GPT**, TableauGPT, Flow GPT, and Apex GPT.\n- Each of these applications is designed to cater to specific businessneeds, such as personalizing text generation for emails, automatingmundane tasks, customizing messages for different audience segments, andcreating workflows from natural language prompts. You can see that ourwork is already a customer here and this could be built upon.\n- This suite emphasizes the ‘Einstein GPT Trust Layer’, designed to ensureno potential data leakage, allowing enterprises to use AI for their mostsensitive needs. They say this trust layer sits between customer dataand the AI models, ensuring that the AI capability can providepredictions without actually looking inside the data. This approachwould allow our work to leverage the power of enterprise AI withoutsacrificing data privacy and/or security.\n- Elsewhere in enterprise AI:\n-\n\t- Accenture announced a $3 billion investment into their data and AI practice. This investment includes doubling their talent to 80,000 people, launching an AI navigator for Enterprise platform, and starting accelerators for data and AI readiness across 19 different industries.\n-\n\t- Contextual, an enterprise-focused AI startup, recently launched out of stealth with $20 million in seed funding.\n-\n\t- Glean announced a $100 million round and introduced a workplace chatbot called the Glean Chat Business.\n-\n\t- Olive: This healthcare automation startup has raised $450 million in fresh capital to build out its enterprise AI for hospitals 1.\n-\n\t- Welltok: This company provides a cloud-based employee wellness platform and has raised $355 million 1.\n-\n\t- Outreach: This sales engagement platform has raised $114 million in series E funding 1.\n-\n\t- Stackpath: This cybersecurity startup has raised $396 million in funding 1.\n-\n\t- Cohere, which is another business-focused AI startup, recently announced a $270 million series C funding round 2, and are partnered by [Oracle](https://www.oracle.com/news/announcement/oracle-to-deliver-powerful-and-secure-generative-ai-service-for-business-2023-06-13/).\n-\n\t- Tunisian enterprise AI startup InstaDeep has also raised $100 million in Series B financing led by Alpha 3.\n-\n\t- There’s a raft of tools like [CustomGPT](https://customgpt.ai/), or day planner [Before Sunset](https://www.beforesunset.ai/) that promise to take your data and make it smart by leveraging their deals with the big cloud providers. The prime example of this approach is [Dropbox AI](https://blog.dropbox.com/topics/product/introducing-AI-powered-tools), which claims to bring the Apple spotlight experience, with ChatGPT smarts, to clouds data. I don’t have confidence that I know enough about this, and it seems to be the purview of the AI-Club. If you have a use case there’s likely a product, but we’d have to project plan it in specifically and find the right fit and cost/benefit.\n- Taken overall these investments indicate a strong belief in thetransformative potential of AI in the enterprise sector.\n- With all this said it’s possible the technology is over-hyped. Whilesome believe that AI will disrupt industries in unimaginable ways,others argue that the technology still has a long way to go. Some evenargue that the distracting nature of the platforms may be net negativein the short term. Regardless, the current state of Enterprise AIrepresents a pivotal moment, with companies trying to productise AI andchange workflows within large corporations. The impact of thesedevelopments could range from a promise of transformation with AI beingevery bit as disruptive as everyone says it is, to an overhyped flop, asoften happens with new technologies. Some industry analysts argue thatwe’re still in the early stages of AI’s potential impact. Tech analystDan Ives likens the current state of AI to a “Gold Rush” moment,suggesting that we’re closer to 1995 than 1999 in terms of AI’sevolution and impact on industries. This perspective suggests that whileAI has made significant strides, there’s still a long way to go beforeit fully transforms the business landscape. I would tentatively agreewith this analysis, and avoid over investing in low confidence FOMOplays.\n- ##### Brute forcing ChatGPT4 with contexts\n- While we await ‘ChatGPT Business’ it’s still possible to explore usingthe tooling. The ChatGPT4 system is so far out ahead of everyone elseit’s sometimes useful to consider using it for business by adding incarefully crafted chunks of context data to refine how it answers. Thisis prompt engineered. A fine example of this everyday use of thetechnology can be found in [this clarify capitalreport](https://clarifycapital.com/the-future-of-investment-pitching)which finds that ChatGPT created pitch decks are **far** more compellingthan human created ones. It can be developed onward for more complexcorporate proposals like this through the API, which is a subscriptionservice, with additional tiers for heavy corporate use cases(<a href=\"#sec:microsoft\" data-reference-type=\"ref\" data-reference=\"sec:microsoft\">8.2.8.8</a>).\n-\n\t- **Advantages**\n-\n\t- Can be trialled in the web interface, spending a few hours or        perhaps days building a custom context that solves a specific        use case for the business, then simply copy/pasting.\n-\n\t- **OpenAI GPT is incredibly cost effective** ($1 for around 700        pages for GPT3.5 performance), or $20 a month for the web        subscription.\n-\n\t- GPT4 is **way** ahead in terms of performance. Possibly 18        months ahead of open source.\n-\n\t- Incredible team and partnership. Plug-ins are arriving very fast        to solve real business problems. They have scale and velocity.\n-\n\t- **Disadvantages**\n-\n\t- In terms of future project planning 18 months isn’t that long,        open source is worth investing in for the sake of        differentiation in those time-scales.\n-\n\t- It’s a very general model, refining this through the API means        programming work. This is a known unknown with staffing costs.\n-\n\t- You’re necessarily giving your commercial data to a cloud        service.\n-\n\t- Their “corporate” private package has trust implications, and        cost implications (more in the next section).\n-\n\t- **It’s reliant on a reliable internet connection, so it’s        suitable for the office but perhaps not ‘site’. Using it might        therefore mean ending up investing time in two development        tracks**.\n- One of the incredibly frustrating things about the GPT series is thatOpenAI are changing the code behind the models all the time as seen infigure<a href=\"#fig:GPTchanges\" data-reference-type=\"ref\" data-reference=\"fig:GPTchanges\">8.5</a>.This makes it hard to build upon in a trustable way.[[chen2023chatgpts]]The team built a dataset with 50 easy problems from LeetCode andmeasured how many GPT-4 answers ran without any changes. The Marchversion succeeded in 52% of the problems, but this dropped to a pale 10%using the model from June. I assume that OpenAI pushes changescontinuously, but we don’t know how the process works and how theyevaluate whether the models are improving or regressing. In my opinion,this is a red flag for anyone building applications that rely on GPT-4.Having the behavior of an LLM change over time is not acceptable.\n  ![]./assets/6bdb61174f6f0a8b5312c6a308bb3899bfc592fd.jpg\n  Changes to GPT models performance over time\n\t- denied by OpenAI (Chen et al)<span class=\"citation\" data-cites=\"chen2023chatgpts\"><a href=\"#fn1\" class=\"footnote-ref\" id=\"fnref1\" role=\"doc-noteref\"><sup>1</sup></a></span><section class=\"footnotes\" role=\"doc-endnotes\"><hr /><ol><li id=\"fn1\" role=\"doc-endnote\"><p>Ibid.<a href=\"#fnref1\" class=\"footnote-back\" role=\"doc-backlink\">↩︎</a></p></li></ol></section>\n- ##### ChatGPT\n\t- Code Interpreter\n- The ChatGPT Code Interpreter Plugin, introduced in March 2023, offers asandboxed environment featuring a working Python interpreter. Thisenvironment, which is isolated from other users and the Internet,supports an impressive array of functionalities. It comes pre-loadedwith over 330 libraries, including popular ones such as pandas,matplotlib, seaborn, and TensorFlow, among others.\n- As illustrated in Figure<a href=\"#fig:chatGPTdata\" data-reference-type=\"ref\" data-reference=\"fig:chatGPTdata\">8.8</a>,the Code Interpreter Plugin is capable of performing a myriad of tasks.For example, it can visualize any data inputted by the user, generateGIFs of the visualizations, and perform file uploads and downloads. Itcan extract colors from an image to create a color palette, andautonomously compress large images when memory is running low. Moreover,the plugin can clean and process data, generate insightfulvisualizations, and convert files to different formats quickly andefficiently.\n- The Code Interpreter Plugin can be installed by ChatGPT Plus users in afew simple steps. However, it is worth noting that while this plugin ispowerful, it does have certain limitations, such as frequent environmentresets, limited Optical Character Recognition (OCR) capabilities, and aninability to access the web. Despite these limitations, OpenAI continuesto work on improving the capabilities of the Code Interpreter Plugin,promising a future with substantial impacts on the world of programming.\n-\n\t- The Code Interpreter Plugin introduces a sandbox and an advanced language model, both of which are critical to its functionality.\n-\n\t- The emphasis of the plugin is on the quality of the model, which can generate code, debug it, and even decide when not to proceed without human input.\n-\n\t- The plugin offers substantial model autonomy, enabling it to work through multiple steps of code generation autonomously.\n-\n\t- Despite its powerful capabilities, the plugin does have limitations, such as frequent environment resets and limited OCR capabilities.\n-\n\t- The plugin is only available to ChatGPT Plus users, and requires a few simple steps for installation.\n-\n\t- The Code Interpreter Plugin represents a significant advancement in the realm of programming, changing the way programmers interact with AI systems.\n- ##### Go all in with Microsoft\n- Microsoft have -just- released GPT4 which privately works on your owndata. [This is likely the best option on the market rightnow](https://techcommunity.microsoft.com/t5/ai-cognitive-services-blog/introducing-azure-openai-service-on-your-data-in-public-preview/ba-p/3847000).\n- ##### Anthropic\n\t- Claude 2\n- Claude-2, Anthropic’s ChatGPT competitor was just released. It’scheaper, stronger, faster, can handle PDFs, and supports longerconversations.\n-\n\t- Claude is 5x cheaper than GPT-4.\n-\n\t- It has more recent data. A a mix of websites, licensed data sets from third parties and voluntarily-supplied user data from early 2023.\n-\n\t- It outperforms GPT4 on the GRE writing and HumanEval coding benchmarks.\n-\n\t- It features a context window of 100,000 tokens, the largest of any commercially available model.\n-\n\t- It can analyze roughly 75,000 words, about the length of “The Great Gatsby\".\n-\n\t- It can easily handle any code related tasks.\n- ##### Llama 2\n- The new Llama 2 model from Meta looks initially exciting but is prettymired in legal detail compared to the emerging open source communityefforts.\n- ###### License Grant\n- You are granted a non-exclusive, worldwide, non-transferable,royalty-free license to use, reproduce, distribute, modify, and createderivative works of Llama 2.\n- ###### Attribution and Acceptable Use\n- You must retain the attribution notice in all copies of Llama 2. Youruse must comply with Meta’s Acceptable Use Policy, which prohibitsillegal, deceptive, dangerous, or harmful uses.\n- ###### Commercial Use and Model Improvement\n- You cannot use Llama 2 to improve any other large language model besidesLlama 2. If your products or services have over 700 million monthlyactive users, you must obtain a separate license from Meta.\n- ###### Disclaimer, Liability, and Ownership\n- Llama 2 is provided \"as is\" with no warranties. You assume all risksfrom use. Meta has no liability for damages arising from use of Llama 2.You own any derivative works and modifications you create, subject toMeta’s ownership of Llama 2.\n- ###### Termination and Risks\n- Meta can terminate the license if you breach it. You must delete Llama 2on termination. Be aware of regulations like Article 28b of the AI Actin the EU. Do appropriate diligence to comply with laws and addressrisks around bias, fairness, transparency, and safety.\n- ###### Key Takeaways\n- Understand attribution requirements, acceptable use policy, commercialuse limits, disclaimer, risks, and ownership provisions. Seek legalcounsel given complexities.\n- ##### Roll your own trained LLM\n- This costs around $500k to train something up from a trillion tokensthat you bring to the party. This gets to ‘last years’ GPT3 level. It’stoo much, but it’s worth being aware of. It’s worth noting that Cerebrasare offering access to their [Andromedacluster](https://www.cerebras.net/press-release/cerebras-unveils-andromeda-a-13.5-million-core-ai-supercomputer-that-delivers-near-perfect-linear-scaling-for-large-language-models)which can train a significant model in the vein of Llama in around 11days.\n- ##### Wait for the Google integrations\n- I very strongly suspect that corporate level ML assistance is coming inforce to the Google stack already employed at our work. This is **by farthe most likely and pragmatic solution for the ‘project planningassistant’ business case**. [Vertex AI]() cloud based generative artsupport shows the direction of travel in this regard. \\[enhanced, framestyle=fill=lightgray, interior style=fill=lightgray\\] With this update,developers can access our text model powered by PaLM 2, Embeddings APIfor text, and other foundation models in Model Garden, as well asleverage user-friendly tools in Generative AI Studio for model tuningand deployment. Backed by enterprise-grade data governance, security,and safety features, Vertex AI can make it easier than ever forcustomers to access foundation models, customize them with their owndata, and quickly build generative AI applications.\n-\n\t- Advantages\n-\n\t- our work already trusts Google with it’s business data\n-\n\t- Single repository potential to leverage that fact\n-\n\t- Will likely be very cheap as a customer incentive. Currently        it’s around 700 pages per dollar.\n-\n\t- Disadvantages\n-\n\t- GCHQ have [taken the unusual        step](https://www.ncsc.gov.uk/blog-post/chatgpt-and-large-language-models-whats-the-risk)        of warning that the data put into these systems goes into their        training and can thereby resurface in competitors searches later\n-\n\t- The likes of Apple and Samsung have banned the use of these        cloud tools as a result. There’s anecdotal evidence of        commercially sensitive details surfacing, though it’s hard to        validate these\n-\n\t- The products can change over time, in ways that are outside of        your control\n- With all that said there is potentially business advantage to learninghow these systems work through doing.\n- ##### Build something custom self hosted\n- Building something custom here means taking an open source model, with apermissive license. There’s a lot of these now and they are ‘decent’.It’s possible to add in some code engineering around the edges to giveit access to private datasets through a chat interface.\n-\n\t- **Advantages**\n-\n\t- It’s private, local, under your control, and so you can trust        your data will be within the company walled garden\n-\n\t- It’s building toward IP and knowledge, in the likely scenario        where GPT4 level models are less than 2 years away. This is real        internal investment\n-\n\t- There are NO legal repercussions to using it in a purely        off-line way. You don’t even need to tell people you’re doing        it. ‘Probably’ no GPDR, data governance, compliance overheads if        designed right.\n-\n\t- **Disadvantages**\n-\n\t- It will still lie, and the company and individuals are still        responsible for the legal repercussions of acting on what the        model says.\n-\n\t- In a live deployment to the public it will occasionally say “bad        things”. It’s just impossible to control edge cases. Even        without the issues incurred by being online, the exposure around        that in terms of waivers is uncertain.\n-\n\t- Making it fit for purpose by adding memory is hard. The token        limits on these models are really small. They’re just not that        smart yet.\n- Below is output from the top two models from the [global leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard).The second highest rated worked better (supercot). Into it I uploadedthe ‘International Code of Practice for Entertainment Rigging’ which isa [meagre 35 pagedocument](https://www.plasa.org/wp-content/uploads/2017/11/ICOPER_V1.0.pdf).I have done 500 pages in the past but the quality starts to break downwith scale. Section 3.5 of the document says \\[enhanced, framestyle=fill=lightgray, interior style=fill=lightgray\\]“All custom builtequipment selected for a project must be reviewed and approved by aqualified person. Custom built equipment must be provided with necessarymarkings and documentation to ensure safe use.” You can see the outputfrom the model running at home (without internet) in Figure<a href=\"#fig:ICOPERLLM\" data-reference-type=\"ref\" data-reference=\"fig:ICOPERLLM\">8.6</a>.It has royally embellished the facts, but it’s actually delivering upgood advice, and the memory injection from the uploaded document meansit’s specific enough to the context of the question.\n  ![]./assets/ed4ec4e9acf8ecb80219042a6dc1a2d1b7235a02.png\n  Results from home hosted LLM\n- I have very much seen this when playing with both my own \"landmarkattention\" models, and Claude. I expect they will find a way round it atsome point. Because humans write with a focus on the beginning and endof documents, so large language models pay far more attention at thebeginning and end of their context input.[[liu2023lost]]\n- That’s tricky because if you load in a stack of PDFs and they gettranslated by the system into one big chunk (which may or may not be inthe expected order of the PDFs), then the PDFs in the middle of thesubmission are subject to more hallucination, garbling, forgetting etc.\n- The \"other\" system, vector databases is more even handed, and socommensurately more predictable. Still got huge problems though.\n- I have seen both effects. They’re currently both bad enough that Iwouldn’t trust document interrogation to these things. There areworkarounds obviously: Ask for references, ask if to tell you if itthinks it’s making it up, as for quotes, ask for locations, check theworking, ask in many sperate ways etc.\n- These \"tips\" can be encoded into the preamble that goes into everyquery, as a standing command, so you don’t have to, except for thechecking bit of course. You can do that with either self host models orthe web ones, but it’s something you need to do often with the webinterfaces. There’s no silver bullet yet but it feels like months away,so I am not advocating learning these tricks. Might be better to waitfor the fixes. Just don’t trust them, these are the reasons, and theyare all basically seeded in the way humans write.\n- There’s a lot that can be done here, but the cost benefit is unclear. Ifthis were sensitive internal planning documents, with a lot ofcomplexity, then there’s potentially a strong case, but we’d need tightprocedures to check on it’s homework. This tool, as usual, is best as away to rapidly construct a framework.\n- As a side note there’s a lot of open source tools like[SuperAGI](https://github.com/TransformerOptimus/SuperAGI) for ‘Agents’and [AnythingLLM](https://github.com/Mintplex-Labs/anything-llm) fordocument analysis which straddle the line between using cloud vendorsand local hosting. Building on one of these gives optionality, but theyare new and ‘flakey’.\n- ##### LoRA training\n- LoRA stands for Low-Rank Adaptation and it’s a cheap way to optimisemodels. A few hundred dollars of rented GPU time can nuance a model tobe more performant for a specific task. It sounds great but basicallyyou’re optimising for a task that you need to understand very well,possibly/probably to the detriment of the generality of the tool. Ifthere’s something you know you want, then this is an option that’sachievable and affordable.\n- Goat, a 7B LLaMA model finetuned for arithmetic tasks notablyoutperformed the  75x larger 540B PaLM model and GPT-4. Goat’s successcan be attributed to two primary factors: task-specific finetuning andLLaMA’s unique digit tokenization. The problem here is Llama is arguablyderived from data with a non-commercial license. OpenLlama gets aroundthis with their Apache2 (do what you like) copy of the model. Thesituation is rapidly evolving. Another example of task-specificfinetuning is the Gorilla project, where the LLM was trained to generateAPI calls. This is a really important area and we might be able to getahead in this niche. This controlling complex whole site systems withvoice control. The model was finetuned using 1,645 API calls fromvarious sources and demonstrated superior performance compared tonon-finetuned models. We can easily repeat that.\n- Recent findings suggest that less LoRA training gets better results, sothis is increasingly being adopted as a way to improve businessfit.[[xue2023repeat]]\n- ##### Enormous token limits\n- There’s a few models now boasting staggering token input limits. With 1million token windows it’s possible that each query to the LLM can be‘the whole corporate database and filesystem’ and then the question youwant an answer to. This is a beguiling option, probably the mostperformant (though I have not played with one), and also pretty muchuseless as nobody yet offers a legally trustable way to upload yourwhole business into a chat window.\n-\n\t- **Advantages**\n-\n\t- Likely excellent at it’s job\n-\n\t- Will ‘hallucinate’ less\n-\n\t- Probably stay on target\n-\n\t- Knows all about the business\n-\n\t- **Disadvantages**\n-\n\t- Web based right now, and knows all about the business\n-\n\t- Not actually available, I’m on wait lists.\n-\n\t- Longer latency as everything will need uploading in the token        window (assuming you don’t buy a service)\n- Very recently this in beginning to shift with the[emergence](https://github.com/eugenepentland/landmark-attention-qlora)of a locally run [‘landmark attention’ model](). There are some issueswith it at time of writing but this could be run on an our work cloudinstance. I have 4000 tokens running at home. I am exploring tree ofthought which needs these bigger windows and elevates reasoning by 70percent.[[yao2023tree]]\n- ##### Memory and other systems\n- There’s many ways to use a database (in this case a ‘vector’ DB) toprime the context window with appropriate information. It’s alsopotentially useful to look at combining older and well understood MLtechniques like knowledge graphs in combination.[[pan2023unifying]] I’venot personally had much luck with these yet.\n- ##### Cost implication for self build\n- It’s less than $20k to build a system that can do anything you need, onsite, rack mount in a ship, whatever. You can do it for less in thecloud, see the next section. You can also spend much, much more.Use-case dependent.\n- Here’s the output from the ‘local’ open source model for clarity. Noteit drifts off into nonsense toward the end then stops. This can be dealtwith but I chose not to: \\[enhanced, frame style=fill=lightgray,interior style=fill=lightgray\\] ASSISTANT:Based on the providedinformation, here are some steps you can take regarding custom builtequipment: 1. Ensure that the equipment is properly marked and traceableto the manufacturer. 2. Keep up-to-date with maintenance schedules andensure that maintenance records are readily available. 3. Follow safetyprocedures and guidelines for rigging work, including visualinspections, PPE usage, and clear communication. 4. Prevent anymalfunctioning or faulty equipment from being used and quarantine it ifnecessary. 5. Use equipment with a capacity that meets or exceeds thedesign specifications in the lifting plan. 6. Attach loads in a mannerthat does not damage the equipment or the load, and consider sling anglefactors. 7. Inspect electrical connections and repair or replaceequipment as necessary. 8. Maintain good housekeeping and prepare a post\n- ##### Build something custom in a private cloud\n- This is exactly the same as the previous section but you hire a privatecloud system ‘on demand’ to do the work. This is [currently pricedat](https://lambdalabs.com/service/gpu-cloud/pricing) $1.10/hr and onlycosts you money when you’re using it (though you have to shut it downyourself). This is both secure, and fairly cost effective. Also, itscales in that if you find a real serious application you can just getbigger rental GPUs and open a private/public interface. **It’s mypreferred path of all the private use cases except for mission criticalon site stuff**, and edge cases like boats at sea etc. For that you needto buy GPUs. AMD have [recentlyannounced](https://www.amd.com/en/newsroom/press-releases/2023-6-13-amd-expands-leadership-data-center-portfolio-with-.html)a partnership with opensource behemoth Huggingface to allow access tolarge and capable models like Falcon in their enormous new memoryarchitecture. Falcon is from the UAE and has odd views on human rights.This is one to watch.\n\n## Current Landscape (2025)\n\n- Industry adoption and implementations\n  - Metaverse platforms continue to evolve with focus on interoperability and open standards\n  - Web3 integration accelerating with decentralised identity and asset ownership\n  - Enterprise adoption growing in virtual collaboration, training, and digital twins\n  - UK companies increasingly active in metaverse development and immersive technologies\n\n- Technical capabilities\n  - Real-time rendering at photorealistic quality levels\n  - Low-latency networking enabling seamless multi-user experiences\n  - AI-driven content generation and procedural world building\n  - Spatial audio and haptics enhancing immersion\n\n- UK and North England context\n  - Manchester: Digital Innovation Factory supports metaverse startups and research\n  - Leeds: Holovis leads in immersive experiences for entertainment and training\n  - Newcastle: University research in spatial computing and interactive systems\n  - Sheffield: Advanced manufacturing using digital twin technology\n\n- Standards and frameworks\n  - Metaverse Standards Forum driving interoperability protocols\n  - WebXR enabling browser-based immersive experiences\n  - glTF and USD for 3D asset interchange\n  - Open Metaverse Interoperability Group defining cross-platform standards\n\n## Metadata\n\n- **Last Updated**: 2025-11-16\n- **Review Status**: Automated remediation with 2025 context\n- **Verification**: Academic sources verified\n- **Regional Context**: UK/North England where applicable\n\n## Related Content: AI Video\n\npublic:: true\n\n#Public page automatically published\n\n- # Real-Time\n- [H. Jensons -TWISTED : r/aivideo (reddit.com)](https://www.reddit.com/r/aivideo/comments/1eryhk6/h_jensons_twisted/)\n- ![rapidsave.com_h_jensons_twisted-zyldr5g40mid1.mp4](../assets/rapidsave.com_h_jensons_twisted-zyldr5g40mid1_1723965805126_0.mp4)\n- [EndlessDreams - Real-Time videos made easy - YouTube](https://www.youtube.com/watch?v=irUpybVgdDY)\n- {{video https://www.youtube.com/watch?v=irUpybVgdDY}}\n- # Commercial Offerings\n- {{video https://www.youtube.com/watch?v=_XtS_4PzEyk&}}\n- ## Runway Gen 3\n\t- [Introducing Gen-3 Alpha: A New Frontier for Video Generation (runwayml.com)](https://runwayml.com/blog/introducing-gen-3-alpha/)\n\t-\n- ## Luma Dream Machine?\n\t- Luma Dream Machine is a browser-based AI video generator [developed by Luma Labs](https://www.creativebloq.com/ai/luma-dream-machine-makes-ai-video-open-to-all), a San Francisco-based startup. It allows users to generate short videos (around 5 seconds) by simply entering a text or image prompt.\n\t\t- **Free to Use**: Luma Dream Machine is free to try, with no waiting list or subscription required. Users get 30 free video generations per month.\n\t\t- **High-Quality Output**: The AI produces impressively clean and detailed videos, adhering to prompts accurately and generating relatively coherent motion.\n\t\t- **Fast Generation**: Videos are generated in around 2 minutes after entering the prompt.\n\t\t- **Consistent Subjects**: Characters and subjects appear consistent throughout the video, capable of expressing emotion better than many previous AI video models.\n\t- ## Limitations\n\t  While groundbreaking, and crucially, \"available\", Luma Dream Machine still has some limitations, as acknowledged by the company:\n\t\t- Morphing, warping, and unnatural movements\n\t\t- Difficulty with complex scenes or full-body shots\n\t\t- Text in videos may appear garbled\n\t\t- Anatomical issues like extra limbs or heads\n\t- [(1) Professor John Keeting on X: \"this was created with Luma AI I am very impressed. Made by Kevin Van Witt and the talented team at The Monster Library https://t.co/IXLWO1Be91\" / X (twitter.com)](https://twitter.com/ProfKeeting/status/1801632319536607623)\n\t- {{twitter https://twitter.com/ProfKeeting/status/1801632319536607623}}\n- {{embed ((661d5f76-bd9c-493d-afc1-efcec299ed24))}}\n- {{embed ((664465de-5bd3-4169-a90b-c03f117bef04))}}\n- # The Rest\n\t- **Lumiere: Google's Contribution** Google's Lumiere project also signifies progress in video generation capabilities, though full details remain undisclosed. This suggests ongoing competition and development  in the field.\n\t- **Meta's Approach: Foundational World Modeling** Meta (formerly Facebook) is taking a distinct approach, focusing on the underlying world modeling needed for video encoding and generation. This emphasis on understanding the principles of physics and object interactions could contribute to  more realistic AI-generated videos.\n\t- **Technical Capabilities and Limitations**\n\t\t- **Capabilities** Current AI video generators demonstrate proficiency in producing high-resolution images and videos. They are capable of style adaptation, simulating complex scenes with multiple elements, and handling variations in aspect ratio and resolution.\n\t\t- **Limitations**  Despite their strengths, these models still struggle to accurately simulate physics and lack a complete understanding of cause and effect. Occasional errors regarding object permanence highlight the existing gap between pattern recognition and a comprehensive understanding of the world.\n\t- **Ethical and Creative Considerations**\n\t\t- **Potential Impacts**  Advancements in AI video generation raise questions about the future of creative professions and the ethical implications of AI-generated content. Balancing technological innovation with safeguarding the integrity of human creativity is an important consideration.\n\t\t- **Challenges**  Distinguishing between pattern recognition and genuine understanding is pivotal in the ethical use of AI. The potential for misuse or the creation of harmful content underscores the need for clear guidelines and responsible practices.\n- # Open systems\n- [Stable Video 4D — Stability AI](https://stability.ai/news/stable-video-4d)\n-\n- ## OpenSora\n\t- [[Update Cycle]]\n- ## Stable Video\n\t- ![1708218759791.mp4](../assets/1708218759791_1708282889322_0.mp4)\n\t- [(16) Discarded Androids (Midjourney to Stable Video beta) : midjourney (reddit.com)](https://www.reddit.com/r/midjourney/comments/1ar6uz0/discarded_androids_midjourney_to_stable_video_beta/)\n\t\t- ![m2-res_960p.mp4](../assets/m2-res_960p_1708014022595_0.mp4)\n- ## Pika\n\t- [twitter link to the render loading below](https://twitter.com/BMaursky/status/1747700852226199815)\n\t  {{twitter https://twitter.com/BMaursky/status/1747700852226199815}} -\n- # Misc links being integrated.\n- [[MotionDirector]], with a dual-path LoRAs architecture to decouple the learning of appearance and motion. Further, we design a novel appearance-debiased temporal loss to mitigate the influence of appearance on the temporal training objective. Experimental results show the proposed method can generate videos of diverse appearances for the customized motions. Our method also supports various downstream applications, such as the mixing of different videos with their appearance and motion respectively, and animating a single image with customized motions.\n- [RAVE: Randomized Noise Shuffling for Fast and Consistent Video Editing with Diffusion Models (rave-video.github.io)](https://rave-video.github.io/)\n\t- https://discord.com/channels/1076117621407223829/1192162917395730635/1192162917395730635\n\t- Here's one way to use the brand new RAVE node from here: https://github.com/spacepxl/ComfyUI-RAVE\n\t\t- First pass often has flickering (depending a lot on the input), so I made a workflow to smooth even harsh flickering with AD. This allows for utilizing the transformative and often more detailed vid2vid from RAVE and still get smooth results in [[ComfyUI]]\n\t\t\t- Updated LCM version: https://discord.com/channels/1076117621407223829/1192162917395730635/1192212692354748427\n\t\t\t  using the \"video/controlgif/animatediff\" contolnet from here: https://huggingface.co/crishhh/animatediff_controlnet/blob/main/controlnet_checkpoint.ckpt\n\t\t- {{video https://rave-video.github.io/static/teaser/car-turn_final.mp4}}\n- ## Style transfer for humans\n\t- Multiple techniques tested with the same [[LoRA DoRA etc]] for [comparison](https://discord.com/channels/1076117621407223829/1198416106554130552/1198416106554130552)\n- [ActAnywhere](https://actanywhere.github.io/)\n- [AI-Enhanced Creator (beehiiv.com)](https://nejcsusec.beehiiv.com/)\n- AnimateAnyone for [[ComfyUI]] [MrForExample/ComfyUI-AnimateAnyone-Evolved: Improved AnimateAnyone implementation that allows you to use the opse image sequence and reference image to generate stylized video (github.com)](https://github.com/MrForExample/ComfyUI-AnimateAnyone-Evolved)\n- [CG Renders to AI ANIMATION\n\t- NIKE video — MOONWALKERS PICTURE](https://www.moonwalkerspicture.com/newslounge/cg-renders-to-ai-workflow-vol-02-anim)\n- Motion Control\n\t- [MotionCtrl (wzhouxiff.github.io)](https://wzhouxiff.github.io/projects/MotionCtrl/)\n- [[2401.12945] Lumiere: A Space-Time Diffusion Model for Video Generation (arxiv.org)](https://arxiv.org/abs/2401.12945)\n- [I2VGen-XL\n\t- a Hugging Face Space by damo-vilab](https://huggingface.co/spaces/damo-vilab/I2VGen-XL)\n- [ali-vilab/i2vgen-xl: Official repo for VGen: a holistic video generation ecosystem for video generation building on diffusion models (github.com)](https://github.com/ali-vilab/i2vgen-xl)\n\t- [MagicVideo-V2: Multi-Stage High-Aesthetic Video Generation (magicvideov2.github.io)](https://magicvideov2.github.io/)\n- Interpolation and interframe consistency\n\t- [controlnet and ebsynth temporal consistency](https://www.reddit.com/r/StableDiffusion/comments/114zmh3/controlnet_and_ebsynth_make_incredible_temporally/)\n\t- [Motion-Conditioned Diffusion Model for Controllable Video Synthesis](https://tsaishien-chen.github.io/MCDiff/)\n\t- [Interframe consistency is now here](https://twitter.com/cut_pow/status/1576748659051749377)\n\t- [Interpolation between two frames](https://film-net.github.io/)\n\t- [FILM frame interpolator](https://film-net.github.io/)\n\t- [ProPainter for Video Inpainting (shangchenzhou.com)](https://shangchenzhou.com/projects/ProPainter/)\n\t- [zengyh1900/Awesome-Image-Inpainting: A curated list of image inpainting and video inpainting papers and resources (github.com)](https://github.com/zengyh1900/Awesome-Image-Inpainting)\n\t-\n- [Runway AI video editing](https://www.youtube.com/c/RunwayML)\n\t- Gen2 examples\n- Multishot [VideoDrafter: Content-Consistent Multi-Scene Video Generation with LLM](https://videodrafter.github.io/)\n- [vienna with prompts](https://www.linkedin.com/posts/dr-andreas-fraunberger_marketinginnovation-digitaltourism-ar-ugcPost-7073039429417730048-BKfQ?utm_source=share&utm_medium=member_desktop)\n- [Video slowmo and enhance](http://zeyuan-chen.com/VideoINR/)\n- [deforum stable diffusion video](https://github.com/HelixNGC7293/DeforumStableDiffusionLocal)\n- [Phenaki](https://phenaki.video/)\n- Collaborative video pipeline\n- [Magicvideo (faster)](https://magicvideo.github.io/)\n- [Production ready re aging](https://studios.disneyresearch.com/2022/11/30/production-ready-face-re-aging-for-visual-effects/)\n- [distilled models for 25fps](https://arxiv.org/abs/2202.00512)\n- [Stable warpfusion](https://www.linkedin.com/posts/rainisto_stablediffusion-musicvideo-remix-activity-7018207241522614272-YT1y?utm_source=share&utm_medium=member_desktop)\n- [Video talking heads from text service](https://www.synthesia.io/)\n- [Tune a video](https://tuneavideo.github.io/)\n- [Vidyo: Generates videos for social networks from longer videos.](https://vidyo.ai/)\n- [Stylegan-T video transformer from google](https://sites.google.com/view/stylegan-t)\n- [Houdini](https://github.com/proceduralit/StableDiffusion_Houdini)\n- [Dream Mix video to video remix](https://dreamix-video-editing.github.io/)\n- [RIFE frame interpolation](https://github.com/megvii-research/ECCV2022-RIFE)\n- [example github for sd](https://github.com/vladmandic/rife)\n- [Synthesia corporate video generation](https://www.youtube.com/watch?v=4uzzD9sD-PI)\n- [pix2pixHD nextframe google colab](https://colab.research.google.com/github/dvschultz/ml-art-colabs/blob/master/Pix2PixHD_Next_Frame_Prediction.ipynb)\n- [minecraft demo codebase](https://github.com/TSFSean/InvokeAI-DiffusionCraftAI)\n- [animation from mixamo](https://www.reddit.com/r/StableDiffusion/comments/zecyc7/mixamo_animations_stable_diffusion_v2_depth2img/)\n- [Intel enhance photorealism in realtime](https://github.com/isl-org/PhotorealismEnhancement)\n- custom SD video to video script\n- [Testing a custom video2video script I'm working on. (These used RealisticVision1.4 & ControlNet) : r/StableDiffusion](https://www.reddit.com/r/StableDiffusion/comments/11iviep/testing_a_custom_video2video_script_im_working_on/)\n- [consistency tools for character tooning](https://www.reddit.com/r/StableDiffusion/comments/11okvc8/how_about_another_joke_murraaaay/)\n- Alibaba system\n\t- [website](https://videocomposer.github.io/)\n\t- [github](https://github.com/damo-vilab/videocomposer)\n\t- [model cards](https://huggingface.co/damo-vilab/MS-Image2Video)\n- [9 new tools](https://twitter.com/mreflow/status/1637957302073565184)\n- [Automatic1111 plugin](https://www.reddit.com/r/StableDiffusion/comments/11w0ba9/modelscope_17b_text2video_model_is_now_available/)\n- [Next frame prediction with controlnet](https://www.reddit.com/r/StableDiffusion/comments/11f8i0g/next_frame_prediction_with_controlnet/)\n- [Will smith eating spaghetti](https://www.reddit.com/r/StableDiffusion/comments/1244h2c/will_smith_eating_spaghetti/)\n- [Transform Video to Animation in Stable Diffusion | How to Install + BEST Consistency Settings: Learn how to use AI to create animations from real videos. We'll use Stable Diffusion and other tools for maximum consistencyProject Files:https://bit.ly/3...](https://www.youtube.com/watch?v=sVmi2Yp43c0&t=22)\n- [How to Use ModelScope text2video with Automatic1111’s Stable Diffusion Web UI | kombitz: Enable the Extension Click on the Extension tab and then click on Install from URL. Enter https://github.com/deforum-art/sd-webui-modelscope-text2video in the URL box and click on Install. Click on Installed and click on Apply and restart UI. Go to your stable-diffusion-webui/models folder and create a folder called ModelScope and then create a folder called t2v under ModelScope. This is your models folder for text2video.](https://www.kombitz.com/2023/03/28/how-to-use-modelscope-text2video-with-automatic1111s-stable-diffusion-web-ui/)\n- This article provides instructions on how to use ModelScope's text2video feature with Automatic1111's Stable Diffusion Web UI.\n- [latent consistency pipeline](https://www.reddit.com/r/StableDiffusion/comments/17fvxrq/60_frame_video_generated_in_646_seconds/)\n- [GitHub\n\t- Picsart-AI-Research/Text2Video-Zero: Text-to-Image Diffusion Models are Zero-Shot Video Generators: Text-to-Image Diffusion Models are Zero-Shot Video Generators\n\t- GitHub\n\t- Picsart-AI-Research/Text2Video-Zero: Text-to-Image Diffusion Models are Zero-Shot Video Generators](https://github.com/Picsart-AI-Research/Text2Video-Zero)\n- The Picsart-AI-Research/Text2Video-Zero repository contains code for a text-to-image diffusion model that can be used to generate videos from text input. The model is a zero-shot video generator, meaning that it does not require any training data in order to generate videos.\n- [LVDM for long video creation](https://yingqinghe.github.io/LVDM/)\n- [The Text2Room algorithm generates textured 3D meshes from a given text prompt by leveraging pre-trained 2D text-to-image models. The core idea is to select camera poses that will result in a seamless, textured 3D mesh. The algorithm iteratively fuses scene frames with the existing geometry to create the final mesh. Evaluation shows that the algorithm is able to generate room-scale 3D geometry with compelling textures from only text as input.](https://lukashoel.github.io/text-to-room/)\n- [The VMesh system models a scene with a triangular mesh and a sparse volume for efficient view synthesis. It is trained on multi-view images of an object to create a contiguous representation of the object's surface and volume. This representation is then used to generate a simplified triangular mesh and a sparse volume, which can be stored and rendered efficiently. The system is designed for real-time applications and can render at 2K 60FPS on common consumer devices.](https://bennyguo.github.io/vmesh/)\n- [LLM guided video generation paper](https://arxiv.org/abs/2309.15091)\n- [LVM video gen using LLM paper](https://arxiv.org/pdf/2309.17444.pdf)\n- [Temporal stable automatic plugin](https://www.reddit.com/r/StableDiffusion/comments/12sd4bi/results_from_latest_version_of_temporal_stable/)\n- [We present a method for high-resolution video synthesis using latent diffusion models (LDMs). Our approach first pre-trains an LDM on images, then introduces a temporal dimension to the latent space diffusion model and fine-tunes it on encoded image sequences (i.e. videos). We focus on two real-world applications: simulation of in-the-wild driving data and creative content creation with text-to-video modeling. Our method achieves state-of-the-art performance on real driving videos of 512 x 1024 resolution. Additionally, our approach can leverage off-the-shelf pre-trained image LDMs, turning the publicly available, state-of-the-art text-to-image LDM Stable Diffusion into an efficient and expressive text-to-video model.](https://buff.ly/41FgQrb)\n- [This script allows for the automation of video stylization using StableDiffusion and ControlNet.](https://github.com/volotat/SD-CN-Animation)\n- [Really easy videos in A1111](https://www.reddit.com/r/StableDiffusion/comments/12otdo0/the_secret_to_really_easy_videos_in_a1111_easier/)\n- [Dancer 4 keyframes, low noise, controlnet approach](https://www.reddit.com/r/StableDiffusion/comments/12nwpdx/dancer_4_keyframes_guide_and_source_files_for/)\n- [Flicker free video workflow paper (good!)](https://anonymous-31415926.github.io/)\n- [Pika labs](https://twitter.com/pika_labs)\n- [Realtime lip-sync API](https://getsynchronicity.io/)\n- [ms image to video on huggingface](https://huggingface.co/spaces/fffiloni/MS-Image2Video)\n- [model to video blender modules](https://github.com/tin2tin/Generative_AI)\n- [videocomposer in python 3.9](https://github.com/mindspore-lab/mindone/tree/master/examples/videocomposer)\n- [motionagent image to video](https://github.com/modelscope/motionagent)\n- [Animatediff comfy workflows on discord](https://discord.com/channels/1076117621407223829/1149372684220768367)\n- [fluid animation youtube](https://www.youtube.com/watch?v=ak_az6ZNYFM)\n- [Controlnet tutorial](https://www.youtube.com/watch?v=WHxIrY2wLQE)\n- [LCM loras for fast inferencing](https://huggingface.co/collections/latent-consistency/latent-consistency-models-loras-654cdd24e111e16f0865fba6)\n- Animatediff is a new animation software that provides a range of tools and features for creating high-quality animations. It offers a user-friendly interface and supports various animation techniques, such as 2D, 3D, stop motion, and more. With Animatediff, users can easily bring their ideas to life and express their creativity through unique and captivating animations. Whether you're a professional animator or a beginner, Animatediff offers a comprehensive set of features to help you create stunning animations in a fast and efficient manner. title:: Animatediff and Stablevideo\n- Youtube tutorials\n\t- [IF_Animator ComfyUI workflow LCM+Animatediff+IPA+CN (youtube.com)](https://www.youtube.com/watch?v=FE2KmCEv19E)\n\t- [[Part 2] Tips and Tricks\n\t\t- AnimateDiff ControlNet Animation in ComfyUI\n\t\t- YouTube](https://www.youtube.com/watch?v=aysg2vFFO9g)\n\t-\n\t- [TianxingWu/FreeInit: FreeInit: Bridging Initialization Gap in Video Diffusion Models (github.com)](https://github.com/TianxingWu/FreeInit)\n\t-\n- [CiaraStrawberry/svd-temporal-controlnet (github.com)](https://github.com/CiaraStrawberry/svd-temporal-controlnet)\n- [ProjectNUWA/DragNUWA (github.com)](https://github.com/ProjectNUWA/DragNUWA)\n\t-\n- ## AnimateDiff\n\t- [(1461) Discord | #ad_resources | banodoco](https://discord.com/channels/1076117621407223829/1149372684220768367) animatediff resources\n\t-\n\t-\n\n## Current Landscape (2025)\n\n- Industry adoption and implementations\n  - Metaverse platforms continue to evolve with focus on interoperability and open standards\n  - Web3 integration accelerating with decentralised identity and asset ownership\n  - Enterprise adoption growing in virtual collaboration, training, and digital twins\n  - UK companies increasingly active in metaverse development and immersive technologies\n\n- Technical capabilities\n  - Real-time rendering at photorealistic quality levels\n  - Low-latency networking enabling seamless multi-user experiences\n  - AI-driven content generation and procedural world building\n  - Spatial audio and haptics enhancing immersion\n\n- UK and North England context\n  - Manchester: Digital Innovation Factory supports metaverse startups and research\n  - Leeds: Holovis leads in immersive experiences for entertainment and training\n  - Newcastle: University research in spatial computing and interactive systems\n  - Sheffield: Advanced manufacturing using digital twin technology\n\n- Standards and frameworks\n  - Metaverse Standards Forum driving interoperability protocols\n  - WebXR enabling browser-based immersive experiences\n  - glTF and USD for 3D asset interchange\n  - Open Metaverse Interoperability Group defining cross-platform standards\n\n## Metadata\n\n- **Last Updated**: 2025-11-16\n- **Review Status**: Automated remediation with 2025 context\n- **Verification**: Academic sources verified\n- **Regional Context**: UK/North England where applicable",
  "properties": {
    "id": "generative-ai-owl-axioms",
    "collapsed": "true",
    "- ontology": "true",
    "- term-id": "AI-0860",
    "- preferred-term": "Generative AI",
    "- source-domain": "ai",
    "- status": "complete",
    "- public-access": "true",
    "- version": "2.0.0",
    "- last-updated": "2025-01-15",
    "- quality-score": "0.90",
    "- definition": "[[Generative AI]] encompasses [[Machine Learning]] systems capable of creating new content across modalities including text, images, audio, video, and code through [[Neural Networks]] trained on large datasets. These systems use [[Deep Learning]] architectures like [[Transformer]]s, [[Diffusion Models]], [[GANs]], and [[VAEs]] to learn data distributions and generate novel, coherent outputs. Generative AI represents a paradigm shift from discriminative models, enabling creative applications in [[Content Creation]], [[Design]], [[Art]], [[Music Generation]], and [[Code Synthesis]].",
    "- maturity": "mature",
    "- source": "[[OpenAI]], [[Stability AI]], [[Midjourney]], [[Anthropic]], [[Google DeepMind]], [[NIST AI Standards]]",
    "- authority-score": "0.93",
    "- owl:class": "ai:GenerativeAI",
    "- owl:physicality": "VirtualEntity",
    "- owl:role": "Process",
    "- owl:inferred-class": "ai:VirtualProcess",
    "- belongsToDomain": "[[AI-GroundedDomain]], [[CreativeMediaDomain]], [[ComputationAndIntelligenceDomain]]",
    "- bridges-to": "",
    "- bridges-from": "[[BehaviouralFeedbackLoop]] via has-part",
    "- implementedInLayer": "[[ComputeLayer]], [[DataLayer]], [[AlgorithmicLayer]]",
    "- is-subclass-of": "[[Artificial Intelligence]], [[Machine Learning]], [[Deep Learning]]",
    "- has-part": "[[Large Language Models]], [[Diffusion Models]], [[GAN]], [[VAE]], [[Neural Network]], [[Training Data]]",
    "- implements": "[[Text Generation]], [[Image Generation]], [[Audio Generation]], [[Video Generation]], [[Code Generation]]",
    "- requires": "[[Neural Network Training]], [[Large-Scale Datasets]], [[GPU Compute]], [[Model Architecture]]",
    "- enables": "[[Content Creation]], [[Creative AI]], [[Automated Design]], [[Synthetic Media]], [[AI Art]]",
    "- related-to": "[[Foundation Models]], [[Prompt Engineering]], [[Fine-Tuning]], [[Model Training]], [[Transfer Learning]]"
  },
  "backlinks": [
    "AI Risk",
    "Deep Learning",
    "BC-0072-node",
    "Virtual Production",
    "ETSI_Domain_AI___Creative_Media",
    "Ontology in LLM Operations",
    "Runes and Glyphs",
    "Education Metaverse",
    "Bitcoin",
    "AI Governance Principle",
    "Large language models",
    "Safety Laser Scanner",
    "Neural 3D Generation",
    "Generative AI"
  ],
  "wiki_links": [
    "Learning",
    "liu2023lost",
    "Reasoning",
    "ComputationAndIntelligenceDomain",
    "NIST AI Standards",
    "Audio Generation",
    "Neural Network Training",
    "Text Generation",
    "Inpainting",
    "Fractal Geometry",
    "CreativeMediaDomain",
    "Midjourney",
    "Video Generation",
    "Generative AI",
    "Transformer",
    "DreamBooth",
    "Code Synthesis",
    "Update Cycle",
    "Decision-Making",
    "Generative Art",
    "Prompt Engineering",
    "Semantic Understanding",
    "Machine Learning",
    "Google DeepMind",
    "Creative AI",
    "Discord",
    "Lightning Network",
    "Training Data",
    "Smart Contract",
    "AlgorithmicLayer",
    "ComputeLayer",
    "Diffusion Models",
    "Fine-Tuning",
    "Stability AI",
    "MachineLearning",
    "VAE",
    "Model Training",
    "Neural Network",
    "VAEs",
    "Pay-Per-Use",
    "Decentralized Storage",
    "Cognitive Abilities",
    "Outpainting",
    "Automated Design",
    "Large Language Models",
    "Decentralized AI",
    "xue2023repeat",
    "Perception",
    "Bitcoin",
    "L402 Protocol",
    "Synthetic Media",
    "Art",
    "Open Source",
    "LoRA DoRA etc",
    "Deep Learning",
    "MotionDirector",
    "Text Prompts",
    "Prompts",
    "Image-to-Image",
    "Memory",
    "GPU Compute",
    "Foundation Models",
    "chen2023chatgpts",
    "Artificial Intelligence",
    "ComfyUI",
    "Model Architecture",
    "LoRA",
    "NFT",
    "Diffusion Model",
    "ControlNet",
    "Community Curation",
    "Music Generation",
    "OpenAI API",
    "DataLayer",
    "Large-Scale Datasets",
    "Anthropic",
    "AI-GroundedDomain",
    "Design",
    "yao2023tree",
    "Content Creation",
    "GANs",
    "BehaviouralFeedbackLoop",
    "Computational Creativity",
    "AI Art",
    "OpenAI",
    "Procedural Generation",
    "Code Generation",
    "Computer Art",
    "ChatGPT",
    "Neural Networks",
    "pan2023unifying",
    "Artistic Style",
    "GAN",
    "Algorithmic Art",
    "Image Generation",
    "Transfer Learning",
    "Latent Diffusion"
  ],
  "ontology": {
    "term_id": "AI-0860",
    "preferred_term": "Generative AI",
    "definition": "[[Generative AI]] encompasses [[Machine Learning]] systems capable of creating new content across modalities including text, images, audio, video, and code through [[Neural Networks]] trained on large datasets. These systems use [[Deep Learning]] architectures like [[Transformer]]s, [[Diffusion Models]], [[GANs]], and [[VAEs]] to learn data distributions and generate novel, coherent outputs. Generative AI represents a paradigm shift from discriminative models, enabling creative applications in [[Content Creation]], [[Design]], [[Art]], [[Music Generation]], and [[Code Synthesis]].",
    "source_domain": "ai",
    "maturity_level": null,
    "authority_score": 0.93
  }
}