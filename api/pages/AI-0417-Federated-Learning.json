{
  "title": "Federated Learning",
  "content": "- ### OntologyBlock\n  id:: federated-learning-ontology\n  collapsed:: true\n\n  - **Identification**\n    - public-access:: true\n    - ontology:: true\n    - term-id:: AI-0417\n    - preferred-term:: Federated Learning\n    - source-domain:: ai-grounded\n    - status:: in-progress\n    - version:: 1.0\n    - last-updated:: 2025-10-29\n\n  - **Definition**\n    - definition:: Federated Learning is a distributed machine learning paradigm enabling collaborative model training across multiple decentralized data sources without centralizing sensitive data, preserving privacy by keeping raw data at source locations while sharing only model updates or gradients. This approach implements iterative training cycles where a central coordinator initializes a global model, selected clients download the model and train locally on private data, clients compute model updates (gradients or weights) based on local training, updates are transmitted to coordinator (optionally with differential privacy noise or secure aggregation), coordinator aggregates updates using methods like federated averaging (FedAvg) computing weighted average based on dataset sizes, and the updated global model is distributed for the next training round. The framework addresses key challenges including statistical heterogeneity where clients have non-IID (non-independent and identically distributed) data requiring techniques like personalized federated learning and federated multi-task learning, systems heterogeneity involving varying computational capabilities and network conditions necessitating asynchronous aggregation and client selection strategies, communication efficiency achieved through compression techniques like gradient quantization and sparsification reducing bandwidth requirements, and privacy protection enhanced through secure multi-party computation for secure aggregation preventing coordinator from seeing individual updates, differential privacy mechanisms adding calibrated noise to updates, and homomorphic encryption enabling encrypted model update aggregation. The 2024-2025 period witnessed federated learning transition from academic research to production infrastructure with healthcare consortia training diagnostic models across hospitals while maintaining patient privacy, financial institutions collaborating on fraud detection without sharing transaction data, and major implementations including Google's Federated Analytics and TensorFlow Federated becoming de facto standards while Apple deployed federated learning across device ecosystems for keyboard suggestions and photo identification, though challenges remained including convergence difficulties with non-IID data, vulnerability to poisoning attacks from malicious participants, and substantial communication overhead despite optimization techniques.\n    - maturity:: mature\n    - source:: [[McMahan et al. (2017)]], [[Google Federated Learning]], [[TensorFlow Federated]]\n    - authority-score:: 0.95\n\n  - **Semantic Classification**\n    - owl:class:: aigo:FederatedLearning\n    - owl:physicality:: VirtualEntity\n    - owl:role:: Process\n    - owl:inferred-class:: aigo:VirtualProcess\n    - belongsToDomain:: [[AIEthicsDomain]]\n    - implementedInLayer:: [[ConceptualLayer]]\n\n  - #### Relationships\n    id:: federated-learning-relationships\n\n  - #### OWL Axioms\n    id:: federated-learning-owl-axioms\n    collapsed:: true\n    - ```clojure\n      \n      ```",
  "properties": {
    "id": "federated-learning-owl-axioms",
    "collapsed": "true",
    "- public-access": "true",
    "- ontology": "true",
    "- term-id": "AI-0417",
    "- preferred-term": "Federated Learning",
    "- source-domain": "ai-grounded",
    "- status": "in-progress",
    "- version": "1.0",
    "- last-updated": "2025-10-29",
    "- definition": "Federated Learning is a distributed machine learning paradigm enabling collaborative model training across multiple decentralized data sources without centralizing sensitive data, preserving privacy by keeping raw data at source locations while sharing only model updates or gradients. This approach implements iterative training cycles where a central coordinator initializes a global model, selected clients download the model and train locally on private data, clients compute model updates (gradients or weights) based on local training, updates are transmitted to coordinator (optionally with differential privacy noise or secure aggregation), coordinator aggregates updates using methods like federated averaging (FedAvg) computing weighted average based on dataset sizes, and the updated global model is distributed for the next training round. The framework addresses key challenges including statistical heterogeneity where clients have non-IID (non-independent and identically distributed) data requiring techniques like personalized federated learning and federated multi-task learning, systems heterogeneity involving varying computational capabilities and network conditions necessitating asynchronous aggregation and client selection strategies, communication efficiency achieved through compression techniques like gradient quantization and sparsification reducing bandwidth requirements, and privacy protection enhanced through secure multi-party computation for secure aggregation preventing coordinator from seeing individual updates, differential privacy mechanisms adding calibrated noise to updates, and homomorphic encryption enabling encrypted model update aggregation. The 2024-2025 period witnessed federated learning transition from academic research to production infrastructure with healthcare consortia training diagnostic models across hospitals while maintaining patient privacy, financial institutions collaborating on fraud detection without sharing transaction data, and major implementations including Google's Federated Analytics and TensorFlow Federated becoming de facto standards while Apple deployed federated learning across device ecosystems for keyboard suggestions and photo identification, though challenges remained including convergence difficulties with non-IID data, vulnerability to poisoning attacks from malicious participants, and substantial communication overhead despite optimization techniques.",
    "- maturity": "mature",
    "- source": "[[McMahan et al. (2017)]], [[Google Federated Learning]], [[TensorFlow Federated]]",
    "- authority-score": "0.95",
    "- owl:class": "aigo:FederatedLearning",
    "- owl:physicality": "VirtualEntity",
    "- owl:role": "Process",
    "- owl:inferred-class": "aigo:VirtualProcess",
    "- belongsToDomain": "[[AIEthicsDomain]]",
    "- implementedInLayer": "[[ConceptualLayer]]"
  },
  "backlinks": [],
  "wiki_links": [
    "Google Federated Learning",
    "McMahan et al. (2017)",
    "AIEthicsDomain",
    "ConceptualLayer",
    "TensorFlow Federated"
  ],
  "ontology": {
    "term_id": "AI-0417",
    "preferred_term": "Federated Learning",
    "definition": "Federated Learning is a distributed machine learning paradigm enabling collaborative model training across multiple decentralized data sources without centralizing sensitive data, preserving privacy by keeping raw data at source locations while sharing only model updates or gradients. This approach implements iterative training cycles where a central coordinator initializes a global model, selected clients download the model and train locally on private data, clients compute model updates (gradients or weights) based on local training, updates are transmitted to coordinator (optionally with differential privacy noise or secure aggregation), coordinator aggregates updates using methods like federated averaging (FedAvg) computing weighted average based on dataset sizes, and the updated global model is distributed for the next training round. The framework addresses key challenges including statistical heterogeneity where clients have non-IID (non-independent and identically distributed) data requiring techniques like personalized federated learning and federated multi-task learning, systems heterogeneity involving varying computational capabilities and network conditions necessitating asynchronous aggregation and client selection strategies, communication efficiency achieved through compression techniques like gradient quantization and sparsification reducing bandwidth requirements, and privacy protection enhanced through secure multi-party computation for secure aggregation preventing coordinator from seeing individual updates, differential privacy mechanisms adding calibrated noise to updates, and homomorphic encryption enabling encrypted model update aggregation. The 2024-2025 period witnessed federated learning transition from academic research to production infrastructure with healthcare consortia training diagnostic models across hospitals while maintaining patient privacy, financial institutions collaborating on fraud detection without sharing transaction data, and major implementations including Google's Federated Analytics and TensorFlow Federated becoming de facto standards while Apple deployed federated learning across device ecosystems for keyboard suggestions and photo identification, though challenges remained including convergence difficulties with non-IID data, vulnerability to poisoning attacks from malicious participants, and substantial communication overhead despite optimization techniques.",
    "source_domain": "ai-grounded",
    "maturity_level": null,
    "authority_score": 0.95
  }
}