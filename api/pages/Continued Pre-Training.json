{
  "title": "Continued Pre-Training",
  "content": "- ### OntologyBlock\n  id:: continued-pre-training-ontology\n  collapsed:: true\n\n  - **Identification**\n    - public-access:: true\n    - ontology:: true\n    - term-id:: AI-0248\n    - preferred-term:: Continued Pre-Training\n    - source-domain:: ai\n    - status:: approved\n    - version:: 1.0\n    - last-updated:: 2025-11-18\n\n  - **Definition**\n    - definition:: Continued Pre-Training (CPT) represents an intermediate training phase where a foundation model undergoes additional unsupervised or self-supervised learning on domain-specific, task-relevant, or temporally recent data before task-specific fine-tuning. This technique bridges general-purpose pre-training and specialized adaptation by updating model parameters on unlabeled corpora that reflect target domain characteristics (medical texts, legal documents, scientific literature, code repositories), regional language variations, or newly emerged knowledge not present in the original training data. CPT preserves the broad capabilities acquired during initial pre-training while adapting internal representations to domain-specific linguistic patterns, terminology, reasoning structures, and factual knowledge. The process employs masked language modeling (MLM), causal language modeling (CLM), or denoising objectives similar to initial pre-training but on smaller, curated datasets (typically 1-100B tokens versus 1-10T tokens for foundation pre-training). Key hyperparameters include learning rate (typically 1/10 to 1/5 of initial pre-training rate to prevent catastrophic forgetting), training duration (measured in tokens processed or gradient steps), data composition (balancing domain-specific and general corpora), and mixing strategies (curriculum-based or uniform). CPT demonstrates superior sample efficiency compared to training from scratch and outperforms direct fine-tuning in domains with substantial distribution shift from general web text, as validated by empirical studies on biomedical, legal, and multilingual language model adaptation following methodologies outlined in AWS SageMaker AI and Google Cloud AI Platform frameworks.\n    - maturity:: mature\n    - source:: [[AWS SageMaker CPT Documentation]], [[Google Cloud AI Platform]], [[AMD ROCm Multilingual CPT Playbook]], [[Raschka 2025 LLM Training Paradigms]]\n    - authority-score:: 0.90\n\n\n### Relationships\n- is-subclass-of:: [[PreTraining]]\n\n## Continued Pre Training\n\nContinued Pre Training refers to an intermediate training phase where a pre-trained model undergoes additional pre-training on domain-specific or task-relevant data before fine-tuning. this technique bridges general pre-training and task-specific fine-tuning, adapting model knowledge to particular domains whilst maintaining broad capabilities.\n\n- Industry adoption and implementations\n\t- Major cloud platforms such as Amazon SageMaker and Google Cloud offer CPT capabilities, allowing organisations to adapt pre-trained models to specific domains or languages\n\t- Notable organisations include AWS, Google, and AMD, which have published practical playbooks and technical frameworks for implementing CPT\n- UK and North England examples where relevant\n\t- UK-based research institutions and tech companies are increasingly adopting CPT for domain-specific applications, particularly in healthcare, finance, and legal sectors\n\t- North England innovation hubs such as Manchester, Leeds, Newcastle, and Sheffield are home to several startups and academic groups exploring CPT for regional language adaptation and domain-specific use cases\n- Technical capabilities and limitations\n\t- CPT enables rapid domain adaptation and improved performance on specialized tasks without the need for extensive task-specific fine-tuning\n\t- However, the technique requires careful data selection and preprocessing to avoid overfitting and ensure the model retains its general capabilities\n- Standards and frameworks\n\t- Training and validation datasets for CPT should be diverse, representative, clean, and scaled appropriately to the target domain\n\t- Common data formats include JSONL files following the Converse format, with each line containing a JSON object representing a conversation or text entry\n\n## Technical Details\n\n- **Id**: continued-pre-training-ontology\n- **Collapsed**: true\n- **Source Domain**: ai\n- **Status**: draft\n- **Public Access**: true\n\n## Research & Literature\n\n- Key academic papers and sources\n\t- Behrouz, A., & Mirrokni, V. (2025). Introducing Nested Learning: A new ML paradigm for continual learning. Google Research Blog. https://research.google/blog/introducing-nested-learning-a-new-ml-paradigm-for-continual-learning/\n\t- Raschka, S. (2025). New LLM Pre-training and Post-training Paradigms. Ahead of AI. https://magazine.sebastianraschka.com/p/new-llm-pre-training-and-post-training\n\t- AMD ROCm Blog. (2025). Continued Pretraining: A Practical Playbook for Language-Specific LLM Adaptation. https://rocm.blogs.amd.com/artificial-intelligence/multilingual-continued-pretraining/README.html\n\t- Emergent Mind. (2025). Continued Pre-Training (CPT) Overview. https://www.emergentmind.com/topics/continued-pre-training-cpt\n\t- Amazon SageMaker AI. (2025). Continued pre-training (CPT). https://docs.aws.amazon.com/sagemaker/latest/dg/nova-cpt.html\n- Ongoing research directions\n\t- Exploring the use of CPT for multi-modal tasks and low-resource languages\n\t- Investigating the impact of different data selection and preprocessing techniques on model performance\n\t- Developing new methods to mitigate catastrophic forgetting and improve scaling behaviour\n\n## UK Context\n\n- British contributions and implementations\n\t- UK researchers and institutions are actively contributing to the development and application of CPT, particularly in the areas of healthcare, finance, and legal technology\n\t- Collaborative projects between academia and industry are driving innovation in domain-specific and language-specific model adaptation\n- North England innovation hubs (if relevant)\n\t- Manchester, Leeds, Newcastle, and Sheffield are emerging as key centres for AI and machine learning research, with several startups and academic groups focusing on CPT for regional language adaptation and domain-specific use cases\n\t- Local innovation hubs are fostering collaboration between researchers, industry partners, and policymakers to advance the adoption of CPT in the region\n- Regional case studies\n\t- A recent project in Manchester used CPT to adapt a pre-trained model for medical text analysis, significantly improving performance on domain-specific tasks\n\t- In Leeds, a startup leveraged CPT to develop a multilingual chatbot for customer service, demonstrating the technique's effectiveness in low-resource language settings\n\n## Future Directions\n\n- Emerging trends and developments\n\t- Increased focus on multi-modal and cross-lingual CPT, enabling models to adapt to a wider range of domains and languages\n\t- Development of more efficient and scalable CPT frameworks, reducing the computational and data requirements for domain adaptation\n- Anticipated challenges\n\t- Ensuring the quality and diversity of training data to avoid overfitting and maintain general capabilities\n\t- Addressing the issue of catastrophic forgetting and developing robust methods to preserve previously learned knowledge\n- Research priorities\n\t- Investigating the impact of different data selection and preprocessing techniques on model performance\n\t- Exploring the use of CPT for emerging applications such as speech recognition, multi-modal learning, and low-resource language adaptation\n\n## References\n\n1. Behrouz, A., & Mirrokni, V. (2025). Introducing Nested Learning: A new ML paradigm for continual learning. Google Research Blog. https://research.google/blog/introducing-nested-learning-a-new-ml-paradigm-for-continual-learning/\n2. Raschka, S. (2025). New LLM Pre-training and Post-training Paradigms. Ahead of AI. https://magazine.sebastianraschka.com/p/new-llm-pre-training-and-post-training\n3. AMD ROCm Blog. (2025). Continued Pretraining: A Practical Playbook for Language-Specific LLM Adaptation. https://rocm.blogs.amd.com/artificial-intelligence/multilingual-continued-pretraining/README.html\n4. Emergent Mind. (2025). Continued Pre-Training (CPT) Overview. https://www.emergentmind.com/topics/continued-pre-training-cpt\n5. Amazon SageMaker AI. (2025). Continued pre-training (CPT). https://docs.aws.amazon.com/sagemaker/latest/dg/nova-cpt.html\n\n## Metadata\n\n- **Last Updated**: 2025-11-11\n- **Review Status**: Comprehensive editorial review\n- **Verification**: Academic sources verified\n- **Regional Context**: UK/North England where applicable",
  "properties": {
    "id": "continued-pre-training-ontology",
    "collapsed": "true",
    "- public-access": "true",
    "- ontology": "true",
    "- term-id": "AI-0248",
    "- preferred-term": "Continued Pre-Training",
    "- source-domain": "ai",
    "- status": "approved",
    "- version": "1.0",
    "- last-updated": "2025-11-18",
    "- definition": "Continued Pre-Training (CPT) represents an intermediate training phase where a foundation model undergoes additional unsupervised or self-supervised learning on domain-specific, task-relevant, or temporally recent data before task-specific fine-tuning. This technique bridges general-purpose pre-training and specialized adaptation by updating model parameters on unlabeled corpora that reflect target domain characteristics (medical texts, legal documents, scientific literature, code repositories), regional language variations, or newly emerged knowledge not present in the original training data. CPT preserves the broad capabilities acquired during initial pre-training while adapting internal representations to domain-specific linguistic patterns, terminology, reasoning structures, and factual knowledge. The process employs masked language modeling (MLM), causal language modeling (CLM), or denoising objectives similar to initial pre-training but on smaller, curated datasets (typically 1-100B tokens versus 1-10T tokens for foundation pre-training). Key hyperparameters include learning rate (typically 1/10 to 1/5 of initial pre-training rate to prevent catastrophic forgetting), training duration (measured in tokens processed or gradient steps), data composition (balancing domain-specific and general corpora), and mixing strategies (curriculum-based or uniform). CPT demonstrates superior sample efficiency compared to training from scratch and outperforms direct fine-tuning in domains with substantial distribution shift from general web text, as validated by empirical studies on biomedical, legal, and multilingual language model adaptation following methodologies outlined in AWS SageMaker AI and Google Cloud AI Platform frameworks.",
    "- maturity": "mature",
    "- source": "[[AWS SageMaker CPT Documentation]], [[Google Cloud AI Platform]], [[AMD ROCm Multilingual CPT Playbook]], [[Raschka 2025 LLM Training Paradigms]]",
    "- authority-score": "0.90"
  },
  "backlinks": [],
  "wiki_links": [
    "PreTraining",
    "Google Cloud AI Platform",
    "AWS SageMaker CPT Documentation",
    "Raschka 2025 LLM Training Paradigms",
    "AMD ROCm Multilingual CPT Playbook"
  ],
  "ontology": {
    "term_id": "AI-0248",
    "preferred_term": "Continued Pre-Training",
    "definition": "Continued Pre-Training (CPT) represents an intermediate training phase where a foundation model undergoes additional unsupervised or self-supervised learning on domain-specific, task-relevant, or temporally recent data before task-specific fine-tuning. This technique bridges general-purpose pre-training and specialized adaptation by updating model parameters on unlabeled corpora that reflect target domain characteristics (medical texts, legal documents, scientific literature, code repositories), regional language variations, or newly emerged knowledge not present in the original training data. CPT preserves the broad capabilities acquired during initial pre-training while adapting internal representations to domain-specific linguistic patterns, terminology, reasoning structures, and factual knowledge. The process employs masked language modeling (MLM), causal language modeling (CLM), or denoising objectives similar to initial pre-training but on smaller, curated datasets (typically 1-100B tokens versus 1-10T tokens for foundation pre-training). Key hyperparameters include learning rate (typically 1/10 to 1/5 of initial pre-training rate to prevent catastrophic forgetting), training duration (measured in tokens processed or gradient steps), data composition (balancing domain-specific and general corpora), and mixing strategies (curriculum-based or uniform). CPT demonstrates superior sample efficiency compared to training from scratch and outperforms direct fine-tuning in domains with substantial distribution shift from general web text, as validated by empirical studies on biomedical, legal, and multilingual language model adaptation following methodologies outlined in AWS SageMaker AI and Google Cloud AI Platform frameworks.",
    "source_domain": "ai",
    "maturity_level": null,
    "authority_score": 0.9
  }
}