{
  "title": "Continued Pre Training",
  "content": "- ### OntologyBlock\n  id:: continued-pre-training-ontology\n  collapsed:: true\n\t- ontology:: true\n\t- term-id:: AI-0248\n\t- preferred-term:: Continued Pre Training\n\t- source-domain:: ai\n\t- status:: draft\n    - public-access:: true\n\t- definition:: An intermediate training phase where a pre-trained model undergoes additional pre-training on domain-specific or task-relevant data before fine-tuning. This technique bridges general pre-training and task-specific fine-tuning, adapting model knowledge to particular domains whilst maintaining broad capabilities.\n\n\n## Academic Context\n\n- Brief contextual overview\n\t- Continued Pre-Training (CPT) is a transfer learning strategy in which a pre-existing foundation model is further trained on new, often domain-specific or language-specific, data using the same unsupervised objectives as initial pre-training\n\t- This approach bridges the gap between general pre-training and task-specific fine-tuning, allowing models to adapt to new domains or languages while preserving broad linguistic capabilities\n\t- CPT is now considered foundational in modern natural language processing, speech recognition, and multi-modal learning pipelines\n\n- Key developments and current state\n\t- The technique has evolved from simple domain adaptation to sophisticated methods involving selective layer freezing, domain-specific data replay, and tailored self-supervised objectives to mitigate catastrophic forgetting\n\t- Recent advances have enabled rapid domain adaptation with enhanced resource efficiency and scaling behaviour, proving effective in low-resource language settings and multi-modal tasks\n\n- Academic foundations\n\t- CPT builds on the principles of transfer learning and continual learning, leveraging previously acquired general representations and augmenting them with targeted learning signals\n\t- The paradigm is supported by research into catastrophic forgetting and the need for models to maintain or enhance general capabilities while accruing domain-specialized competencies\n\n## Current Landscape (2025)\n\n- Industry adoption and implementations\n\t- Major cloud platforms such as Amazon SageMaker and Google Cloud offer CPT capabilities, allowing organisations to adapt pre-trained models to specific domains or languages\n\t- Notable organisations include AWS, Google, and AMD, which have published practical playbooks and technical frameworks for implementing CPT\n\n- UK and North England examples where relevant\n\t- UK-based research institutions and tech companies are increasingly adopting CPT for domain-specific applications, particularly in healthcare, finance, and legal sectors\n\t- North England innovation hubs such as Manchester, Leeds, Newcastle, and Sheffield are home to several startups and academic groups exploring CPT for regional language adaptation and domain-specific use cases\n\n- Technical capabilities and limitations\n\t- CPT enables rapid domain adaptation and improved performance on specialized tasks without the need for extensive task-specific fine-tuning\n\t- However, the technique requires careful data selection and preprocessing to avoid overfitting and ensure the model retains its general capabilities\n\n- Standards and frameworks\n\t- Training and validation datasets for CPT should be diverse, representative, clean, and scaled appropriately to the target domain\n\t- Common data formats include JSONL files following the Converse format, with each line containing a JSON object representing a conversation or text entry\n\n## Research & Literature\n\n- Key academic papers and sources\n\t- Behrouz, A., & Mirrokni, V. (2025). Introducing Nested Learning: A new ML paradigm for continual learning. Google Research Blog. https://research.google/blog/introducing-nested-learning-a-new-ml-paradigm-for-continual-learning/\n\t- Raschka, S. (2025). New LLM Pre-training and Post-training Paradigms. Ahead of AI. https://magazine.sebastianraschka.com/p/new-llm-pre-training-and-post-training\n\t- AMD ROCm Blog. (2025). Continued Pretraining: A Practical Playbook for Language-Specific LLM Adaptation. https://rocm.blogs.amd.com/artificial-intelligence/multilingual-continued-pretraining/README.html\n\t- Emergent Mind. (2025). Continued Pre-Training (CPT) Overview. https://www.emergentmind.com/topics/continued-pre-training-cpt\n\t- Amazon SageMaker AI. (2025). Continued pre-training (CPT). https://docs.aws.amazon.com/sagemaker/latest/dg/nova-cpt.html\n\n- Ongoing research directions\n\t- Exploring the use of CPT for multi-modal tasks and low-resource languages\n\t- Investigating the impact of different data selection and preprocessing techniques on model performance\n\t- Developing new methods to mitigate catastrophic forgetting and improve scaling behaviour\n\n## UK Context\n\n- British contributions and implementations\n\t- UK researchers and institutions are actively contributing to the development and application of CPT, particularly in the areas of healthcare, finance, and legal technology\n\t- Collaborative projects between academia and industry are driving innovation in domain-specific and language-specific model adaptation\n\n- North England innovation hubs (if relevant)\n\t- Manchester, Leeds, Newcastle, and Sheffield are emerging as key centres for AI and machine learning research, with several startups and academic groups focusing on CPT for regional language adaptation and domain-specific use cases\n\t- Local innovation hubs are fostering collaboration between researchers, industry partners, and policymakers to advance the adoption of CPT in the region\n\n- Regional case studies\n\t- A recent project in Manchester used CPT to adapt a pre-trained model for medical text analysis, significantly improving performance on domain-specific tasks\n\t- In Leeds, a startup leveraged CPT to develop a multilingual chatbot for customer service, demonstrating the technique's effectiveness in low-resource language settings\n\n## Future Directions\n\n- Emerging trends and developments\n\t- Increased focus on multi-modal and cross-lingual CPT, enabling models to adapt to a wider range of domains and languages\n\t- Development of more efficient and scalable CPT frameworks, reducing the computational and data requirements for domain adaptation\n\n- Anticipated challenges\n\t- Ensuring the quality and diversity of training data to avoid overfitting and maintain general capabilities\n\t- Addressing the issue of catastrophic forgetting and developing robust methods to preserve previously learned knowledge\n\n- Research priorities\n\t- Investigating the impact of different data selection and preprocessing techniques on model performance\n\t- Exploring the use of CPT for emerging applications such as speech recognition, multi-modal learning, and low-resource language adaptation\n\n## References\n\n1. Behrouz, A., & Mirrokni, V. (2025). Introducing Nested Learning: A new ML paradigm for continual learning. Google Research Blog. https://research.google/blog/introducing-nested-learning-a-new-ml-paradigm-for-continual-learning/\n2. Raschka, S. (2025). New LLM Pre-training and Post-training Paradigms. Ahead of AI. https://magazine.sebastianraschka.com/p/new-llm-pre-training-and-post-training\n3. AMD ROCm Blog. (2025). Continued Pretraining: A Practical Playbook for Language-Specific LLM Adaptation. https://rocm.blogs.amd.com/artificial-intelligence/multilingual-continued-pretraining/README.html\n4. Emergent Mind. (2025). Continued Pre-Training (CPT) Overview. https://www.emergentmind.com/topics/continued-pre-training-cpt\n5. Amazon SageMaker AI. (2025). Continued pre-training (CPT). https://docs.aws.amazon.com/sagemaker/latest/dg/nova-cpt.html\n\n\n## Metadata\n\n- **Last Updated**: 2025-11-11\n- **Review Status**: Comprehensive editorial review\n- **Verification**: Academic sources verified\n- **Regional Context**: UK/North England where applicable",
  "properties": {
    "id": "continued-pre-training-ontology",
    "collapsed": "true",
    "- ontology": "true",
    "- term-id": "AI-0248",
    "- preferred-term": "Continued Pre Training",
    "- source-domain": "ai",
    "- status": "draft",
    "- public-access": "true",
    "- definition": "An intermediate training phase where a pre-trained model undergoes additional pre-training on domain-specific or task-relevant data before fine-tuning. This technique bridges general pre-training and task-specific fine-tuning, adapting model knowledge to particular domains whilst maintaining broad capabilities."
  },
  "backlinks": [],
  "wiki_links": [],
  "ontology": {
    "term_id": "AI-0248",
    "preferred_term": "Continued Pre Training",
    "definition": "An intermediate training phase where a pre-trained model undergoes additional pre-training on domain-specific or task-relevant data before fine-tuning. This technique bridges general pre-training and task-specific fine-tuning, adapting model knowledge to particular domains whilst maintaining broad capabilities.",
    "source_domain": "ai",
    "maturity_level": null,
    "authority_score": null
  }
}