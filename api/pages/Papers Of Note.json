{
  "title": "Papers Of Note",
  "content": "- ### OntologyBlock\n  id:: papers-of-note-ontology\n  collapsed:: true\n\t- ontology:: true\n\t- term-id:: mv-892246369703\n\t- preferred-term:: Papers Of Note\n\t- source-domain:: metaverse\n\t- status:: active\n\t- public-access:: true\n\t- definition:: A component of the metaverse ecosystem focusing on papers of note.\n\t- maturity:: mature\n\t- authority-score:: 0.85\n\t- owl:class:: mv:PapersOfNote\n\t- owl:physicality:: ConceptualEntity\n\t- owl:role:: Concept\n\t- belongsToDomain:: [[MetaverseDomain]]\n\t- #### Relationships\n\t  id:: papers-of-note-relationships\n\t\t- enables:: [[ImmersiveExperience]], [[Presence]], [[SpatialComputing]]\n\t\t- requires:: [[DisplayTechnology]], [[TrackingSystem]], [[RenderingEngine]]\n\t\t- bridges-to:: [[HumanComputerInteraction]], [[ComputerVision]], [[Robotics]]\n\n\t- #### OWL Axioms\n\t  id:: papers-of-note-owl-axioms\n\t  collapsed:: true\n\t\t- ```clojure\n\t\t  Declaration(Class(mv:PapersOfNote))\n\n\t\t  # Classification\n\t\t  SubClassOf(mv:PapersOfNote mv:ConceptualEntity)\n\t\t  SubClassOf(mv:PapersOfNote mv:Concept)\n\n\t\t  # Domain classification\n\t\t  SubClassOf(mv:PapersOfNote\n\t\t    ObjectSomeValuesFrom(mv:belongsToDomain mv:MetaverseDomain)\n\t\t  )\n\n\t\t  # Annotations\n\t\t  AnnotationAssertion(rdfs:label mv:PapersOfNote \"Papers Of Note\"@en)\n\t\t  AnnotationAssertion(rdfs:comment mv:PapersOfNote \"A component of the metaverse ecosystem focusing on papers of note.\"@en)\n\t\t  AnnotationAssertion(dcterms:identifier mv:PapersOfNote \"mv-892246369703\"^^xsd:string)\n\t\t  ```\n\npublic:: true\n\n- From Ahead of AI newsletter\n-\n- 1 Jan, *Astraios: Parameter-Efficient Instruction Tuning Code Large Language Models*, [https://arxiv.org/abs/2401.00788](https://substack.com/redirect/79786d4e-a69d-45d9-8b7b-6b4b8db4774a?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 2 Jan, *A Comprehensive Study of Knowledge Editing for Large Language Models*, [https://arxiv.org/abs/2401.01286](https://substack.com/redirect/8415b085-728b-445a-8b2e-26748f99f4cd?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 2 Jan, *LLM Maybe LongLM: Self-Extend LLM Context Window Without Tuning*, [https://arxiv.org/abs/2401.01325](https://substack.com/redirect/80d178e9-9f26-4fd3-b7dc-05b7201ebb18?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 2 Jan, *Self-Play Fine-Tuning Converts Weak Language Models to Strong Language Models*, [https://arxiv.org/abs/2401.01335](https://substack.com/redirect/ae67c483-c26a-453a-b9c9-927cb26ccc0c?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 2 Jan, *LLaMA Beyond English: An Empirical Study on Language Capability Transfer*, [https://arxiv.org/abs/2401.01055](https://substack.com/redirect/386a7d59-fd93-4532-9ba1-ca60be2a44de?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 3 Jan, *A Mechanistic Understanding of Alignment Algorithms: A Case Study on DPO and Toxicity*, [https://arxiv.org/abs/2401.01967](https://substack.com/redirect/bf66a892-a628-41df-9895-b6ad35a3b36e?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 4 Jan, *LLaMA Pro: Progressive LLaMA with Block Expansion*, [https://arxiv.org/abs/2401.02415](https://substack.com/redirect/05113354-4602-4d37-9dce-4a9e62e56db3?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 4 Jan, *LLM Augmented LLMs: Expanding Capabilities through Composition*, [https://arxiv.org/abs/2401.02412](https://substack.com/redirect/61fc2c45-e536-4936-8bbb-91896472a870?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 4 Jan, *Blending Is All You Need: Cheaper, Better Alternative to Trillion-Parameters LLM*, [https://arxiv.org/abs/2401.02994](https://substack.com/redirect/728a3b9f-872c-475f-a2fe-3d70477c2e38?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 5 Jan, *DeepSeek LLM: Scaling Open-Source Language Models with Longtermism*, [https://arxiv.org/abs/2401.02954](https://substack.com/redirect/610e990e-7d64-40b6-bc05-b2324ecd7e95?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 5 Jan, *Denoising Vision Transformers*, [https://arxiv.org/abs/2401.02957](https://substack.com/redirect/5eed5e56-c3e8-4eed-90bf-c7a46897a058?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 7 Jan, *Soaring from 4K to 400K: Extending LLM’s Context with Activation Beacon*, [https://arxiv.org/abs/2401.03462](https://substack.com/redirect/2716aa8a-b46b-472b-b510-6b5c7a4ec1b8?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 8 Jan, *Mixtral of Experts*, [https://arxiv.org/abs/2401.04088](https://substack.com/redirect/b56cec75-5d8b-4467-8a81-1ac4a28556c4?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 8 Jan, *MoE-Mamba: Efficient Selective State Space Models with Mixture of Experts*, [https://arxiv.org/abs/2401.04081](https://substack.com/redirect/ac40afc9-6683-4680-af84-567156ab830c?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 8 Jan, *A Minimaximalist Approach to Reinforcement Learning from Human Feedback*, [https://arxiv.org/abs/2401.04056](https://substack.com/redirect/e2be01e3-f7f2-476d-a12a-8329b5f94d96?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 9 Jan, *RoSA: Accurate Parameter-Efficient Fine-Tuning via Robust Adaptation*, [https://arxiv.org/abs/2401.04679](https://substack.com/redirect/500c62f1-57ff-4c8e-933e-bce9ee189503?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 10 Jan, *Sleeper Agents: Training Deceptive LLMs that Persist Through Safety Training*, [https://arxiv.org/abs/2401.05566](https://substack.com/redirect/933f5613-becb-4425-aa2d-d1cbd9851278?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 11 Jan, *Transformers are Multi-State RNNs*, [https://arxiv.org/abs/2401.06104](https://substack.com/redirect/e6c46b40-512c-417d-89bd-bdf8192c41b7?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 11 Jan, *A Closer Look at AUROC and AUPRC under Class Imbalance*, [https://arxiv.org/abs/2401.06091](https://substack.com/redirect/e48406a0-1c08-4477-92fc-52c9154aedb4?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 12 Jan, *An Experimental Design Framework for Label-Efficient Supervised Finetuning of Large Language Models*, [https://arxiv.org/abs/2401.06692](https://substack.com/redirect/8e8efc39-5a81-4324-8568-97c4be006dad?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 16 Jan, *Tuning Language Models by Proxy*, [https://arxiv.org/abs/2401.08565](https://substack.com/redirect/bccdf908-93e9-4a5f-ba08-bcfa0e09c56d?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 16 Jan, *Scalable Pre-training of Large Autoregressive Image Models*, [https://arxiv.org/abs/2401.08541](https://substack.com/redirect/566e30b9-23ca-459f-8f1d-f551054a7608?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 16 Jan, *Code Generation with AlphaCodium: From Prompt Engineering to Flow Engineering*, [https://arxiv.org/abs/2401.08500](https://substack.com/redirect/2695754b-a034-4901-a1a6-fe152d3f238d?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 16 Jan, *RAG vs Fine-tuning: Pipelines, Tradeoffs, and a Case Study on Agriculture*, [https://arxiv.org/abs/2401.08406](https://substack.com/redirect/a9e02776-d3c8-4f88-b390-d4fffc8f1905?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 17 Jan, *ReFT: Reasoning with Reinforced Fine-Tuning*, [https://arxiv.org/abs/2401.08967](https://substack.com/redirect/77379c5d-c672-420d-a1a1-cdfa0bb0c7a5?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 18 Jan, *DiffusionGPT: LLM-Driven Text-to-Image Generation System*, [https://arxiv.org/abs/2401.10061](https://substack.com/redirect/3d37b8a5-5a83-4c42-a193-6ed04f15a785?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 18 Jan, *Self-Rewarding Language Models*, [https://arxiv.org/abs/2401.10020](https://substack.com/redirect/2c8812f3-746b-49d7-bb35-f7777bea9193?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 18 Jan, *VMamba: Visual State Space Model*, [https://arxiv.org/abs/2401.10166](https://substack.com/redirect/b1ae3527-1b59-4890-b1a7-fae354be2f00?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 19 Jan, *Knowledge Fusion of Large Language Models*, [https://arxiv.org/abs/2401.10491](https://substack.com/redirect/53cbb0c3-8314-48be-aa35-d4a493b8bf3a?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 22 Jan, *SpatialVLM: Endowing Vision-Language Models with Spatial Reasoning Capabilities*, [https://arxiv.org/abs/2401.12168](https://substack.com/redirect/f122379a-1d40-4c35-91dd-3fbf4384ed37?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 22 Jan, *WARM: On the Benefits of Weight Averaged Reward Models*, [https://arxiv.org/abs/2401.12187](https://substack.com/redirect/301db30f-2301-45c0-9b4e-cccf54af80ce?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 22 Jan, *Spotting LLMs With Binoculars: Zero-Shot Detection of Machine-Generated Text*, [https://arxiv.org/abs/2401.12070](https://substack.com/redirect/69846790-fce4-490f-ad66-872bb0096113?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 24 Jan, *MambaByte: Token-free Selective State Space Model*, [https://arxiv.org/abs/2401.13660](https://substack.com/redirect/5c25a170-d994-4879-b098-761a68fab7ea?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 24 Jan, *SpacTor-T5: Pre-training T5 Models with Span Corruption and Replaced Token Detection*, [https://arxiv.org/abs/2401.13160](https://substack.com/redirect/6cb2ee53-e417-4fb9-937c-3fd4e4a716b2?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 25 Jan, *Rethinking Patch Dependence for Masked Autoencoders*, [https://arxiv.org/abs/2401.14391](https://substack.com/redirect/ec8eb548-bcc3-47ae-a584-f8aa3fa01bd9?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 25 Jan, *Pix2gestalt: Amodal Segmentation by Synthesizing Wholes*, [https://arxiv.org/abs/2401.14398](https://substack.com/redirect/2c049fb8-8fec-4eff-b27e-2afc9b0319d9?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 25 Jan, *Multimodal Pathway: Improve Transformers with Irrelevant Data from Other Modalities*, [https://arxiv.org/abs/2401.14405](https://substack.com/redirect/34087c7f-b556-4c01-be4d-220f6f7224da?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 26 Jan, *EAGLE: Speculative Sampling Requires Rethinking Feature Uncertainty*, [https://arxiv.org/abs/2401.15077](https://substack.com/redirect/7c73bf10-bc71-408d-97fa-f75aed7a5e40?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 29 Jan, *MoE-LLaVA: Mixture of Experts for Large Vision-Language Models*, [https://arxiv.org/abs/2401.15947](https://substack.com/redirect/2109ca30-087e-4c41-acc7-3e6528b28151?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 29 Jan, *Rephrasing the Web: A Recipe for Compute and Data-Efficient Language Modeling*, [https://arxiv.org/abs/2401.16380](https://substack.com/redirect/9a04bcc7-5c7c-4501-8d94-cd39a98fb965?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 31 Jan, *KVQuant: Towards 10 Million Context Length LLM Inference with KV Cache Quantization*, [https://arxiv.org/abs/2401.18079](https://substack.com/redirect/773df725-e457-48fa-84c7-62cc9402a89d?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- ## **February 2024**\n- 1 Feb, *Efficient Exploration for LLMs*, [https://arxiv.org/abs/2402.00396](https://substack.com/redirect/6337bcce-9da1-46cb-ac0e-d348c51a4174?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 1 Feb, *OLMo: Accelerating the Science of Language Models*, [https://arxiv.org/abs/2402.00838](https://substack.com/redirect/0190868f-d342-4489-b2ff-79391083472f?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 1 Feb, *Tiny Titans: Can Smaller Large Language Models Punch Above Their Weight in the Real World for Meeting Summarization?*, [https://arxiv.org/abs/2402.00841](https://substack.com/redirect/4dcc03d5-f11f-48f3-89cc-95d1387f4752?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 1 Feb, *Repeat After Me: Transformers are Better than State Space Models at Copying*, [https://arxiv.org/abs/2402.01032](https://substack.com/redirect/fc724550-573b-41ff-aef9-bd76a2a6ffc2?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 2 Feb, *LiPO: Listwise Preference Optimization through Learning-to-Rank*, [https://arxiv.org/abs/2402.01878](https://substack.com/redirect/f24801af-42c8-4051-bb6b-98b32511bf9c?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 2 Feb, *FindingEmo: An Image Dataset for Emotion Recognition in the Wild*, [https://arxiv.org/abs/2402.01355](https://substack.com/redirect/789d0c85-6cae-4c2a-8c66-aac5b6df5e00?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 3 Feb, *More Agents Is All You Need*, [https://arxiv.org/abs/2402.05120](https://substack.com/redirect/ed2fc1c5-c2df-4bb7-8d71-920bf06383c3?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 5 Feb, *DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models*, [https://arxiv.org/abs/2402.03300](https://substack.com/redirect/0d4eed8d-78a8-4f6e-bf73-d8bce805848c?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 6 Feb, *MobileVLM V2: Faster and Stronger Baseline for Vision Language Model*, [https://arxiv.org/abs/2402.03766](https://substack.com/redirect/23f525b9-daaf-46cb-bf8f-7a6fc93648ad?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 6 Feb, *A Phase Transition Between Positional and Semantic Learning in a Solvable Model of Dot-Product Attention*, [https://arxiv.org/abs/2402.03902](https://substack.com/redirect/11339467-8ea5-4416-9408-dbef856bd64c?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 6 Feb, *Scaling Laws for Downstream Task Performance of Large Language Models*, [https://arxiv.org/abs/2402.04177](https://substack.com/redirect/14bdf9b0-2d2c-4628-93a7-46a8dac8d393?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 6 Feb, *MOMENT: A Family of Open Time-series Foundation Models*, [https://arxiv.org/abs/2402.03885](https://substack.com/redirect/a29596ff-57d9-44c4-9763-984ac22aca5b?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 6 Feb, *Vision Superalignment: Weak-to-Strong Generalization for Vision Foundation Models*, [https://arxiv.org/abs/2402.03749](https://substack.com/redirect/41d4401a-7bad-443e-b9d8-62a5e2f6834f?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 6 Feb, *Self-Discover: Large Language Models Self-Compose Reasoning Structures*, [https://arxiv.org/abs/2402.03620](https://substack.com/redirect/6ba4ac1a-b4fa-463c-8279-65ce5cbdfa9b?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 7 Feb, *Grandmaster-Level Chess Without Search*, [https://arxiv.org/abs/2402.04494](https://substack.com/redirect/ecf1d304-6b4c-4379-8f3f-98c8cea2df80?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 7 Feb, *Direct Language Model Alignment from Online AI Feedback*, [https://arxiv.org/abs/2402.04792](https://substack.com/redirect/fea30f6f-7126-4e11-a698-a79d30992066?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 8 Feb, *Buffer Overflow in Mixture of Experts*, [https://arxiv.org/abs/2402.05526](https://substack.com/redirect/1e111953-e08d-4231-b194-dee4e4f92c64?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 9 Feb, *The Boundary of Neural Network Trainability is Fractal*, [https://arxiv.org/abs/2402.06184](https://substack.com/redirect/0d7e0d57-04df-4249-80c4-a2118aefe303?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 11 Feb, *ODIN: Disentangled Reward Mitigates Hacking in RLHF*, [https://arxiv.org/abs/2402.07319](https://substack.com/redirect/87e1a8ab-585b-40f0-b137-c69d08a13c78?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 12 Feb, *Policy Improvement using Language Feedback Models*, [https://arxiv.org/abs/2402.07876](https://substack.com/redirect/3928654c-28a1-4ea7-933a-661a664d7f6f?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 12 Feb, *Scaling Laws for Fine-Grained Mixture of Experts*, [https://arxiv.org/abs/2402.07871](https://substack.com/redirect/264b2b21-99ab-4c1f-84a1-c63b2ef96161?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 12 Feb, *Aya Model: An Instruction Finetuned Open-Access Multilingual Language Model*, [https://arxiv.org/abs/2402.07610](https://substack.com/redirect/7fddb107-1885-42e1-abfe-a97f09e0d94b?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 12 Feb, *Step-On-Feet Tuning: Scaling Self-Alignment of LLMs via Bootstrapping*, [https://arxiv.org/abs/2402.07610](https://substack.com/redirect/7fddb107-1885-42e1-abfe-a97f09e0d94b?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 12 Feb, *Suppressing Pink Elephants with Direct Principle Feedback*, [https://arxiv.org/abs/2402.07896](https://substack.com/redirect/706bc856-06df-4858-b39f-fee51403dbc5?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 13 Feb, *World Model on Million-Length Video And Language With RingAttention*, [https://arxiv.org/abs/2402.08268](https://substack.com/redirect/e22fb3c7-bba5-4382-9d9a-ed25b9ce60a5?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 13 Feb, *Mixtures of Experts Unlock Parameter Scaling for Deep RL*, [https://arxiv.org/abs/2402.08609](https://substack.com/redirect/d9ec89b7-9d9d-4bd1-b58d-08299e989fdd?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 14 Feb, *DoRA: Weight-Decomposed Low-Rank Adaptation*, [https://arxiv.org/abs/2402.09353](https://substack.com/redirect/69f38313-caec-475f-9616-cbefd1e3c879?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 14 Feb, *Transformers Can Achieve Length Generalization But Not Robustly*, [https://arxiv.org/abs/2402.09371](https://substack.com/redirect/7469cd7a-613a-4d31-be69-9d93de53fc23?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 15 Feb, *BASE TTS: Lessons From Building a Billion-Parameter Text-to-Speech Model on 100K Hours of Data*, [https://arxiv.org/abs/2402.08093](https://substack.com/redirect/111cbecd-5de9-485c-a26a-1a6a5e17afc0?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 15 Feb, *Recovering the Pre-Fine-Tuning Weights of Generative Models*, [https://arxiv.org/abs/2402.10208](https://substack.com/redirect/5682bb1d-d66e-4d50-8c49-977eeeb86c49?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 15 Feb, *Generative Representational Instruction Tuning*, [https://arxiv.org/abs/2402.09906](https://substack.com/redirect/d71e8c90-0e4e-4eb9-b1b0-db982af62371?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 16 Feb, *FinTral: A Family of GPT-4 Level Multimodal Financial Large Language Models*, [https://arxiv.org/abs/2402.10986](https://substack.com/redirect/a8906a2a-e9ff-4fa0-803e-6d8cb180090c?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 17 Feb, *OneBit: Towards Extremely Low-bit Large Language Models*, [https://arxiv.org/abs/2402.11295](https://substack.com/redirect/bd24335f-0818-429c-8c73-611f51a61366?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 18 Feb, *LongAgent: Scaling Language Models to 128k Context through Multi-Agent Collaboration*, [https://arxiv.org/abs/2402.11550](https://substack.com/redirect/ddb3e5fc-4a46-423b-b534-db18a1016ae9?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 19 Feb, *Reformatted Alignment*, [https://arxiv.org/abs/2402.12219](https://substack.com/redirect/5ab208b3-5479-4ab6-9aa2-b7b003998443?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 19 Feb, *AnyGPT: Unified Multimodal LLM with Discrete Sequence Modeling*, [https://arxiv.org/abs/2402.12226](https://substack.com/redirect/c5f72d21-b502-48b4-9bea-0fee871e9c0e?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 19 Feb, *Towards Cross-Tokenizer Distillation: the Universal Logit Distillation Loss for LLMs*, [https://arxiv.org/abs/2402.12030](https://substack.com/redirect/b4aeb6df-c9b1-48a2-9e01-d1cdca16a74b?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 19 Feb, *LoRA+: Efficient Low Rank Adaptation of Large Models*, [https://arxiv.org/abs/2402.12354](https://substack.com/redirect/43aa1e82-17f1-4067-a5b5-aabfaa37c8bd?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 20 Feb, *Neural Network Diffusion*, [https://arxiv.org/abs/2402.13144](https://substack.com/redirect/0561a8f8-c92a-4d5f-a561-ab61adf04f7f?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 21 Feb, *YOLOv9: Learning What You Want to Learn Using Programmable Gradient Information*, [https://arxiv.org/abs/2402.13616](https://substack.com/redirect/cb4428de-49c3-4daa-ba7b-5cc112d56b73?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 21 Feb, *LongRoPE: Extending LLM Context Window Beyond 2 Million Tokens*, [https://arxiv.org/abs/2402.13753](https://substack.com/redirect/9d13cb3f-dde5-4605-90a7-c987f49c2c70?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 21 Feb, *Large Language Models for Data Annotation: A Survey*, [https://arxiv.org/abs/2402.13446](https://substack.com/redirect/70d95e24-3ef2-4626-b92d-046bd0e2ce21?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 22 Feb, *TinyLLaVA: A Framework of Small-scale Large Multimodal Models*, [https://arxiv.org/abs/2402.14289](https://substack.com/redirect/a2eedacd-b0af-466a-87dd-ef019f68d307?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 22 Feb, *Back to Basics: Revisiting REINFORCE Style Optimization for Learning from Human Feedback in LLMs*, [https://arxiv.org/abs/2402.14740](https://substack.com/redirect/3b9cc140-a59c-4cc3-b7c9-a0593b19b25b?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 23 Feb, *Genie: Generative Interactive Environments*, [https://arxiv.org/abs/2402.15391](https://substack.com/redirect/308c15cf-c211-49db-ac29-c3f47693f1ca?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 27 Feb, *The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits*, [https://arxiv.org/abs/2402.17764](https://substack.com/redirect/d9e23c78-dab0-4410-bbf0-70d85d521026?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 27 Feb, *Sora Generates Videos with Stunning Geometrical Consistency*, [https://arxiv.org/abs/2402.17403](https://substack.com/redirect/5125bc10-90a9-4b7e-884a-8cd04098ecf0?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 27 Feb, *When Scaling Meets LLM Finetuning: The Effect of Data, Model and Finetuning Method*, [https://arxiv.org/abs/2402.17193](https://substack.com/redirect/1de5ffef-c7be-4cd3-856e-bd706f2cca29?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 29 Feb, *Griffin: Mixing Gated Linear Recurrences with Local Attention for Efficient Language Models*, [https://arxiv.org/abs/2402.19427](https://substack.com/redirect/a5ea3c53-25c4-499a-933f-cc50fc39be4f?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- ## **March 2024**\n- 1 Mar, *Learning and Leveraging World Models in Visual Representation Learning*, [https://arxiv.org/abs/2403.00504](https://substack.com/redirect/bf69c896-c761-4420-b830-d91c18d0031f?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 3 Mar, *Improving LLM Code Generation with Grammar Augmentation*, [https://arxiv.org/abs/2403.01632](https://substack.com/redirect/df4e40a7-62ce-4099-b8e3-7251057b27d6?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 3 Mar, *The Hidden Attention of Mamba Models*, [https://arxiv.org/abs/2403.01590](https://substack.com/redirect/65a73b15-53dd-4f6b-9321-32fce73c61da?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 4 Mar, *Training-Free Pretrained Model Merging*, [https://arxiv.org/abs/2403.01753](https://substack.com/redirect/0377b714-763f-4a47-9465-4659e3e07a32?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 4 Mar, *Vision-RWKV: Efficient and Scalable Visual Perception with RWKV-Like Architectures*, [https://arxiv.org/abs/2403.02308](https://substack.com/redirect/4722a591-4c74-4665-8b83-9462190f60c7?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 5 Mar, *The WMDP Benchmark: Measuring and Reducing Malicious Use With Unlearning*, [https://arxiv.org/abs/2403.03218](https://substack.com/redirect/c65d115d-4f6f-4dee-8eb3-facb36cf9a87?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 5 Mar, *Evolution Transformer: In-Context Evolutionary Optimization*, [https://arxiv.org/abs/2403.02985](https://substack.com/redirect/b6c9c26f-483e-428e-abda-383ad795e445?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 5 Mar, *Enhancing Vision-Language Pre-training with Rich Supervisions*, [https://arxiv.org/abs/2403.03346](https://substack.com/redirect/29cb5301-fd6b-4576-be34-058628d0bb11?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 5 Mar, *Scaling Rectified Flow Transformers for High-Resolution Image Synthesis*, [https://arxiv.org/abs/2403.03206](https://substack.com/redirect/002e31cd-1b8c-4392-8100-3fa60278b34a?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 5 Mar, *Design2Code: How Far Are We From Automating Front-End Engineering?*, [https://arxiv.org/abs/2403.03163](https://substack.com/redirect/26d8afdd-0405-425d-85a1-abcb78278565?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 6 Mar, *ShortGPT: Layers in Large Language Models are More Redundant Than You Expect*, [https://arxiv.org/abs/2403.03853](https://substack.com/redirect/1d975c39-b01f-4f6e-82d2-bbdc61aff72c?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 6 Mar, *Backtracing: Retrieving the Cause of the Query*, [https://arxiv.org/abs/2403.03956](https://substack.com/redirect/ad2c3d3e-802f-4dfa-b3b4-caca26943b93?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 6 Mar, *Learning to Decode Collaboratively with Multiple Language Models*, [https://arxiv.org/abs/2403.03870](https://substack.com/redirect/91cc124a-7cfa-4db7-b6e7-6bf9f8083722?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 6 Mar, *SaulLM-7B: A pioneering Large Language Model for Law*, [https://arxiv.org/abs/2403.03883](https://substack.com/redirect/f50f70ba-d988-48e5-8a84-318c818acc32?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 6 Mar, *Are Language Models Puzzle Prodigies? Algorithmic Puzzles Unveil Serious Challenges in Multimodal Reasoning*, [https://arxiv.org/abs/2403.03864](https://substack.com/redirect/a64cd561-81a5-4151-87a6-4b7a3f0bc24f?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 6 Mar, *3D Diffusion Policy*, [https://arxiv.org/abs/2403.03954](https://substack.com/redirect/b5e9c814-0ef1-4d20-8927-2f2b2ae64b34?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 6 Mar, *MedMamba: Vision Mamba for Medical Image Classification*, [https://arxiv.org/abs/2403.03849](https://substack.com/redirect/deda3cc5-b387-4207-ad9b-afc9995201ea?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 6 Mar, *GaLore: Memory-Efficient LLM Training by Gradient Low-Rank Projection*, [https://arxiv.org/abs/2403.03507](https://substack.com/redirect/c4c6298b-05b0-43b5-b393-b878cb005f58?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 6 Mar, *Stop Regressing: Training Value Functions via Classification for Scalable Deep RL*, [https://arxiv.org/abs/2403.03950](https://substack.com/redirect/bdf046ea-6f89-476d-a922-8a7b8e06a6cb?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 7 Mar, *How Far Are We from Intelligent Visual Deductive Reasoning?*, [https://arxiv.org/abs/2403.04732](https://substack.com/redirect/210d2935-5b7a-41a1-b249-78c71b2faf73?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 7 Mar, *Common 7B Language Models Already Possess Strong Math Capabilities*, [https://arxiv.org/abs/2403.04706](https://substack.com/redirect/53edf6c3-6121-4f41-bc7c-24e1963c39bc?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 8 Mar, *Gemini 1.5: Unlocking Multimodal Understanding Across Millions of Tokens of Context*, [https://arxiv.org/abs/2403.05530](https://substack.com/redirect/4f32e4c5-64f3-49a7-b097-6d42e3a39ba7?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 8 Mar, *Is Cosine-Similarity of Embeddings Really About Similarity?*, [https://arxiv.org/abs/2403.05440](https://substack.com/redirect/9b2e2c27-a377-4a6a-bfef-ab2df79d585a?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 8 Mar, *LLM4Decompile: Decompiling Binary Code with Large Language Models*, [https://arxiv.org/abs/2403.05286](https://substack.com/redirect/11cc1be9-d4f6-403f-b98a-0397a2a44036?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 9 Mar, *Algorithmic Progress in Language Models*, [https://arxiv.org/abs/2403.05812](https://substack.com/redirect/6aaddd29-c19b-4344-b857-bf9861aae9c9?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 11 Mar, *Stealing Part of a Production Language Model*, [https://arxiv.org/abs/2403.06634](https://substack.com/redirect/a9d86223-2cce-4a70-902c-3ccce62ee4b3?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 12 Mar, *Chronos: Learning the Language of Time Series*, [https://arxiv.org/abs/2403.07815](https://substack.com/redirect/943d16a3-7acb-4cac-93c8-0cbecc99b3c5?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 13 Mar, *Simple and Scalable Strategies to Continually Pre-train Large Language Models*, [https://arxiv.org/abs/2403.08763](https://substack.com/redirect/e20a4bd2-eb63-4c33-9c69-adde7ca6ed5f?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 13 Mar, *Language Models Scale Reliably With Over-Training and on Downstream Tasks*, [https://arxiv.org/abs/2403.08540](https://substack.com/redirect/df309c37-3e65-4a5a-873d-da46527d8d84?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 14 Mar, *BurstAttention: An Efficient Distributed Attention Framework for Extremely Long Sequences*, [https://arxiv.org/abs/2403.09347](https://substack.com/redirect/2beaa761-08ec-441d-96dd-3ce86e2fe0ba?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 14 Mar, *LocalMamba: Visual State Space Model with Windowed Selective Scan*, [https://arxiv.org/abs/2403.09338](https://substack.com/redirect/21ab52c1-3a65-4d0d-8867-65ceac234255?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 14 Mar, *GiT: Towards Generalist Vision Transformer through Universal Language Interface*, [https://arxiv.org/abs/2403.09394](https://substack.com/redirect/dd5bc2fc-d41e-4607-a5a8-29d26e7d237c?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 14 Mar, *MM1: Methods, Analysis & Insights from Multimodal LLM Pre-training*, [https://arxiv.org/abs/2403.09611](https://substack.com/redirect/9c27a132-3fe7-48c6-bfb7-4e21dfcba617?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 15 Mar, *RAFT: Adapting Language Model to Domain Specific RAG*, [https://arxiv.org/abs/2403.10131](https://substack.com/redirect/9f469270-4bd0-4277-bd66-9d9705038867?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 18 Mar, *TnT-LLM: Text Mining at Scale with Large Language Models*, [https://arxiv.org/abs/2403.12173](https://substack.com/redirect/c8b67876-8509-44b9-80cf-4374ca4808fd?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 18 Mar, *Decoding Compressed Trust: Scrutinizing the Trustworthiness of Efficient LLMs Under Compression*, [https://arxiv.org/abs/2403.15447](https://substack.com/redirect/a4f945af-8c10-4542-95ee-5178bcd54037?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 19 Mar, *PERL: Parameter Efficient Reinforcement Learning from Human Feedback*, [https://arxiv.org/abs/2403.10704](https://substack.com/redirect/ce0a0e40-3f29-41b7-ae58-a1093d80d622?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 20 Mar, *RewardBench: Evaluating Reward Models for Language Modeling*, [https://arxiv.org/abs/2403.13787](https://substack.com/redirect/ad69cddd-fbfd-4e63-80c9-7ae8359ecdc9?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 20 Mar, *LlamaFactory: Unified Efficient Fine-Tuning of 100+ Language Models*, [https://arxiv.org/abs/2403.13372](https://substack.com/redirect/49694772-deff-469c-8582-3de7e8e84781?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 21 Mar, *RakutenAI-7B: Extending Large Language Models for Japanese*, [https://arxiv.org/abs/2403.15484](https://substack.com/redirect/0dc30d9a-9d6a-4e9f-ba0e-e1b64e674c9a?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 22 Mar, *SiMBA: Simplified Mamba-Based Architecture for Vision and Multivariate Time Series*, [https://arxiv.org/abs/2403.15360](https://substack.com/redirect/b6bcb5fc-34bd-42dc-a993-19aa3b5f617a?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 22 Mar, *Can Large Language Models Explore In-Context?*, [https://arxiv.org/abs/2403.15371](https://substack.com/redirect/25b141fb-1157-4f4d-98d3-cfe9aa077905?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 22 Mar, *LLM2LLM: Boosting LLMs with Novel Iterative Data Enhancement*, [https://arxiv.org/abs/2403.15042](https://substack.com/redirect/f8a27da6-8170-4811-b3bb-d6d49debdff3?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 25 Mar, *LLM Agent Operating System*, [https://arxiv.org/abs/2403.16971](https://substack.com/redirect/b9bd58f7-5b30-4cd4-8b95-9f8f3dc87e1b?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 26 Mar, *The Unreasonable Ineffectiveness of the Deeper Layers*, [https://arxiv.org/abs/2403.17887](https://substack.com/redirect/0a715f4c-dfed-4497-917a-3e643cd0e30f?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 27 Mar, *BioMedLM: A 2.7B Parameter Language Model Trained On Biomedical Text*, [https://arxiv.org/abs/2403.18421](https://substack.com/redirect/9887f35d-de62-48ec-811a-f7eda02ae04b?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 27 Mar, *ViTAR: Vision Transformer with Any Resolution*, [https://arxiv.org/abs/2403.18361](https://substack.com/redirect/f5a8f5fc-6e93-46cb-b335-10d46e7878cc?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 27 Mar, *Long-form Factuality in Large Language Models*, [https://arxiv.org/abs/2403.18802](https://substack.com/redirect/c59b2db5-e4d8-498f-b3e1-2bdc3ea6ca07?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 27 Mar, *Mini-Gemini: Mining the Potential of Multi-modality Vision Language Models*, [https://arxiv.org/abs/2403.18814](https://substack.com/redirect/b0a94af2-e141-4118-98b2-b0b549836ee9?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 26 Mar, *LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning*, [https://arxiv.org/abs/2403.17919](https://substack.com/redirect/44aeb66f-8e1e-4a9b-8ae1-351393348b8a?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 26 Mar, *Mechanistic Design and Scaling of Hybrid Architectures*, [https://arxiv.org/abs/2403.17844](https://substack.com/redirect/6b2c74e7-370c-4595-9685-ebc886108665?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 28 Mar, *MagicLens: Self-Supervised Image Retrieval with Open-Ended Instructions*, [https://arxiv.org/abs/2403.19651](https://substack.com/redirect/3fee28cb-79c1-4251-b00d-ad559623e003?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 28 Mar, *Model Stock: All We Need Is Just a Few Fine-Tuned Models*, [https://arxiv.org/abs/2403.19522](https://substack.com/redirect/8a9297ca-a6de-4079-9b45-ef221458494d?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- ## **April 2024**\n- 1 Apr, *Do Language Models Plan Ahead for Future Tokens?*, [https://arxiv.org/abs/2404.00859](https://substack.com/redirect/674dfd1a-800a-4cfd-9e67-e7d8bdf45570?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 1 Apr, *Bigger is not Always Better: Scaling Properties of Latent Diffusion Models*, [https://arxiv.org/abs/2404.01367](https://substack.com/redirect/a09d2339-253e-4b9c-893a-5e458139c2bb?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 1 Apr, *The Fine Line: Navigating Large Language Model Pretraining with Down-streaming Capability Analysis*, [https://arxiv.org/abs/2404.01204](https://substack.com/redirect/656ad167-f982-44b3-a45e-4bba6155f068?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 1 Apr, *Diffusion-RWKV: Scaling RWKV-Like Architectures for Diffusion Models*, [https://arxiv.org/abs/2404.04478](https://substack.com/redirect/e1260dcf-e6e8-4499-a23b-cd165a9044ff?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 2 Apr, *Mixture-of-Depths: Dynamically Allocating Compute in Transformer-Based Language Models*, [https://arxiv.org/abs/2404.02258](https://substack.com/redirect/3abf27d6-c45c-4105-badc-39912c1e33ab?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 2 Apr, *Long-context LLMs Struggle with Long In-context Learning*, [https://arxiv.org/abs/2404.02060](https://substack.com/redirect/e312eee6-b991-416c-9863-9bfb6a6a8df2?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 2 Apr, *Emergent Abilities in Reduced-Scale Generative Language Models*, [https://arxiv.org/abs/2404.02204](https://substack.com/redirect/52532fa9-285a-4c0e-8bde-1591eb953e2a?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 2 Apr, *Jailbreaking Leading Safety-Aligned LLMs with Simple Adaptive Attacks*, [https://arxiv.org/abs/2404.02151](https://substack.com/redirect/02b51392-a673-443e-acb7-8dae9dad43bf?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 3 Apr, *On the Scalability of Diffusion-based Text-to-Image Generation*, [https://arxiv.org/abs/2404.02883](https://substack.com/redirect/db89b6ae-15d5-493b-87e9-554fce1cb840?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 3 Apr, *BAdam: A Memory Efficient Full Parameter Training Method for Large Language Models*, [https://arxiv.org/abs/2404.02827](https://substack.com/redirect/a6eeedc6-5257-494b-8bd4-f1f8102110e8?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 3 Apr, *Cross-Attention Makes Inference Cumbersome in Text-to-Image Diffusion Models*, [https://arxiv.org/abs/2404.02747](https://substack.com/redirect/ea27317e-f84c-4a88-bd7d-e42e7d45bcb2?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 4 Apr, *Direct Nash Optimization: Teaching Language Models to Self-Improve with General Preferences*, [https://arxiv.org/abs/2404.02151](https://substack.com/redirect/02b51392-a673-443e-acb7-8dae9dad43bf?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 4 Apr, *Training LLMs over Neurally Compressed Text*, [https://arxiv.org/abs/2404.03626](https://substack.com/redirect/873f7c27-d879-40e1-9020-451aae9e187e?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 4 Apr, *CantTalkAboutThis: Aligning Language Models to Stay on Topic in Dialogues*, [https://arxiv.org/abs/2404.03820](https://substack.com/redirect/1b840346-40c1-4538-8c45-8acc3eea6905?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 5 Apr, *ReFT: Representation Finetuning for Language Models*, [https://arxiv.org/abs/2404.03592](https://substack.com/redirect/24f64667-d62c-46db-97ab-26d09d7f3770?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 5 Apr, *Verifiable by Design: Aligning Language Models to Quote from Pre-Training Data*, [https://arxiv.org/abs/2404.03862](https://substack.com/redirect/78a6ad55-4c02-470e-a1fa-94a32fa1cac9?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 5 Apr, *Sigma: Siamese Mamba Network for Multi-Modal Semantic Segmentation*, [https://arxiv.org/abs/2404.04256](https://substack.com/redirect/6716e58a-8ae7-4375-8034-746d87f35906?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 8 Apr, *AutoCodeRover: Autonomous Program Improvement*, [https://arxiv.org/abs/2404.05427](https://substack.com/redirect/6c5eaf40-07b9-459b-9d9f-460f05ae39a6?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 8 Apr, *Eagle and Finch: RWKV with Matrix-Valued States and Dynamic Recurrence*, [https://arxiv.org/abs/2404.05892](https://substack.com/redirect/961ef83e-bc7b-406d-8cc5-65d09baca8fe?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 8 Apr, *CodecLM: Aligning Language Models with Tailored Synthetic Data*, [https://arxiv.org/abs/2404.05875](https://substack.com/redirect/15058c05-b607-4c42-b9f3-e2085ad7a880?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 9 Apr, *MiniCPM: Unveiling the Potential of Small Language Models with Scalable Training Strategies*, [https://arxiv.org/abs/2404.06395](https://substack.com/redirect/ab6771aa-fc59-47f8-ba6e-70313a8e853a?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 9 Apr, *Elephants Never Forget: Memorization and Learning of Tabular Data in Large Language Models*, [https://arxiv.org/abs/2404.06209](https://substack.com/redirect/24f21c50-8eef-4da2-a5a0-b5474403ef9e?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 9 Apr, *LLM2Vec: Large Language Models Are Secretly Powerful Text Encoders*, [https://arxiv.org/abs/2404.05961](https://substack.com/redirect/82d98b9c-870b-4a38-b09d-0e01cf8de874?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 10 Apr, *Adapting LLaMA Decoder to Vision Transformer*, [https://arxiv.org/abs/2404.06773](https://substack.com/redirect/dea6e17b-7459-438c-98f7-cb449bcc2236?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 10 Apr, *Leave No Context Behind: Efficient Infinite Context Transformers with Infini-attention*, [https://arxiv.org/abs/2404.07143](https://substack.com/redirect/1e26e5f6-4ec1-41dc-bd68-660b99bec89b?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 11 Apr, *LLoCO: Learning Long Contexts Offline*, [https://arxiv.org/abs/2404.07979](https://substack.com/redirect/3eb559bb-67bf-4b82-9408-8a05504794d8?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 11 Apr, *JetMoE: Reaching Llama2 Performance with 0.1M Dollars*, [https://arxiv.org/abs/2404.07413](https://substack.com/redirect/e3e6ab16-dcf2-43cb-a765-54a34ee0db51?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 11 Apr, *Best Practices and Lessons Learned on Synthetic Data for Language Models*, [https://arxiv.org/abs/2404.07503](https://substack.com/redirect/632edc85-5cd6-44da-b4f5-98d184559eab?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 11 Apr, *Rho-1: Not All Tokens Are What You Need*, [https://arxiv.org/abs/2404.07965](https://substack.com/redirect/0bbc0f80-b388-4a1a-a410-177be37676fb?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 12 Apr, *Pre-training Small Base LMs with Fewer Tokens*, [https://arxiv.org/abs/2404.08634](https://substack.com/redirect/2c319b6e-2323-4041-a54b-5a02b67eb625?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 12 Apr, *Dataset Reset Policy Optimization for RLHF*, [https://arxiv.org/abs/2404.08495](https://substack.com/redirect/04c8e8b1-1332-4388-b9e3-a9356c1cc5b0?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 13 Apr, *LLM In-Context Recall is Prompt Dependent*, [https://arxiv.org/abs/2404.08865](https://substack.com/redirect/69e70412-c86e-4e2f-a70b-300e876ca039?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 15 Apr, *State Space Model for New-Generation Network Alternative to Transformers: A Survey*, [https://arxiv.org/abs/2404.09516](https://substack.com/redirect/1d2cdfb1-78ec-4193-9f85-a62eed7476b5?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 15 Apr, *Chinchilla Scaling: A Replication Attempt*, [https://arxiv.org/abs/2404.10102](https://substack.com/redirect/3992d9c8-d8cd-465c-a6a6-7fbde19dd0fd?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 15 Apr, *Learn Your Reference Model for Real Good Alignment*, [https://arxiv.org/abs/2404.09656](https://substack.com/redirect/662f3085-f8e2-4c6a-98f7-d6fcc2442f89?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 16 Apr, *Is DPO Superior to PPO for LLM Alignment? A Comprehensive Study*, [https://arxiv.org/abs/2404.10719](https://substack.com/redirect/602dc6c8-6650-4de6-a778-22f5706b0df6?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 16 Apr, *Scaling (Down) CLIP: A Comprehensive Analysis of Data, Architecture, and Training Strategies*, [https://arxiv.org/abs/2404.08197](https://substack.com/redirect/d1da519e-dcfc-4e4d-b28d-26bf125626e4?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 16 Apr, *How Faithful Are RAG Models? Quantifying the Tug-of-War Between RAG and LLMs' Internal Prior*, [https://arxiv.org/abs/2404.10198](https://substack.com/redirect/ed0924d9-5786-4774-ab6d-4ff7e388e0ab?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 17 Apr, *A Survey on Retrieval-Augmented Text Generation for Large Language Models*, [https://arxiv.org/abs/2404.10981](https://substack.com/redirect/9857b8f9-d1af-44e6-af6b-83d43bf17476?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 18 Apr, *When LLMs are Unfit Use FastFit: Fast and Effective Text Classification with Many Classes*, [https://arxiv.org/abs/2404.12365](https://substack.com/redirect/d3b56899-524d-4a86-b884-e1644b5ebb36?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 18 Apr, *Toward Self-Improvement of LLMs via Imagination, Searching, and Criticizing*, [https://arxiv.org/abs/2404.12253](https://substack.com/redirect/bd2dc2c2-6813-42c6-b8d9-038c2680ed7d?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 18 Apr, *OpenBezoar: Small, Cost-Effective and Open Models Trained on Mixes of Instruction Data*, [https://arxiv.org/abs/2404.12195](https://substack.com/redirect/2204e7d7-1170-42d7-aaa9-ca9cd694a940?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 19 Apr, *The Instruction Hierarchy: Training LLMs to Prioritize Privileged Instructions*, [https://arxiv.org/abs/2404.13208](https://substack.com/redirect/f48d6944-eb56-4db7-bc3a-e8612e066776?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 22 Apr, *How Good Are Low-bit Quantized LLaMA3 Models? An Empirical Study*, [https://arxiv.org/abs/2404.14047](https://substack.com/redirect/a4fbc1f5-38c2-469a-961d-991bc6bb12bb?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 22 Apr, *Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone*, [https://arxiv.org/abs/2404.14219](https://substack.com/redirect/3950d2ed-0a07-4041-ae24-a0ae41b951b2?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 22 Apr, *OpenELM: An Efficient Language Model Family with Open-source Training and Inference Framework*, [https://arxiv.org/abs/2404.14619](https://substack.com/redirect/8d85605d-dcef-4eea-92c1-1a176ded516b?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 22 Apr, *A Survey on Self-Evolution of Large Language Models*, [https://arxiv.org/abs/2404.14662](https://substack.com/redirect/b309b37a-4f29-483a-ac03-9baa441d2dc8?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 23 Apr, *Multi-Head Mixture-of-Experts*, [https://arxiv.org/abs/2404.15045](https://substack.com/redirect/c4fcb7a0-0368-4ed4-af5c-4cf18f32ca58?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 23 Apr, *NExT: Teaching Large Language Models to Reason about Code Execution*, [https://arxiv.org/abs/2404.14662](https://substack.com/redirect/b309b37a-4f29-483a-ac03-9baa441d2dc8?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 23 Apr, *Graph Machine Learning in the Era of Large Language Models (LLMs)*, [https://arxiv.org/abs/2404.14928](https://substack.com/redirect/bf83e10a-8a6a-4ddc-b4f7-ec0414c93d27?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 24 Apr, *Retrieval Head Mechanistically Explains Long-Context Factuality*, [https://arxiv.org/abs/2404.15574](https://substack.com/redirect/1d55f31c-d63c-4eeb-a742-4475874a71fa?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 25 Apr, *Layer Skip: Enabling Early Exit Inference and Self-Speculative Decoding*, [https://arxiv.org/abs/2404.16710](https://substack.com/redirect/88c327a5-c85d-40e3-9e0f-cfea3d2d45d7?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 25 Apr, *Make Your LLM Fully Utilize the Context*, [https://arxiv.org/abs/2404.16811](https://substack.com/redirect/8f97c6e7-6429-4845-b3f5-138bffffbe5a?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 28 Apr, *LoRA Land: 310 Fine-tuned LLMs that Rival GPT-4, A Technical Report*, [https://arxiv.org/abs/2405.00732](https://substack.com/redirect/a2e9594d-b621-47f2-935a-188c4573451a?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 30 Apr, *Better & Faster Large Language Models via Multi-token Prediction*, [https://arxiv.org/abs/2404.19737](https://substack.com/redirect/1840a07d-bb38-460e-a984-2ab218bbc418?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 30 Apr, *RAG and RAU: A Survey on Retrieval-Augmented Language Model in Natural Language Processing*, [https://arxiv.org/abs/2404.19543](https://substack.com/redirect/050292b6-0565-4320-9ac0-f724d8fe7c6a?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 30 Apr, *A Primer on the Inner Workings of Transformer-based Language Models*, [https://arxiv.org/abs/2405.00208](https://substack.com/redirect/ef974363-87cb-43cf-95d8-943e567bb180?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 30 Apr, *When to Retrieve: Teaching LLMs to Utilize Information Retrieval Effectively*, [https://arxiv.org/abs/2404.19705](https://substack.com/redirect/17c0916d-1b71-4e46-9ac9-4442f68772b7?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 30 Apr, *KAN: Kolmogorov–Arnold Networks*, [https://arxiv.org/abs/2404.19756](https://substack.com/redirect/f372cf13-200c-47b8-a7dc-57f70ca72476?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- ## **May 2024**\n- 1 May, *Is Bigger Edit Batch Size Always Better? An Empirical Study on Model Editing with Llama-3*, [https://arxiv.org/abs/2405.00664](https://substack.com/redirect/1c5de279-35a8-4f42-ae89-5d18d41ef69d?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 1 May, *Self-Play Preference Optimization for Language Model Alignment*, [https://arxiv.org/abs/2405.00675](https://substack.com/redirect/f8577768-a966-4d6b-a3a1-8b2b168d3fd3?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 1 May, *A Careful Examination of Large Language Model Performance on Grade School Arithmetic*, [https://arxiv.org/abs/2405.00332](https://substack.com/redirect/58cbad1d-c504-46eb-b3f3-ff3bd45d9468?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 2 May, *Prometheus 2: An Open Source Language Model Specialized in Evaluating Other Language Models*, [https://arxiv.org/abs/2405.01535](https://substack.com/redirect/fea83775-d04b-4224-850c-dd9465d57099?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 3 May, *What Matters When Building Vision-Language Models?*, [https://arxiv.org/abs/2405.02246](https://substack.com/redirect/6dbee6af-0ab3-4b33-8a56-5fef31c50b5c?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 5 May, *Is Flash Attention Stable?*, [https://arxiv.org/abs/2405.02803](https://substack.com/redirect/ead40040-52c8-4b1d-9f9f-f55bda495cf4?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 7 May, *vAttention: Dynamic Memory Management for Serving LLMs without PagedAttention*, [https://arxiv.org/abs/2405.04437](https://substack.com/redirect/4a6c6cc8-a525-4e24-977e-db9515a0686b?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 7 May, *xLSTM: Extended Long Short-Term Memory*, [https://arxiv.org/abs/2405.04517](https://substack.com/redirect/028cbcd0-7cd9-427b-85ba-321e33f19b1e?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 8 May, *You Only Cache Once: Decoder-Decoder Architectures for Language Models*, [https://arxiv.org/abs/2405.05254](https://substack.com/redirect/80f79e0f-71a8-4531-8bf1-afba8c4594c3?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 8 May, *DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model*, [https://arxiv.org/abs/2405.04434](https://substack.com/redirect/b2acaaf5-d3fa-46f4-b0ac-77353678c3a1?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 8 May, *Fishing for Magikarp: Automatically Detecting Under-trained Tokens in Large Language Models*, [https://arxiv.org/abs/2405.05417](https://substack.com/redirect/d861a78d-8d48-4c52-a20c-ab5e803f684e?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 9 May, *Does Fine-Tuning LLMs on New Knowledge Encourage Hallucinations?*, [https://arxiv.org/abs/2405.05904](https://substack.com/redirect/5ca7614d-1c58-4870-ade7-56b8241f7192?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 10 May, *Value Augmented Sampling for Language Model Alignment and Personalization*, [https://arxiv.org/abs/2405.06639](https://substack.com/redirect/5cbcdea6-ee7c-4d35-b9fa-2031976c6936?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 12 May, *PHUDGE: Phi-3 as Scalable Judge*, [https://arxiv.org/abs/2405.08029](https://substack.com/redirect/9da1f2c7-1d1f-434d-9010-2fa15055e31e?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 13 May, *RLHF Workflow: From Reward Modeling to Online RLHF*, [https://arxiv.org/abs/2405.07863](https://substack.com/redirect/dc2f9eae-d382-455e-9d28-d65f9fa42e0f?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 15 May, *LoRA Learns Less and Forgets Less*, [https://arxiv.org/abs/2405.09673](https://substack.com/redirect/cbe29308-4d8e-42cc-9294-1893fa9cf09c?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 15 May, *Xmodel-VLM: A Simple Baseline for Multimodal Vision Language Model*, [https://arxiv.org/abs/2405.09215](https://substack.com/redirect/1f38e9ea-099c-406d-ad19-9e2c00c1ef66?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 16 May, *Chameleon: Mixed-Modal Early-Fusion Foundation Models*, [https://arxiv.org/abs/2405.09818](https://substack.com/redirect/0f9f0c6a-7427-41c5-adb5-d5f72cbfb07f?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 17 May, *Towards Modular LLMs by Building and Reusing a Library of LoRAs*, [https://arxiv.org/abs/2405.11157](https://substack.com/redirect/c63a6f65-2a90-4386-a617-5a5946dfea3d?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 19 May, *SLAB: Efficient Transformers with Simplified Linear Attention and Progressive Re-parameterized Batch Normalization*, [https://arxiv.org/abs/2405.11582](https://substack.com/redirect/833e05af-1fb7-42ed-b9e6-9ee3ffa588c4?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 20 May, *MoRA: High-Rank Updating for Parameter-Efficient Fine-Tuning*, [https://arxiv.org/abs/2405.12130](https://substack.com/redirect/1167ff9d-ee5f-4487-b0f7-412f2f6916ee?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 22 May, *Attention as an RNN*, [https://arxiv.org/abs/2405.13956](https://substack.com/redirect/d1b2f765-f51a-4422-ade5-5abd88545ce0?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 22 May, *Dense Connector for MLLMs*, [https://arxiv.org/abs/2405.13800](https://substack.com/redirect/afb265bf-3f9d-43fa-a5f6-adb1ab5413a1?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 23 May, *AlignGPT: Multi-modal Large Language Models with Adaptive Alignment Capability*, [https://arxiv.org/abs/2405.14129](https://substack.com/redirect/3837fa51-610c-4656-84fa-d90c02e6e4b1?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 23 May, *SimPO: Simple Preference Optimization with a Reference-Free Reward*, [https://arxiv.org/abs/2405.14734](https://substack.com/redirect/3957d273-659b-4c61-918c-27d4c1f272e1?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 23 May, *Instruction Tuning With Loss Over Instructions*, [https://arxiv.org/abs/2405.14394](https://substack.com/redirect/a066d705-8698-4454-9ec2-e3eb7abc43d0?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 24 May, *The Road Less Scheduled*, [https://arxiv.org/abs/2405.15682](https://substack.com/redirect/b6ae4f49-86a5-41d1-aeef-41a5c4443963?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 26 May, *Stacking Your Transformers: A Closer Look at Model Growth for Efficient LLM Pre-Training*, [https://arxiv.org/abs/2405.15319](https://substack.com/redirect/5761bf59-1d9a-4e66-94ed-e02ae7dca17c?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 26 May, *gzip Predicts Data-dependent Scaling Laws*, [https://arxiv.org/abs/2405.16684](https://substack.com/redirect/8606bdd7-cfbc-47fe-8cae-728c793f2af9?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 27 May, *Trans-LoRA: Towards Data-free Transferable Parameter Efficient Finetuning*, [https://arxiv.org/abs/2405.17258](https://substack.com/redirect/a3905700-d0b8-4252-8e48-71be2b05c137?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 28 May, *VeLoRA: Memory Efficient Training using Rank-1 Sub-Token Projections*, [https://arxiv.org/abs/2405.17991](https://substack.com/redirect/868af1f1-4c40-4c59-a4e1-671e8e6fdcfb?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 28 May, *LLaMA-NAS: Efficient Neural Architecture Search for Large Language Models*, [https://arxiv.org/abs/2405.18377](https://substack.com/redirect/28a44073-bd2a-4967-9889-dc801e611b0c?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 29 May, *Contextual Position Encoding: Learning to Count What's Important*, [https://arxiv.org/abs/2405.18719](https://substack.com/redirect/b7207da1-165c-4a8d-8bba-c44a1e747187?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- ## **June 2024**\n- 2 Jun, *Show, Don't Tell: Aligning Language Models with Demonstrated Feedback*, [https://arxiv.org/abs/2406.00888](https://substack.com/redirect/e9f5bcc2-a5fd-4771-9618-f91606c3857f?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 3 Jun, *Skywork-MoE: A Deep Dive into Training Techniques for Mixture-of-Experts Language Models*, [https://arxiv.org/abs/2406.06563](https://substack.com/redirect/6eb782e0-7345-4271-8104-472d1783d2e6?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 3 Jun, *OLoRA: Orthonormal Low-Rank Adaptation of Large Language Models*, [https://arxiv.org/abs/2406.01775](https://substack.com/redirect/587a7280-4b61-445b-a72c-f1b5d038100b?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 3 Jun, *The Geometry of Categorical and Hierarchical Concepts in Large Language Models*, [https://arxiv.org/abs/2406.01506](https://substack.com/redirect/a986036f-ca86-4a4a-aeef-53fc6ff3da77?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 3 Jun, *Towards Scalable Automated Alignment of LLMs: A Survey*, [https://arxiv.org/abs/2406.01252](https://substack.com/redirect/f2faba08-70cd-4535-b783-f43588e8c323?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 4 Jun, *Scalable MatMul-free Language Modeling*, [https://arxiv.org/abs/2406.02528](https://substack.com/redirect/0499aa92-2e24-4e96-b27c-2839185a4d8f?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 4 Jun, *Block Transformer: Global-to-Local Language Modeling for Fast Inference*, [https://arxiv.org/abs/2406.02657](https://substack.com/redirect/4d3ee96b-7ee4-43ca-b682-f78789a6aaa7?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 6 Jun, *Buffer of Thoughts: Thought-Augmented Reasoning with Large Language Models*, [https://arxiv.org/abs/2406.04271](https://substack.com/redirect/40a8c64c-ecb5-43db-80de-16d3b732ba90?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 6 Jun, *The Prompt Report: A Systematic Survey of Prompting Techniques*, [https://arxiv.org/abs/2406.06608](https://substack.com/redirect/5e07252a-2fe2-4565-a82f-115327caaaf2?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 6 Jun, *Transformers Need Glasses! Information Over-Squashing in Language Tasks*, [https://arxiv.org/abs/2406.04267](https://substack.com/redirect/62598196-fbef-43fa-b559-f4cbc332928c?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 6 Jun, *Are We Done with MMLU?*, [https://arxiv.org/abs/2406.04127](https://substack.com/redirect/0042574d-86f1-4ee0-8712-b939ad3db01d?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 6 Jun, *Step-aware Preference Optimization: Aligning Preference with Denoising Performance at Each Step*, [https://arxiv.org/abs/2406.04314](https://substack.com/redirect/b1ec70b6-1852-499e-922d-d0fb53a2ed15?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 7 Jun, *Boosting Large-scale Parallel Training Efficiency with C4: A Communication-Driven Approach*, [https://arxiv.org/abs/2406.04594](https://substack.com/redirect/8d0dc753-c379-41c9-b51d-abc3b3e526e5?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 7 Jun, *CRAG -- Comprehensive RAG Benchmark*, [https://arxiv.org/abs/2406.04744](https://substack.com/redirect/fc03c3a8-4b4d-4dcb-b829-ec049a22ced1?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 7 Jun, *WildBench: Benchmarking LLMs with Challenging Tasks from Real Users in the Wild*, [https://arxiv.org/abs/2406.04770](https://substack.com/redirect/8e89c55a-39b8-46bf-adcf-c40931cbadb3?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 7 Jun, *Mixture-of-Agents Enhances Large Language Model Capabilities*, [https://arxiv.org/abs/2406.04692](https://substack.com/redirect/682798ac-335a-40dd-ad58-0054e98b32a0?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 7 Jun, *BERTs are Generative In-Context Learners*, [https://arxiv.org/abs/2406.04823](https://substack.com/redirect/eb7c305d-02dd-4205-b0ff-d1b8cf1d866f?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 7 Jun, *3D-GRAND: A Million-Scale Dataset for 3D-LLMs with Better Grounding and Less Hallucination*, [https://arxiv.org/abs/2406.05132](https://substack.com/redirect/f5a8d752-69e6-4ff8-9e5e-643c804cd8e7?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 8 Jun, *Creativity Has Left the Chat: The Price of Debiasing Language Models*, [https://arxiv.org/abs/2406.05587](https://substack.com/redirect/77146882-2b7a-4f43-9fbd-b98a46e4387b?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 10 Jun, *Autoregressive Model Beats Diffusion: Llama for Scalable Image Generation*, [https://arxiv.org/abs/2406.06525](https://substack.com/redirect/43936148-1bb4-48cf-a50e-21085699a285?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 10 Jun, *Margin-aware Preference Optimization for Aligning Diffusion Models Without Reference*, [https://arxiv.org/abs/2406.06424](https://substack.com/redirect/4256d872-ae41-44c5-84a1-0c3f2b9bdba2?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 10 Jun, *Husky: A Unified, Open-Source Language Agent for Multi-Step Reasoning*, [https://arxiv.org/abs/2406.06469](https://substack.com/redirect/cfd33125-8a3a-404f-a40b-8e4ef2ba9338?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 10 Jun, *Turbo Sparse: Achieving LLM SOTA Performance with Minimal Activated Parameters*, [https://arxiv.org/abs/2406.05955](https://substack.com/redirect/65737152-6e65-4f71-bed9-ef684ede240c?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 10 Jun, *Self-Tuning: Instructing LLMs to Effectively Acquire New Knowledge through Self-Teaching*, [https://arxiv.org/abs/2406.06326](https://substack.com/redirect/dc5634d1-1a37-43ea-b6bf-21f7b62aa425?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 11 Jun, *An Image is Worth 32 Tokens for Reconstruction and Generation*, [https://arxiv.org/abs/2406.07550](https://substack.com/redirect/e3c753f9-cc8d-4251-9ea1-921d8c3fbcd9?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 11 Jun, *TextGrad: Automatic \"Differentiation\" via Text*, [https://arxiv.org/abs/2406.07496](https://substack.com/redirect/2304b8a4-d698-4364-8c2a-620353292eb6?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 11 Jun, *Simple and Effective Masked Diffusion Language Models*, [https://arxiv.org/abs/2406.07524](https://substack.com/redirect/c10e95b8-37e3-42ad-b72c-413abb1f581b?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 11 Jun, *Never Miss A Beat: An Efficient Recipe for Context Window Extension of Large Language Models with Consistent \"Middle\" Enhancement*, [https://arxiv.org/abs/2406.07138](https://substack.com/redirect/eb16a1d4-ef25-4125-b2ce-c572ea08d759?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 11 Jun, *Samba: Simple Hybrid State Space Models for Efficient Unlimited Context Language Modeling*, [https://arxiv.org/abs/2406.07522](https://substack.com/redirect/cde9c6ce-2768-4a4f-903a-213c443cedf3?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 12 Jun, *Magpie: Alignment Data Synthesis from Scratch by Prompting Aligned LLMs with Nothing*, [https://arxiv.org/abs/2406.08464](https://substack.com/redirect/22a31068-f2d0-404f-8eb5-dbe1f0d1081d?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 12 Jun, *What If We Recaption Billions of Web Images with LLaMA-3?*, [https://arxiv.org/abs/2406.08478](https://substack.com/redirect/dd33481a-7c47-4376-9874-1fa155a1b8c1?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 12 Jun, *Large Language Model Unlearning via Embedding-Corrupted Prompts*, [https://arxiv.org/abs/2406.07933](https://substack.com/redirect/f57585f0-8c30-4c6c-87e9-02ff2b234454?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 12 Jun, *Large Language Models Must Be Taught to Know What They Don't Know*, [https://arxiv.org/abs/2406.08391](https://substack.com/redirect/2473dd81-4f73-45df-82eb-bfb323d84aa3?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 12 Jun, *An Empirical Study of Mamba-based Language Models*, [https://arxiv.org/abs/2406.07887](https://substack.com/redirect/22fff2cf-aebf-4356-a180-3d4764a00317?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 12 Jun, *Discovering Preference Optimization Algorithms with and for Large Language Models*, [https://arxiv.org/abs/2406.08414](https://substack.com/redirect/b0ed64fa-29e1-4111-a03b-ab31b8e67310?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 13 Jun, *Transformers Meet Neural Algorithmic Reasoners*, [https://arxiv.org/abs/2406.09308](https://substack.com/redirect/46173fda-7004-43f2-a255-83ad4807fd7a?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 13 Jun, *MLKV: Multi-Layer Key-Value Heads for Memory Efficient Transformer Decoding*, [https://arxiv.org/abs/2406.09297](https://substack.com/redirect/195d8e93-36d5-4037-bb1c-91242858c938?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 13 Jun, *An Image is Worth More Than 16x16 Patches: Exploring Transformers on Individual Pixels*, [https://arxiv.org/abs/2406.09415](https://substack.com/redirect/597c0730-90e8-4d01-81b7-91c261f76dd3?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 13 Jun, *FouRA: Fourier Low Rank Adaptation*, [https://arxiv.org/abs/2406.08798](https://substack.com/redirect/5fc9066f-6759-41b0-b64a-480a861f9823?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 14 Jun, *Bootstrapping Language Models with DPO Implicit Rewards*, [https://arxiv.org/abs/2406.09760](https://substack.com/redirect/a7c39b55-53d5-4ac0-ab2e-29f2d4fe3a62?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 14 Jun, *Be like a Goldfish, Don't Memorize! Mitigating Memorization in Generative LLMs*, [https://arxiv.org/abs/2406.10209](https://substack.com/redirect/8e2bf065-883d-459e-a849-538dd1b8a907?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 14 Jun, *Regularizing Hidden States Enables Learning Generalizable Reward Model for LLMs*, [https://arxiv.org/abs/2406.10216](https://substack.com/redirect/ff0b2d72-a41e-4ee5-afec-5082257daf69?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 16 Jun, *THEANINE: Revisiting Memory Management in Long-term Conversations with Timeline-augmented Response Generation*, [https://arxiv.org/abs/2406.10996](https://substack.com/redirect/2a7babdc-dcbd-418e-ab51-ac241ebf6657?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 17 Jun, *Task Me Anything*, [https://arxiv.org/abs/2406.11775](https://substack.com/redirect/0c190f0d-4136-4fdb-9f87-e8c9e443c211?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 17 Jun, *How Do Large Language Models Acquire Factual Knowledge During Pretraining?*, [https://arxiv.org/abs/2406.11813](https://substack.com/redirect/a8f8ae1d-cc22-4d15-a3fc-f50268a991d6?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 17 Jun, *mDPO: Conditional Preference Optimization for Multimodal Large Language Models*, [https://arxiv.org/abs/2406.11839](https://substack.com/redirect/05cf60c0-3c7d-4e70-a44c-ad39204b5008?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 17 Jun, *Nemotron-4 340B Technical Report*, [https://arxiv.org/abs/2406.11704](https://substack.com/redirect/e89cb194-fbf4-40ea-be72-6888d98c7f20?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 17 Jun, *DataComp-LM: In Search of the Next Generation of Training Sets for Language Models*, [https://arxiv.org/abs/2406.11794](https://substack.com/redirect/a58c6304-6cec-4ec5-a4ed-30da7aa4e865?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 17 Jun, *Tokenization Falling Short: The Curse of Tokenization*, [https://arxiv.org/abs/2406.11687](https://substack.com/redirect/fd01315f-bd2c-4299-8b77-a29e60d6abd4?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 17 Jun, *DeepSeek-Coder-V2: Breaking the Barrier of Closed-Source Models in Code Intelligence*, [https://arxiv.org/abs/2406.11931](https://substack.com/redirect/eb88705f-574a-40b7-b57f-6c754679c061?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 17 Jun, *Unveiling Encoder-Free Vision-Language Models*, [https://arxiv.org/abs/2406.11832](https://substack.com/redirect/22bdc3b8-c13d-4991-b9bb-fa081fdb82d3?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 17 Jun, *Iterative Length-Regularized Direct Preference Optimization: A Case Study on Improving 7B Language Models to GPT-4 Level*, [https://arxiv.org/abs/2406.11817](https://substack.com/redirect/afe584f5-7792-4d5c-8abc-aa1aa6a8ffa9?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 17 Jun, *HARE: HumAn pRiors, a key to small language model Efficiency*, [https://arxiv.org/abs/2406.11410](https://substack.com/redirect/2540534a-369f-4c11-909b-e58c148fdfab?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 17 Jun, *Measuring memorization in RLHF for code completion*, [https://arxiv.org/abs/2406.11715](https://substack.com/redirect/61c33d89-6e3f-4f21-8b05-f03be439a6dd?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 17 Jun, *Self-MoE: Towards Compositional Large Language Models with Self-Specialized Experts*, [https://arxiv.org/abs/2406.12034](https://substack.com/redirect/20acf033-f09f-4e59-a5d8-0f52166a59e4?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 18 Jun, *From RAGs to Rich Parameters: Probing How Language Models Utilize External Knowledge Over Parametric Information for Factual Queries*, [https://arxiv.org/abs/2406.12824](https://substack.com/redirect/8aec63af-ddd6-4bf8-a4e8-bfc481202df9?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 18 Jun, *Judging the Judges: Evaluating Alignment and Vulnerabilities in LLMs-as-Judges*, [https://arxiv.org/abs/2406.12624](https://substack.com/redirect/54c74154-6118-4b6f-b194-0f1052cc159e?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 19 Jun, *Can Long-Context Language Models Subsume Retrieval, RAG, SQL, and More?*, [https://arxiv.org/abs/2406.13121](https://substack.com/redirect/f8b25a5d-719b-4ac8-b716-23cd6edd5e2d?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 20 Jun, *Instruction Pre-Training: Language Models are Supervised Multitask Learners*, [https://arxiv.org/abs/2406.14491](https://substack.com/redirect/39831f46-9107-4a8c-b62d-eeaf56c6cb63?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 20 Jun, *Can LLMs Learn by Teaching? A Preliminary Study*, [https://arxiv.org/abs/2406.14629](https://substack.com/redirect/e73e0790-96ea-49fa-a6af-68b85aadefc2?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 21 Jun, *A Tale of Trust and Accuracy: Base vs. Instruct LLMs in RAG Systems*, [https://arxiv.org/abs/2406.14972](https://substack.com/redirect/33664c47-aa2e-49af-8725-cf72cd11e13f?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 21 Jun, *LongRAG: Enhancing Retrieval-Augmented Generation with Long-context LLMs*, [https://arxiv.org/abs/2406.15319](https://substack.com/redirect/bff23d51-7c62-4929-aa35-a980f4fbad82?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 21 Jun, *MoA: Mixture of Sparse Attention for Automatic Large Language Model Compression*, [https://arxiv.org/abs/2406.14909](https://substack.com/redirect/35768c1c-7852-4be9-abd9-f86db1a54fff?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 21 Jun, *Efficient Continual Pre-training by Mitigating the Stability Gap*, [https://arxiv.org/abs/2406.14833](https://substack.com/redirect/539aa684-092f-4114-a1c0-ae72add460a6?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 24 Jun, *Sparser is Faster and Less is More: Efficient Sparse Attention for Long-Range Transformers*, [https://arxiv.org/abs/2406.16747](https://substack.com/redirect/65c023bd-1d3d-4821-a8bc-24133440cd71?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 24 Jun, *WARP: On the Benefits of Weight Averaged Rewarded Policies*, [https://arxiv.org/abs/2406.16768](https://substack.com/redirect/f513a5fd-2f30-469a-8ba0-4a616546a57d?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 24 Jun, *Adam-mini: Use Fewer Learning Rates To Gain More*, [https://arxiv.org/abs/2406.16793](https://substack.com/redirect/054e02fa-36d1-4433-86e1-7b2c153c16d2?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 25 Jun, *The FineWeb Datasets: Decanting the Web for the Finest Text Data at Scale*, [https://arxiv.org/abs/2406.17557](https://substack.com/redirect/9c9cc982-643e-4614-b6c1-88bf9f542f2a?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 25 Jun, *LongIns: A Challenging Long-context Instruction-based Exam for LLMs*, [https://arxiv.org/abs/2406.17588](https://substack.com/redirect/ae2700c1-4869-4834-8608-7cd5c4580f3f?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 25 Jun, *Following Length Constraints in Instructions*, [https://arxiv.org/abs/2406.17744](https://substack.com/redirect/fec907c2-3c9b-4341-a88d-55c37640722c?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 26 Jun, *A Closer Look into Mixture-of-Experts in Large Language Models*, [https://arxiv.org/abs/2406.18219](https://substack.com/redirect/90be2123-599b-41c8-bbdd-bc1955b73a51?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 26 Jun, *RouteLLM: Learning to Route LLMs with Preference Data*, [https://arxiv.org/abs/2406.18665](https://substack.com/redirect/908747d7-e5f3-4864-951a-5a27f6e4ac48?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 26 Jun, *Step-DPO: Step-wise Preference Optimization for Long-chain Reasoning of LLMs*, [https://arxiv.org/abs/2406.18629](https://substack.com/redirect/7a473686-2b3c-4ea0-aa74-c914eb9652bb?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 27 Jun, *Dataset Size Recovery from LoRA Weights*, [https://arxiv.org/abs/2406.19395](https://substack.com/redirect/dbc97b8c-c395-4eeb-9331-cbe054ed9c8e?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 27 Jun, *From Artificial Needles to Real Haystacks: Improving Retrieval Capabilities in LLMs by Finetuning on Synthetic Data*, [https://arxiv.org/abs/2406.19292](https://substack.com/redirect/67e9900c-0729-4da3-90b1-b6240bd7f21a?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 27 Jun, *Changing Answer Order Can Decrease MMLU Accuracy*, [https://arxiv.org/abs/2406.19470](https://substack.com/redirect/53f73e9b-bac1-457a-9616-6ac445adb589?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 28 Jun, *Direct Preference Knowledge Distillation for Large Language Models*, [https://arxiv.org/abs/2406.19774](https://substack.com/redirect/bcb3f081-90e0-4458-8211-1f0cd1805ff8?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 28 Jun, *LLM Critics Help Catch LLM Bugs*, [https://arxiv.org/abs/2407.00215](https://substack.com/redirect/f65f67fe-7998-49c4-9262-68c7a1bd6d3a?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 28 Jun, *Scaling Synthetic Data Creation with 1,000,000,000 Personas*, [https://arxiv.org/abs/2406.20094](https://substack.com/redirect/f54eb3e8-fbba-4d4c-a0a7-009b0a7cd7c6?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- ## **Jul 2024**\n- 1 Jul, *LLM See, LLM Do: Guiding Data Generation to Target Non-Differentiable Objectives*, [https://arxiv.org/abs/2407.01490](https://substack.com/redirect/2fd6a88b-7382-4731-a298-421c755e3870?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 1 Jul, *Searching for Best Practices in Retrieval-Augmented Generation*, [https://arxiv.org/abs/2407.01219](https://substack.com/redirect/080de8ff-79c7-48f6-89ce-2bc502b10a91?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 1 Jul, *Let the Expert Stick to His Last: Expert-Specialized Fine-Tuning for Sparse Architectural Large Language Models*, [https://arxiv.org/abs/2407.01906](https://substack.com/redirect/dcb9cacb-9cb9-47e2-a3c5-2f0b5445bc9d?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 1 Jul, *Diffusion Forcing: Next-token Prediction Meets Full-Sequence Diffusion*, [https://arxiv.org/abs/2407.01392](https://substack.com/redirect/3cfab629-a36a-4ce1-b1ff-c6cb8b514efc?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 1 Jul, *Eliminating Position Bias of Language Models: A Mechanistic Approach*, [https://arxiv.org/abs/2407.01100](https://substack.com/redirect/836df518-f78b-4f25-9f84-0ea349f49a9e?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 2 Jul, *JMInference 1.0: Accelerating Pre-filling for Long-Context LLMs via Dynamic Sparse Attention*, [https://arxiv.org/abs/2407.02490](https://substack.com/redirect/7e7f09c9-8930-4a52-8b88-e07da23475d4?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 2 Jul, *TokenPacker: Efficient Visual Projector for Multimodal LLM*, [https://arxiv.org/abs/2407.02392](https://substack.com/redirect/08a522a0-43cd-4f1a-828e-c6ccb3241810?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 2 Jul, *Reasoning in Large Language Models: A Geometric Perspective*, [https://arxiv.org/abs/2407.02678](https://substack.com/redirect/ef28f920-895f-4a85-af62-7e8ee6b0b5d6?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 2 Jul, *RankRAG: Unifying Context Ranking with Retrieval-Augmented Generation in LLMs*, [https://arxiv.org/abs/2407.02485](https://substack.com/redirect/68de2d14-60b3-438b-b250-338e4c6e4ffc?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 3 Jul, *AgentInstruct: Toward Generative Teaching with Agentic Flows*, [https://arxiv.org/abs/2407.03502](https://substack.com/redirect/147a9e57-bb91-4b75-81fa-f77c71156192?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 3 Jul, *HEMM: Holistic Evaluation of Multimodal Foundation Models*, [https://arxiv.org/abs/2407.03418](https://substack.com/redirect/0d2ec668-fb31-47bc-8a5e-c85f9ab60942?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 4 Jul, *Mixture of A Million Experts*, [https://arxiv.org/abs/2407.04153](https://substack.com/redirect/484eeb9f-e814-4eaf-9580-00e25f1d107b?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 5 Jul, *Learning to (Learn at Test Time): RNNs with Expressive Hidden States*, [https://arxiv.org/abs/2407.04620](https://substack.com/redirect/dfe98848-f037-4712-a04b-bf92177a984b?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 9 Jul, *Vision Language Models Are Blind*, [https://arxiv.org/abs/2407.06581](https://substack.com/redirect/71adece7-a665-4762-a0d2-a50f978d8b00?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 9 Jul, *Self-Recognition in Language Models*, [https://arxiv.org/abs/2407.06946](https://substack.com/redirect/5a96e7af-b32c-4f83-ba7d-67d598684b6e?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 10 Jul, *Inference Performance Optimization for Large Language Models on CPUs*, [https://arxiv.org/abs/2407.07304](https://substack.com/redirect/71e30c17-fc00-4d9a-b647-a26aed446ae4?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 11 Jul, *Gradient Boosting Reinforcement Learning*, [https://arxiv.org/abs/2407.08250](https://substack.com/redirect/6df1b840-869b-4b04-a938-be80024f5c13?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 11 Jul, *FlashAttention-3: Fast and Accurate Attention with Asynchrony and Low-precision*, [https://arxiv.org/abs/2407.08608](https://substack.com/redirect/e1822754-60e9-4ba4-9f70-9243d0d1cb99?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 12 Jul, *SpreadsheetLLM: Encoding Spreadsheets for Large Language Models*, [https://arxiv.org/abs/2407.09025](https://substack.com/redirect/d1835a20-e58e-4a7d-b2ec-d9b06a308ffa?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 12 Jul, *New Desiderata for Direct Preference Optimization*, [https://arxiv.org/abs/2407.09072](https://substack.com/redirect/0b92a067-0d0c-47a1-a535-5a3616a2ecc4?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 12 Jul, *Context Embeddings for Efficient Answer Generation in RAG*, [https://arxiv.org/abs/2407.09252](https://substack.com/redirect/46067b2e-5d0d-4b8f-a605-405c4c4334e2?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 15 Jul, *Qwen2 Technical Report*, [https://arxiv.org/abs/2407.10671](https://substack.com/redirect/eacd3eda-9be7-4b1d-8d45-400c9d4fb36f?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 15 Jul, *The Good, The Bad, and The Greedy: Evaluation of LLMs Should Not Ignore Non-Determinism*, [https://arxiv.org/abs/2407.10457](https://substack.com/redirect/0ca630c7-5615-4670-b9be-bc363f5004b9?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 15 Jul, *From GaLore to WeLore: How Low-Rank Weights Non-uniformly Emerge from Low-Rank Gradients*, [https://arxiv.org/abs/2407.11239](https://substack.com/redirect/2cca313f-dc95-4fbc-ba3f-d640835eb533?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 16 Jul, *GoldFinch: High Performance RWKV/Transformer Hybrid with Linear Pre-Fill and Extreme KV-Cache Compression*, [https://arxiv.org/abs/2407.12077](https://substack.com/redirect/3887df3e-2996-4565-82c6-c8cf7fc6269d?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 16 Jul, *Scaling Diffusion Transformers to 16 Billion Parameters*, [https://arxiv.org/abs/2407.11633](https://substack.com/redirect/3188c3fc-4325-458d-bfe9-cab7e1c32fd0?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 16 Jul, *NeedleBench: Can LLMs Do Retrieval and Reasoning in 1 Million Context Window?*, [https://arxiv.org/abs/2407.11963](https://substack.com/redirect/67ac222f-8592-42d1-b3da-b73c9842672e?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 17 Jul, *Patch-Level Training for Large Language Models*, [https://arxiv.org/abs/2407.12665](https://substack.com/redirect/268f8f5c-dca6-442f-ab46-384a0f71b0cd?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 17 Jul, *LMMs-Eval: Reality Check on the Evaluation of Large Multimodal Models*, [https://arxiv.org/abs/2407.12772](https://substack.com/redirect/8752b807-c023-4812-98cb-d2ace7616cf5?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 17 Jul, *A Survey of Prompt Engineering Methods in Large Language Models for Different NLP Tasks*, [https://arxiv.org/abs/2407.12994](https://substack.com/redirect/d33d2168-d276-43d8-acf7-8de1ca542646?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 17 Jul, *Spectra: A Comprehensive Study of Ternary, Quantized, and FP16 Language Models*, [https://arxiv.org/abs/2407.12327](https://substack.com/redirect/b19629e8-34af-4ff0-aca5-99a20cf98369?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 18 Jul, *Attention Overflow: Language Model Input Blur during Long-Context Missing Items Recommendation*, [https://arxiv.org/abs/2407.13481](https://substack.com/redirect/e118401d-8cd6-4433-9e41-6ddf11456838?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 18 Jul, *Weak-to-Strong Reasoning*, [https://arxiv.org/abs/2407.13647](https://substack.com/redirect/de908ec6-7ea1-4131-9e64-b2938c2ab984?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 18 Jul, *Understanding Reference Policies in Direct Preference Optimization*, [https://arxiv.org/abs/2407.13709](https://substack.com/redirect/0abb42d9-a35c-42f8-a032-b2784e2997f6?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 18 Jul, *Scaling Laws with Vocabulary: Larger Models Deserve Larger Vocabularies*, [https://arxiv.org/abs/2407.13623](https://substack.com/redirect/dbde33b1-b9ea-4cd9-89a5-b195fa2916b2?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 19 Jul, *BOND: Aligning LLMs with Best-of-N Distillation*, [https://arxiv.org/abs/2407.14622](https://substack.com/redirect/81f45ec1-ec9c-4a52-8f62-1e40fa02cc5e?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 19 Jul, *Compact Language Models via Pruning and Knowledge Distillation*, [https://arxiv.org/abs/2407.14679](https://substack.com/redirect/f21dd1a7-0f5c-4936-9f9e-2c00b5b2c765?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 19 Jul, *LazyLLM: Dynamic Token Pruning for Efficient Long Context LLM Inference*, [https://arxiv.org/abs/2407.14057](https://substack.com/redirect/84a1612c-c70e-4d7d-9656-e5d06dacae70?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 22 Jul, *Mini-Sequence Transformer: Optimizing Intermediate Memory for Long Sequences Training*, [https://arxiv.org/abs/2407.15892](https://substack.com/redirect/8e7a64b0-0cda-4ee2-87ed-a12d225d57ea?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 22 Jul, *DDK: Distilling Domain Knowledge for Efficient Large Language Models*, [https://arxiv.org/abs/2407.16154](https://substack.com/redirect/f5272a99-d15d-4a9c-9be3-8d404a8dfcf6?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 23 Jul, *Generation Constraint Scaling Can Mitigate Hallucination*, [https://arxiv.org/abs/2407.16908](https://substack.com/redirect/97607b59-cb58-4a87-895b-6d8ae55a994f?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 23 Jul, *Retrieval Augmented Generation or Long-Context LLMs? A Comprehensive Study and Hybrid Approach*, [https://arxiv.org/abs/2407.16833](https://substack.com/redirect/f03df3c9-2186-49b6-94c9-b303931dd2eb?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 23 Jul, *Course-Correction: Safety Alignment Using Synthetic Preferences*, [https://arxiv.org/abs/2407.16637](https://substack.com/redirect/6a54128e-eee2-44a4-ab63-7cf3f2e57925?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 26 Jul, *Data Mixture Inference: What do BPE Tokenizers Reveal about their Training Data?*, [https://arxiv.org/abs/2407.16607](https://substack.com/redirect/edeceea6-d9f4-40b2-a14c-e407521d512a?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 28 Jul, *Meta-Rewarding Language Models: Self-Improving Alignment with LLM-as-a-Meta-Judge*, [https://arxiv.org/abs/2407.19594](https://substack.com/redirect/4b22ca22-ecde-4486-9bae-8b24411f2111?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 29 Jul, *Improving Retrieval Augmented Language Model with Self-Reasoning*, [https://arxiv.org/abs/2407.19813](https://substack.com/redirect/d2524387-2d9c-49a2-8d7c-5d02e2b18ab4?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 29 Jul, *Apple Intelligence Foundation Language Models*, [https://arxiv.org/abs/2407.21075](https://substack.com/redirect/034daf6f-a410-4183-985b-60f649fd9784?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 30 Jul, *ThinK: Thinner Key Cache by Query-Driven Pruning*, [https://arxiv.org/abs/2407.21018](https://substack.com/redirect/5b5af585-7537-4189-9218-dca69f7c3bf1?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 31 Jul, *The Llama 3 Herd of Models*, [https://arxiv.org/abs/2407.21783](https://substack.com/redirect/b0f741ee-b18b-42b2-b528-bc97462db587?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 31 Jul, *Gemma 2: Improving Open Language Models at a Practical Size*, [https://arxiv.org/abs/2408.00118](https://substack.com/redirect/59ebdaf0-d2fe-4958-9505-1295639d8528?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- ## **August 2024**\n- 1 Aug, S*AM 2: Segment Anything in Images and Videos,* [https://arxiv.org/abs/2408.00714](https://substack.com/redirect/f9108c66-74c8-4629-a1f0-91d3fc48be11?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 2 Aug, *POA: Pre-training Once for Models of All Sizes,* [https://arxiv.org/abs/2408.01031](https://substack.com/redirect/a7980e91-473d-41d0-a3a4-388c7d228783?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 2 Aug, *RAGEval: Scenario Specific RAG Evaluation Dataset Generation Framework,* [https://arxiv.org/abs/2408.01262](https://substack.com/redirect/36908db5-a36d-41f3-88a7-415386a07f9e?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 2 Aug, *A Survey of Mamba,* [https://arxiv.org/abs/2408.01129](https://substack.com/redirect/dfc99042-7b05-4a46-99ca-cb9da6847308?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 3 Aug, *MiniCPM-V: A GPT-4V Level MLLM on Your Phone,* [https://arxiv.org/abs/2408.01800](https://substack.com/redirect/f2d6df02-2c99-47d9-a587-20f57b8f2b79?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 5 Aug, *RAG Foundry: A Framework for Enhancing LLMs for Retrieval Augmented Generation,* [https://arxiv.org/abs/2408.02545](https://substack.com/redirect/f0ca6040-7e8c-472c-94eb-795a6d46fcfc?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 5 Aug, *Self-Taught Evaluators,* [https://arxiv.org/abs/2408.02666](https://substack.com/redirect/267ef5b5-84df-4e5c-acf4-8ff80cd13d1b?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 5 Aug, *BioMamba: A Pre-trained Biomedical Language Representation Model Leveraging Mamba,* [https://arxiv.org/abs/2408.02600](https://substack.com/redirect/a94dd61e-184b-4375-88b9-5eb5d6874d3e?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 5 Aug, *Self-Taught Evaluators,* [https://arxiv.org/abs/2408.02666](https://substack.com/redirect/267ef5b5-84df-4e5c-acf4-8ff80cd13d1b?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 7 Aug, *EXAONE 3.0 7.8B Instruction Tuned Language Model,* [https://arxiv.org/abs/2408.03541](https://substack.com/redirect/7bcf15be-bdb1-4292-85fc-cf0d96a220ef?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 7 Aug, *1.5-Pints Technical Report: Pretraining in Days, Not Months -- Your Language Model Thrives on Quality Data,* [https://arxiv.org/abs/2408.03506](https://substack.com/redirect/0a7d20ca-bee1-4a6e-bdf0-add0dc036f33?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 8 Aug, *Conversational Prompt Engineering,* [https://arxiv.org/abs/2408.04560](https://substack.com/redirect/a6861968-ad21-4151-8433-7b692f6f01c5?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 8 Aug, *Trans-Tokenization and Cross-lingual Vocabulary Transfers: Language Adaptation of LLMs for Low-Resource NLP,* [https://arxiv.org/abs/2408.04303](https://substack.com/redirect/8f6d35b4-563a-4e08-99fe-47ec7f80caff?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 12 Aug, *The AI Scientist: Towards Fully Automated Open-Ended Scientific Discovery,* [https://arxiv.org/abs/2408.06292](https://substack.com/redirect/5aa3f2f5-78a5-418c-9e3a-124d14f04a00?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 15 Aug, *Hermes 3 Technical Report,* [https://arxiv.org/abs/2408.12570](https://substack.com/redirect/7d8a39c8-da09-4f34-8206-93e415297c1f?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 19 Aug, *Customizing Language Models with Instance-wise LoRA for Sequential Recommendation,* [https://arxiv.org/abs/2408.10159](https://substack.com/redirect/ee75cbc3-c061-4151-9b5d-b5f88635c5ab?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 20 Aug*, Enhancing Robustness in Large Language Models: Prompting for Mitigating the Impact of Irrelevant Information,* [https://arxiv.org/abs/2408.10615](https://substack.com/redirect/ea396dc0-0a9f-4f30-8acd-d48c770b6b84?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 20 Aug, *To Code, or Not To Code? Exploring Impact of Code in Pre-training,* [https://arxiv.org/abs/2408.10914](https://substack.com/redirect/a736b6e3-1c73-4c59-9bf6-5da614014ece?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 21 Aug ,* LLM Pruning and Distillation in Practice: The Minitron Approach, *[https://arxiv.org/abs/2408.11796](https://substack.com/redirect/0dd4085f-2153-4593-98f4-423faaff6d27?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 22 Aug, *Jamba-1.5: Hybrid Transformer-Mamba Models at Scale,* [https://arxiv.org/abs/2408.12570](https://substack.com/redirect/7d8a39c8-da09-4f34-8206-93e415297c1f?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 22 Aug, *Controllable Text Generation for Large Language Models: A Survey,* [https://arxiv.org/abs/2408.12599](https://substack.com/redirect/6e4408c3-5faf-4a9d-b45c-aa2da38ed656?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 23 Aug, *Multi-Layer Transformers Gradient Can be Approximated in Almost Linear Time,* [https://arxiv.org/abs/2408.13233](https://substack.com/redirect/5e036add-9e91-4eb6-b2c3-0a04dddc108d?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 26 Aug, *A Practitioner's Guide to Continual Multimodal Pretraining,* [https://arxiv.org/abs/2408.14471](https://substack.com/redirect/8c09403f-f600-492e-91d8-544a0385901f?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 26 Aug, *Building and better understanding vision-language models: insights and future directions,* [https://arxiv.org/abs/2408.12637](https://substack.com/redirect/94cf7b23-443c-42f4-b7f4-116bc19a0ebb?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 26 Aug, *CURLoRA: Stable LLM Continual Fine-Tuning and Catastrophic Forgetting Mitigation,* [https://arxiv.org/abs/2408.14572](https://substack.com/redirect/2cd9eff1-1efb-4800-b61e-af64b4dd932a?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 27 Aug, *The Mamba in the Llama: Distilling and Accelerating Hybrid Models,* [https://arxiv.org/abs/2408.15237](https://substack.com/redirect/bfd4ba78-fe7a-406f-adc9-41f3de52fe11?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 28 Aug, *ReMamba: Equip Mamba with Effective Long-Sequence Modeling,* [https://arxiv.org/abs/2408.15496](https://substack.com/redirect/d2da6482-73df-4cb7-a0aa-915673460ccd?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 29 Aug, *Smaller, Weaker, Yet Better: Training LLM Reasoners via Compute-Optimal Sampling,* [https://arxiv.org/abs/2408.16737](https://substack.com/redirect/a5cb8ada-f6d8-4e2b-9e74-dbdf42c6e4aa?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 31 Aug, *LongRecipe: Recipe for Efficient Long Context Generalization in Large Languge Models,* [https://arxiv.org/abs/2409.00509](https://substack.com/redirect/b42dfc08-770d-4b31-82ce-ece859db7016?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- ## **September 2024**\n- 3 Sep, *OLMoE: Open Mixture-of-Experts Language Models,* [https://arxiv.org/abs/2409.02060](https://substack.com/redirect/67b4ea52-d88f-477c-9b2a-cbfc011a632d?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 3 Sep 2024, *In Defense of RAG in the Era of Long-Context Language Models,* [https://arxiv.org/abs/2409.01666](https://substack.com/redirect/8c123e4d-661d-4a18-9c3e-9f6ad97bb121?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 5 Sep, *Attention Heads of Large Language Models: A Survey,* [https://arxiv.org/abs/2409.03752](https://substack.com/redirect/25b86a0a-d934-43fb-ba3f-f4990946b67d?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 5 Sep, *LongCite: Enabling LLMs to Generate Fine-grained Citations in Long-context QA*, [https://arxiv.org/abs/2409.02897](https://substack.com/redirect/c4ab3c32-67e8-46a3-b95b-68616b8c2ac9?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 5 Sep, *How Do Your Code LLMs Perform? Empowering Code Instruction Tuning with High-Quality Data,* [https://arxiv.org/abs/2409.03810](https://substack.com/redirect/79ec2ede-0a39-492c-9b6a-37a970e00a69?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 6 Sep, T*heory, Analysis, and Best Practices for Sigmoid Self-Attention,* [https://arxiv.org/abs/2409.04431](https://substack.com/redirect/3ef7937e-3176-4bb4-8691-2bb83f4e2569?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 10 Sep, *LLaMA-Omni: Seamless Speech Interaction with Large Language Models,* [https://arxiv.org/abs/2409.06666](https://substack.com/redirect/0c0e0192-2a35-40cc-9bf1-7b3fae8e3d57?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 10 Sep, *What is the Role of Small Models in the LLM Era: A Survey,* [https://arxiv.org/abs/2409.06857](https://substack.com/redirect/dd97a86a-6d4c-40e4-abdd-c922644f1a92?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 11 Sep, *Policy Filtration in RLHF to Fine-Tune LLM for Code Generation,* [https://arxiv.org/abs/2409.06957](https://substack.com/redirect/7922fa19-815c-4efe-88f2-0baca9d76400?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 16 Sep, *RetrievalAttention: Accelerating Long-Context LLM Inference via Vector Retrieval*, [https://arxiv.org/abs/2409.10516](https://substack.com/redirect/4c16d216-0395-4b2b-9b52-31964f2ebbc6?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 18 Sep, *Qwen2.5-Math Technical Report: Toward Mathematical Expert Model via Self-Improvement*, [https://arxiv.org/abs/2409.12122](https://substack.com/redirect/e1ab1834-169e-403d-b326-d524192056a5?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 18 Sep, *Qwen2.5-Coder Technical Report*, [https://arxiv.org/abs/2409.12186](https://substack.com/redirect/78421dc8-0411-4e4b-b4c8-4866b9744ab6?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 21 Sep, *Instruction Following without Instruction Tuning,* [https://arxiv.org/abs/2409.14254](https://substack.com/redirect/142edb56-86a1-40da-90d6-f942fbb908e3?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 30 Sep, I*s Preference Alignment Always the Best Option to Enhance LLM-Based Translation? An Empirical Analysis,* [https://arxiv.org/abs/2409.20059](https://substack.com/redirect/f4490e8c-5215-4357-b996-67422339d948?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 30 Sep, *The Perfect Blend: Redefining RLHF with Mixture of Judges,* [https://arxiv.org/abs/2409.20370](https://substack.com/redirect/eee4c54c-2915-4735-b9d5-c4f7edb12932?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA) (New paper by Meta on how they did RLHF for Llama 3)\n- ## **October 2024**\n- 1 Oct, *Addition is All You Need for Energy-efficient Language Models,* [https://arxiv.org/abs/2410.00907](https://substack.com/redirect/e4dd26cb-0c66-4de4-a582-09952a275c07?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 2 Oct *Quantifying Generalization Complexity for Large Language Models,* [https://arxiv.org/abs/2410.01769](https://substack.com/redirect/c4dc657e-b900-41b5-8882-25cabe381930?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 2 Oct, *When a language model is optimized for reasoning, does it still show embers of autoregression? An analysis of OpenAI o1*, [https://arxiv.org/abs/2410.01792](https://substack.com/redirect/d261b63a-d90e-4b2a-9ce0-34f98bafb6b5?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 2 Oct, W*ere RNNs All We Needed?*, [https://arxiv.org/abs/2410.01201](https://substack.com/redirect/0501d1fb-6912-4ff7-b2ee-4ca367f54bc3?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 3 Oct, *Selective Attention Improves Transformer*, [https://arxiv.org/abs/2410.02703](https://substack.com/redirect/4c25dab1-ebc9-4746-bc44-2478e334a3a7?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 3 Oct, *LLMs Know More Than They Show: On the Intrinsic Representation of LLM Hallucinations*, [https://arxiv.org/abs/2410.02707](https://substack.com/redirect/a5ec9a1c-9698-4c9b-8b99-65e251b9c269?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 3 Oct, *LLaVA-Critic: Learning to Evaluate Multimodal Models*, [https://arxiv.org/abs/2410.02712](https://substack.com/redirect/bbe709fd-4722-431f-968c-8bfb80c54304?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 7 Oct, *Differential Transformer*, [https://arxiv.org/abs/2410.05258](https://substack.com/redirect/32d6329b-616b-49f4-8f4a-3c5727a802ca?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 7 Oct, *GSM-Symbolic: Understanding the Limitations of Mathematical Reasoning in Large Language Models*, [https://arxiv.org/abs/2410.05229](https://substack.com/redirect/444c7a19-4e98-4896-a29c-c0829bef6050?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 8 Oct, *ARIA: An Open Multimodal Native Mixture-of-Experts Model*, [https://arxiv.org/abs/2410.05993](https://substack.com/redirect/f23331b3-59ec-4f83-a3bd-66f23adca811?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 8 Oct, *O1 Replication Journey: A Strategic Progress Report -- Part 1*, [https://arxiv.org/abs/2410.18982](https://substack.com/redirect/fa9f2d76-6ac0-4870-b5bb-7806e79058b5?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 8 Oct, *Long-Context LLMs Meet RAG: Overcoming Challenges for Long Inputs in RAG,* [https://arxiv.org/abs/2410.05983](https://substack.com/redirect/59604f29-a60d-4734-adc6-e5433e3d35df?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 9 Oct, *From Generalist to Specialist: Adapting Vision Language Models via Task-Specific Visual Instruction Tuning*, [https://arxiv.org/abs/2410.06456](https://substack.com/redirect/f6fc5979-992d-40e2-8909-af7c3933ff89?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 10 Oct, *KV Prediction for Improved Time to First Token*, [https://arxiv.org/abs/2410.08391](https://substack.com/redirect/830313c5-f3d6-4821-9782-2205c633c434?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 11 Oct, *Baichuan-Omni Technical Report*, [https://arxiv.org/abs/2410.08565](https://substack.com/redirect/d00ca40f-004d-4f8e-82d3-c9f40686db03?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 13 Oct, *MMIE: Massive Multimodal Interleaved Comprehension Benchmark for Large Vision-Language Models*, [https://arxiv.org/abs/2410.10139](https://substack.com/redirect/cdeabdbf-e920-4b94-98a2-417cdef244ea?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 13 Oct, *LOKI: A Comprehensive Synthetic Data Detection Benchmark using Large Multimodal Models*, [https://arxiv.org/abs/2410.09732](https://substack.com/redirect/7fc3a7ab-ef22-4411-82d3-b318d89f47b9?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 15 Oct, *AFlow: Automating Agentic Workflow Generation*, [https://arxiv.org/abs/2410.10762](https://substack.com/redirect/e89323bf-d9d1-4dd7-84e8-ab20dd16ed99?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 15 Oct, *Toward General Instruction-Following Alignment for Retrieval-Augmented Generation*, [https://arxiv.org/abs/2410.09584](https://substack.com/redirect/109b3f65-d329-4c4c-abbf-8e2167f467c2?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 21 Oct, *Pre-training Distillation for Large Language Models: A Design Space Exploration*, [https://arxiv.org/abs/2410.16215](https://substack.com/redirect/c2c2f034-ff24-427a-8648-fa81f5032d9e?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 23 Oct, *MIA-DPO: Multi-Image Augmented Direct Preference Optimization For Large Vision-Language Models*, [https://arxiv.org/abs/2410.17637](https://substack.com/redirect/e22b2dfb-e6c2-4dc7-9c76-64f8253a9937?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 23 Oct, *Scalable Ranked Preference Optimization for Text-to-Image Generation*, [https://arxiv.org/abs/2410.18013](https://substack.com/redirect/7dac9daf-da68-41e3-ab0f-34271b9d6695?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 23 Oct, *Scaling Diffusion Language Models via Adaptation from Autoregressive Models*, [https://arxiv.org/abs/2410.17891](https://substack.com/redirect/3ae874f0-2a48-4065-8675-6a8cad3d5b15?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 24 Oct, *Hybrid Preferences: Learning to Route Instances for Human vs. AI Feedback*, [https://arxiv.org/abs/2410.19133](https://substack.com/redirect/16c95243-e15a-4be7-ad7d-4664259615ed?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 25 Oct, *Counting Ability of Large Language Models and Impact of Tokenization*, [https://arxiv.org/abs/2410.19730](https://substack.com/redirect/ebfc5000-34c4-4e8c-bc27-93dc79c4e167?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 25 Oct, *A Survey of Small Language Models*, [https://arxiv.org/abs/2410.20011](https://substack.com/redirect/b999b87f-bb38-4b1a-9891-7685d41b664a?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 26 Oct, *Accelerating Direct Preference Optimization with Prefix Sharing*, [https://arxiv.org/abs/2410.20305](https://substack.com/redirect/7a404691-c9b1-43da-9ba5-436aa1e18ed0?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 27 Oct, *Mind Your Step (by Step): Chain-of-Thought can Reduce Performance on Tasks where Thinking Makes Humans Worse*, [https://arxiv.org/abs/2410.21333](https://substack.com/redirect/e4b07e0b-687c-4755-9f0e-4c2bfad698a4?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 28 Oct, *LongReward: Improving Long-context Large Language Models with AI Feedback*, [https://arxiv.org/abs/2410.21252](https://substack.com/redirect/732a1624-7ae4-4670-a6a1-cd3621924dde?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 28 Oct, *ShadowKV: KV Cache in Shadows for High-Throughput Long-Context LLM Inference*, [https://arxiv.org/abs/2410.21465](https://substack.com/redirect/2cc86730-af8d-4a07-b5ef-2012b7f34350?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 29 Oct, *Beyond Text: Optimizing RAG with Multimodal Inputs for Industrial Applications*, [https://arxiv.org/abs/2410.21943](https://substack.com/redirect/af25eb9e-4abc-4581-9ec6-5286f3ef430d?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 30 Oct, *CORAL: Benchmarking Multi-turn Conversational Retrieval-Augmentation Generation*, [https://arxiv.org/abs/2410.23090](https://substack.com/redirect/3126287b-fb58-4f8e-a596-cff2c5d52f40?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 31 Oct, *What Happened in LLMs Layers when Trained for Fast vs. Slow Thinking: A Gradient Perspective*, [https://arxiv.org/abs/2410.23743](https://substack.com/redirect/b20dfc07-6c5c-4714-899b-f17090e0060f?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 31 Oct, *GPT or BERT: why not both?*, [https://arxiv.org/abs/2410.24159](https://substack.com/redirect/cc0e1510-715e-4f8b-9bac-c25500e19949?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 31 Oct, *Language Models can Self-Lengthen to Generate Long Texts*, [https://arxiv.org/abs/2410.23933](https://substack.com/redirect/3671e167-5145-4c21-ac86-87437df3efed?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- ## **November 2024**\n- 1 Nov, *Adding Error Bars to Evals: A Statistical Approach to Language Model Evaluations*, [https://arxiv.org/abs/2411.00640](https://substack.com/redirect/54f7c841-4f1b-40c5-b813-6d6eec518212?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 1 Nov 2024, *Adapting While Learning: Grounding LLMs for Scientific Problems with Intelligent Tool Usage Adaptation*, [https://arxiv.org/abs/2411.00412](https://substack.com/redirect/826f004c-b9a1-4f81-bbe2-8a41a128cfd3?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 1 Nov 2024, *Multi-expert Prompting Improves Reliability, Safety, and Usefulness of Large Language Models*, [https://arxiv.org/abs/2411.00492](https://substack.com/redirect/6c3b5229-1e78-4e6c-b9e5-2707d00ba778?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 3 Nov, S*ample-Efficient Alignment for LLMs*, [https://arxiv.org/abs/2411.01493](https://substack.com/redirect/240651cf-8138-4af1-a1d1-af31433b77ef?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 4 Nov 2024, *A Comprehensive Survey of Small Language Models in the Era of Large Language Models: Techniques, Enhancements, Applications, Collaboration with LLMs, and Trustworthiness*, [https://arxiv.org/abs/2411.03350](https://substack.com/redirect/f32e1ad1-a406-4198-b9ab-aa4b2c0f431b?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 4 Nov, *\"Give Me BF16 or Give Me Death\"? Accuracy-Performance Trade-Offs in LLM Quantization*, [https://arxiv.org/abs/2411.02355](https://substack.com/redirect/5d83fef8-ae25-4356-815e-160975bf792b?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 4 Nov, *Parameter-Efficient Fine-Tuning of Large Language Models for Unit Test Generation: An Empirical Study*, [https://arxiv.org/abs/2411.02462](https://substack.com/redirect/7778c822-74f0-41ec-a2f7-cad21e8c40a6?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 5 Nov, *HtmlRAG: HTML is Better Than Plain Text for Modeling Retrieved Knowledge in RAG Systems*, [https://arxiv.org/abs/2411.02959](https://substack.com/redirect/d759a229-793c-44ef-9510-05a1f9924bdc?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 6 Nov, *Both Text and Images Leaked! A Systematic Analysis of Multimodal LLM Data Contamination*, [https://arxiv.org/abs/2411.03823](https://substack.com/redirect/3c24948f-b3d3-4af6-955a-e4b75d065797?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 6 Nov, *Language Models are Hidden Reasoners: Unlocking Latent Reasoning Capabilities via Self-Rewarding*, [https://arxiv.org/abs/2411.04282](https://substack.com/redirect/3c921cb4-e59c-495c-8297-7782441c41e9?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 6 Nov, *Number Cookbook: Number Understanding of Language Models and How to Improve It*, [https://arxiv.org/abs/2411.03766](https://substack.com/redirect/c9b2910c-ab97-4c33-bab2-705f02c8a2b2?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 7 Nov, *Mixture-of-Transformers: A Sparse and Scalable Architecture for Multi-Modal Foundation Models*, [https://arxiv.org/abs/2411.04996](https://substack.com/redirect/d6843086-7122-49ef-9604-5f6deb075c93?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 7 Nov, *BitNet a4.8: 4-bit Activations for 1-bit LLMs*, [https://arxiv.org/abs/2411.04965](https://substack.com/redirect/6777d927-412f-4ff2-b334-c5d8381638cc?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 7 Nov, *Scaling Laws for Precision*, [https://arxiv.org/abs/2411.04330](https://substack.com/redirect/f3c15ba7-cbf0-4962-a87a-99263612278b?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 8 Nov, *Energy Efficient Protein Language Models: Leveraging Small Language Models with LoRA for Controllable Protein Generation*, [https://arxiv.org/abs/2411.05966](https://substack.com/redirect/4da9a386-3dbd-48fe-b1ec-4cfaeb276927?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 8 Nov, *Balancing Pipeline Parallelism with Vocabulary Parallelism*, [https://arxiv.org/abs/2411.05288](https://substack.com/redirect/353d44d7-41ff-4867-90a7-37ec001ce5d1?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 11 Nov, *Toward Optimal Search and Retrieval for RAG*, [https://arxiv.org/abs/2411.07396](https://substack.com/redirect/4f8f08ab-ffe5-42ec-ab52-dd65d8e9db52?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 12 Nov, *Large Language Models Can Self-Improve in Long-context Reasoning*, [https://arxiv.org/abs/2411.08147](https://substack.com/redirect/73b810b2-20e1-44b7-9066-92129876da55?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 12 Nov, *Stronger Models are NOT Stronger Teachers for Instruction Tuning*, [https://arxiv.org/abs/2411.07133](https://substack.com/redirect/4018c81d-0e93-4f04-b7e6-74b077492eae?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 12 Nov, *Direct Preference Optimization Using Sparse Feature-Level Constraints*, [https://arxiv.org/abs/2411.07618](https://substack.com/redirect/15e16eae-dc79-4d86-9bcf-3593d4f2636e?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 13 Nov, *Cut Your Losses in Large-Vocabulary Language Models*, [https://arxiv.org/abs/2411.09009](https://substack.com/redirect/86b4d912-bba7-4d87-804e-b9004e58283f?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 15 Nov, *Does Prompt Formatting Have Any Impact on LLM Performance?*, [https://arxiv.org/abs/2411.10541](https://substack.com/redirect/f0b9adaa-3c37-44a2-8c20-4945afcc9aef?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 17 Nov, *SymDPO: Boosting In-Context Learning of Large Multimodal Models with Symbol Demonstration Direct Preference Optimization*, [https://arxiv.org/abs/2411.11909](https://substack.com/redirect/79bc2538-af6e-4c8c-b2bd-d284627da587?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 17 Nov, *SageAttention2 Technical Report: Accurate 4 Bit Attention for Plug-and-play Inference Acceleration*, [https://arxiv.org/abs/2411.10958](https://substack.com/redirect/b687e920-1423-4298-bf17-16b90c7a157d?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 18 Nov, *Bi-Mamba: Towards Accurate 1-Bit State Space Models*, [https://arxiv.org/abs/2411.11843](https://substack.com/redirect/b258e20d-7feb-40a2-aad7-d91c6e79659f?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 19 Nov, RedPajama: an Open Dataset for Training Large Language Models, [https://arxiv.org/abs/2411.12372](https://substack.com/redirect/23068f08-5213-4a2f-84d8-bba03a496967?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 20 Nov, *Hymba: A Hybrid-head Architecture for Small Language Models*, [https://arxiv.org/abs/2411.13676](https://substack.com/redirect/9b518545-f769-488b-b9a9-3778977b1ba7?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 20 Nov, *Loss-to-Loss Prediction: Scaling Laws for All Datasets*, [https://arxiv.org/abs/2411.12925](https://substack.com/redirect/a2bf33a3-a6b3-4009-a62f-117b95c8ddeb?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 21 Nov, *When Precision Meets Position: BFloat16 Breaks Down RoPE in Long-Context Training*, [https://arxiv.org/abs/2411.13476](https://substack.com/redirect/c59fefd8-7501-4433-a0c7-8d570d59237a?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 21 Nov, *Multimodal Autoregressive Pre-training of Large Vision Encoders*, [https://arxiv.org/abs/2411.14402](https://substack.com/redirect/214aff7f-57e7-4a36-802f-fd9e5ff27b2b?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 21 Nov, *Natural Language Reinforcement Learning*, [https://arxiv.org/abs/2411.14251](https://substack.com/redirect/ebb1e570-1551-4f82-ac75-ec519d09c549?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 22 Nov, *Large Multi-modal Models Can Interpret Features in Large Multi-modal Models*, [https://arxiv.org/abs/2411.14982](https://substack.com/redirect/cee124a0-f5d7-4b2b-a2b9-c0ece30c7e97?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 23 Nov, *MME-Survey: A Comprehensive Survey on Evaluation of Multimodal LLMs*, [https://arxiv.org/abs/2411.15296](https://substack.com/redirect/5ef61ef4-1460-4246-b979-268f89302f0b?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 23 Nov, *TÜLU 3: Pushing Frontiers in Open Language Model Post-Training*, [https://arxiv.org/abs/2411.15124](https://substack.com/redirect/451fdd95-ec0e-40df-9736-c2870291a5df?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n- 24 Nov, *LLMs Do Not Think Step-by-step In Implicit Reasoning*, [https://arxiv.org/abs/2411.15862](https://substack.com/redirect/f1b5067f-492d-4a1b-b703-2617da241d25?j=eyJ1IjoiMmRuamR0In0.FbVk83ULjBfeYOuNAZl_CpjTIVAnXyHBXQJHR4cbAoA)\n\n## Current Landscape (2025)\n\n- Industry adoption and implementations\n  - Metaverse platforms continue to evolve with focus on interoperability and open standards\n  - Web3 integration accelerating with decentralised identity and asset ownership\n  - Enterprise adoption growing in virtual collaboration, training, and digital twins\n  - UK companies increasingly active in metaverse development and immersive technologies\n\n- Technical capabilities\n  - Real-time rendering at photorealistic quality levels\n  - Low-latency networking enabling seamless multi-user experiences\n  - AI-driven content generation and procedural world building\n  - Spatial audio and haptics enhancing immersion\n\n- UK and North England context\n  - Manchester: Digital Innovation Factory supports metaverse startups and research\n  - Leeds: Holovis leads in immersive experiences for entertainment and training\n  - Newcastle: University research in spatial computing and interactive systems\n  - Sheffield: Advanced manufacturing using digital twin technology\n\n- Standards and frameworks\n  - Metaverse Standards Forum driving interoperability protocols\n  - WebXR enabling browser-based immersive experiences\n  - glTF and USD for 3D asset interchange\n  - Open Metaverse Interoperability Group defining cross-platform standards\n\n## Metadata\n\n- **Last Updated**: 2025-11-16\n- **Review Status**: Automated remediation with 2025 context\n- **Verification**: Academic sources verified\n- **Regional Context**: UK/North England where applicable",
  "properties": {
    "id": "papers-of-note-owl-axioms",
    "collapsed": "true",
    "- ontology": "true",
    "- term-id": "mv-892246369703",
    "- preferred-term": "Papers Of Note",
    "- source-domain": "metaverse",
    "- status": "active",
    "- public-access": "true",
    "- definition": "A component of the metaverse ecosystem focusing on papers of note.",
    "- maturity": "mature",
    "- authority-score": "0.85",
    "- owl:class": "mv:PapersOfNote",
    "- owl:physicality": "ConceptualEntity",
    "- owl:role": "Concept",
    "- belongsToDomain": "[[MetaverseDomain]]",
    "- enables": "[[ImmersiveExperience]], [[Presence]], [[SpatialComputing]]",
    "- requires": "[[DisplayTechnology]], [[TrackingSystem]], [[RenderingEngine]]",
    "- bridges-to": "[[HumanComputerInteraction]], [[ComputerVision]], [[Robotics]]",
    "public": "true"
  },
  "backlinks": [],
  "wiki_links": [
    "TrackingSystem",
    "ImmersiveExperience",
    "ComputerVision",
    "RenderingEngine",
    "SpatialComputing",
    "Robotics",
    "HumanComputerInteraction",
    "MetaverseDomain",
    "DisplayTechnology",
    "Presence"
  ],
  "ontology": {
    "term_id": "mv-892246369703",
    "preferred_term": "Papers Of Note",
    "definition": "A component of the metaverse ecosystem focusing on papers of note.",
    "source_domain": "metaverse",
    "maturity_level": null,
    "authority_score": 0.85
  }
}