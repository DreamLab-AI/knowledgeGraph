{
  "title": "Group vs Individual Fairness",
  "content": "- ### OntologyBlock\n  id:: 0383-group-vs-individual-fairness-ontology\n  collapsed:: true\n\n  - **Identification**\n    - public-access:: true\n    - ontology:: true\n    - term-id:: AI-0383\n    - preferred-term:: Group vs Individual Fairness\n    - source-domain:: ai\n    - status:: in-progress\n    - version:: 1.0\n    - last-updated:: 2025-10-29\n\n  - **Definition**\n    - definition:: Group vs Individual Fairness represents two distinct paradigms for conceptualizing and operationalizing algorithmic fairness with fundamentally different units of analysis and philosophical foundations. Group Fairness operates at the aggregate level, requiring statistical parity across protected demographic groups such that prediction distributions, error rates, or outcome rates are similar across groups, formalized as P(Ŷ|A=a) being approximately equal for all protected group values a. This paradigm underlies metrics like demographic parity, equalized odds, and predictive parity, and aligns with legal frameworks focused on disparate impact and anti-discrimination compliance. In contrast, Individual Fairness operates at the person level, requiring that similar individuals receive similar predictions regardless of group membership, formalized through a fairness metric d(x₁,x₂) → d(f(x₁),f(f₂)) where the distance between predictions is bounded by the distance between individuals in a task-relevant similarity space. Group fairness is operationally straightforward requiring only protected attribute labels but may permit unfairness to individuals within groups, while individual fairness provides stronger theoretical guarantees but requires defining task-appropriate similarity metrics that avoid encoding prohibited biases. The two paradigms are not necessarily compatible, as satisfying group fairness constraints does not guarantee individual fairness and vice versa, representing a fundamental tension in fair machine learning research explored by Dwork et al. (2012) and subsequent scholarship.\n    - maturity:: mature\n    - source:: [[Dwork et al. (2012)]], [[Hardt et al. (2016)]], [[Barocas et al. (2019)]]\n    - authority-score:: 0.95\n\n  - **Semantic Classification**\n    - owl:class:: aigo:GroupVsIndividualFairness\n    - owl:physicality:: VirtualEntity\n    - owl:role:: Process\n    - owl:inferred-class:: aigo:VirtualProcess\n    - belongsToDomain:: [[AIEthicsDomain]]\n    - implementedInLayer:: [[ConceptualLayer]]\n\n  - #### Relationships\n    id:: 0383-group-vs-individual-fairness-relationships\n\n  - #### OWL Axioms\n    id:: 0383-group-vs-individual-fairness-owl-axioms\n    collapsed:: true\n    - ```clojure\n      (Declaration (Class :FairnessParadigm))\n(SubClassOf :FairnessParadigm :EthicalFramework)\n\n;; Group Fairness\n(Declaration (Class :GroupFairness))\n(SubClassOf :GroupFairness :FairnessParadigm)\n(AnnotationAssertion rdfs:comment :GroupFairness\n  \"Fairness defined over groups: statistical parity across protected groups\"@en)\n(DataPropertyAssertion :formalDefinition :GroupFairness\n  \"P(Ŷ|A=a) similar for all protected groups a\"^^xsd:string)\n\n;; Individual Fairness\n(Declaration (Class :IndividualFairness))\n(SubClassOf :IndividualFairness :FairnessParadigm)\n(AnnotationAssertion rdfs:comment :IndividualFairness\n  \"Fairness defined over individuals: similar individuals treated similarly\"@en)\n(DataPropertyAssertion :formalDefinition :IndividualFairness\n  \"d(x₁,x₂) small → d(f(x₁),f(x₂)) small\"^^xsd:string)\n\n;; Disjointness\n(DisjointClasses :GroupFairness :IndividualFairness)\n\n;; Properties\n(Declaration (ObjectProperty :operatesOn))\n(ObjectPropertyDomain :operatesOn :FairnessParadigm)\n(ObjectPropertyRange :operatesOn :AnalysisLevel)\n\n(DataPropertyAssertion :operatesOn :GroupFairness :AggregateLevel)\n(DataPropertyAssertion :operatesOn :IndividualFairness :PersonLevel)\n      ```\n\n- ## About Group vs Individual Fairness\n  id:: 0383-group-vs-individual-fairness-about\n\n  - \n  -\n  \n\n\t\t\t- ### Metaverse Instances\n\t\t\t\t- Individual virtual worlds with unique themes, functionalities, and communities.\n\t\t\t\t- Examples include:\n\n\t\t\t- ### Metaverse Instances\n\t\t\t\t- Individual virtual worlds with unique themes, functionalities, and communities.\n\t\t\t\t- Examples include:\n\n\n## Current Landscape (2025)\n\n- Industry adoption and implementations\n  - Metaverse platforms continue to evolve with focus on interoperability and open standards\n  - Web3 integration accelerating with decentralised identity and asset ownership\n  - Enterprise adoption growing in virtual collaboration, training, and digital twins\n  - UK companies increasingly active in metaverse development and immersive technologies\n\n- Technical capabilities\n  - Real-time rendering at photorealistic quality levels\n  - Low-latency networking enabling seamless multi-user experiences\n  - AI-driven content generation and procedural world building\n  - Spatial audio and haptics enhancing immersion\n\n- UK and North England context\n  - Manchester: Digital Innovation Factory supports metaverse startups and research\n  - Leeds: Holovis leads in immersive experiences for entertainment and training\n  - Newcastle: University research in spatial computing and interactive systems\n  - Sheffield: Advanced manufacturing using digital twin technology\n\n- Standards and frameworks\n  - Metaverse Standards Forum driving interoperability protocols\n  - WebXR enabling browser-based immersive experiences\n  - glTF and USD for 3D asset interchange\n  - Open Metaverse Interoperability Group defining cross-platform standards\n\n## Metadata\n\n- **Last Updated**: 2025-11-16\n- **Review Status**: Automated remediation with 2025 context\n- **Verification**: Academic sources verified\n- **Regional Context**: UK/North England where applicable",
  "properties": {
    "id": "0383-group-vs-individual-fairness-about",
    "collapsed": "true",
    "- public-access": "true",
    "- ontology": "true",
    "- term-id": "AI-0383",
    "- preferred-term": "Group vs Individual Fairness",
    "- source-domain": "ai",
    "- status": "in-progress",
    "- version": "1.0",
    "- last-updated": "2025-10-29",
    "- definition": "Group vs Individual Fairness represents two distinct paradigms for conceptualizing and operationalizing algorithmic fairness with fundamentally different units of analysis and philosophical foundations. Group Fairness operates at the aggregate level, requiring statistical parity across protected demographic groups such that prediction distributions, error rates, or outcome rates are similar across groups, formalized as P(Ŷ|A=a) being approximately equal for all protected group values a. This paradigm underlies metrics like demographic parity, equalized odds, and predictive parity, and aligns with legal frameworks focused on disparate impact and anti-discrimination compliance. In contrast, Individual Fairness operates at the person level, requiring that similar individuals receive similar predictions regardless of group membership, formalized through a fairness metric d(x₁,x₂) → d(f(x₁),f(f₂)) where the distance between predictions is bounded by the distance between individuals in a task-relevant similarity space. Group fairness is operationally straightforward requiring only protected attribute labels but may permit unfairness to individuals within groups, while individual fairness provides stronger theoretical guarantees but requires defining task-appropriate similarity metrics that avoid encoding prohibited biases. The two paradigms are not necessarily compatible, as satisfying group fairness constraints does not guarantee individual fairness and vice versa, representing a fundamental tension in fair machine learning research explored by Dwork et al. (2012) and subsequent scholarship.",
    "- maturity": "mature",
    "- source": "[[Dwork et al. (2012)]], [[Hardt et al. (2016)]], [[Barocas et al. (2019)]]",
    "- authority-score": "0.95",
    "- owl:class": "aigo:GroupVsIndividualFairness",
    "- owl:physicality": "VirtualEntity",
    "- owl:role": "Process",
    "- owl:inferred-class": "aigo:VirtualProcess",
    "- belongsToDomain": "[[AIEthicsDomain]]",
    "- implementedInLayer": "[[ConceptualLayer]]"
  },
  "backlinks": [],
  "wiki_links": [
    "ConceptualLayer",
    "Dwork et al. (2012)",
    "AIEthicsDomain",
    "Barocas et al. (2019)",
    "Hardt et al. (2016)"
  ],
  "ontology": {
    "term_id": "AI-0383",
    "preferred_term": "Group vs Individual Fairness",
    "definition": "Group vs Individual Fairness represents two distinct paradigms for conceptualizing and operationalizing algorithmic fairness with fundamentally different units of analysis and philosophical foundations. Group Fairness operates at the aggregate level, requiring statistical parity across protected demographic groups such that prediction distributions, error rates, or outcome rates are similar across groups, formalized as P(Ŷ|A=a) being approximately equal for all protected group values a. This paradigm underlies metrics like demographic parity, equalized odds, and predictive parity, and aligns with legal frameworks focused on disparate impact and anti-discrimination compliance. In contrast, Individual Fairness operates at the person level, requiring that similar individuals receive similar predictions regardless of group membership, formalized through a fairness metric d(x₁,x₂) → d(f(x₁),f(f₂)) where the distance between predictions is bounded by the distance between individuals in a task-relevant similarity space. Group fairness is operationally straightforward requiring only protected attribute labels but may permit unfairness to individuals within groups, while individual fairness provides stronger theoretical guarantees but requires defining task-appropriate similarity metrics that avoid encoding prohibited biases. The two paradigms are not necessarily compatible, as satisfying group fairness constraints does not guarantee individual fairness and vice versa, representing a fundamental tension in fair machine learning research explored by Dwork et al. (2012) and subsequent scholarship.",
    "source_domain": "ai",
    "maturity_level": null,
    "authority_score": 0.95
  }
}