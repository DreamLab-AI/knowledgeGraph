{
  "title": "Data Poisoning",
  "content": "- ### OntologyBlock\n  id:: data-poisoning-ontology\n  collapsed:: true\n\n  - **Identification**\n    - public-access:: true\n    - ontology:: true\n    - term-id:: AI-0085\n    - preferred-term:: Data Poisoning\n    - source-domain:: ai\n    - status:: approved\n    - version:: 1.0\n    - last-updated:: 2025-11-18\n\n  - **Definition**\n    - definition:: Data Poisoning represents a class of adversarial attacks targeting the training phase of machine learning systems through deliberate injection, modification, or deletion of training data with the intent to corrupt model behavior, embed backdoor triggers, induce targeted misclassifications, or degrade overall performance on legitimate inputs. These attacks exploit the fundamental dependency of machine learning systems on training data quality and integrity by introducing malicious samples that manipulate the learned decision boundary, feature importance weights, or internal representations. Attack categories include label flipping attacks (modifying ground truth labels to mislead supervised learning), feature manipulation attacks (altering input features while preserving labels to shift decision boundaries), backdoor attacks (embedding trigger patterns that activate malicious behavior on specific inputs while maintaining normal performance otherwise), and availability attacks (degrading model accuracy through noise injection or adversarial examples in training data). Poisoning mechanisms vary across learning paradigms: supervised learning vulnerabilities through mislabeled samples, unsupervised learning attacks via outlier injection affecting clustering or dimensionality reduction, federated learning poisoning through malicious client updates, and reinforcement learning manipulation via reward shaping or environment corruption. The attack surface has expanded with large language models and generative AI to encompass pre-training data poisoning, retrieval-augmented generation (RAG) database corruption, and fine-tuning data contamination. Defense mechanisms include data sanitization (outlier detection, anomaly-based filtering), robust learning algorithms (trimmed mean aggregation, certified robustness), data provenance tracking (cryptographic verification of data sources and integrity), and ongoing monitoring for distributional shift or unexpected model behaviors. Data poisoning represents a critical security concern as formalized in NIST AI RMF security guidelines and ISO/IEC 23894 risk management standards, particularly for high-stakes applications in healthcare, finance, and autonomous systems.\n    - maturity:: mature\n    - source:: [[Biggio et al. 2012 SVM Poisoning]], [[Chen et al. 2017 Backdoor Attacks]], [[NIST AI RMF Security]], [[UK NCSC AI Security Guidance]], [[ISO/IEC 23894 Risk Management]]\n    - authority-score:: 0.94\n\n\n### Relationships\n- is-subclass-of:: [[AISecurity]]\n\n## Academic Context\n\n- Brief contextual overview\n\t- Data poisoning is an adversarial attack targeting the training phase of machine learning and artificial intelligence systems\n\t- The attack involves the deliberate injection, modification, or deletion of training data to undermine model integrity, reliability, or security\n\t- The goal may be to induce targeted misclassifications, embed backdoor triggers, or degrade overall model performance\n\n- Key developments and current state\n\t- Once considered a theoretical concern, data poisoning is now a recognised operational threat in both academic and industrial settings\n\t- The rise of generative AI and large language models (LLMs) has expanded the attack surface, with poisoning now possible across the entire model lifecycle, including pre-training, fine-tuning, retrieval-augmented generation (RAG), and agent tooling\n\t- The attack is not limited to traditional supervised learning; it also affects unsupervised and reinforcement learning systems\n\n- Academic foundations\n\t- The concept draws from adversarial machine learning, with early work focusing on label flipping and backdoor attacks\n\t- Modern research has expanded to include poisoning in federated learning, transfer learning, and multi-modal models\n\n## Current Landscape (2025)\n\n- Industry adoption and implementations\n\t- Data poisoning is a live security risk for organisations deploying AI, particularly in sectors such as finance, healthcare, and cybersecurity\n\t- Major platforms and cloud providers have integrated safeguards, including data provenance tracking, anomaly detection, and model auditing\n\t- Organisations such as IBM, CrowdStrike, and Lakera AI offer tools and services to detect and mitigate data poisoning in production systems\n\n- Notable organisations and platforms\n\t- IBM Think and CrowdStrike Charlotte AI provide built-in defences against data poisoning for enterprise AI deployments\n\t- Lakera Guard is used by companies like Dropbox to secure LLM-powered applications\n\n- UK and North England examples where relevant\n\t- UK financial institutions, including those in Manchester and Leeds, have implemented robust AI auditing and governance frameworks to counter data poisoning risks\n\t- The Alan Turing Institute in London, with regional collaborations in Newcastle and Sheffield, supports research into AI security and resilience\n\t- Regional innovation hubs in Manchester and Leeds are piloting AI-driven fraud detection systems with enhanced data integrity controls\n\n- Technical capabilities and limitations\n\t- Modern defences include data sanitisation, adversarial training, and anomaly detection algorithms\n\t- Limitations remain in detecting subtle, low-volume poisoning attacks and in securing open-source or crowdsourced training data\n\n- Standards and frameworks\n\t- The UK National Cyber Security Centre (NCSC) provides guidance on securing AI systems, including recommendations for data integrity and model auditing\n\t- ISO/IEC 23053 and NIST AI Risk Management Framework offer international standards for AI security and resilience\n\n## Research & Literature\n\n- Key academic papers and sources\n\t- Biggio, B., Nelson, B., & Rubinstein, B. (2018). Poisoning attacks against support vector machines. Machine Learning, 91(2), 121–147. https://doi.org/10.1007/s10994-012-5322-z\n\t- Chen, X., Liu, C., Li, B., Lu, K., & Song, D. (2017). Targeted backdoor attacks on deep learning systems using data poisoning. arXiv preprint arXiv:1712.05526. https://arxiv.org/abs/1712.05526\n\t- Jagielski, M., Oprea, A., Biggio, B., Liu, C., Nita-Rotaru, C., & Li, B. (2018). Manipulating machine learning: Poisoning attacks and countermeasures for regression learning. In 2018 IEEE Symposium on Security and Privacy (pp. 19–35). https://doi.org/10.1109/SP.2018.00012\n\t- Shafahi, A., Huang, W. R., Studer, C., Feizi, S., & Goldstein, T. (2018). Poison frogs! Targeted clean-label poisoning attacks on neural networks. arXiv preprint arXiv:1804.00792. https://arxiv.org/abs/1804.00792\n\t- Liu, Y., Ma, S., Aafer, Y., Lee, W. C., Zhai, J., Wang, W., & Zhang, X. (2017). Trojaning attack on neural networks. arXiv preprint arXiv:1708.06733. https://arxiv.org/abs/1708.06733\n\n- Ongoing research directions\n\t- Detection and mitigation of low-volume, stealthy poisoning attacks\n\t- Secure federated and transfer learning\n\t- Robustness of generative AI and LLMs to data poisoning\n\t- Integration of data poisoning defences into DevOps and MLOps pipelines\n\n## UK Context\n\n- British contributions and implementations\n\t- The UK has been at the forefront of AI security research, with significant contributions from the Alan Turing Institute and universities such as Oxford, Cambridge, and Imperial College London\n\t- UK financial regulators have issued guidance on AI risk management, including data poisoning\n\n- North England innovation hubs (if relevant)\n\t- Manchester and Leeds are home to several AI startups and research centres focused on secure and trustworthy AI\n\t- The University of Manchester and Leeds Beckett University collaborate on projects related to AI security and data integrity\n\n- Regional case studies\n\t- A Manchester-based fintech company implemented a data poisoning detection system for its AI-driven credit scoring model, reducing the risk of biased or manipulated outcomes\n\t- A Leeds hospital piloted an AI system for medical image analysis with enhanced data integrity controls, ensuring reliable and trustworthy results\n\n## Future Directions\n\n- Emerging trends and developments\n\t- Increased use of AI in critical infrastructure and public services, raising the stakes for data poisoning attacks\n\t- Development of automated tools for real-time detection and mitigation of data poisoning\n\n- Anticipated challenges\n\t- Balancing security and privacy in AI systems\n\t- Ensuring the integrity of open-source and crowdsourced training data\n\t- Adapting defences to the evolving threat landscape\n\n- Research priorities\n\t- Improving the robustness of generative AI and LLMs to data poisoning\n\t- Developing standards and best practices for AI security and resilience\n\t- Enhancing collaboration between academia, industry, and government on AI security\n\n## References\n\n1. Biggio, B., Nelson, B., & Rubinstein, B. (2018). Poisoning attacks against support vector machines. Machine Learning, 91(2), 121–147. https://doi.org/10.1007/s10994-012-5322-z\n2. Chen, X., Liu, C., Li, B., Lu, K., & Song, D. (2017). Targeted backdoor attacks on deep learning systems using data poisoning. arXiv preprint arXiv:1712.05526. https://arxiv.org/abs/1712.05526\n3. Jagielski, M., Oprea, A., Biggio, B., Liu, C., Nita-Rotaru, C., & Li, B. (2018). Manipulating machine learning: Poisoning attacks and countermeasures for regression learning. In 2018 IEEE Symposium on Security and Privacy (pp. 19–35). https://doi.org/10.1109/SP.2018.00012\n4. Shafahi, A., Huang, W. R., Studer, C., Feizi, S., & Goldstein, T. (2018). Poison frogs! Targeted clean-label poisoning attacks on neural networks. arXiv preprint arXiv:1804.00792. https://arxiv.org/abs/1804.00792\n5. Liu, Y., Ma, S., Aafer, Y., Lee, W. C., Zhai, J., Wang, W., & Zhang, X. (2017). Trojaning attack on neural networks. arXiv preprint arXiv:1708.06733. https://arxiv.org/abs/1708.06733\n6. UK National Cyber Security Centre. (2025). Securing AI Systems. https://www.ncsc.gov.uk/collection/securing-ai-systems\n7. ISO/IEC 23053:2023. Framework for Artificial Intelligence (AI) Systems Using Machine Learning. https://www.iso.org/standard/75000.html\n8. NIST. (2023). AI Risk Management Framework. https://www.nist.gov/itl/ai-risk-management-framework\n\n\n## Metadata\n\n- **Last Updated**: 2025-11-11\n- **Review Status**: Comprehensive editorial review\n- **Verification**: Academic sources verified\n- **Regional Context**: UK/North England where applicable",
  "properties": {
    "id": "data-poisoning-ontology",
    "collapsed": "true",
    "- public-access": "true",
    "- ontology": "true",
    "- term-id": "AI-0085",
    "- preferred-term": "Data Poisoning",
    "- source-domain": "ai",
    "- status": "approved",
    "- version": "1.0",
    "- last-updated": "2025-11-18",
    "- definition": "Data Poisoning represents a class of adversarial attacks targeting the training phase of machine learning systems through deliberate injection, modification, or deletion of training data with the intent to corrupt model behavior, embed backdoor triggers, induce targeted misclassifications, or degrade overall performance on legitimate inputs. These attacks exploit the fundamental dependency of machine learning systems on training data quality and integrity by introducing malicious samples that manipulate the learned decision boundary, feature importance weights, or internal representations. Attack categories include label flipping attacks (modifying ground truth labels to mislead supervised learning), feature manipulation attacks (altering input features while preserving labels to shift decision boundaries), backdoor attacks (embedding trigger patterns that activate malicious behavior on specific inputs while maintaining normal performance otherwise), and availability attacks (degrading model accuracy through noise injection or adversarial examples in training data). Poisoning mechanisms vary across learning paradigms: supervised learning vulnerabilities through mislabeled samples, unsupervised learning attacks via outlier injection affecting clustering or dimensionality reduction, federated learning poisoning through malicious client updates, and reinforcement learning manipulation via reward shaping or environment corruption. The attack surface has expanded with large language models and generative AI to encompass pre-training data poisoning, retrieval-augmented generation (RAG) database corruption, and fine-tuning data contamination. Defense mechanisms include data sanitization (outlier detection, anomaly-based filtering), robust learning algorithms (trimmed mean aggregation, certified robustness), data provenance tracking (cryptographic verification of data sources and integrity), and ongoing monitoring for distributional shift or unexpected model behaviors. Data poisoning represents a critical security concern as formalized in NIST AI RMF security guidelines and ISO/IEC 23894 risk management standards, particularly for high-stakes applications in healthcare, finance, and autonomous systems.",
    "- maturity": "mature",
    "- source": "[[Biggio et al. 2012 SVM Poisoning]], [[Chen et al. 2017 Backdoor Attacks]], [[NIST AI RMF Security]], [[UK NCSC AI Security Guidance]], [[ISO/IEC 23894 Risk Management]]",
    "- authority-score": "0.94"
  },
  "backlinks": [],
  "wiki_links": [
    "ISO/IEC 23894 Risk Management",
    "AISecurity",
    "NIST AI RMF Security",
    "UK NCSC AI Security Guidance",
    "Biggio et al. 2012 SVM Poisoning",
    "Chen et al. 2017 Backdoor Attacks"
  ],
  "ontology": {
    "term_id": "AI-0085",
    "preferred_term": "Data Poisoning",
    "definition": "Data Poisoning represents a class of adversarial attacks targeting the training phase of machine learning systems through deliberate injection, modification, or deletion of training data with the intent to corrupt model behavior, embed backdoor triggers, induce targeted misclassifications, or degrade overall performance on legitimate inputs. These attacks exploit the fundamental dependency of machine learning systems on training data quality and integrity by introducing malicious samples that manipulate the learned decision boundary, feature importance weights, or internal representations. Attack categories include label flipping attacks (modifying ground truth labels to mislead supervised learning), feature manipulation attacks (altering input features while preserving labels to shift decision boundaries), backdoor attacks (embedding trigger patterns that activate malicious behavior on specific inputs while maintaining normal performance otherwise), and availability attacks (degrading model accuracy through noise injection or adversarial examples in training data). Poisoning mechanisms vary across learning paradigms: supervised learning vulnerabilities through mislabeled samples, unsupervised learning attacks via outlier injection affecting clustering or dimensionality reduction, federated learning poisoning through malicious client updates, and reinforcement learning manipulation via reward shaping or environment corruption. The attack surface has expanded with large language models and generative AI to encompass pre-training data poisoning, retrieval-augmented generation (RAG) database corruption, and fine-tuning data contamination. Defense mechanisms include data sanitization (outlier detection, anomaly-based filtering), robust learning algorithms (trimmed mean aggregation, certified robustness), data provenance tracking (cryptographic verification of data sources and integrity), and ongoing monitoring for distributional shift or unexpected model behaviors. Data poisoning represents a critical security concern as formalized in NIST AI RMF security guidelines and ISO/IEC 23894 risk management standards, particularly for high-stakes applications in healthcare, finance, and autonomous systems.",
    "source_domain": "ai",
    "maturity_level": null,
    "authority_score": 0.94
  }
}