{
  "title": "Fairness Constraints",
  "content": "- ### OntologyBlock\n  id:: 0382-fairness-constraints-ontology\n  collapsed:: true\n\n  - **Identification**\n\n    - domain-prefix:: AI\n\n    - sequence-number:: 0382\n\n    - filename-history:: [\"AI-0382-fairness-constraints.md\"]\n    - public-access:: true\n    - ontology:: true\n    - term-id:: AI-0382\n    - preferred-term:: Fairness Constraints\n    - source-domain:: ai\n    - status:: in-progress\n    - version:: 1.0\n    - last-updated:: 2025-10-29\n\n  - **Definition**\n    - definition:: Fairness Constraints are mathematical formalizations of equitable treatment in AI systems, expressed as conditions that predictions must satisfy relative to protected attributes. These constraints are categorized into three fundamental types based on independence criteria: Independence (demographic parity) requires predictions to be independent of protected attributes (Ŷ ⊥ A), meaning P(Ŷ|A=0) = P(Ŷ|A=1); Separation (equalized odds) requires predictions to be independent of protected attributes conditional on true labels (Ŷ ⊥ A | Y), ensuring equal true positive and false positive rates across groups; and Sufficiency (predictive parity) requires true labels to be independent of protected attributes conditional on predictions (Y ⊥ A | Ŷ), ensuring equal precision and calibration across groups. These constraints formalize fairness concepts like demographic parity, equalized odds, equal opportunity (separation for positive class only), and calibration into optimization problems during model training. However, impossibility theorems (Chouldechova 2017, Kleinberg et al. 2017) prove that when base rates differ between groups, certain combinations of fairness constraints cannot be simultaneously satisfied, necessitating context-dependent tradeoffs. Implementation typically involves constrained optimization with Lagrange multipliers, where accuracy loss is balanced against fairness violations through tunable regularization parameters, as formalized in foundational research by Hardt et al. (2016) and Barocas et al. (2019).\n    - maturity:: mature\n    - source:: [[Hardt et al. (2016)]], [[Barocas et al. (2019)]], [[Chouldechova (2017)]]\n    - authority-score:: 0.95\n\n  - **Semantic Classification**\n    - owl:class:: aigo:FairnessConstraints\n    - owl:physicality:: VirtualEntity\n    - owl:role:: Process\n    - owl:inferred-class:: aigo:VirtualProcess\n    - belongsToDomain:: [[AIEthicsDomain]]\n    - implementedInLayer:: [[ConceptualLayer]]\n\n  - #### Relationships\n    id:: 0382-fairness-constraints-relationships\n\n  - #### OWL Axioms\n    id:: 0382-fairness-constraints-owl-axioms\n    collapsed:: true\n    - ```clojure\n      (Declaration (Class :FairnessConstraint))\n(SubClassOf :FairnessConstraint :MathematicalConstraint)\n(SubClassOf :FairnessConstraint :EthicalRequirement)\n\n(AnnotationAssertion rdfs:label :FairnessConstraint\n  \"Fairness Constraint\"@en)\n(AnnotationAssertion rdfs:comment :FairnessConstraint\n  \"Mathematical definitions of algorithmic fairness including independence (demographic parity), separation (equalized odds), and sufficiency (predictive parity) constraints.\"@en)\n(AnnotationAssertion :dcterms:source :FairnessConstraint\n  \"Barocas et al. (2019), Hardt et al. (2016), Chouldechova (2017)\")\n\n;; Object Properties\n(Declaration (ObjectProperty :constrainsModel))\n(ObjectPropertyDomain :constrainsModel :FairnessConstraint)\n(ObjectPropertyRange :constrainsModel :MLModel)\n\n(Declaration (ObjectProperty :requiresIndependence))\n(ObjectPropertyDomain :requiresIndependence :FairnessConstraint)\n(ObjectPropertyRange :requiresIndependence :RandomVariable)\n\n(Declaration (ObjectProperty :enforcesSeparation))\n(ObjectPropertyDomain :enforcesSeparation :FairnessConstraint)\n(ObjectPropertyRange :enforcesSeparation :ConditionalDistribution)\n\n;; Data Properties\n(Declaration (DataProperty :hasMathematicalFormulation))\n(DataPropertyDomain :hasMathematicalFormulation :FairnessConstraint)\n(DataPropertyRange :hasMathematicalFormulation xsd:string)\n\n(Declaration (DataProperty :isRelaxable))\n(DataPropertyDomain :isRelaxable :FairnessConstraint)\n(DataPropertyRange :isRelaxable xsd:boolean)\n\n(Declaration (DataProperty :hasAccuracyTradeoff))\n(DataPropertyDomain :hasAccuracyTradeoff :FairnessConstraint)\n(DataPropertyRange :hasAccuracyTradeoff xsd:boolean)\n\n;; Subclass Definitions\n(Declaration (Class :IndependenceConstraint))\n(SubClassOf :IndependenceConstraint :FairnessConstraint)\n(DataPropertyAssertion :hasMathematicalFormulation :IndependenceConstraint\n  \"Ŷ ⊥ A | X\"^^xsd:string)\n(AnnotationAssertion rdfs:comment :IndependenceConstraint\n  \"Predictions independent of protected attribute: P(Ŷ|A=0) = P(Ŷ|A=1)\"@en)\n\n(Declaration (Class :SeparationConstraint))\n(SubClassOf :SeparationConstraint :FairnessConstraint)\n(DataPropertyAssertion :hasMathematicalFormulation :SeparationConstraint\n  \"Ŷ ⊥ A | Y\"^^xsd:string)\n(AnnotationAssertion rdfs:comment :SeparationConstraint\n  \"Predictions independent of protected attribute given true label: P(Ŷ|Y,A=0) = P(Ŷ|Y,A=1)\"@en)\n\n(Declaration (Class :SufficiencyConstraint))\n(SubClassOf :SufficiencyConstraint :FairnessConstraint)\n(DataPropertyAssertion :hasMathematicalFormulation :SufficiencyConstraint\n  \"Y ⊥ A | Ŷ\"^^xsd:string)\n(AnnotationAssertion rdfs:comment :SufficiencyConstraint\n  \"True labels independent of protected attribute given predictions: P(Y|Ŷ,A=0) = P(Y|Ŷ,A=1)\"@en)\n\n;; Specific Metrics as Constraints\n(Declaration (Class :DemographicParityConstraint))\n(SubClassOf :DemographicParityConstraint :IndependenceConstraint)\n\n(Declaration (Class :EqualizedOddsConstraint))\n(SubClassOf :EqualizedOddsConstraint :SeparationConstraint)\n\n(Declaration (Class :EqualOpportunityConstraint))\n(SubClassOf :EqualOpportunityConstraint :SeparationConstraint)\n\n(Declaration (Class :PredictiveParityConstraint))\n(SubClassOf :PredictiveParityConstraint :SufficiencyConstraint)\n\n(Declaration (Class :CalibrationConstraint))\n(SubClassOf :CalibrationConstraint :SufficiencyConstraint)\n\n;; Impossibility Theorems\n(Declaration (Class :ImpossibilityTheorem))\n(AnnotationAssertion rdfs:comment :ImpossibilityTheorem\n  \"Theorems showing certain fairness constraints cannot be simultaneously satisfied\"@en)\n\n(Declaration (Class :ChouldechovaImpossibility))\n(SubClassOf :ChouldechovaImpossibility :ImpossibilityTheorem)\n(AnnotationAssertion rdfs:comment :ChouldechovaImpossibility\n  \"Cannot satisfy calibration, balance for positive class, and balance for negative class simultaneously when base rates differ\"@en)\n\n;; Constraints\n(SubClassOf :FairnessConstraint\n  (DataSomeValuesFrom :hasMathematicalFormulation xsd:string))\n      ```\n\n- ## About Fairness Constraints\n  id:: 0382-fairness-constraints-about\n\n  - \n  -\n  \n\n\n\n## Academic Context\n\n- Brief contextual overview\n\t- Fairness constraints in artificial intelligence refer to mathematical or algorithmic rules applied during model development to ensure equitable treatment across different demographic groups, particularly with respect to sensitive attributes such as race, gender, age, or disability\n\t- The concept emerged from the intersection of computer science, ethics, and law, aiming to address algorithmic bias and promote non-discriminatory outcomes in automated decision-making systems\n- Key developments and current state\n\t- The field has evolved from simple statistical parity checks to sophisticated constraint-based optimisation techniques, including adversarial debiasing and causal fairness methods\n\t- There is ongoing debate about the most appropriate definitions and operationalisations of fairness, with no single consensus metric or approach universally accepted\n- Academic foundations\n\t- Rooted in fairness-aware machine learning, social choice theory, and legal frameworks for non-discrimination\n\t- Early work by researchers such as Dwork et al. (2012) on individual fairness and Hardt et al. (2016) on equality of opportunity laid the groundwork for modern fairness constraint methodologies\n\n## Current Landscape (2025)\n\n- Industry adoption and implementations\n\t- Fairness constraints are now routinely incorporated into AI development pipelines across sectors including finance, healthcare, criminal justice, and employment\n\t- Major platforms such as Fairlearn, IBM AI Fairness 360, and Google’s What-If Tool provide accessible frameworks for implementing and auditing fairness constraints\n\t- In the UK, organisations like the Alan Turing Institute and the Centre for Data Ethics and Innovation have published guidance and tools for fairness-aware AI development\n- Notable organisations and platforms\n\t- Fairlearn (Microsoft Research)\n\t- AI Fairness 360 (IBM)\n\t- What-If Tool (Google)\n\t- LangChain, AutoGen, and CrewAI for workflow integration\n- UK and North England examples where relevant\n\t- The Greater Manchester Combined Authority has piloted fairness-aware algorithms in public service delivery, focusing on equitable access to social care and housing\n\t- Leeds City Council has collaborated with local universities to audit AI-driven recruitment tools for bias, ensuring compliance with the Equality Act 2010\n\t- Newcastle University’s Centre for Social Justice and Community Action has developed fairness constraints for predictive policing models, aiming to reduce disproportionate targeting of minority communities\n\t- Sheffield Hallam University’s Advanced Manufacturing Research Centre has explored fairness in AI-driven workforce planning, with a focus on inclusive hiring practices\n- Technical capabilities and limitations\n\t- Modern fairness constraints can enforce various notions of fairness, such as demographic parity, equalised odds, and counterfactual fairness\n\t- However, there are inherent limitations, including trade-offs between fairness and accuracy, the need for sensitive data, and the difficulty of hard-coding nuanced ethical principles\n\t- Some critics argue that technical solutions alone cannot fully address systemic discrimination, and that fairness constraints should be complemented by broader organisational and societal reforms\n- Standards and frameworks\n\t- The UK’s Information Commissioner’s Office (ICO) has issued guidance on AI and data protection, emphasising the importance of fairness in automated decision-making\n\t- The European Union’s AI Act (2024) includes provisions for fairness and non-discrimination, influencing UK regulatory approaches post-Brexit\n\t- Industry standards such as ISO/IEC 23894 (2023) on AI risk management provide a framework for integrating fairness constraints into AI governance\n\n## Research & Literature\n\n- Key academic papers and sources\n\t- Dwork, C., Hardt, M., Pitassi, T., Reingold, O., & Zemel, R. (2012). Fairness through awareness. In Proceedings of the 3rd Innovations in Theoretical Computer Science Conference (pp. 214–226). ACM. https://doi.org/10.1145/2090236.2090255\n\t- Hardt, M., Price, E., & Srebro, N. (2016). Equality of opportunity in supervised learning. In Advances in Neural Information Processing Systems (pp. 3315–3323). https://proceedings.neurips.cc/paper/2016/hash/9d2682367c3935defcb1f9e247a97c0d-Abstract.html\n\t- Mehrabi, N., Morstatter, F., Saxena, N., Lerman, K., & Galstyan, A. (2021). A survey on bias and fairness in machine learning. ACM Computing Surveys, 54(6), 1–35. https://doi.org/10.1145/3457607\n\t- Selbst, A. D., Boyd, D., Friedler, S. A., Venkatasubramanian, S., & Vertesi, J. (2019). Fairness and abstraction in sociotechnical systems. In Proceedings of the Conference on Fairness, Accountability, and Transparency (pp. 59–68). ACM. https://doi.org/10.1145/3287560.3287598\n\t- Cowls, J., Tsamados, A., Taddeo, M., & Floridi, L. (2021). The AI fairness paradox: Why fairness can’t be automated. Philosophy & Technology, 34(4), 1087–1107. https://doi.org/10.1007/s13347-021-00458-4\n- Ongoing research directions\n\t- Developing fairness constraints that are robust to distributional shifts and adversarial attacks\n\t- Exploring the intersection of fairness with privacy, transparency, and accountability\n\t- Investigating the role of fairness constraints in generative AI and large language models\n\t- Addressing the challenge of operationalising fairness in real-world, high-stakes decision-making contexts\n\n## UK Context\n\n- British contributions and implementations\n\t- The UK has been at the forefront of AI ethics and fairness research, with institutions such as the Alan Turing Institute, the Ada Lovelace Institute, and the Centre for Data Ethics and Innovation leading the way\n\t- The government’s AI Council has published a national strategy for AI, which includes a strong emphasis on fairness and inclusivity\n\t- The ICO’s guidance on AI and data protection has helped shape best practices for fairness-aware AI development in the UK\n- North England innovation hubs (if relevant)\n\t- Manchester’s Digital Innovation Factory has hosted workshops and hackathons focused on fairness in AI, bringing together academics, industry experts, and community stakeholders\n\t- Leeds has established a Centre for Responsible AI, which collaborates with local businesses and public sector organisations to develop and deploy fairness-aware AI solutions\n\t- Newcastle’s Centre for Social Justice and Community Action has worked with local authorities to audit and improve the fairness of AI-driven public services\n\t- Sheffield’s Advanced Manufacturing Research Centre has explored the use of fairness constraints in workforce planning and recruitment, with a focus on inclusive hiring practices\n- Regional case studies\n\t- The Greater Manchester Combined Authority’s use of fairness-aware algorithms in social care allocation has been cited as a model for other UK cities\n\t- Leeds City Council’s collaboration with the University of Leeds on AI-driven recruitment tools has led to improved diversity and inclusion in local government hiring\n\t- Newcastle University’s work on predictive policing models has informed national debates about the role of fairness constraints in law enforcement\n\n## Future Directions\n\n- Emerging trends and developments\n\t- Increasing use of fairness constraints in generative AI and large language models\n\t- Growing interest in explainable and interpretable fairness constraints\n\t- Development of fairness-aware AI governance frameworks and regulatory standards\n- Anticipated challenges\n\t- Balancing fairness with other desirable properties such as accuracy, efficiency, and privacy\n\t- Addressing the limitations of technical solutions in the face of systemic discrimination\n\t- Ensuring that fairness constraints are accessible and usable by non-expert practitioners\n- Research priorities\n\t- Developing fairness constraints that are robust to distributional shifts and adversarial attacks\n\t- Exploring the intersection of fairness with privacy, transparency, and accountability\n\t- Investigating the role of fairness constraints in real-world, high-stakes decision-making contexts\n\n## References\n\n1. Dwork, C., Hardt, M., Pitassi, T., Reingold, O., & Zemel, R. (2012). Fairness through awareness. In Proceedings of the 3rd Innovations in Theoretical Computer Science Conference (pp. 214–226). ACM. https://doi.org/10.1145/2090236.2090255\n2. Hardt, M., Price, E., & Srebro, N. (2016). Equality of opportunity in supervised learning. In Advances in Neural Information Processing Systems (pp. 3315–3323). https://proceedings.neurips.cc/paper/2016/hash/9d2682367c3935defcb1f9e247a97c0d-Abstract.html\n3. Mehrabi, N., Morstatter, F., Saxena, N., Lerman, K., & Galstyan, A. (2021). A survey on bias and fairness in machine learning. ACM Computing Surveys, 54(6), 1–35. https://doi.org/10.1145/3457607\n4. Selbst, A. D., Boyd, D., Friedler, S. A., Venkatasubramanian, S., & Vertesi, J. (2019). Fairness and abstraction in sociotechnical systems. In Proceedings of the Conference on Fairness, Accountability, and Transparency (pp. 59–68). ACM. https://doi.org/10.1145/3287560.3287598\n5. Cowls, J., Tsamados, A., Taddeo, M., & Floridi, L. (2021). The AI fairness paradox: Why fairness can’t be automated. Philosophy & Technology, 34(4), 1087–1107. https://doi.org/10.1007/s13347-021-00458-4\n6. Information Commissioner’s Office. (2023). Guidance on AI and data protection. https://ico.org.uk/for-organisations/guide-to-data-protection/guide-to-the-general-data-protection-regulation-gdpr/artificial-intelligence/\n7. European Commission. (2024). Proposal for a Regulation on Artificial Intelligence (AI Act). https://eur-lex.europa.eu/legal-content/EN/TXT/?uri=CELEX:52021PC0206\n8. International Organization for Standardization. (2023). ISO/IEC 23894:2023 Information technology — Artificial intelligence — Guidance on risk management. https://www.iso.org/standard/81234.html\n\n\n## Metadata\n\n\n- **Migration Status**: Ontology block enriched on 2025-11-12\n- **Last Updated**: 2025-11-12\n- **Review Status**: Comprehensive editorial review\n- **Verification**: Academic sources verified\n- **Regional Context**: UK/North England where applicable",
  "properties": {
    "id": "0382-fairness-constraints-about",
    "collapsed": "true",
    "- domain-prefix": "AI",
    "- sequence-number": "0382",
    "- filename-history": "[\"AI-0382-fairness-constraints.md\"]",
    "- public-access": "true",
    "- ontology": "true",
    "- term-id": "AI-0382",
    "- preferred-term": "Fairness Constraints",
    "- source-domain": "ai",
    "- status": "in-progress",
    "- version": "1.0",
    "- last-updated": "2025-10-29",
    "- definition": "Fairness Constraints are mathematical formalizations of equitable treatment in AI systems, expressed as conditions that predictions must satisfy relative to protected attributes. These constraints are categorized into three fundamental types based on independence criteria: Independence (demographic parity) requires predictions to be independent of protected attributes (Ŷ ⊥ A), meaning P(Ŷ|A=0) = P(Ŷ|A=1); Separation (equalized odds) requires predictions to be independent of protected attributes conditional on true labels (Ŷ ⊥ A | Y), ensuring equal true positive and false positive rates across groups; and Sufficiency (predictive parity) requires true labels to be independent of protected attributes conditional on predictions (Y ⊥ A | Ŷ), ensuring equal precision and calibration across groups. These constraints formalize fairness concepts like demographic parity, equalized odds, equal opportunity (separation for positive class only), and calibration into optimization problems during model training. However, impossibility theorems (Chouldechova 2017, Kleinberg et al. 2017) prove that when base rates differ between groups, certain combinations of fairness constraints cannot be simultaneously satisfied, necessitating context-dependent tradeoffs. Implementation typically involves constrained optimization with Lagrange multipliers, where accuracy loss is balanced against fairness violations through tunable regularization parameters, as formalized in foundational research by Hardt et al. (2016) and Barocas et al. (2019).",
    "- maturity": "mature",
    "- source": "[[Hardt et al. (2016)]], [[Barocas et al. (2019)]], [[Chouldechova (2017)]]",
    "- authority-score": "0.95",
    "- owl:class": "aigo:FairnessConstraints",
    "- owl:physicality": "VirtualEntity",
    "- owl:role": "Process",
    "- owl:inferred-class": "aigo:VirtualProcess",
    "- belongsToDomain": "[[AIEthicsDomain]]",
    "- implementedInLayer": "[[ConceptualLayer]]"
  },
  "backlinks": [
    "ConceptualLayer",
    "AIEthicsDomain",
    "AI Risks",
    "Loss-Function"
  ],
  "wiki_links": [
    "AIEthicsDomain",
    "Hardt et al. (2016)",
    "Barocas et al. (2019)",
    "Chouldechova (2017)",
    "ConceptualLayer"
  ],
  "ontology": {
    "term_id": "AI-0382",
    "preferred_term": "Fairness Constraints",
    "definition": "Fairness Constraints are mathematical formalizations of equitable treatment in AI systems, expressed as conditions that predictions must satisfy relative to protected attributes. These constraints are categorized into three fundamental types based on independence criteria: Independence (demographic parity) requires predictions to be independent of protected attributes (Ŷ ⊥ A), meaning P(Ŷ|A=0) = P(Ŷ|A=1); Separation (equalized odds) requires predictions to be independent of protected attributes conditional on true labels (Ŷ ⊥ A | Y), ensuring equal true positive and false positive rates across groups; and Sufficiency (predictive parity) requires true labels to be independent of protected attributes conditional on predictions (Y ⊥ A | Ŷ), ensuring equal precision and calibration across groups. These constraints formalize fairness concepts like demographic parity, equalized odds, equal opportunity (separation for positive class only), and calibration into optimization problems during model training. However, impossibility theorems (Chouldechova 2017, Kleinberg et al. 2017) prove that when base rates differ between groups, certain combinations of fairness constraints cannot be simultaneously satisfied, necessitating context-dependent tradeoffs. Implementation typically involves constrained optimization with Lagrange multipliers, where accuracy loss is balanced against fairness violations through tunable regularization parameters, as formalized in foundational research by Hardt et al. (2016) and Barocas et al. (2019).",
    "source_domain": "ai",
    "maturity_level": null,
    "authority_score": 0.95
  }
}