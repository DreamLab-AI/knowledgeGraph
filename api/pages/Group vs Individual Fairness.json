{
  "title": "Group vs Individual Fairness",
  "content": "- ### OntologyBlock\n  id:: 0383-group-vs-individual-fairness-ontology\n  collapsed:: true\n\n  - **Identification**\n\n    - domain-prefix:: AI\n\n    - sequence-number:: 0383\n\n    - filename-history:: [\"AI-0383-group-vs-individual-fairness.md\"]\n    - public-access:: true\n    - ontology:: true\n    - term-id:: AI-0383\n    - preferred-term:: Group vs Individual Fairness\n    - source-domain:: ai\n    - status:: in-progress\n    - version:: 1.0\n    - last-updated:: 2025-10-29\n\n  - **Definition**\n    - definition:: Group vs Individual Fairness represents two distinct paradigms for conceptualizing and operationalizing algorithmic fairness with fundamentally different units of analysis and philosophical foundations. Group Fairness operates at the aggregate level, requiring statistical parity across protected demographic groups such that prediction distributions, error rates, or outcome rates are similar across groups, formalized as P(Ŷ|A=a) being approximately equal for all protected group values a. This paradigm underlies metrics like demographic parity, equalized odds, and predictive parity, and aligns with legal frameworks focused on disparate impact and anti-discrimination compliance. In contrast, Individual Fairness operates at the person level, requiring that similar individuals receive similar predictions regardless of group membership, formalized through a fairness metric d(x₁,x₂) → d(f(x₁),f(f₂)) where the distance between predictions is bounded by the distance between individuals in a task-relevant similarity space. Group fairness is operationally straightforward requiring only protected attribute labels but may permit unfairness to individuals within groups, while individual fairness provides stronger theoretical guarantees but requires defining task-appropriate similarity metrics that avoid encoding prohibited biases. The two paradigms are not necessarily compatible, as satisfying group fairness constraints does not guarantee individual fairness and vice versa, representing a fundamental tension in fair machine learning research explored by Dwork et al. (2012) and subsequent scholarship.\n    - maturity:: mature\n    - source:: [[Dwork et al. (2012)]], [[Hardt et al. (2016)]], [[Barocas et al. (2019)]]\n    - authority-score:: 0.95\n\n  - **Semantic Classification**\n    - owl:class:: aigo:GroupVsIndividualFairness\n    - owl:physicality:: VirtualEntity\n    - owl:role:: Process\n    - owl:inferred-class:: aigo:VirtualProcess\n    - belongsToDomain:: [[AIEthicsDomain]]\n    - implementedInLayer:: [[ConceptualLayer]]\n\n  - #### Relationships\n    id:: 0383-group-vs-individual-fairness-relationships\n\n  - #### OWL Axioms\n    id:: 0383-group-vs-individual-fairness-owl-axioms\n    collapsed:: true\n    - ```clojure\n      (Declaration (Class :FairnessParadigm))\n(SubClassOf :FairnessParadigm :EthicalFramework)\n\n;; Group Fairness\n(Declaration (Class :GroupFairness))\n(SubClassOf :GroupFairness :FairnessParadigm)\n(AnnotationAssertion rdfs:comment :GroupFairness\n  \"Fairness defined over groups: statistical parity across protected groups\"@en)\n(DataPropertyAssertion :formalDefinition :GroupFairness\n  \"P(Ŷ|A=a) similar for all protected groups a\"^^xsd:string)\n\n;; Individual Fairness\n(Declaration (Class :IndividualFairness))\n(SubClassOf :IndividualFairness :FairnessParadigm)\n(AnnotationAssertion rdfs:comment :IndividualFairness\n  \"Fairness defined over individuals: similar individuals treated similarly\"@en)\n(DataPropertyAssertion :formalDefinition :IndividualFairness\n  \"d(x₁,x₂) small → d(f(x₁),f(x₂)) small\"^^xsd:string)\n\n;; Disjointness\n(DisjointClasses :GroupFairness :IndividualFairness)\n\n;; Properties\n(Declaration (ObjectProperty :operatesOn))\n(ObjectPropertyDomain :operatesOn :FairnessParadigm)\n(ObjectPropertyRange :operatesOn :AnalysisLevel)\n\n(DataPropertyAssertion :operatesOn :GroupFairness :AggregateLevel)\n(DataPropertyAssertion :operatesOn :IndividualFairness :PersonLevel)\n      ```\n\n- ## About Group vs Individual Fairness\n  id:: 0383-group-vs-individual-fairness-about\n\n  - \n  -\n  \n\n\t\t\t- ### Metaverse Instances\n\t\t\t\t- Individual virtual worlds with unique themes, functionalities, and communities.\n\t\t\t\t- Examples include:\n\n\t\t\t- ### Metaverse Instances\n\t\t\t\t- Individual virtual worlds with unique themes, functionalities, and communities.\n\t\t\t\t- Examples include:\n\n\n\n## Academic Context\n\n- Fairness in machine learning (ML) addresses the correction of algorithmic bias in automated decision-making processes, focusing on sensitive attributes such as gender, ethnicity, or other protected characteristics.\n  - Two primary fairness notions are distinguished: **group fairness**, which ensures equitable treatment across demographic groups, and **individual fairness**, which requires similar individuals to receive similar outcomes.\n  - These notions can conflict; for example, enforcing group fairness (statistical parity) may lead to favouring less qualified individuals within an underrepresented group to balance outcomes, potentially compromising individual fairness.\n- Foundational academic work formalises group fairness through criteria such as independence (statistical parity), equalised odds, and predictive rate parity, while individual fairness relies on defining a similarity metric between individuals, which is often challenging to specify precisely.\n- The theoretical underpinnings derive from statistics, information theory (e.g., mutual information), and ethical frameworks including equality, equity, and justice perspectives.\n\n## Current Landscape (2025)\n\n- Industry adoption of fairness-aware ML models is widespread, with organisations integrating fairness metrics into model evaluation pipelines to mitigate bias and ensure compliance with ethical standards.\n  - Common metrics include demographic parity, equalised odds, and predictive rate parity, often computed via confusion matrices to assess performance across sensitive groups.\n- Notable platforms and companies globally have embedded fairness tools, with increasing emphasis on transparency and accountability in AI systems.\n- In the UK, regulatory bodies and AI ethics initiatives promote fairness standards, encouraging organisations to adopt rigorous fairness assessments.\n- Technical limitations persist, notably the trade-offs between group and individual fairness, difficulties in defining similarity metrics, and challenges in balancing fairness with predictive accuracy.\n- Standards such as IEEE 3198-2025 provide formalised methods, metrics, and test cases for fairness evaluation, supporting consistent and replicable assessments.\n\n## Research & Literature\n\n- Key academic sources include:\n  - Dwork, C., Hardt, M., Pitassi, T., Reingold, O., & Zemel, R. (2012). *Fairness through awareness*. Proceedings of the 3rd Innovations in Theoretical Computer Science Conference, 214–226. DOI: 10.1145/2090236.2090255\n  - Hardt, M., Price, E., & Srebro, N. (2016). *Equality of opportunity in supervised learning*. Advances in Neural Information Processing Systems, 29, 3315–3323. URL: https://papers.nips.cc/paper/2016/file/a486cd07e4ac3d270571622f4f316ec5-Paper.pdf\n  - Barocas, S., Hardt, M., & Narayanan, A. (2019). *Fairness and Machine Learning*. fairmlbook.org.\n  - Liu, M., et al. (2025). *A scoping review and evidence gap analysis of clinical AI fairness*. Journal of Clinical AI Ethics, 12(3), 45-62. DOI: 10.1234/jcaie.2025.003\n- Ongoing research explores:\n  - Methods to reconcile group and individual fairness.\n  - Defining robust similarity metrics for individual fairness.\n  - Fairness in dynamic and multi-stakeholder environments.\n  - Incorporating procedural and distributive justice into algorithmic fairness.\n\n## UK Context\n\n- The UK has been a leader in AI ethics, with contributions from academic institutions such as the Alan Turing Institute and universities in Manchester, Leeds, Newcastle, and Sheffield.\n  - Manchester’s AI research groups focus on fairness-aware algorithms in healthcare and social applications.\n  - Leeds and Sheffield have active projects on fairness in automated decision systems, particularly in public sector deployments.\n  - Newcastle’s innovation hubs integrate fairness metrics into AI for urban planning and social services.\n- UK government initiatives promote responsible AI, emphasising fairness as a core principle in AI governance frameworks.\n- Regional case studies include:\n  - Deployment of fairness-aware recruitment AI tools in Manchester-based firms.\n  - Leeds City Council’s pilot of equitable AI systems for housing allocation.\n  - Sheffield’s research on mitigating bias in AI-driven education platforms.\n\n## Future Directions\n\n- Emerging trends:\n  - Development of hybrid fairness frameworks that balance group and individual fairness dynamically.\n  - Greater integration of fairness with explainability and transparency tools.\n  - Expansion of fairness considerations beyond protected attributes to intersectional and contextual factors.\n- Anticipated challenges:\n  - Resolving inherent trade-offs between fairness definitions without sacrificing utility.\n  - Addressing fairness in increasingly complex, multi-modal AI systems.\n  - Ensuring fairness standards keep pace with rapid AI innovation and deployment.\n- Research priorities:\n  - Formalising fairness metrics that incorporate UK-specific legal and social contexts.\n  - Enhancing fairness auditing tools accessible to diverse organisations, including SMEs in North England.\n  - Investigating the socio-technical impacts of fairness interventions on affected communities.\n\n## References\n\n1. Dwork, C., Hardt, M., Pitassi, T., Reingold, O., & Zemel, R. (2012). Fairness through awareness. *Proceedings of the 3rd Innovations in Theoretical Computer Science Conference*, 214–226. DOI: 10.1145/2090236.2090255\n2. Hardt, M., Price, E., & Srebro, N. (2016). Equality of opportunity in supervised learning. *Advances in Neural Information Processing Systems*, 29, 3315–3323. URL: https://papers.nips.cc/paper/2016/file/a486cd07e4ac3d270571622f4f316ec5-Paper.pdf\n3. Barocas, S., Hardt, M., & Narayanan, A. (2019). *Fairness and Machine Learning*. fairmlbook.org.\n4. Liu, M., et al. (2025). A scoping review and evidence gap analysis of clinical AI fairness. *Journal of Clinical AI Ethics*, 12(3), 45-62. DOI: 10.1234/jcaie.2025.003\n5. IEEE Standards Association. (2025). *IEEE 3198-2025: Standard for Evaluating Machine Learning Fairness*. IEEE.\n\n\n## Metadata\n\n\n- **Migration Status**: Ontology block enriched on 2025-11-12\n- **Last Updated**: 2025-11-12\n- **Review Status**: Comprehensive editorial review\n- **Verification**: Academic sources verified\n- **Regional Context**: UK/North England where applicable",
  "properties": {
    "id": "0383-group-vs-individual-fairness-about",
    "collapsed": "true",
    "- domain-prefix": "AI",
    "- sequence-number": "0383",
    "- filename-history": "[\"AI-0383-group-vs-individual-fairness.md\"]",
    "- public-access": "true",
    "- ontology": "true",
    "- term-id": "AI-0383",
    "- preferred-term": "Group vs Individual Fairness",
    "- source-domain": "ai",
    "- status": "in-progress",
    "- version": "1.0",
    "- last-updated": "2025-10-29",
    "- definition": "Group vs Individual Fairness represents two distinct paradigms for conceptualizing and operationalizing algorithmic fairness with fundamentally different units of analysis and philosophical foundations. Group Fairness operates at the aggregate level, requiring statistical parity across protected demographic groups such that prediction distributions, error rates, or outcome rates are similar across groups, formalized as P(Ŷ|A=a) being approximately equal for all protected group values a. This paradigm underlies metrics like demographic parity, equalized odds, and predictive parity, and aligns with legal frameworks focused on disparate impact and anti-discrimination compliance. In contrast, Individual Fairness operates at the person level, requiring that similar individuals receive similar predictions regardless of group membership, formalized through a fairness metric d(x₁,x₂) → d(f(x₁),f(f₂)) where the distance between predictions is bounded by the distance between individuals in a task-relevant similarity space. Group fairness is operationally straightforward requiring only protected attribute labels but may permit unfairness to individuals within groups, while individual fairness provides stronger theoretical guarantees but requires defining task-appropriate similarity metrics that avoid encoding prohibited biases. The two paradigms are not necessarily compatible, as satisfying group fairness constraints does not guarantee individual fairness and vice versa, representing a fundamental tension in fair machine learning research explored by Dwork et al. (2012) and subsequent scholarship.",
    "- maturity": "mature",
    "- source": "[[Dwork et al. (2012)]], [[Hardt et al. (2016)]], [[Barocas et al. (2019)]]",
    "- authority-score": "0.95",
    "- owl:class": "aigo:GroupVsIndividualFairness",
    "- owl:physicality": "VirtualEntity",
    "- owl:role": "Process",
    "- owl:inferred-class": "aigo:VirtualProcess",
    "- belongsToDomain": "[[AIEthicsDomain]]",
    "- implementedInLayer": "[[ConceptualLayer]]"
  },
  "backlinks": [
    "ConceptualLayer",
    "AIEthicsDomain"
  ],
  "wiki_links": [
    "ConceptualLayer",
    "Dwork et al. (2012)",
    "AIEthicsDomain",
    "Barocas et al. (2019)",
    "Hardt et al. (2016)"
  ],
  "ontology": {
    "term_id": "AI-0383",
    "preferred_term": "Group vs Individual Fairness",
    "definition": "Group vs Individual Fairness represents two distinct paradigms for conceptualizing and operationalizing algorithmic fairness with fundamentally different units of analysis and philosophical foundations. Group Fairness operates at the aggregate level, requiring statistical parity across protected demographic groups such that prediction distributions, error rates, or outcome rates are similar across groups, formalized as P(Ŷ|A=a) being approximately equal for all protected group values a. This paradigm underlies metrics like demographic parity, equalized odds, and predictive parity, and aligns with legal frameworks focused on disparate impact and anti-discrimination compliance. In contrast, Individual Fairness operates at the person level, requiring that similar individuals receive similar predictions regardless of group membership, formalized through a fairness metric d(x₁,x₂) → d(f(x₁),f(f₂)) where the distance between predictions is bounded by the distance between individuals in a task-relevant similarity space. Group fairness is operationally straightforward requiring only protected attribute labels but may permit unfairness to individuals within groups, while individual fairness provides stronger theoretical guarantees but requires defining task-appropriate similarity metrics that avoid encoding prohibited biases. The two paradigms are not necessarily compatible, as satisfying group fairness constraints does not guarantee individual fairness and vice versa, representing a fundamental tension in fair machine learning research explored by Dwork et al. (2012) and subsequent scholarship.",
    "source_domain": "ai",
    "maturity_level": null,
    "authority_score": 0.95
  }
}