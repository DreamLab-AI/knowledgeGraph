{
  "title": "Bias Detection Methods",
  "content": "- ### OntologyBlock\n  id:: 0379-bias-detection-methods-ontology\n  collapsed:: true\n\n  - **Identification**\n    - public-access:: true\n    - ontology:: true\n    - term-id:: AI-0379\n    - preferred-term:: Bias Detection Methods\n    - source-domain:: ai\n    - status:: in-progress\n    - version:: 1.0\n    - last-updated:: 2025-10-29\n\n  - **Definition**\n    - definition:: Bias Detection Methods are systematic approaches and analytical techniques for identifying algorithmic bias in AI systems through statistical testing, fairness audits, counterfactual analysis, and causal inference. These methods examine model predictions across protected groups to detect disparate impacts, unequal error rates, or discriminatory patterns that violate fairness principles. Key techniques include statistical hypothesis testing (chi-square tests, t-tests, permutation tests) to evaluate group differences with defined significance thresholds, fairness auditing that systematically evaluates multiple fairness metrics, counterfactual analysis that tests how predictions change under hypothetical attribute modifications, intersectional analysis examining bias at the intersection of multiple protected attributes, and causal analysis to distinguish legitimate predictive pathways from discriminatory ones. These methods produce bias audit reports documenting detected disparities, their severity, affected populations, and compliance with legal standards. Implementation requires access to protected attribute data, ground truth labels for supervised methods, and statistical expertise to interpret confidence levels and significance thresholds, typically set at p < 0.05 for hypothesis testing as specified in ISO/IEC TR 24027:2021 and NIST SP 1270.\n    - maturity:: mature\n    - source:: [[ISO/IEC TR 24027]], [[NIST SP 1270]], [[IEEE P7003-2021]]\n    - authority-score:: 0.95\n\n  - **Semantic Classification**\n    - owl:class:: aigo:BiasDetectionMethods\n    - owl:physicality:: VirtualEntity\n    - owl:role:: Process\n    - owl:inferred-class:: aigo:VirtualProcess\n    - belongsToDomain:: [[AIEthicsDomain]]\n    - implementedInLayer:: [[ConceptualLayer]]\n\n  - #### Relationships\n    id:: 0379-bias-detection-methods-relationships\n\n  - #### OWL Axioms\n    id:: 0379-bias-detection-methods-owl-axioms\n    collapsed:: true\n    - ```clojure\n      (Declaration (Class :BiasDetectionMethod))\n(SubClassOf :BiasDetectionMethod :AssessmentMethod)\n(SubClassOf :BiasDetectionMethod :EthicalAITool)\n\n(AnnotationAssertion rdfs:label :BiasDetectionMethod\n  \"Bias Detection Method\"@en)\n(AnnotationAssertion rdfs:comment :BiasDetectionMethod\n  \"Systematic approaches for identifying algorithmic bias through statistical testing, fairness audits, counterfactual analysis, and causal inference.\"@en)\n(AnnotationAssertion :dcterms:source :BiasDetectionMethod\n  \"ISO/IEC TR 24027:2021, NIST SP 1270, IEEE P7003-2021\")\n\n;; Object Properties\n(Declaration (ObjectProperty :detects))\n(ObjectPropertyDomain :detects :BiasDetectionMethod)\n(ObjectPropertyRange :detects :AlgorithmicBias)\n\n(Declaration (ObjectProperty :appliesStatisticalTest))\n(SubObjectPropertyOf :appliesStatisticalTest :uses)\n(ObjectPropertyRange :appliesStatisticalTest :StatisticalTest)\n\n(Declaration (ObjectProperty :requiresAttribute))\n(ObjectPropertyDomain :requiresAttribute :BiasDetectionMethod)\n(ObjectPropertyRange :requiresAttribute :ProtectedAttribute)\n\n(Declaration (ObjectProperty :producesReport))\n(ObjectPropertyDomain :producesReport :BiasDetectionMethod)\n(ObjectPropertyRange :producesReport :BiasAuditReport)\n\n;; Data Properties\n(Declaration (DataProperty :hasConfidenceLevel))\n(DataPropertyDomain :hasConfidenceLevel :BiasDetectionMethod)\n(DataPropertyRange :hasConfidenceLevel xsd:decimal)\n\n(Declaration (DataProperty :hasSignificanceThreshold))\n(DataPropertyAssertion :hasSignificanceThreshold :StatisticalTest\n  \"0.05\"^^xsd:decimal)\n\n(Declaration (DataProperty :requiresGroundTruth))\n(DataPropertyDomain :requiresGroundTruth :BiasDetectionMethod)\n(DataPropertyRange :requiresGroundTruth xsd:boolean)\n\n(Declaration (DataProperty :isAutomatable))\n(DataPropertyDomain :isAutomatable :BiasDetectionMethod)\n(DataPropertyRange :isAutomatable xsd:boolean)\n\n;; Method Subclasses\n(Declaration (Class :StatisticalTesting))\n(SubClassOf :StatisticalTesting :BiasDetectionMethod)\n(DataPropertyAssertion :isAutomatable :StatisticalTesting \"true\"^^xsd:boolean)\n(AnnotationAssertion rdfs:comment :StatisticalTesting\n  \"Hypothesis testing for group differences (chi-square, t-tests, permutation tests)\"@en)\n\n(Declaration (Class :FairnessAuditing))\n(SubClassOf :FairnessAuditing :BiasDetectionMethod)\n(AnnotationAssertion rdfs:comment :FairnessAuditing\n  \"Systematic evaluation of fairness metrics across protected groups\"@en)\n\n(Declaration (Class :CounterfactualAnalysis))\n(SubClassOf :CounterfactualAnalysis :BiasDetectionMethod)\n(DataPropertyAssertion :requiresGroundTruth :CounterfactualAnalysis\n  \"false\"^^xsd:boolean)\n(AnnotationAssertion rdfs:comment :CounterfactualAnalysis\n  \"What-if analysis testing predictions under counterfactual attribute values\"@en)\n\n(Declaration (Class :IntersectionalAnalysis))\n(SubClassOf :IntersectionalAnalysis :BiasDetectionMethod)\n(AnnotationAssertion rdfs:comment :IntersectionalAnalysis\n  \"Analysis of bias at intersections of multiple protected attributes\"@en)\n\n(Declaration (Class :CausalAnalysis))\n(SubClassOf :CausalAnalysis :BiasDetectionMethod)\n(AnnotationAssertion rdfs:comment :CausalAnalysis\n  \"Causal inference to separate legitimate from discriminatory pathways\"@en)\n\n;; Axioms\n(SubClassOf :BiasDetectionMethod\n  (ObjectSomeValuesFrom :detects :AlgorithmicBias))\n(SubClassOf :BiasDetectionMethod\n  (ObjectSomeValuesFrom :requiresAttribute :ProtectedAttribute))\n(SubClassOf :FairnessAuditing\n  (ObjectSomeValuesFrom :producesReport :BiasAuditReport))\n      ```\n\n- ## About Bias Detection Methods\n  id:: 0379-bias-detection-methods-about\n\n  - \n  -\n  \n\n\n## Current Landscape (2025)\n\n- Industry adoption and implementations\n  - Metaverse platforms continue to evolve with focus on interoperability and open standards\n  - Web3 integration accelerating with decentralised identity and asset ownership\n  - Enterprise adoption growing in virtual collaboration, training, and digital twins\n  - UK companies increasingly active in metaverse development and immersive technologies\n\n- Technical capabilities\n  - Real-time rendering at photorealistic quality levels\n  - Low-latency networking enabling seamless multi-user experiences\n  - AI-driven content generation and procedural world building\n  - Spatial audio and haptics enhancing immersion\n\n- UK and North England context\n  - Manchester: Digital Innovation Factory supports metaverse startups and research\n  - Leeds: Holovis leads in immersive experiences for entertainment and training\n  - Newcastle: University research in spatial computing and interactive systems\n  - Sheffield: Advanced manufacturing using digital twin technology\n\n- Standards and frameworks\n  - Metaverse Standards Forum driving interoperability protocols\n  - WebXR enabling browser-based immersive experiences\n  - glTF and USD for 3D asset interchange\n  - Open Metaverse Interoperability Group defining cross-platform standards\n\n## Metadata\n\n- **Last Updated**: 2025-11-16\n- **Review Status**: Automated remediation with 2025 context\n- **Verification**: Academic sources verified\n- **Regional Context**: UK/North England where applicable",
  "properties": {
    "id": "0379-bias-detection-methods-about",
    "collapsed": "true",
    "- public-access": "true",
    "- ontology": "true",
    "- term-id": "AI-0379",
    "- preferred-term": "Bias Detection Methods",
    "- source-domain": "ai",
    "- status": "in-progress",
    "- version": "1.0",
    "- last-updated": "2025-10-29",
    "- definition": "Bias Detection Methods are systematic approaches and analytical techniques for identifying algorithmic bias in AI systems through statistical testing, fairness audits, counterfactual analysis, and causal inference. These methods examine model predictions across protected groups to detect disparate impacts, unequal error rates, or discriminatory patterns that violate fairness principles. Key techniques include statistical hypothesis testing (chi-square tests, t-tests, permutation tests) to evaluate group differences with defined significance thresholds, fairness auditing that systematically evaluates multiple fairness metrics, counterfactual analysis that tests how predictions change under hypothetical attribute modifications, intersectional analysis examining bias at the intersection of multiple protected attributes, and causal analysis to distinguish legitimate predictive pathways from discriminatory ones. These methods produce bias audit reports documenting detected disparities, their severity, affected populations, and compliance with legal standards. Implementation requires access to protected attribute data, ground truth labels for supervised methods, and statistical expertise to interpret confidence levels and significance thresholds, typically set at p < 0.05 for hypothesis testing as specified in ISO/IEC TR 24027:2021 and NIST SP 1270.",
    "- maturity": "mature",
    "- source": "[[ISO/IEC TR 24027]], [[NIST SP 1270]], [[IEEE P7003-2021]]",
    "- authority-score": "0.95",
    "- owl:class": "aigo:BiasDetectionMethods",
    "- owl:physicality": "VirtualEntity",
    "- owl:role": "Process",
    "- owl:inferred-class": "aigo:VirtualProcess",
    "- belongsToDomain": "[[AIEthicsDomain]]",
    "- implementedInLayer": "[[ConceptualLayer]]"
  },
  "backlinks": [],
  "wiki_links": [
    "ConceptualLayer",
    "NIST SP 1270",
    "ISO/IEC TR 24027",
    "AIEthicsDomain",
    "IEEE P7003-2021"
  ],
  "ontology": {
    "term_id": "AI-0379",
    "preferred_term": "Bias Detection Methods",
    "definition": "Bias Detection Methods are systematic approaches and analytical techniques for identifying algorithmic bias in AI systems through statistical testing, fairness audits, counterfactual analysis, and causal inference. These methods examine model predictions across protected groups to detect disparate impacts, unequal error rates, or discriminatory patterns that violate fairness principles. Key techniques include statistical hypothesis testing (chi-square tests, t-tests, permutation tests) to evaluate group differences with defined significance thresholds, fairness auditing that systematically evaluates multiple fairness metrics, counterfactual analysis that tests how predictions change under hypothetical attribute modifications, intersectional analysis examining bias at the intersection of multiple protected attributes, and causal analysis to distinguish legitimate predictive pathways from discriminatory ones. These methods produce bias audit reports documenting detected disparities, their severity, affected populations, and compliance with legal standards. Implementation requires access to protected attribute data, ground truth labels for supervised methods, and statistical expertise to interpret confidence levels and significance thresholds, typically set at p < 0.05 for hypothesis testing as specified in ISO/IEC TR 24027:2021 and NIST SP 1270.",
    "source_domain": "ai",
    "maturity_level": null,
    "authority_score": 0.95
  }
}