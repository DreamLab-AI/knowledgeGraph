{
  "title": "ComfyUI",
  "content": "- ### OntologyBlock\n  id:: comfyui-ontology\n  collapsed:: true\n\t- ontology:: true\n\t- term-id:: mv-258351002389\n\t- preferred-term:: ComfyUI\n\t- source-domain:: metaverse\n\t- status:: active\n\t- public-access:: true\n\t- definition:: A component of the metaverse ecosystem focusing on comfyui.\n\t- maturity:: mature\n\t- authority-score:: 0.85\n\t- owl:class:: mv:Comfyui\n\t- owl:physicality:: ConceptualEntity\n\t- owl:role:: Concept\n\t- belongsToDomain:: [[MetaverseDomain]]\n\t- #### Relationships\n\t  id:: comfyui-relationships\n\t\t- enables:: [[ImmersiveExperience]], [[Presence]], [[SpatialComputing]]\n\t\t- requires:: [[DisplayTechnology]], [[TrackingSystem]], [[RenderingEngine]]\n\t\t- bridges-to:: [[HumanComputerInteraction]], [[ComputerVision]], [[Robotics]]\n\n\t- #### OWL Axioms\n\t  id:: comfyui-owl-axioms\n\t  collapsed:: true\n\t\t- ```clojure\n\t\t  Declaration(Class(mv:Comfyui))\n\n\t\t  # Classification\n\t\t  SubClassOf(mv:Comfyui mv:ConceptualEntity)\n\t\t  SubClassOf(mv:Comfyui mv:Concept)\n\n\t\t  # Domain classification\n\t\t  SubClassOf(mv:Comfyui\n\t\t    ObjectSomeValuesFrom(mv:belongsToDomain mv:MetaverseDomain)\n\t\t  )\n\n\t\t  # Annotations\n\t\t  AnnotationAssertion(rdfs:label mv:Comfyui \"ComfyUI\"@en)\n\t\t  AnnotationAssertion(rdfs:comment mv:Comfyui \"A component of the metaverse ecosystem focusing on comfyui.\"@en)\n\t\t  AnnotationAssertion(dcterms:identifier mv:Comfyui \"mv-258351002389\"^^xsd:string)\n\t\t  ```\n\npublic:: true\n\n- #Public page automatically published\n-\n- It's not the easiest way to use Stable Diffusion.\n- https://private-user-images.githubusercontent.com/140084057/368715655-f46f769d-f168-454c-9c7b-ed8bcd727c1d.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MzEwNjY1NTEsIm5iZiI6MTczMTA2NjI1MSwicGF0aCI6Ii8xNDAwODQwNTcvMzY4NzE1NjU1LWY0NmY3NjlkLWYxNjgtNDU0Yy05YzdiLWVkOGJjZDcyN2MxZC5wbmc_WC1BbXotQWxnb3JpdGhtPUFXUzQtSE1BQy1TSEEyNTYmWC1BbXotQ3JlZGVudGlhbD1BS0lBVkNPRFlMU0E1M1BRSzRaQSUyRjIwMjQxMTA4JTJGdXMtZWFzdC0xJTJGczMlMkZhd3M0X3JlcXVlc3QmWC1BbXotRGF0ZT0yMDI0MTEwOFQxMTQ0MTFaJlgtQW16LUV4cGlyZXM9MzAwJlgtQW16LVNpZ25hdHVyZT00MjQzYjkyNDU2MjNjOTRhYjgyM2JkN2E1YzQxMDcxMDAwNTQ5ZjYzZThhMjBhYjM4NGZhYzhiYzg0ZTNhNDYzJlgtQW16LVNpZ25lZEhlYWRlcnM9aG9zdCJ9.eOij_sfuzFqJLnC0tcT-OYnMTJF30B_mnj2xM3qjG-Y\n-\n- The [GitHub repository](https://github.com/ZHO-ZHO-ZHO/[[ComfyUI]]-[[Gemini]]) integrates Gemini into ComfyUI, offering models like Gemini-pro, Genimi-pro-vision, and Gemini 1.5 Pro for text, image, and file processing tasks. Users can apply for their own API Key to access Gemini API. The repository provides workflow examples, installation instructions, and updates. Contact information includes zhozho3965@gmail.com and a QQ Group (839821928). Social media links to '-Zho-', Bilibili, Twitter, Little Red Book, and support on Bilibili and Aifadian. Credits to ComfyUI_Custom_Nodes_AlekPet. - Users need to apply for a Gemini_API_Key to use Gemini nodes, ensure a stable connection to [[Google]] Gemini's services, and update the dependency 'google-generativeai' to version 0.4.1 for Gemini 1.5 Pro. Installation can be done using ComfyUI Manager or manually by cloning the repository from GitHub and installing requirements. Workflow versions include V3.0 with Gemini 1.5 Pro, V2.0 with a chatbot workflow, and V1.1 with workflows for Gemini-pro and Genimi-pro-vision. Updates include Version 3.0 adding Gemini 1.5 Pro, system instructions, and file uploads, Version 2.1 fixing a bug, and Version 2.0 adding context chat nodes. - The [status history](https://api.status-histroy.com/svg?repos=ZHO-ZHO-ZHO/ConfyUI-Gemini&type=Timeline) for the repository is available.\n- The [video](https://www.youtube.com/watch?v=fFdYdTzq7Kg) titled 'Expanding Horizons: Outpainting Mastery in [[ComfyUI]]' explores the artistry of Outpainting with ComfyUI's [[Stable Diffusion]] feature. The video delves into Hyper Expansion, Sketch to Render, and Auto Background Regeneration within the realm of Outpainting. Topics discussed include the use of LORAs, Quick Basic Outpainting, and various techniques for Outpainting with ComfyUI.\n- The [Aerial view of the building](https://civitai.com/models/121728/aerial-view-of-the-building) showcases a [[LoRA DoRA etc]] model for urban bird's-eye views, offering high-definition training sets for cityscapes and buildings. The model, based on SD 1.5, has received very positive reviews and was last updated on Aug 3, 2023.\n- The [GitHub repository](https://github.com/comfyanonymous/[[ComfyUI]]) showcases ComfyUI, a powerful and modular [[Stable Diffusion]] GUI, API, and backend with a graph/nodes interface. The interface allows users to design and execute advanced stable diffusion pipelines without needing to code. ComfyUI supports various features like SD1.x, SD2.x, SDXL, Stable Video Diffusion, and Stable Cascade. Users can - back_hashcash-denial_2002 - - [[experiment]] with complex workflows, embeddings/textual inversion, Loras, hypernetworks, and more. The repository provides detailed installation instructions for Windows, Linux, AMD GPUs, NVIDIA GPUs, Intel GPUs, [[Apple]] Mac silicon, and DirectML for AMD cards on Windows. Additionally, it offers shortcuts for workflow management, high-quality previews, TLS/SSL setup, and support channels for users. The repository is licensed under GPL-3.0 and has garnered 35.3k stars and 3.8k forks.\n- The [video](https://www.youtube.com/watch?v=DROM8vfIYUY) titled 'How 2 Canvas Node' is available on YouTube. The video duration is 14 minutes and 22 seconds.\n- The [video](https://www.youtube.com/watch?v=AU8NDSBIS1U) showcases a plugin that provides realtime AI assistance to - ### OntologyBlock id:: krita-ontology collapsed:: true - ontology:: true - term-id:: mv-523525042165 - [[krita]], a digital painting software. The channel, Nerdy Rodent, offers tutorials on [[Stable Diffusion]], Generative AI, [[Large language models]], and other AI tools, catering to AI enthusiasts and professionals. The plugin enhances various aspects of AI technology, such as voice cloning, text-to-speech, and style transfer, making it a valuable resource for AI enthusiasts and artists alike.\n- The [video](https://www.youtube.com/watch?v=reimr3jZ8lI) titled '[[ComfyUI]] Fundamentals - [[Upscaling]] 1' explores the concept of upscaling in the context of ComfyUI. The video delves into how upscaling works and provides tips to enhance the upscaling process. The presenter discusses the ability to go under 1 on the upscale by node, highlighting key aspects of image upscaling and various upscaling options.\n- The [YouTube video](https://www.youtube.com/watch?v=ppE1W0-LJas) titled '[[ComfyUI]] Modular- Ultimate Starter Workflow Usage' provides insights into using ComfyUI Modular for an ultimate starter workflow.\n- The [YouTube](https://www.youtube.com/@sedetweiler) page provides information about the use of cookies and data for various purposes, including [[Google]] services, audience engagement, and site statistics. By accepting all, users allow for the development of new services, displaying ads, and showing personalised content and ads based on their settings. Non-personalised content and ads are influenced by factors like the viewed content and location. Users can manage privacy settings and find more information by selecting 'Meer opties'.\n- The [GitHub repository](https://github.com/phineas-pta/comfy-trt-test) is an attempt to use TensorRT with [[ComfyUI]], focusing on specific models and workflows. The repository provides instructions for installation and usage, highlighting compatible models like [[Stable Diffusion]] and SDXL. It also mentions limitations and future improvements for user-friendliness and compatibility with ComfyUI-Manager. The repository is a ComfyUI port from the official A1111 extension, with potential maintenance issues addressed by an alternative repository. The README outlines conversion scripts, dependencies, and error messages encountered during the process.\n- The [GitHub repository](https://github.com/gameltb/ComfyUI_stable_fast) showcases experimental usage of stable-fast and TensorRT. It includes speed tests, installation instructions, and features of stable-fast and TensorRT.\n\t- The repository provides guidance on enabling stable-fast nodes and installing TensorRT for testing purposes.\n\t- It highlights the compatibility of stable-fast with [[LoRA DoRA etc]], ControlNet, and other models, along with speed optimizations and node support.\n\t- Speed tests on a GeForce RTX 3060 Mobile show performance metrics for stable-fast and TensorRT implementations.\n\t- The repository also includes a table detailing features, tested nodes, and performance benchmarks for different workflows.\n- The [GitHub repository](https://github.com/Acly/[[krita]]-ai-diffusion?tab=readme-ov-file) provides a streamlined interface for generating images with AI in Krita. Users can inpaint and outpaint with optional text prompts without the need for tweaking. The plugin allows for creating new images from scratch, refining existing content, and live painting. It supports various resolutions, job queues, and history tracking. Customization options are available for advanced users. The technology used includes [[Stable Diffusion]] for image generation, [[ComfyUI]] for the diffusion backend, ControlNet for [[Inpainting]], and IP-Adapter for outpainting. The repository is licensed under GPL-3.0 and has garnered 4.8k stars and 216 forks. Contributors have added features like object selection tools and GPU cloud support.\n- The [[ComfyUI]] [models](https://civitai.com/tag/comfyui) tag on Civitai features 379 models. Civitai offers a range of AI models for various applications and purposes.\n- The [GitHub repository](https://github.com/fictions-ai/sharing-is-caring/tree/main) titled 'sharing-is-caring' is maintained by the fictions.ai team. They focus on collaboration and sharing knowledge related to A.I. generation. The repository contains various workflows, scripts, and tools for A.I. generation. Some key [[contents]] include Comfy Workflows, upscale workflows, and specific workflow requirements. Contact the fictions.ai team for questions or feedback.\n- The [Index](https://wyrde.github.io/ComfyResources/nodes/) page of [[ComfyUI]] Resources provides a comprehensive list of custom nodes and tools for ComfyUI. Various nodes such as CLIP BLIP Node, GPT node ComfyUI, and Vid2vid Node Suite are available for installation. Instructions for installing custom nodes are included on the page.\n- The [Comfy Workflows](https://[[ComfyWorkFlows]].com/) website offers a platform for sharing art and workflows, with a focus on [[ComfyUI]]. Users can explore thousands of workflows created by the community and run them with zero setup using the ComfyUI Launcher. The site features a variety of workflows, images, and videos created by different creators.\n- The [GitHub repository](https://github.com/ZHO-ZHO-ZHO/[[ComfyUI]]-[[Gemini]]) integrates Gemini into ComfyUI, enabling users to generate prompts, describe images, and converse with Gemini. The repository features the latest Gemini 1.5 Pro model with system instruction settings, multi-modal conversations, and file reading capabilities. Users can request their API Key for Gemini API. Various models and nodes are provided for different functionalities, along with installation methods, workflows, changelog, and contact details.\n- The [video](https://www.youtube.com/watch?v=fFdYdTzq7Kg) titled 'Expanding Horizons: Outpainting Mastery in [[ComfyUI]]' on YouTube showcases the mastery of outpainting in ComfyUI. The video explores the revolutionary design aspects, including Vignette Mastery, ComfyUI Magic, and Font Previews Galore.\n- The [paper](https://arxiv.org/abs/2309.11497) titled FreeU: Free Lunch in Diffusion U-Net explores the potential of enhancing generation quality in [[Diffusion Models]] without additional training. The authors propose a method called 'FreeU' that strategically re-weights contributions from U-Net's skip connections and backbone feature maps to improve generation quality. The results show promising outcomes for image and video generation tasks, demonstrating the ease of integration with existing diffusion models.\n- The [GitHub repository](https://github.com/phineas-pta/comfy-trt-test) explores the attempt to use TensorRT with [[ComfyUI]], focusing on compatibility and installation instructions. The repository provides information on supported models like [[Stable Diffusion]] and SDXL, along with a list of working and non-working models. The project aims to make the process more user-friendly and automatic in the future. The README.md file outlines the steps for installing Python dependencies, TensorRT versions, and converting [[Checkpoints]] to TensorRT engines. It also discusses the usage of converted engines in ComfyUI and common error messages. The repository includes links to original implementations and download links for testing with various checkpoints and models.\n- The [AP Workflow 9.0 for [[ComfyUI]]](https://perilli.com/ai/comfyui/#soon) introduces new features such as upscalers, image generation with [[Dall-e]], advanced XYZ plot, face cloner, face analyzer, and training helper for batch captioning. Instructions for troubleshooting custom node installation, switching to SD 1.5 models, using LM Studio for prompt enrichment, securing ComfyUI connection with SSL, and FAQs are provided. Special thanks to custom node creators and a full changelog for version 9.0 are included. The [AP Workflow](https://perilli.com/ai/comfyui/#soon) version 8.0 offers features like bookmark nodes, [[IPAdapter]] plus v2 nodes, uploader function, caption generator function, image evaluators, face analyzer, aesthetic score predictor, image chooser, prompt enricher function, [[LoRA DoRA etc]] info node, face detailer function, and reorganized L2 pipeline layout with removed functions like ReVision and Image Enhancer.\n- The [[Google]] [Spreadsheets](https://docs.google.com/spreadsheets/d/1IYJw4Iv9M_vX507MPbdX4thhVYxOr6-IThbaRjdpVgM/edit#gid=0) document titled 'SDXL Model Compare' provides information on various data models and comparison metrics. The content includes data on different styles, illustrations, and financial aspects. The document also offers support for screen readers and data cleaning suggestions.\n- The [GitHub repository](https://github.com/chaojie/[[ComfyUI]]-MotionCtrl) for ComfyUI-MotionCtrl contains an implementation of MotionCtrl for video generation. The repository includes nodes for loading motion control [[Checkpoints]], motion control conditioning, and motion control [[Sampling]]. Tools for generating motion trajectories and camera points are also provided. Examples of workflows for generating LVDM/VideoCrafter videos and using [[AnimateDiff]] for scribbling are available. The repository is licensed under Apache-2.0 and has received 121 stars and 4 forks.\n- The [YouTube](https://www.youtube.com/@ferniclestix) page provides information about cookies and data usage for [[Google]] services. Users can choose to accept or reject cookies for various purposes, including delivering Google services, tracking outages, measuring audience engagement, and showing personalised content and ads based on user settings.\n- The [video](https://www.youtube.com/watch?v=DROM8vfIYUY) titled 'How 2 Canvas Node' is available on YouTube.\n- The [YouTube](https://www.youtube.com/@NerdyRodent) page provides information about cookies and data usage by [[Google]] services. Users can choose to accept or reject cookies for various purposes, including delivering Google services, measuring audience engagement, and showing personalised content and ads. More options are available for managing privacy settings.\n- The [GitHub repository](https://github.com/Acly/[[krita]]-ai-diffusion?tab=readme-ov-file) provides a streamlined interface for generating images with AI in Krita. It allows users to inpaint and outpaint images with optional text prompts, requiring no tweaking. The plugin supports features like generating images from scratch, refining existing content, live painting, and job queue management. Customization options are available for advanced users. The plugin is open source and free to use. [[krita-plugin]], [[stable-diffusion]], [[Generative AI]]\n- The [YouTube video](https://www.youtube.com/watch?v=AU8NDSBIS1U) showcases a plugin that adds real-time AI assistance to [[krita]]. The video covers topics such as [[Stable Diffusion]], Generative AI, [[Large language models]], AI Animation, Voice Cloning, and more. The channel, Nerdy Rodent, provides tutorials on [[Artificial Intelligence]] in an easy-to-digest format. The AI enthusiast behind the channel recommends specific hardware for the best AI experience at home. Not suitable for children. Please use AI responsibly.\n- The [GitHub repository](https://github.com/comfyanonymous/[[ComfyUI]]) showcases ComfyUI, a powerful and modular [[Stable Diffusion]] GUI, API, and backend with a graph/nodes interface. The interface allows users to design and execute advanced stable diffusion pipelines without needing to code. ComfyUI supports various [[Diffusion Models]], asynchronous queue system, and many optimizations to enhance workflow efficiency. Users can - back_hashcash-denial_2002 - - [[experiment]] with complex workflows, including area composition, [[Inpainting]], controlnet, upscale models, and more. The repository provides detailed installation instructions for Windows, Linux, AMD GPUs, NVIDIA GPUs, Intel GPUs, [[Apple]] Mac silicon, and DirectML for AMD cards on Windows. Additionally, it offers shortcuts for workflow management, high-quality preview setup, TLS/SSL configuration, and support channels for users. The repository is licensed under GPL-3.0 and has garnered significant community engagement with 35.3k stars and 3.8k forks.\n- The [video](https://www.youtube.com/watch?v=OdMtJMzjNLg) titled 'LATENT Tricks - Amazing ways to use [[ComfyUI]]' features Olivio Sarikas, an AI Expert and passionate Artist, showcasing the exciting world of AI art. The video invites viewers to explore AI art and creative visions with live streams. Olivio Sarikas, a professional Designer with a Masters Degree in Fine Arts, shares [[Tips and Tricks]] for using ComfyUI.\n- The [[ComfyUI]][models](https://civitai.com/tag/comfyui) tag on Civitai features 379 models for [[Stable Diffusion]] AI. Users can explore and access these models for various applications. Civitai offers a range of services and resources for creators, including terms of service, privacy policies, and safety guidelines.\n- The [[ComfyUI]]-[extension-tutorials/ComfyUI-Impact-Pack/workflow](https://github.com/ltdrdata/ComfyUI-extension-tutorials/tree/Main/ComfyUI-Impact-Pack/workflow) on GitHub provides a collection of workflow-related files for various image processing tasks. The repository includes files for tasks such as [[Upscaling]], animation, segmentation, and more. Each file represents a specific workflow step or technique, showcasing the versatility and capabilities of the ComfyUI Impact Pack.\n- The [video](https://www.youtube.com/watch?v=KvZ8ucBqyqw) titled '[[ComfyUI]] Impact Pack - Q&A;: Detailer Options' on YouTube provides explanations on the important parameters of the detailer.\n- The [Index](https://wyrde.github.io/ComfyResources/nodes/#index) page of [[ComfyUI]] Resources provides a comprehensive list of custom nodes and tools available for use. The nodes cover a wide range of functionalities, from image processing to AI installation tools. Installation instructions can be found on the respective node pages, with the option to streamline the process using ltdrdata's Comfy Manager.\n- The [YouTube video](https://www.youtube.com/watch?v=reimr3jZ8lI) titled '[[ComfyUI]] Fundamentals - [[Upscaling]] 1' provides insights into the fundamentals of ComfyUI and the concept of upscaling in the context of user interface design.\n- The [YouTube video](https://www.youtube.com/watch?v=ppE1W0-LJas) titled '[[ComfyUI]] Modular- Ultimate Starter Workflow Usage' provides insights into using ComfyUI Modular for an ultimate starter workflow.\n- The [GitHub repository](https://github.com/SHI-Labs/Prompt-Free-Diffusion) for Prompt-Free Diffusion discusses a diffusion model that generates images using only visual inputs, replacing text encoders with a Semantic Context Encoder (SeeCoder). The model is reusable across various T2I models and adaptive layers. The repository includes pretrained models and tools for model conversion. The implementation is based on a research paper presented at arXiv 2023 / CVPR 2024.\n- The [file](https://gist.github.com/comfyanonymous/343e5675f9a2c8281fde0c440df2e2c6#file-workflow-[[json]]) contains Python code for a custom node in the [[ComfyUI]]/custom_nodes directory. The code defines a class for handling model references and latent data. The JSON file included in the gist outlines a workflow with various nodes like VAEDecode, CLIPTextEncode, KSampler, SaveImage, CheckpointLoaderSimple, ImageScale, LoadImage, and ReferenceOnlySimple.\n- The [GitHub repository](https://github.com/itsKaynine/comfy-ui-client) contains a Node.js WebSockets API client for [[ComfyUI]]. The client is based on the WebSockets API example and is licensed under the MIT [[license]]. The repository includes folders and files such as examples, source code, and configuration files. The client allows users to connect to a server, generate images based on prompts, and save the images to a specified directory. Topics related to the project include nodejs, api, stable-diffusion, comfyui, and sdxl.\n- The [YouTube](https://www.youtube.com/@ArchAi3D/videos) page provides information about cookies and data usage by [[Google]] services. Users can choose to accept or reject cookies for various purposes, including personalised content and ads. More options are available for managing privacy settings.\n- The [video](https://www.youtube.com/watch?v=js4JeDF3v4g) showcases ComyUI, a tool for video animation rendering using AI technologies like WAS, Seecoder, Style, and Semantic segmentation. The creator, Amir Ferdos, a seasoned 3D artist and designer, explores the fusion of AI with design processes, offering unique workflows and tutorials on their YouTube channel. The videos aim to educate and inspire designers on the transformative impact of AI in design. For a deeper dive into the creator's work, exclusive tutorials and source files are available on their Patreon page.\n- The [[ComfyUI]] [Community Manual](https://blenderneko.github.io/ComfyUI-docs/) provides documentation for ComfyUI, a [[Stable Diffusion]] GUI and backend. It covers topics such as installation, downloading models, first steps with Comfy, loading other flows, and further support. The manual includes detailed information on interface, core nodes (including advanced, conditioning, experimental, image, latent, loaders, mask, and [[Sampling]]), custom nodes, developing custom nodes, and contributing documentation.\n- The [GitHub repository](https://github.com/pydn/[[ComfyUI]]-to-Python-Extension) provides a powerful tool that translates ComfyUI workflows into executable Python code. The tool bridges the gap between ComfyUI's visual interface and Python's programming environment, streamlining the process for data scientists, software developers, and AI enthusiasts. Use cases include creating lean app deployments, programmatic experiments, and large image generation queues. The v1.0.0 release notes highlight support for custom nodes. To use the tool, clone the repository, enable Dev mode options in ComfyUI, save workflows in API format, and run the script to generate Python code for image generation without launching a server. The repository is primarily focused on topics like pytorch, generative art, image generation, AI art, [[Stable Diffusion]], and ComfyUI.\n- The [website](https://comfy.icu/) offers [[ComfyUI]] Cloud services for running and deploying workflows without the need for downloads or installs. Users can pay only for active GPU usage, avoiding idle time and unnecessary costs. ComfyICU provides ready-to-use creative workflows and a simple, scalable API for production. The platform aims to simplify workflow creation and deployment, offering fast performance and cost-efficiency. Users can access over 5000 happy users' testimonials and FAQs for more information.\n- The [video](https://www.youtube.com/watch?v=SMOM1bIY5yA) titled 'EASY [[Inpainting]] in [[ComfyUI]] with SAM (segment Anything) | Creative Workflow Tutorial' provides a tutorial on using SAM for inpainting in ComfyUI.\n- The [wiki page](https://github.com/chrisgoringe/Comfy-Custom-Node-How-To/wiki) provides a practical and [[collaborative]] guide on developing custom nodes for [[ComfyUI]]. The guide is unofficial and focuses on practicality over formality, encouraging collaboration through Q&A-style discussions. It covers various topics related to custom node development, such as control flow, data types, and UI design.\n- The [Comfy Workflows](https://[[ComfyWorkFlows]].com/) website offers a platform for sharing art and workflows, with features like [[ComfyUI]] Launcher for running workflows with zero setup. Users can explore thousands of workflows created by the community. Trending creators and the latest images and videos are showcased on the site.\n- Custom nodes for interpolating between, well, everything in the [[Stable Diffusion]] [[ComfyUI]]. The [GitHub repository](https://github.com/shockz0rz/ComfyUI_InterpolateEverything) contains functionality to create preprocessed ControlNet OpenPose inputs midway between two images. Future features include line-art interpolation. To install, follow the provided instructions.\n- The [GitHub repository](https://github.com/xXAdonesXx/NodeGPT) contains [[ComfyUI]] Extension Nodes for Automated Text Generation. The repository is under development, with features like autogen, automated task solving, and group chat capabilities. The repository includes various folders and files for different functionalities. To contribute, users can submit pull requests, suggestions, or issue reports. The repository is licensed under AGPL-3.0. The repository has 314 stars, 24 forks, and 4 contributors.\n- The [AutoGen Advanced Tutorial](https://www.youtube.com/watch?v=PUPO2tTyPOo) on YouTube focuses on building incredible AI AGENT teams. The tutorial delves into advanced techniques for creating AI teams and enhancing their capabilities.\n- The [GitHub repository](https://github.com/olegchomp/TDComfyUI) provides a TouchDesigner interface for [[ComfyUI]], offering features like workflow creation and image send/receive. The repository includes installation instructions and resources for using the TDComfyUI component. It also offers guidance on connecting to [[Stable Diffusion]] and utilising ComfyUI settings for optimal performance.\n- The [GitHub repository](https://github.com/NimaNzrii/[[ComfyUI]]-photoshop) showcases the ComfyUI plugin for Photoshop, offering AI-powered image generation features. The plugin enables unlimited generative fill, customizable back-end workflow, and one-click image transformation. System requirements include a minimum of 6GB graphics memory and 12GB RAM. Installation involves downloading the plugin from a provided link or locally via a .CCX file. Additional files are required for specific functionalities. Support and contributions are encouraged through GitHub.\n- The [model](https://civitai.com/models/121728/aerial-view-of-the-building) titled 'Aerial view of the building' offers a high-definition training set for urban bird's-eye views, encompassing a variety of domestic and foreign architectural drawings. The model is based on [[LoRA DoRA etc]] technology and has received positive reviews. The training set is designed for cityscape and building enthusiasts.\n- ## Unsorted links\n\t- [ZHO-ZHO-ZHO/ComfyUI-Gemini: Using Gemini in ComfyUI (github.com)](https://github.com/ZHO-ZHO-ZHO/ComfyUI-Gemini)\n\t- A GitHub repository that provides instructions on using Gemini in ComfyUI.\n\t- [Expanding Horizons: Outpainting Mastery in ComfyUI (youtube.com)](https://www.youtube.com/watch?v=fFdYdTzq7Kg)\n\t- A YouTube video tutorial that demonstrates how to master outpainting in ComfyUI.\n\t- [Chaoses-Ib/ComfyScript: A Python front end for ComfyUI (github.com)](https://github.com/Chaoses-Ib/ComfyScript)\n\t- This GitHub repository contains a Python front end for ComfyUI, known as ComfyScript.\n\t- [Aerial view of the buildingÔºàÂª∫Á≠ëÈ∏üÁû∞ÂõæÔºâ\n\t- v1.0 | Stable Diffusion LoRA | Civitai](https://civitai.com/models/121728/aerial-view-of-the-building)\n\t- Civitai provides a stable diffusion LoRA model for generating an aerial view of a building using ComfyUI.\n\t- [ComfyUI nodes based](https://github.com/comfyanonymous/ComfyUI)\n\t- A GitHub repository that provides nodes-based examples and workflows for ComfyUI.\n\t- üé¨\n\t\t- [How 2 Canvas Node YouTube](https://www.youtube.com/watch?v=DROM8vfIYUY)\n\t\t- A YouTube video tutorial that explains how to use the Canvas Node in ComfyUI.\n\t\t- [This One Simple Plugin Adds Realtime AI Assistance to Krita\n\t\t- YouTube](https://www.youtube.com/watch?v=AU8NDSBIS1U)\n\t\t- A YouTube video that introduces a plugin for Krita that adds realtime AI assistance using ComfyUI.\n\t\t- [Tutorials from first principles](https://www.youtube.com/watch?v=reimr3jZ8lI)\n\t\t- A YouTube video tutorial series that covers ComfyUI from the basic principles.\n\t\t- [modular workflow](https://www.youtube.com/watch?v=ppE1W0-LJas)\n\t\t- A YouTube video that showcases a modular workflow in ComfyUI.\n\t\t- [Detailed youtube tutorials](https://www.youtube.com/@sedetweiler)\n\t\t- A YouTube channel with detailed tutorials on using ComfyUI.\n\t\t- üîß\n\t\t\t- [phineas-pta/comfy-trt-test: attempt to use TensorRT with ComfyUI (github.com)](https://github.com/phineas-pta/comfy-trt-test)\n\t\t\t- A GitHub repository that attempts to use TensorRT with ComfyUI.\n\t\t\t- [gameltb/ComfyUI_stable_fast: Experimental usage of stable-fast and TensorRT. (github.com)](https://github.com/gameltb/ComfyUI_stable_fast)\n\t\t\t- This GitHub repository provides an experimental usage of stable-fast and TensorRT in ComfyUI.\n\t\t\t- [Acly/krita-ai-diffusion: Streamlined interface for generating images with AI in Krita. Inpaint and outpaint with optional text prompt, no tweaking required. (github.com)](https://github.com/Acly/krita-ai-diffusion?tab=readme-ov-file)\n\t\t\t- A GitHub repository that offers a streamlined interface for generating images with AI in Krita using ComfyUI.\n\t\t\t- [Sytan workflow (contains js!)](https://github.com/SytanSD/Sytan-SDXL-ComfyUI)\n\t\t\t- This GitHub repository contains a workflow for ComfyUI that includes JavaScript files.\n\t\t- üåê\n\t\t\t- [(2) Plush-for-ComfyUI style_prompt, can now use ChatGPT to create prompts from images : comfyui (reddit.com)](https://www.reddit.com/r/comfyui/comments/18uincm/plushforcomfyui_style_prompt_can_now_use_chatgpt/)\n\t\t\t- A Reddit post that discusses the Plush-for-ComfyUI style_prompt and its capability to create prompts from images using ChatGPT.\n\t\t\t- [(1) AP Workflow 6.0 for ComfyUI\n\t\t\t- Now with support for SD 1.5 and HiRes Fix, IPAdapter, Prompt Enricher via local LLMs (and OpenAI), and a new Object Swapper + Face Swapper, FreeU v2, XY Plot, ControlNet and ControlLoRAs, SDXL Base + Refiner, Hand Detailer, Face Detailer, Upscalers, ReVision, etc. : StableDiffusion (reddit.com)](https://www.reddit.com/r/StableDiffusion/comments/17v0bo3/ap_workflow_60_for_comfyui_now_with_support_for/)\n\t\t\t- A Reddit post about the AP Workflow 6.0 for ComfyUI, which includes various features and enhancements.\n\t\t\t- [UI node packs on civitai](https://civitai.com/tag/comfyui)\n\t\t\t- Civitai provides UI node packs for ComfyUI.\n\t\t- üìë\n\t\t\t- [fictions-ai/sharing-is-caring (github.com)](https://github.com/fictions-ai/sharing-is-caring/tree/main)\n\t\t\t- This GitHub repository contains various resources related to ComfyUI and its applications.\n\t\t\t- [Wiki full of links](https://wyrde.github.io/ComfyResources/nodes/)\n\t\t\t- A comprehensive wiki filled with links to resources, tutorials, and examples related to ComfyUI.\n\t\t\t- [Images / workflows](https://comfyworkflows.com/)\n\t\t\t- ComfyWorkflows offers a collection of images and workflows created using ComfyUI.\n\t\t- Using LLMs in ComfyUI\n\t\t\t- [ZHO-ZHO-ZHO/ComfyUI-Gemini: Using Gemini in ComfyUI (github.com)](https://github.com/ZHO-ZHO-ZHO/ComfyUI-Gemini)\n\t\t-\n\t\t- [Expanding Horizons: Outpainting Mastery in ComfyUI (youtube.com)](https://www.youtube.com/watch?v=fFdYdTzq7Kg)\n\t\t- https://arxiv.org/abs/2309.11497\n\t\t- TensorRT converter [phineas-pta/comfy-trt-test: attempt to use TensorRT with ComfyUI (github.com)](https://github.com/phineas-pta/comfy-trt-test)\n\t\t- [gameltb/ComfyUI_stable_fast: Experimental usage of stable-fast and TensorRT. (github.com)](https://github.com/gameltb/ComfyUI_stable_fast)\n\t\t- https://perilli.com/ai/comfyui/#soon\n\t\t-\n\t\t- [Chaoses-Ib/ComfyScript: A Python front end for ComfyUI (github.com)](https://github.com/Chaoses-Ib/ComfyScript)\n\t\t- Model leaderboard  [SDXL Model Compare\n\t\t\t- Google Sheets](https://docs.google.com/spreadsheets/d/1IYJw4Iv9M_vX507MPbdX4thhVYxOr6-IThbaRjdpVgM/edit#gid=0)\n\t\t- [fictions-ai/sharing-is-caring (github.com)](https://github.com/fictions-ai/sharing-is-caring/tree/main)\n\t\t- [chaojie/ComfyUI-MotionCtrl (github.com)](https://github.com/chaojie/ComfyUI-MotionCtrl)\n\t\t- [(2) Plush-for-ComfyUI style_prompt, can now use ChatGPT to create prompts from images : comfyui (reddit.com)](https://www.reddit.com/r/comfyui/comments/18uincm/plushforcomfyui_style_prompt_can_now_use_chatgpt/)\n\t\t- [Ferniclestix](https://www.youtube.com/@ferniclestix)\n\t\t\t- [How 2 Canvas Node YouTube](https://www.youtube.com/watch?v=DROM8vfIYUY)\n\t\t- [Nerdy Rodent YouTube](https://www.youtube.com/@NerdyRodent)\n\t\t- [Acly/krita-ai-diffusion: Streamlined interface for generating images with AI in [[Krita]]. Inpaint and outpaint with optional text prompt, no tweaking required. (github.com)](https://github.com/Acly/krita-ai-diffusion?tab=readme-ov-file)\n\t\t\t- TODO this needs the live view debugging\n\t\t\t- [This One Simple Plugin Adds Realtime AI Assistance to Krita YouTube](https://www.youtube.com/watch?v=AU8NDSBIS1U) [[Courses and Training]]\n\t\t- [ComfyUI nodes based](https://github.com/comfyanonymous/ComfyUI)\n\t\t- [Controlnet auto installer](https://github.com/Fannovel16/comfy_controlnet_preprocessors)\n\t\t- [LATENT Tricks Amazing ways to use ComfyUI](https://www.youtube.com/watch?v=OdMtJMzjNLg)\n\t\t- [latent consistency model](https://github.com/0xbitches/ComfyUI-LCM#img2img--vid2vid)\n\t\t- [UI node packs on civitai](https://civitai.com/tag/comfyui)\n\t\t- [(1) AP Workflow 6.0 for ComfyUI\n\t\t\t- Now with support for SD 1.5 and HiRes Fix, IPAdapter, Prompt Enricher via local LLMs (and OpenAI), and a new Object Swapper + Face Swapper, FreeU v2, XY Plot, ControlNet and ControlLoRAs, SDXL Base + Refiner, Hand Detailer, Face Detailer, Upscalers, ReVision, etc. : StableDiffusion (reddit.com)](https://www.reddit.com/r/StableDiffusion/comments/17v0bo3/ap_workflow_60_for_comfyui_now_with_support_for/)\n\t\t- [Workflows that can be loaded](https://github.com/comfyanonymous/ComfyUI_examples)\n\t\t- [Sytan workflow (contains js!)](https://github.com/SytanSD/Sytan-SDXL-ComfyUI)\n\t\t- [Impact pack and youtube](https://github.com/ltdrdata/ComfyUI-extension-tutorials/tree/Main/ComfyUI-Impact-Pack/workflow)\n\t\t- [youtube](https://www.youtube.com/watch?v=KvZ8ucBqyqw)\n\t\t- [Wiki full of links](https://wyrde.github.io/ComfyResources/nodes/)\n\t\t- [Tutorials from first principles](https://www.youtube.com/watch?v=reimr3jZ8lI)\n\t\t- [modular workflow](https://www.youtube.com/watch?v=ppE1W0-LJas)\n\t\t- [Detailed youtube tutorials](https://www.youtube.com/@sedetweiler)\n\t\t- [Prompt free diffusion](https://github.com/SHI-Labs/Prompt-Free-Diffusion)\n\t\t- Motion brush [chaojie/ComfyUI-DragNUWA (github.com)](https://github.com/chaojie/ComfyUI-DragNUWA)\n\t\t- [reference_only controlnet](https://gist.github.com/comfyanonymous/343e5675f9a2c8281fde0c440df2e2c6#file-workflow-json)\n\t\t- [Citivia autoprompt](https://civitai.com/models/123358/sdvn-comfyui-workflow-autoprompt-sdxl)\n\t\t- [Typescript client for comfyui](https://github.com/itsKaynine/comfy-ui-client)\n\t\t- [Animation workflow](https://www.reddit.com/r/comfyui/comments/15s6lpr/short_animation_img2img_in_comfyui_with/)\n\t\t- [Complex workflow tutorials](https://www.youtube.com/@ArchAi3D/videos)\n\t\t- [animation](https://www.youtube.com/watch?v=js4JeDF3v4g)\n\t\t- [Manual](https://blenderneko.github.io/ComfyUI-docs/)\n\t\t- [Turn comfyui to python](https://github.com/pydn/ComfyUI-to-Python-Extension)\n\t\t- [Share workflows](https://comfy.icu/)\n\t\t- [consistent character creation](https://www.reddit.com/r/comfyui/comments/16ceh10/i_succeeded_to_adapt_the_tutorial_character/)\n\t\t- [Edit in another tab](https://www.reddit.com/r/comfyui/comments/16d0wtx/workflow_using_15_scribble_controlnet_to_feed/)\n\t\t- [semi automated inpainting](https://www.youtube.com/watch?v=SMOM1bIY5yA)\n\t\t- [Canvas editor with layers](https://github.com/Lerc/canvas_tab)\n\t\t- [Build custom nodes howto](https://github.com/chrisgoringe/Comfy-Custom-Node-How-To/wiki)\n\t\t- [Images / workflows](https://comfyworkflows.com/)\n\t\t- [Interpolate everything (openpose)](https://github.com/shockz0rz/ComfyUI_InterpolateEverything)\n\t\t- [Autogen inside comfyui](https://github.com/xXAdonesXx/NodeGPT)\n\t\t- [autogen tutorial](https://www.youtube.com/watch?v=PUPO2tTyPOo)\n\t\t- [lcm consistency lora](https://github.com/0xbitches/ComfyUI-LCM)\n\t\t- [touch designer](https://github.com/olegchomp/TDComfyUI)\n\t\t- [NimaNzrii/comfyui-photoshop:](github.com)](https://github.com/NimaNzrii/comfyui-photoshop) [[Photoshop]] node inside of ComfyUi, send and get data from Photoshop\n\t\t\t- Reddit post on the matter  [(2) NEW AI NEWS! Photoshop to Comfyui V1 is Finally Released! : comfyui (reddit.com)](https://www.reddit.com/r/comfyui/comments/18jygtn/new_ai_news_photoshop_to_comfyui_v1_is_finally/)\n\t\t- [Aerial view of the buildingÔºàÂª∫Á≠ëÈ∏üÁû∞ÂõæÔºâ\n\t\t\t- v1.0 | Stable Diffusion LoRA | Civitai](https://civitai.com/models/121728/aerial-view-of-the-building)\n-\n\n## Current Landscape (2025)\n\n- Industry adoption and implementations\n  - Metaverse platforms continue to evolve with focus on interoperability and open standards\n  - Web3 integration accelerating with decentralised identity and asset ownership\n  - Enterprise adoption growing in virtual collaboration, training, and digital twins\n  - UK companies increasingly active in metaverse development and immersive technologies\n\n- Technical capabilities\n  - Real-time rendering at photorealistic quality levels\n  - Low-latency networking enabling seamless multi-user experiences\n  - AI-driven content generation and procedural world building\n  - Spatial audio and haptics enhancing immersion\n\n- UK and North England context\n  - Manchester: Digital Innovation Factory supports metaverse startups and research\n  - Leeds: Holovis leads in immersive experiences for entertainment and training\n  - Newcastle: University research in spatial computing and interactive systems\n  - Sheffield: Advanced manufacturing using digital twin technology\n\n- Standards and frameworks\n  - Metaverse Standards Forum driving interoperability protocols\n  - WebXR enabling browser-based immersive experiences\n  - glTF and USD for 3D asset interchange\n  - Open Metaverse Interoperability Group defining cross-platform standards\n\n## Metadata\n\n- **Last Updated**: 2025-11-16\n- **Review Status**: Automated remediation with 2025 context\n- **Verification**: Academic sources verified\n- **Regional Context**: UK/North England where applicable",
  "properties": {
    "id": "comfyui-owl-axioms",
    "collapsed": "true",
    "- ontology": "true",
    "- term-id": "mv-258351002389",
    "- preferred-term": "ComfyUI",
    "- source-domain": "metaverse",
    "- status": "active",
    "- public-access": "true",
    "- definition": "A component of the metaverse ecosystem focusing on comfyui.",
    "- maturity": "mature",
    "- authority-score": "0.85",
    "- owl:class": "mv:Comfyui",
    "- owl:physicality": "ConceptualEntity",
    "- owl:role": "Concept",
    "- belongsToDomain": "[[MetaverseDomain]]",
    "- enables": "[[ImmersiveExperience]], [[Presence]], [[SpatialComputing]]",
    "- requires": "[[DisplayTechnology]], [[TrackingSystem]], [[RenderingEngine]]",
    "- bridges-to": "[[HumanComputerInteraction]], [[ComputerVision]], [[Robotics]]",
    "public": "true",
    "- The [video](https://www.youtube.com/watch?v=AU8NDSBIS1U) showcases a plugin that provides realtime AI assistance to - ### OntologyBlock id": "krita-ontology collapsed:: true - ontology:: true - term-id:: mv-523525042165 - [[krita]], a digital painting software. The channel, Nerdy Rodent, offers tutorials on [[Stable Diffusion]], Generative AI, [[Large language models]], and other AI tools, catering to AI enthusiasts and professionals. The plugin enhances various aspects of AI technology, such as voice cloning, text-to-speech, and style transfer, making it a valuable resource for AI enthusiasts and artists alike."
  },
  "backlinks": [
    "Open Generative AI tools",
    "Logseq",
    "Real Time",
    "AI Video",
    "BC-0072-node",
    "Open Webui and Pipelines",
    "Prompt Engineering",
    "AnimateDiff",
    "ComfyUI",
    "Python and PyTorch",
    "BC-0037-public-key",
    "Flux",
    "relighting",
    "PlayerTwo",
    "ollama",
    "Upscaling",
    "BC-0074-light-node"
  ],
  "wiki_links": [
    "experiment",
    "Upscaling",
    "collaborative",
    "LoRA DoRA etc",
    "Artificial Intelligence",
    "ComfyWorkFlows",
    "Diffusion Models",
    "DisplayTechnology",
    "krita",
    "Apple",
    "json",
    "Inpainting",
    "contents",
    "ImmersiveExperience",
    "SpatialComputing",
    "Photoshop",
    "Google",
    "TrackingSystem",
    "Robotics",
    "stable-diffusion",
    "Courses and Training",
    "HumanComputerInteraction",
    "krita-plugin",
    "Generative AI",
    "Checkpoints",
    "Krita",
    "Dall-e",
    "MetaverseDomain",
    "ComfyUI",
    "AnimateDiff",
    "Large language models",
    "Gemini",
    "Sampling",
    "license",
    "RenderingEngine",
    "Tips and Tricks",
    "IPAdapter",
    "Stable Diffusion",
    "Presence",
    "ComputerVision"
  ],
  "ontology": {
    "term_id": "mv-258351002389",
    "preferred_term": "ComfyUI",
    "definition": "A component of the metaverse ecosystem focusing on comfyui.",
    "source_domain": "metaverse",
    "maturity_level": null,
    "authority_score": 0.85
  }
}