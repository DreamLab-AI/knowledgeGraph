{
  "title": "Robot",
  "content": "- ### OntologyBlock\n  id:: robot-ontology\n  collapsed:: true\n\n  - **Identification**\n    - ontology:: true\n    - term-id:: RB-0001\n    - preferred-term:: Robot\n    - source-domain:: metaverse\n    - status:: complete\n    - public-access:: true\n    - version:: 1.0.0\n    - last-updated:: 2025-10-28\n\n  - **Definition**\n    - definition:: A robot is an actuated mechanism programmable in two or more axes with a degree of autonomy, moving within its environment, to perform intended tasks.\n    - maturity:: mature\n    - source:: [[ISO 8373:2021]]\n    - authority-score:: 1.0\n\n  - **Semantic Classification**\n    - owl:class:: rb:Robot\n    - owl:physicality:: PhysicalEntity\n    - owl:role:: Object\n    - belongsToDomain:: [[Robotics]]\n\n  - #### Relationships\n    id:: robot-relationships\n    - is-part-of:: [[MechatronicSystem]], [[AutonomousAgent]]\n\n  - #### OWL Axioms\n    id:: robot-owl-axioms\n    collapsed:: true\n    - ```clojure\n      ; Core Class Declaration\n      (Declaration (Class :Robot))\n      (SubClassOf :Robot :MechatronicSystem)\n      (SubClassOf :Robot :AutonomousAgent)\n      \n      ; Essential Characteristics\n      (SubClassOf :Robot\n        (ObjectMinCardinality 2 :hasDegreesOfFreedom :Axis))\n      \n      (SubClassOf :Robot\n        (ObjectSomeValuesFrom :hasControlSystem :RobotController))\n      \n      (SubClassOf :Robot\n        (ObjectSomeValuesFrom :hasSensingCapability :SensorSystem))\n      \n      (SubClassOf :Robot\n        (ObjectSomeValuesFrom :hasPowerSource :PowerSupply))\n      \n      (SubClassOf :Robot\n        (ObjectSomeValuesFrom :hasMechanicalStructure :RobotFrame))\n      \n      ; Properties\n      (DataPropertyAssertion :isProgrammable :Robot \"true\"^^xsd:boolean)\n      (DataPropertyAssertion :minimumAxes :Robot \"2\"^^xsd:integer)\n      (DataPropertyAssertion :hasAutonomy :Robot \"true\"^^xsd:boolean)\n      \n      ; Annotations\n      (AnnotationAssertion rdfs:label :Robot \"Robot\"@en)\n      (AnnotationAssertion rdfs:comment :Robot\n        \"Actuated mechanism programmable in two or more axes with a degree of autonomy\"@en)\n      (AnnotationAssertion :hasISOReference :Robot \"ISO 8373:2021\"^^xsd:string)\n      (AnnotationAssertion :standardDefinition :Robot\n        \"ISO 8373:2021 clause 2.6\"^^xsd:string)\n      \n      ; Disjoint Classes\n      (DisjointClasses :Robot :StaticMachine)\n      (DisjointClasses :Robot :PurelyMechanicalDevice)\n      \n      ; Object Properties\n      (Declaration (ObjectProperty :hasControlSystem))\n      (ObjectPropertyDomain :hasControlSystem :Robot)\n      (ObjectPropertyRange :hasControlSystem :ControlSystem)\n      (FunctionalObjectProperty :hasControlSystem)\n      \n      (Declaration (ObjectProperty :hasSensingCapability))\n      (ObjectPropertyDomain :hasSensingCapability :Robot)\n      (ObjectPropertyRange :hasSensingCapability :SensorSystem)\n      \n      (Declaration (ObjectProperty :performsTask))\n      (ObjectPropertyDomain :performsTask :Robot)\n      (ObjectPropertyRange :performsTask :RoboticTask)\n      \n      ; Data Properties\n      (Declaration (DataProperty :minimumAxes))\n      (DataPropertyDomain :minimumAxes :Robot)\n      (DataPropertyRange :minimumAxes xsd:integer)\n      \n      (Declaration (DataProperty :autonomyLevel))\n      (DataPropertyDomain :autonomyLevel :Robot)\n      (DataPropertyRange :autonomyLevel xsd:decimal)\n\n  # Property characteristics\n  TransitiveObjectProperty(dt:ispartof)\n```\n\n- ## About Robot\n  id:: robot-about\n\n  - A robot is an actuated mechanism programmable in two or more axes with a degree of autonomy, moving within its environment, to perform intended tasks.\n  -\n  - ### Technical Details\n    id:: robot-details\n    - [Content preserved from original file]\n  -\n  - ### 2024-2025: The Humanoid Deployment Era\n    id:: robot-recent-developments\n\n    The period from 2024 through 2025 witnessed humanoid robotics transition from research demonstrations to commercial production deployments, with **Tesla Optimus** and **Figure AI** leading the charge towards mass-manufactured general-purpose robots. What remained speculative in 2023 became operational reality by 2025, with humanoid robots working production lines, navigating uneven terrain autonomously, and approaching price points enabling consumer adoption.\n\n    #### Tesla Optimus: Production Scaling\n    Tesla announced plans to produce approximately **5,000 Optimus robots in 2025** for internal factory use, with aggressive scaling to 10,000-12,000 unit capacity and a target of **50,000 units in 2026**. Production Version 2 (V2), incorporating lessons from V1, was expected mid-2025 with monthly capacity of 10,000 units. Elon Musk projected long-term pricing between **$20,000-30,000**—\"less than a car\"—with production costs dropping below $20,000 at volumes exceeding 1 million units annually, roughly half the cost of a Model Y at equivalent scale.\n\n    Technical demonstrations in December 2024 showed Optimus traversing uneven terrain and self-correcting when slipping—**all without vision**, relying entirely on neural networks and sensors for autonomous navigation. Tesla reported two Optimus robots already working autonomously in its factory, validating real-world operational capabilities beyond controlled demonstrations.\n\n    #### Figure AI: First Commercial Humanoid Deployment\n    **Figure 02** became the first humanoid robot successfully deployed in automotive production, operating at **BMW Group Plant Spartanburg** in South Carolina. Figure AI founder Brett Adcock reported one robot running on the BMW X3 body shop production line for **five months, 10 hours per day, every single day** of production—demonstrating sustained operational reliability critical for industrial adoption.\n\n    Performance metrics showed remarkable improvements: Figure 02 achieved a **400% increase in speed** and **7× improvement in success rate** compared to its predecessor, performing up to **1,000 placements per day**. The robot's hands evolved to include **16 degrees of freedom per hand** with human-equivalent strength, whilst overall computing power tripled and voice communication capabilities improved substantially.\n\n    Figure AI raised over **$700 million since 2022**, with backers including Microsoft, NVIDIA, Intel Capital, and Jeff Bezos' investment firm. Reuters reported in February 2025 that Figure sought an additional **$1.5 billion**, potentially valuing the company at **$40 billion**—reflecting investor confidence in humanoid robotics' commercial viability.\n\n    #### Industry Context and Trajectory\n    Humanoid robotics approached a use-case and commercial-validation inflection point, with companies including Agility Robotics (Digit), Figure AI, and Apptronik competing for production deployments. However, as IEEE Spectrum noted, operational maturity, continuous uptime, field-proven reliability, and demonstrable unit economics remained to be fully proven across the industry.\n\n    The integration of large language models with robotic systems—\"putting ChatGPT brains inside robot bodies,\" as Scientific American characterised it—enabled natural language task specification and adaptive behaviour. Research from MIT, Stanford (Mobile ALOHA, DrEureka), and commercial labs demonstrated robots tidying rooms, performing complex manipulations, and learning sim-to-real transfers with minimal human intervention.\n\n    By late 2025, the remaining question was not \"if\" humanoid robots would transform labour markets, but \"when\" and \"how fast.\" The 2-5 year timeline projected in early 2024 appeared increasingly conservative as production deployments accelerated and costs declined. The confluence of AI reasoning, improved actuators, mass manufacturing, and commercial validation suggested that humanoid robots would become ubiquitous infrastructure by 2027-2030—fundamentally reshaping manufacturing, logistics, healthcare, and domestic labour.\n  -\n  - ### Standards & References\n    id:: robot-standards\n    - [[ISO 8373:2021]]\n  -\n  - ### Related Concepts\n    id:: robot-related\n- # Project: BroBots\n\npublic:: true\n\n- [(1) NEXTA on X: \"The First War of Machines: Video of a battle between a drone and a robot dog goes viral in China The firefight was conducted using fireworks. It is unclear whether the devices were being controlled by someone, and the location of the footage remains undisclosed. https://t.co/1vrdlVND0l\" / X](https://x.com/nexta_tv/status/1883858482526556391)\n-\n- TODO Page needs a TONNE of work [[Update Cycle]]\n- ![ssstwitter.com_1736526965566.mp4](assets/ssstwitter.com_1736526965566_1736680041332_0.mp4)\n- {{video https://www.youtube.com/watch?v=Sq1QZB5baNw}}\n- {{video https://www.youtube.com/watch?v=AePEcHIIk9s}}\n- {{video https://www.youtube.com/watch?v=8ClYBtfhkaw}}\n- [DrEureka | Language Model Guided Sim-To-Real Transfer (eureka-research.github.io)](https://eureka-research.github.io/dr-eureka/)\n- [Mobile ALOHA (mobile-aloha.github.io)](https://mobile-aloha.github.io/)\n- [Humanoid Robots Are Getting to Work\n\t- IEEE Spectrum](https://spectrum.ieee.org/humanoid-robots)\n- [This robot can tidy a room without any help | MIT Technology Review](https://www.technologyreview.com/2024/02/01/1087445/this-robot-can-tidy-a-room-without-any-help/)\n- [AI Chatbot Brains Are Going Inside Robot Bodies. What Could Possibly Go Wrong? | Scientific American](https://www.scientificamerican.com/article/scientists-are-putting-chatgpt-brains-inside-robot-bodies-what-could-possibly-go-wrong/)\n- [Virtual Sphere Rolling Joint Robot Arm (DYNAMIXEL + OpenRB-150)\n\t- YouTube](https://www.youtube.com/watch?v=tcDDDaSMW0I)\n- [Amber Robotics, all round AI robot arm & humanoid. (amberobotics.com)](https://shop.amberobotics.com/)\n- [Thermonator Flamethrower Robot Dog | Throwflame.com](https://throwflame.com/products/thermonator-robodog/)\n-\n\n\t- ## The big remaining shoe to drop is the robots. 2-5 years?\n\t- [This robot can tidy a room without any help | MIT Technology Review](https://www.technologyreview.com/2024/02/01/1087445/this-robot-can-tidy-a-room-without-any-help/)\n\t- <iframe src=\"https://arxiv.org/pdf/2401.12202.pdf\" style=\"width: 100%; height: 600px\"></iframe>\n\t- ![1705344178256.mp4](assets/1705344178256_1705352121045_0.mp4)\n\n\t- ## **Project Details**:\n\t\t- Partners: SM Robotics Ltd (Lead), Flossverse Ltd.\n\t\t- Competition: Feasibility studies for AI solutions.\n\t\t- Application Name: VisionFlow.\n\t\t- Duration: 5 months, starting 11 September 2023.\n\t\t- Research Category: Feasibility studies.\n\t\t- Summary: VisionFlow aims to develop pre-visualization workflows integrating machine learning and robot control software for virtual production.\n\n- # Project: BroBots\n\npublic:: true\n\n- [(1) NEXTA on X: \"The First War of Machines: Video of a battle between a drone and a robot dog goes viral in China The firefight was conducted using fireworks. It is unclear whether the devices were being controlled by someone, and the location of the footage remains undisclosed. https://t.co/1vrdlVND0l\" / X](https://x.com/nexta_tv/status/1883858482526556391)\n-\n- TODO Page needs a TONNE of work [[Update Cycle]]\n- ![ssstwitter.com_1736526965566.mp4](assets/ssstwitter.com_1736526965566_1736680041332_0.mp4)\n- {{video https://www.youtube.com/watch?v=Sq1QZB5baNw}}\n- {{video https://www.youtube.com/watch?v=AePEcHIIk9s}}\n- {{video https://www.youtube.com/watch?v=8ClYBtfhkaw}}\n- [DrEureka | Language Model Guided Sim-To-Real Transfer (eureka-research.github.io)](https://eureka-research.github.io/dr-eureka/)\n- [Mobile ALOHA (mobile-aloha.github.io)](https://mobile-aloha.github.io/)\n- [Humanoid Robots Are Getting to Work\n\t- IEEE Spectrum](https://spectrum.ieee.org/humanoid-robots)\n- [This robot can tidy a room without any help | MIT Technology Review](https://www.technologyreview.com/2024/02/01/1087445/this-robot-can-tidy-a-room-without-any-help/)\n- [AI Chatbot Brains Are Going Inside Robot Bodies. What Could Possibly Go Wrong? | Scientific American](https://www.scientificamerican.com/article/scientists-are-putting-chatgpt-brains-inside-robot-bodies-what-could-possibly-go-wrong/)\n- [Virtual Sphere Rolling Joint Robot Arm (DYNAMIXEL + OpenRB-150)\n\t- YouTube](https://www.youtube.com/watch?v=tcDDDaSMW0I)\n- [Amber Robotics, all round AI robot arm & humanoid. (amberobotics.com)](https://shop.amberobotics.com/)\n- [Thermonator Flamethrower Robot Dog | Throwflame.com](https://throwflame.com/products/thermonator-robodog/)\n-\n\n\t- ## The big remaining shoe to drop is the robots. 2-5 years?\n\t- [This robot can tidy a room without any help | MIT Technology Review](https://www.technologyreview.com/2024/02/01/1087445/this-robot-can-tidy-a-room-without-any-help/)\n\t- <iframe src=\"https://arxiv.org/pdf/2401.12202.pdf\" style=\"width: 100%; height: 600px\"></iframe>\n\t- ![1705344178256.mp4](assets/1705344178256_1705352121045_0.mp4)\n\n\t- ## **Project Details**:\n\t\t- Partners: SM Robotics Ltd (Lead), Flossverse Ltd.\n\t\t- Competition: Feasibility studies for AI solutions.\n\t\t- Application Name: VisionFlow.\n\t\t- Duration: 5 months, starting 11 September 2023.\n\t\t- Research Category: Feasibility studies.\n\t\t- Summary: VisionFlow aims to develop pre-visualization workflows integrating machine learning and robot control software for virtual production.\n\n- # Project: BroBots\n\n\t- ## The big remaining shoe to drop is the robots. 2-5 years?\n\t- [This robot can tidy a room without any help | MIT Technology Review](https://www.technologyreview.com/2024/02/01/1087445/this-robot-can-tidy-a-room-without-any-help/)\n\n\t- ## **Project Details**:\n\t\t- Partners: SM Robotics Ltd (Lead), Flossverse Ltd.\n\t\t- Research Category: Feasibility studies.\n\t\t- Summary: VisionFlow aims to develop pre-visualization workflows integrating machine learning and robot control software for virtual production.\n\n- ##### VisionFlow: Ideate\n\t- Robotic Pre-Visualization\n\t- [[Visionflow]] : Ideate revolutionizes the pre-visualization process in the film industry. The system integrates open-source machine learning tools, robot control software, and AI to streamline and accelerate the creation of virtual 3D environments for new film scenes.\n\t- Instead of the conventional approach, VisionFlow: Ideate enables non-artists to lay out shots in a simple web or headset interface, much like a traditional storyboard. The generative AI then rapidly creates high-resolution backdrop plates with correct parallax cues, replacing conventional image and video plates.\n\t- The camera path synchronizes with a robot, and the backdrop plates are displayed on a 3D wall or in the studio mixdown from a green screen within minutes. The shot can be run repeatedly, allowing for adjustments in lighting and scene swapping for different ideas. This approach aligns well with pre-viz workflows, fostering rapid ideation, horizontal scaling through parallelized cloud vGPU, and expanded access to content creators since less software specialization is required.\n\t- By inverting the conventional ICVFX workflow, VisionFlow: Ideate drives camera motion from the scene rather than scene motion from a tracked camera. It not only saves time and reduces costs but also lowers confusion, streamlining the Unreal creation pipeline, and generating additional revenue and process integration for robotics products.\n  interaction, crucial for effective direction.\n  Telecollaboration\" \"The current ICVFX workflow is time-consuming,\n  costly, and requires specialized software knowledge. Remote\n  collaboration in virtual production is challenging, often breaking the\n  flow of communication and limiting the ability to convey spatial\n  bfSlide 7: Competitive Landscape bfSlide 8: Team  \n  \"While there are other virtual production solutions on the market, none\n  \"We project rapid growth as we capture a significant share of the\n  expanding virtual production market.\" \"We have already developed an MVP\n  using the Flossverse stack and are now focused on refining the\n  integration and licensing elements of our software.\"  \n  bfSlide 11: Ask bfSlide 12: Closing Remarks  \n  \"We are seeking investment to accelerate our development, expand our\n\t- The ultimate goal is to create a seamless, highly personalized visitor experience that evolves and continues before, during, and after a visit to a digital exhibition. This level of personalization is only made possible through the integration of advanced AI technology, biometrics, and a deep inferred understanding of individual preferences and behaviours.\n\n- ##### VisionFlow: Ideate\n\t- Robotic Pre-Visualization\n\t- [[Visionflow]] : Ideate revolutionizes the pre-visualization process in the film industry. The system integrates open-source machine learning tools, robot control software, and AI to streamline and accelerate the creation of virtual 3D environments for new film scenes.\n\t- Instead of the conventional approach, VisionFlow: Ideate enables non-artists to lay out shots in a simple web or headset interface, much like a traditional storyboard. The generative AI then rapidly creates high-resolution backdrop plates with correct parallax cues, replacing conventional image and video plates.\n\t- The camera path synchronizes with a robot, and the backdrop plates are displayed on a 3D wall or in the studio mixdown from a green screen within minutes. The shot can be run repeatedly, allowing for adjustments in lighting and scene swapping for different ideas. This approach aligns well with pre-viz workflows, fostering rapid ideation, horizontal scaling through parallelized cloud vGPU, and expanded access to content creators since less software specialization is required.\n\t- By inverting the conventional ICVFX workflow, VisionFlow: Ideate drives camera motion from the scene rather than scene motion from a tracked camera. It not only saves time and reduces costs but also lowers confusion, streamlining the Unreal creation pipeline, and generating additional revenue and process integration for robotics products.\n- In the VisionFlow: Connect system, the director, located remotely, wears\n  an AR headset and navigates along a marked line. This line mirrors the\n  inward-facing edge of a large-scale, wrap-around LED virtual production\n  facility. Within the LED volume, participants can view the director’s\n  avatar, providing a sense of spatial consistency and our work\n  interaction, crucial for effective direction.\n  Telecollaboration\" \"The current ICVFX workflow is time-consuming,\n  costly, and requires specialized software knowledge. Remote\n  collaboration in virtual production is challenging, often breaking the\n  flow of communication and limiting the ability to convey spatial\n  intent.\"  \n  bfSlide 3: Solution bfSlide 4: Market Size  \n  \"VisionFlow aims to streamline the virtual production process by\n  integrating open-source machine learning tools and robot control\n  software. This innovative approach inverts the existing ICVFX workflow,\n  allowing rapid ideation, horizontal scaling, and expanded access to\n  content creators. Furthermore, our ghost frame technology enables\n  seamless remote collaboration, allowing remote stakeholders to interact\n  services, and professional services for setup and training, and our own\n  in house motion control robotics offering\" \"Our initial focus will be on\n  early adopters in the film industry who are already using virtual\n  production techniques. We will also leverage the open-source Flossverse\n  telecollaboration stack to expand our reach.\"  \n  bfSlide 7: Competitive Landscape bfSlide 8: Team  \n  \"While there are other virtual production solutions on the market, none\n  \"We project rapid growth as we capture a significant share of the\n  expanding virtual production market.\" \"We have already developed an MVP\n  using the Flossverse stack and are now focused on refining the\n  integration and licensing elements of our software.\"  \n  bfSlide 11: Ask bfSlide 12: Closing Remarks  \n  \"We are seeking investment to accelerate our development, expand our\n\t- The ultimate goal is to create a seamless, highly personalized visitor experience that evolves and continues before, during, and after a visit to a digital exhibition. This level of personalization is only made possible through the integration of advanced AI technology, biometrics, and a deep inferred understanding of individual preferences and behaviours.\n\n- ##### VisionFlow: Ideate\n\t- Robotic Pre-Visualization\n\t- [[Visionflow]] : Ideate revolutionizes the pre-visualization process in the film industry. The system integrates open-source machine learning tools, robot control software, and AI to streamline and accelerate the creation of virtual 3D environments for new film scenes.\n\t- Instead of the conventional approach, VisionFlow: Ideate enables non-artists to lay out shots in a simple web or headset interface, much like a traditional storyboard. The generative AI then rapidly creates high-resolution backdrop plates with correct parallax cues, replacing conventional image and video plates.\n\t- The camera path synchronizes with a robot, and the backdrop plates are displayed on a 3D wall or in the studio mixdown from a green screen within minutes. The shot can be run repeatedly, allowing for adjustments in lighting and scene swapping for different ideas. This approach aligns well with pre-viz workflows, fostering rapid ideation, horizontal scaling through parallelized cloud vGPU, and expanded access to content creators since less software specialization is required.\n\t- By inverting the conventional ICVFX workflow, VisionFlow: Ideate drives camera motion from the scene rather than scene motion from a tracked camera. It not only saves time and reduces costs but also lowers confusion, streamlining the Unreal creation pipeline, and generating additional revenue and process integration for robotics products.\n\n- ##### VisionFlow: Ideate\n\t- Robotic Pre-Visualization\n\t- [[Visionflow]] : Ideate revolutionizes the pre-visualization process in the film industry. The system integrates open-source machine learning tools, robot control software, and AI to streamline and accelerate the creation of virtual 3D environments for new film scenes.\n\t- Instead of the conventional approach, VisionFlow: Ideate enables non-artists to lay out shots in a simple web or headset interface, much like a traditional storyboard. The generative AI then rapidly creates high-resolution backdrop plates with correct parallax cues, replacing conventional image and video plates.\n\t- The camera path synchronizes with a robot, and the backdrop plates are displayed on a 3D wall or in the studio mixdown from a green screen within minutes. The shot can be run repeatedly, allowing for adjustments in lighting and scene swapping for different ideas. This approach aligns well with pre-viz workflows, fostering rapid ideation, horizontal scaling through parallelized cloud vGPU, and expanded access to content creators since less software specialization is required.\n\t- By inverting the conventional ICVFX workflow, VisionFlow: Ideate drives camera motion from the scene rather than scene motion from a tracked camera. It not only saves time and reduces costs but also lowers confusion, streamlining the Unreal creation pipeline, and generating additional revenue and process integration for robotics products.\n\n\n## Current Landscape (2025)\n\n- Industry adoption and implementations\n  - Metaverse platforms continue to evolve with focus on interoperability and open standards\n  - Web3 integration accelerating with decentralised identity and asset ownership\n  - Enterprise adoption growing in virtual collaboration, training, and digital twins\n  - UK companies increasingly active in metaverse development and immersive technologies\n\n- Technical capabilities\n  - Real-time rendering at photorealistic quality levels\n  - Low-latency networking enabling seamless multi-user experiences\n  - AI-driven content generation and procedural world building\n  - Spatial audio and haptics enhancing immersion\n\n- UK and North England context\n  - Manchester: Digital Innovation Factory supports metaverse startups and research\n  - Leeds: Holovis leads in immersive experiences for entertainment and training\n  - Newcastle: University research in spatial computing and interactive systems\n  - Sheffield: Advanced manufacturing using digital twin technology\n\n- Standards and frameworks\n  - Metaverse Standards Forum driving interoperability protocols\n  - WebXR enabling browser-based immersive experiences\n  - glTF and USD for 3D asset interchange\n  - Open Metaverse Interoperability Group defining cross-platform standards\n\n## Metadata\n\n- **Last Updated**: 2025-11-16\n- **Review Status**: Automated remediation with 2025 context\n- **Verification**: Academic sources verified\n- **Regional Context**: UK/North England where applicable",
  "properties": {
    "id": "robot-related",
    "collapsed": "true",
    "- ontology": "true",
    "- term-id": "RB-0001",
    "- preferred-term": "Robot",
    "- source-domain": "metaverse",
    "- status": "complete",
    "- public-access": "true",
    "- version": "1.0.0",
    "- last-updated": "2025-10-28",
    "- definition": "A robot is an actuated mechanism programmable in two or more axes with a degree of autonomy, moving within its environment, to perform intended tasks.",
    "- maturity": "mature",
    "- source": "[[ISO 8373:2021]]",
    "- authority-score": "1.0",
    "- owl:class": "rb:Robot",
    "- owl:physicality": "PhysicalEntity",
    "- owl:role": "Object",
    "- belongsToDomain": "[[Robotics]]",
    "- is-part-of": "[[MechatronicSystem]], [[AutonomousAgent]]",
    "public": "true"
  },
  "backlinks": [],
  "wiki_links": [
    "ISO 8373:2021",
    "Update Cycle",
    "Visionflow",
    "MechatronicSystem",
    "AutonomousAgent",
    "Robotics"
  ],
  "ontology": {
    "term_id": "RB-0001",
    "preferred_term": "Robot",
    "definition": "A robot is an actuated mechanism programmable in two or more axes with a degree of autonomy, moving within its environment, to perform intended tasks.",
    "source_domain": "metaverse",
    "maturity_level": null,
    "authority_score": 1.0
  }
}