{
  "title": "AI Video",
  "content": "- ### OntologyBlock\n  id:: ai-video-ontology\n  collapsed:: true\n\t- ontology:: true\n\t- term-id:: mv-449611067545\n\t- preferred-term:: AI Video\n\t- source-domain:: metaverse\n\t- status:: active\n\t- public-access:: true\n\t- definition:: A component of the metaverse ecosystem focusing on ai video.\n\t- maturity:: mature\n\t- authority-score:: 0.85\n\t- owl:class:: mv:AiVideo\n\t- owl:physicality:: ConceptualEntity\n\t- owl:role:: Concept\n\t- belongsToDomain:: [[MetaverseDomain]]\n\t- #### Relationships\n\t  id:: ai-video-relationships\n\t\t- enables:: [[ImmersiveExperience]], [[Presence]], [[SpatialComputing]]\n\t\t- requires:: [[DisplayTechnology]], [[TrackingSystem]], [[RenderingEngine]]\n\t\t- bridges-to:: [[HumanComputerInteraction]], [[ComputerVision]], [[Robotics]]\n\n\t- #### OWL Axioms\n\t  id:: ai-video-owl-axioms\n\t  collapsed:: true\n\t\t- ```clojure\n\t\t  Declaration(Class(mv:AiVideo))\n\n\t\t  # Classification\n\t\t  SubClassOf(mv:AiVideo mv:ConceptualEntity)\n\t\t  SubClassOf(mv:AiVideo mv:Concept)\n\n\t\t  # Domain classification\n\t\t  SubClassOf(mv:AiVideo\n\t\t    ObjectSomeValuesFrom(mv:belongsToDomain mv:MetaverseDomain)\n\t\t  )\n\n\t\t  # Annotations\n\t\t  AnnotationAssertion(rdfs:label mv:AiVideo \"AI Video\"@en)\n\t\t  AnnotationAssertion(rdfs:comment mv:AiVideo \"A component of the metaverse ecosystem focusing on ai video.\"@en)\n\t\t  AnnotationAssertion(dcterms:identifier mv:AiVideo \"mv-449611067545\"^^xsd:string)\n\t\t  ```\n\npublic:: true\n\n#Public page automatically published\n\n- # Real-Time\n- [H. Jensons -TWISTED : r/aivideo (reddit.com)](https://www.reddit.com/r/aivideo/comments/1eryhk6/h_jensons_twisted/)\n- ![rapidsave.com_h_jensons_twisted-zyldr5g40mid1.mp4](../assets/rapidsave.com_h_jensons_twisted-zyldr5g40mid1_1723965805126_0.mp4)\n- [EndlessDreams - Real-Time videos made easy - YouTube](https://www.youtube.com/watch?v=irUpybVgdDY)\n- {{video https://www.youtube.com/watch?v=irUpybVgdDY}}\n- # Commercial Offerings\n- {{video https://www.youtube.com/watch?v=_XtS_4PzEyk&}}\n- ## Runway Gen 3\n\t- [Introducing Gen-3 Alpha: A New Frontier for Video Generation (runwayml.com)](https://runwayml.com/blog/introducing-gen-3-alpha/)\n\t-\n- ## Luma Dream Machine?\n\t- Luma Dream Machine is a browser-based AI video generator [developed by Luma Labs](https://www.creativebloq.com/ai/luma-dream-machine-makes-ai-video-open-to-all), a San Francisco-based startup. It allows users to generate short videos (around 5 seconds) by simply entering a text or image prompt.\n\t\t- **Free to Use**: Luma Dream Machine is free to try, with no waiting list or subscription required. Users get 30 free video generations per month.\n\t\t- **High-Quality Output**: The AI produces impressively clean and detailed videos, adhering to prompts accurately and generating relatively coherent motion.\n\t\t- **Fast Generation**: Videos are generated in around 2 minutes after entering the prompt.\n\t\t- **Consistent Subjects**: Characters and subjects appear consistent throughout the video, capable of expressing emotion better than many previous AI video models.\n\t- ## Limitations\n\t  While groundbreaking, and crucially, \"available\", Luma Dream Machine still has some limitations, as acknowledged by the company:\n\t\t- Morphing, warping, and unnatural movements\n\t\t- Difficulty with complex scenes or full-body shots\n\t\t- Text in videos may appear garbled\n\t\t- Anatomical issues like extra limbs or heads\n\t- [(1) Professor John Keeting on X: \"this was created with Luma AI I am very impressed. Made by Kevin Van Witt and the talented team at The Monster Library https://t.co/IXLWO1Be91\" / X (twitter.com)](https://twitter.com/ProfKeeting/status/1801632319536607623)\n\t- {{twitter https://twitter.com/ProfKeeting/status/1801632319536607623}}\n- {{embed ((661d5f76-bd9c-493d-afc1-efcec299ed24))}}\n- {{embed ((664465de-5bd3-4169-a90b-c03f117bef04))}}\n- # The Rest\n\t- **Lumiere: Google's Contribution** Google's Lumiere project also signifies progress in video generation capabilities, though full details remain undisclosed. This suggests ongoing competition and development  in the field.\n\t- **Meta's Approach: Foundational World Modeling** Meta (formerly Facebook) is taking a distinct approach, focusing on the underlying world modeling needed for video encoding and generation. This emphasis on understanding the principles of physics and object interactions could contribute to  more realistic AI-generated videos.\n\t- **Technical Capabilities and Limitations**\n\t\t- **Capabilities** Current AI video generators demonstrate proficiency in producing high-resolution images and videos. They are capable of style adaptation, simulating complex scenes with multiple elements, and handling variations in aspect ratio and resolution.\n\t\t- **Limitations**  Despite their strengths, these models still struggle to accurately simulate physics and lack a complete understanding of cause and effect. Occasional errors regarding object permanence highlight the existing gap between pattern recognition and a comprehensive understanding of the world.\n\t- **Ethical and Creative Considerations**\n\t\t- **Potential Impacts**  Advancements in AI video generation raise questions about the future of creative professions and the ethical implications of AI-generated content. Balancing technological innovation with safeguarding the integrity of human creativity is an important consideration.\n\t\t- **Challenges**  Distinguishing between pattern recognition and genuine understanding is pivotal in the ethical use of AI. The potential for misuse or the creation of harmful content underscores the need for clear guidelines and responsible practices.\n- # Open systems\n- [Stable Video 4D — Stability AI](https://stability.ai/news/stable-video-4d)\n-\n- ## OpenSora\n\t- [[Update Cycle]]\n- ## Stable Video\n\t- ![1708218759791.mp4](../assets/1708218759791_1708282889322_0.mp4)\n\t- [(16) Discarded Androids (Midjourney to Stable Video beta) : midjourney (reddit.com)](https://www.reddit.com/r/midjourney/comments/1ar6uz0/discarded_androids_midjourney_to_stable_video_beta/)\n\t\t- ![m2-res_960p.mp4](../assets/m2-res_960p_1708014022595_0.mp4)\n- ## Pika\n\t- [twitter link to the render loading below](https://twitter.com/BMaursky/status/1747700852226199815)\n\t  {{twitter https://twitter.com/BMaursky/status/1747700852226199815}} -\n- # Misc links being integrated.\n- [[MotionDirector]], with a dual-path LoRAs architecture to decouple the learning of appearance and motion. Further, we design a novel appearance-debiased temporal loss to mitigate the influence of appearance on the temporal training objective. Experimental results show the proposed method can generate videos of diverse appearances for the customized motions. Our method also supports various downstream applications, such as the mixing of different videos with their appearance and motion respectively, and animating a single image with customized motions.\n- [RAVE: Randomized Noise Shuffling for Fast and Consistent Video Editing with Diffusion Models (rave-video.github.io)](https://rave-video.github.io/)\n\t- https://discord.com/channels/1076117621407223829/1192162917395730635/1192162917395730635\n\t- Here's one way to use the brand new RAVE node from here: https://github.com/spacepxl/ComfyUI-RAVE\n\t\t- First pass often has flickering (depending a lot on the input), so I made a workflow to smooth even harsh flickering with AD. This allows for utilizing the transformative and often more detailed vid2vid from RAVE and still get smooth results in [[ComfyUI]]\n\t\t\t- Updated LCM version: https://discord.com/channels/1076117621407223829/1192162917395730635/1192212692354748427\n\t\t\t  using the \"video/controlgif/animatediff\" contolnet from here: https://huggingface.co/crishhh/animatediff_controlnet/blob/main/controlnet_checkpoint.ckpt\n\t\t- {{video https://rave-video.github.io/static/teaser/car-turn_final.mp4}}\n- ## Style transfer for humans\n\t- Multiple techniques tested with the same [[LoRA DoRA etc]] for [comparison](https://discord.com/channels/1076117621407223829/1198416106554130552/1198416106554130552)\n- [ActAnywhere](https://actanywhere.github.io/)\n- [AI-Enhanced Creator (beehiiv.com)](https://nejcsusec.beehiiv.com/)\n- AnimateAnyone for [[ComfyUI]] [MrForExample/ComfyUI-AnimateAnyone-Evolved: Improved AnimateAnyone implementation that allows you to use the opse image sequence and reference image to generate stylized video (github.com)](https://github.com/MrForExample/ComfyUI-AnimateAnyone-Evolved)\n- [CG Renders to AI ANIMATION\n\t- NIKE video — MOONWALKERS PICTURE](https://www.moonwalkerspicture.com/newslounge/cg-renders-to-ai-workflow-vol-02-anim)\n- Motion Control\n\t- [MotionCtrl (wzhouxiff.github.io)](https://wzhouxiff.github.io/projects/MotionCtrl/)\n- [[2401.12945] Lumiere: A Space-Time Diffusion Model for Video Generation (arxiv.org)](https://arxiv.org/abs/2401.12945)\n- [I2VGen-XL\n\t- a Hugging Face Space by damo-vilab](https://huggingface.co/spaces/damo-vilab/I2VGen-XL)\n- [ali-vilab/i2vgen-xl: Official repo for VGen: a holistic video generation ecosystem for video generation building on diffusion models (github.com)](https://github.com/ali-vilab/i2vgen-xl)\n\t- [MagicVideo-V2: Multi-Stage High-Aesthetic Video Generation (magicvideov2.github.io)](https://magicvideov2.github.io/)\n- Interpolation and interframe consistency\n\t- [controlnet and ebsynth temporal consistency](https://www.reddit.com/r/StableDiffusion/comments/114zmh3/controlnet_and_ebsynth_make_incredible_temporally/)\n\t- [Motion-Conditioned Diffusion Model for Controllable Video Synthesis](https://tsaishien-chen.github.io/MCDiff/)\n\t- [Interframe consistency is now here](https://twitter.com/cut_pow/status/1576748659051749377)\n\t- [Interpolation between two frames](https://film-net.github.io/)\n\t- [FILM frame interpolator](https://film-net.github.io/)\n\t- [ProPainter for Video Inpainting (shangchenzhou.com)](https://shangchenzhou.com/projects/ProPainter/)\n\t- [zengyh1900/Awesome-Image-Inpainting: A curated list of image inpainting and video inpainting papers and resources (github.com)](https://github.com/zengyh1900/Awesome-Image-Inpainting)\n\t-\n- [Runway AI video editing](https://www.youtube.com/c/RunwayML)\n\t- Gen2 examples\n- Multishot [VideoDrafter: Content-Consistent Multi-Scene Video Generation with LLM](https://videodrafter.github.io/)\n- [vienna with prompts](https://www.linkedin.com/posts/dr-andreas-fraunberger_marketinginnovation-digitaltourism-ar-ugcPost-7073039429417730048-BKfQ?utm_source=share&utm_medium=member_desktop)\n- [Video slowmo and enhance](http://zeyuan-chen.com/VideoINR/)\n- [deforum stable diffusion video](https://github.com/HelixNGC7293/DeforumStableDiffusionLocal)\n- [Phenaki](https://phenaki.video/)\n- Collaborative video pipeline\n- [Magicvideo (faster)](https://magicvideo.github.io/)\n- [Production ready re aging](https://studios.disneyresearch.com/2022/11/30/production-ready-face-re-aging-for-visual-effects/)\n- [distilled models for 25fps](https://arxiv.org/abs/2202.00512)\n- [Stable warpfusion](https://www.linkedin.com/posts/rainisto_stablediffusion-musicvideo-remix-activity-7018207241522614272-YT1y?utm_source=share&utm_medium=member_desktop)\n- [Video talking heads from text service](https://www.synthesia.io/)\n- [Tune a video](https://tuneavideo.github.io/)\n- [Vidyo: Generates videos for social networks from longer videos.](https://vidyo.ai/)\n- [Stylegan-T video transformer from google](https://sites.google.com/view/stylegan-t)\n- [Houdini](https://github.com/proceduralit/StableDiffusion_Houdini)\n- [Dream Mix video to video remix](https://dreamix-video-editing.github.io/)\n- [RIFE frame interpolation](https://github.com/megvii-research/ECCV2022-RIFE)\n- [example github for sd](https://github.com/vladmandic/rife)\n- [Synthesia corporate video generation](https://www.youtube.com/watch?v=4uzzD9sD-PI)\n- [pix2pixHD nextframe google colab](https://colab.research.google.com/github/dvschultz/ml-art-colabs/blob/master/Pix2PixHD_Next_Frame_Prediction.ipynb)\n- [minecraft demo codebase](https://github.com/TSFSean/InvokeAI-DiffusionCraftAI)\n- [animation from mixamo](https://www.reddit.com/r/StableDiffusion/comments/zecyc7/mixamo_animations_stable_diffusion_v2_depth2img/)\n- [Intel enhance photorealism in realtime](https://github.com/isl-org/PhotorealismEnhancement)\n- custom SD video to video script\n- [Testing a custom video2video script I'm working on. (These used RealisticVision1.4 & ControlNet) : r/StableDiffusion](https://www.reddit.com/r/StableDiffusion/comments/11iviep/testing_a_custom_video2video_script_im_working_on/)\n- [consistency tools for character tooning](https://www.reddit.com/r/StableDiffusion/comments/11okvc8/how_about_another_joke_murraaaay/)\n- Alibaba system\n\t- [website](https://videocomposer.github.io/)\n\t- [github](https://github.com/damo-vilab/videocomposer)\n\t- [model cards](https://huggingface.co/damo-vilab/MS-Image2Video)\n- [9 new tools](https://twitter.com/mreflow/status/1637957302073565184)\n- [Automatic1111 plugin](https://www.reddit.com/r/StableDiffusion/comments/11w0ba9/modelscope_17b_text2video_model_is_now_available/)\n- [Next frame prediction with controlnet](https://www.reddit.com/r/StableDiffusion/comments/11f8i0g/next_frame_prediction_with_controlnet/)\n- [Will smith eating spaghetti](https://www.reddit.com/r/StableDiffusion/comments/1244h2c/will_smith_eating_spaghetti/)\n- [Transform Video to Animation in Stable Diffusion | How to Install + BEST Consistency Settings: Learn how to use AI to create animations from real videos. We'll use Stable Diffusion and other tools for maximum consistencyProject Files:https://bit.ly/3...](https://www.youtube.com/watch?v=sVmi2Yp43c0&t=22)\n- [How to Use ModelScope text2video with Automatic1111’s Stable Diffusion Web UI | kombitz: Enable the Extension Click on the Extension tab and then click on Install from URL. Enter https://github.com/deforum-art/sd-webui-modelscope-text2video in the URL box and click on Install. Click on Installed and click on Apply and restart UI. Go to your stable-diffusion-webui/models folder and create a folder called ModelScope and then create a folder called t2v under ModelScope. This is your models folder for text2video.](https://www.kombitz.com/2023/03/28/how-to-use-modelscope-text2video-with-automatic1111s-stable-diffusion-web-ui/)\n- This article provides instructions on how to use ModelScope's text2video feature with Automatic1111's Stable Diffusion Web UI.\n- [latent consistency pipeline](https://www.reddit.com/r/StableDiffusion/comments/17fvxrq/60_frame_video_generated_in_646_seconds/)\n- [GitHub\n\t- Picsart-AI-Research/Text2Video-Zero: Text-to-Image Diffusion Models are Zero-Shot Video Generators: Text-to-Image Diffusion Models are Zero-Shot Video Generators\n\t- GitHub\n\t- Picsart-AI-Research/Text2Video-Zero: Text-to-Image Diffusion Models are Zero-Shot Video Generators](https://github.com/Picsart-AI-Research/Text2Video-Zero)\n- The Picsart-AI-Research/Text2Video-Zero repository contains code for a text-to-image diffusion model that can be used to generate videos from text input. The model is a zero-shot video generator, meaning that it does not require any training data in order to generate videos.\n- [LVDM for long video creation](https://yingqinghe.github.io/LVDM/)\n- [The Text2Room algorithm generates textured 3D meshes from a given text prompt by leveraging pre-trained 2D text-to-image models. The core idea is to select camera poses that will result in a seamless, textured 3D mesh. The algorithm iteratively fuses scene frames with the existing geometry to create the final mesh. Evaluation shows that the algorithm is able to generate room-scale 3D geometry with compelling textures from only text as input.](https://lukashoel.github.io/text-to-room/)\n- [The VMesh system models a scene with a triangular mesh and a sparse volume for efficient view synthesis. It is trained on multi-view images of an object to create a contiguous representation of the object's surface and volume. This representation is then used to generate a simplified triangular mesh and a sparse volume, which can be stored and rendered efficiently. The system is designed for real-time applications and can render at 2K 60FPS on common consumer devices.](https://bennyguo.github.io/vmesh/)\n- [LLM guided video generation paper](https://arxiv.org/abs/2309.15091)\n- [LVM video gen using LLM paper](https://arxiv.org/pdf/2309.17444.pdf)\n- [Temporal stable automatic plugin](https://www.reddit.com/r/StableDiffusion/comments/12sd4bi/results_from_latest_version_of_temporal_stable/)\n- [We present a method for high-resolution video synthesis using latent diffusion models (LDMs). Our approach first pre-trains an LDM on images, then introduces a temporal dimension to the latent space diffusion model and fine-tunes it on encoded image sequences (i.e. videos). We focus on two real-world applications: simulation of in-the-wild driving data and creative content creation with text-to-video modeling. Our method achieves state-of-the-art performance on real driving videos of 512 x 1024 resolution. Additionally, our approach can leverage off-the-shelf pre-trained image LDMs, turning the publicly available, state-of-the-art text-to-image LDM Stable Diffusion into an efficient and expressive text-to-video model.](https://buff.ly/41FgQrb)\n- [This script allows for the automation of video stylization using StableDiffusion and ControlNet.](https://github.com/volotat/SD-CN-Animation)\n- [Really easy videos in A1111](https://www.reddit.com/r/StableDiffusion/comments/12otdo0/the_secret_to_really_easy_videos_in_a1111_easier/)\n- [Dancer 4 keyframes, low noise, controlnet approach](https://www.reddit.com/r/StableDiffusion/comments/12nwpdx/dancer_4_keyframes_guide_and_source_files_for/)\n- [Flicker free video workflow paper (good!)](https://anonymous-31415926.github.io/)\n- [Pika labs](https://twitter.com/pika_labs)\n- [Realtime lip-sync API](https://getsynchronicity.io/)\n- [ms image to video on huggingface](https://huggingface.co/spaces/fffiloni/MS-Image2Video)\n- [model to video blender modules](https://github.com/tin2tin/Generative_AI)\n- [videocomposer in python 3.9](https://github.com/mindspore-lab/mindone/tree/master/examples/videocomposer)\n- [motionagent image to video](https://github.com/modelscope/motionagent)\n- [Animatediff comfy workflows on discord](https://discord.com/channels/1076117621407223829/1149372684220768367)\n- [fluid animation youtube](https://www.youtube.com/watch?v=ak_az6ZNYFM)\n- [Controlnet tutorial](https://www.youtube.com/watch?v=WHxIrY2wLQE)\n- [LCM loras for fast inferencing](https://huggingface.co/collections/latent-consistency/latent-consistency-models-loras-654cdd24e111e16f0865fba6)\n- Animatediff is a new animation software that provides a range of tools and features for creating high-quality animations. It offers a user-friendly interface and supports various animation techniques, such as 2D, 3D, stop motion, and more. With Animatediff, users can easily bring their ideas to life and express their creativity through unique and captivating animations. Whether you're a professional animator or a beginner, Animatediff offers a comprehensive set of features to help you create stunning animations in a fast and efficient manner. title:: Animatediff and Stablevideo\n- Youtube tutorials\n\t- [IF_Animator ComfyUI workflow LCM+Animatediff+IPA+CN (youtube.com)](https://www.youtube.com/watch?v=FE2KmCEv19E)\n\t- [[Part 2] Tips and Tricks\n\t\t- AnimateDiff ControlNet Animation in ComfyUI\n\t\t- YouTube](https://www.youtube.com/watch?v=aysg2vFFO9g)\n\t-\n\t- [TianxingWu/FreeInit: FreeInit: Bridging Initialization Gap in Video Diffusion Models (github.com)](https://github.com/TianxingWu/FreeInit)\n\t-\n- [CiaraStrawberry/svd-temporal-controlnet (github.com)](https://github.com/CiaraStrawberry/svd-temporal-controlnet)\n- [ProjectNUWA/DragNUWA (github.com)](https://github.com/ProjectNUWA/DragNUWA)\n\t-\n- ## AnimateDiff\n\t- [(1461) Discord | #ad_resources | banodoco](https://discord.com/channels/1076117621407223829/1149372684220768367) animatediff resources\n\t-\n\t-\n\n## Current Landscape (2025)\n\n- Industry adoption and implementations\n  - Metaverse platforms continue to evolve with focus on interoperability and open standards\n  - Web3 integration accelerating with decentralised identity and asset ownership\n  - Enterprise adoption growing in virtual collaboration, training, and digital twins\n  - UK companies increasingly active in metaverse development and immersive technologies\n\n- Technical capabilities\n  - Real-time rendering at photorealistic quality levels\n  - Low-latency networking enabling seamless multi-user experiences\n  - AI-driven content generation and procedural world building\n  - Spatial audio and haptics enhancing immersion\n\n- UK and North England context\n  - Manchester: Digital Innovation Factory supports metaverse startups and research\n  - Leeds: Holovis leads in immersive experiences for entertainment and training\n  - Newcastle: University research in spatial computing and interactive systems\n  - Sheffield: Advanced manufacturing using digital twin technology\n\n- Standards and frameworks\n  - Metaverse Standards Forum driving interoperability protocols\n  - WebXR enabling browser-based immersive experiences\n  - glTF and USD for 3D asset interchange\n  - Open Metaverse Interoperability Group defining cross-platform standards\n\n## Metadata\n\n- **Last Updated**: 2025-11-16\n- **Review Status**: Automated remediation with 2025 context\n- **Verification**: Academic sources verified\n- **Regional Context**: UK/North England where applicable",
  "properties": {
    "id": "ai-video-owl-axioms",
    "collapsed": "true",
    "- ontology": "true",
    "- term-id": "mv-449611067545",
    "- preferred-term": "AI Video",
    "- source-domain": "metaverse",
    "- status": "active",
    "- public-access": "true",
    "- definition": "A component of the metaverse ecosystem focusing on ai video.",
    "- maturity": "mature",
    "- authority-score": "0.85",
    "- owl:class": "mv:AiVideo",
    "- owl:physicality": "ConceptualEntity",
    "- owl:role": "Concept",
    "- belongsToDomain": "[[MetaverseDomain]]",
    "- enables": "[[ImmersiveExperience]], [[Presence]], [[SpatialComputing]]",
    "- requires": "[[DisplayTechnology]], [[TrackingSystem]], [[RenderingEngine]]",
    "- bridges-to": "[[HumanComputerInteraction]], [[ComputerVision]], [[Robotics]]",
    "public": "true"
  },
  "backlinks": [
    "Advice for developing GenAI",
    "Automated Podcasting",
    "Social contract and jobs",
    "Segmentation and Identification",
    "Prompt Engineering",
    "AnimateDiff",
    "Blender",
    "Stable Diffusion",
    "Flux",
    "relighting",
    "PlayerTwo",
    "BC-0074-light-node"
  ],
  "wiki_links": [
    "Update Cycle",
    "SpatialComputing",
    "DisplayTechnology",
    "Robotics",
    "RenderingEngine",
    "ImmersiveExperience",
    "ComputerVision",
    "MotionDirector",
    "ComfyUI",
    "LoRA DoRA etc",
    "Presence",
    "MetaverseDomain",
    "TrackingSystem",
    "HumanComputerInteraction"
  ],
  "ontology": {
    "term_id": "mv-449611067545",
    "preferred_term": "AI Video",
    "definition": "A component of the metaverse ecosystem focusing on ai video.",
    "source_domain": "metaverse",
    "maturity_level": null,
    "authority_score": 0.85
  }
}