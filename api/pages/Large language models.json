{
  "title": "Large Language Models",
  "content": "- ### OntologyBlock\n  id:: large-language-models-ontology\n  collapsed:: true\n\n  - **Identification**\n    - ontology:: true\n    - term-id:: AI-0850\n    - preferred-term:: Large Language Models\n    - source-domain:: ai\n    - status:: complete\n    - public-access:: true\n    - version:: 2.0.0\n    - last-updated:: 2025-01-15\n    - quality-score:: 0.92\n\n  - **Definition**\n    - definition:: [[Large Language Models]] (LLMs) are [[Foundation Models]] with billions to trillions of parameters trained on massive text corpora using [[Transformer]] architectures and [[Self-Supervised Learning]], capable of performing diverse [[Natural Language Processing]] tasks through [[Few-Shot Learning]], [[Zero-Shot Learning]], and [[Prompt Engineering]]. LLMs represent a paradigm shift in [[Artificial Intelligence]], demonstrating emergent capabilities in reasoning, code generation, multilingual understanding, and complex task decomposition.\n    - maturity:: mature\n    - source:: [[OpenAI Research]], [[Google DeepMind]], [[Anthropic]], [[Meta AI Research]], [[NIST AI Standards]]\n    - authority-score:: 0.95\n\n  - **Semantic Classification**\n    - owl:class:: ai:LargeLanguageModel\n    - owl:physicality:: VirtualEntity\n    - owl:role:: Process\n    - owl:inferred-class:: ai:VirtualProcess\n    - belongsToDomain:: [[AI-GroundedDomain]], [[ComputationAndIntelligenceDomain]], [[DataManagementDomain]]\n    - implementedInLayer:: [[ComputeLayer]], [[DataLayer]], [[AlgorithmicLayer]]\n\n  - #### Relationships\n    id:: large-language-models-relationships\n    - is-subclass-of:: [[Foundation Models]], [[Neural Network]], [[Deep Learning]], [[Machine Learning]]\n    - has-part:: [[Transformer]], [[Attention Mechanism]], [[Tokenization]], [[Embedding]], [[Training Data]], [[Model Parameters]]\n    - implements:: [[Self-Attention]], [[Multi-Head Attention]], [[Positional Encoding]], [[Backpropagation]], [[Gradient Descent]]\n    - requires:: [[GPU Compute]], [[Distributed Training]], [[Large-Scale Datasets]], [[Model Optimization]], [[Fine-Tuning]]\n    - enables:: [[Natural Language Understanding]], [[Text Generation]], [[Code Generation]], [[Translation]], [[Question Answering]], [[Reasoning]]\n    - related-to:: [[Generative AI]], [[AI Agent System]], [[Prompt Engineering]], [[Retrieval-Augmented Generation]], [[Chain-of-Thought Prompting]]\n    - bridges-to::\n      - [[L402 Payment Protocol]] (domain: bitcoin - for AI service micropayments)\n      - [[Lightning Network]] (domain: bitcoin - for LLM API payment channels)\n      - [[Autonomous Economic Agents]] (domain: bitcoin - for AI-powered bitcoin agents)\n      - [[Smart Contract]] (domain: blockchain - for AI model governance)\n\n  - #### OWL Axioms\n    id:: large-language-models-owl-axioms\n    collapsed:: true\n    - ```clojure\n      Prefix(ai:=<http://purl.org/ai-ontology#>)\n      Prefix(btc:=<http://purl.org/bitcoin-ontology#>)\n      Prefix(dt:=<http://purl.org/disruptive-tech/bridges#>)\n      Prefix(owl:=<http://www.w3.org/2002/07/owl#>)\n      Prefix(xsd:=<http://www.w3.org/2001/XMLSchema#>)\n      Prefix(rdfs:=<http://www.w3.org/2000/01/rdf-schema#>)\n\n      Ontology(<http://purl.org/ai-ontology/AI-0850>\n\n        Declaration(Class(ai:LargeLanguageModel))\n        AnnotationAssertion(rdfs:label ai:LargeLanguageModel \"Large Language Model\"@en)\n        AnnotationAssertion(rdfs:comment ai:LargeLanguageModel\n          \"Neural network models with billions+ parameters trained on massive text corpora\"@en)\n\n        SubClassOf(ai:LargeLanguageModel ai:FoundationModel)\n        SubClassOf(ai:LargeLanguageModel ai:NeuralNetwork)\n\n        # LLM requires transformer architecture\n        SubClassOf(ai:LargeLanguageModel\n          ObjectSomeValuesFrom(ai:requiresArchitecture ai:Transformer))\n\n        # LLM has billions of parameters\n        SubClassOf(ai:LargeLanguageModel\n          DataHasValue(ai:parameterCount DataMinInclusive(\"1000000000\"^^xsd:integer)))\n\n        # Bridge to Bitcoin: LLM can use L402 for API payments\n        SubClassOf(ai:LargeLanguageModel\n          ObjectSomeValuesFrom(dt:canIntegrateWith btc:L402PaymentProtocol))\n\n        # Bridge to Bitcoin: LLM APIs can accept Lightning payments\n        SubClassOf(ai:LargeLanguageModel\n          ObjectSomeValuesFrom(dt:canAcceptPayment btc:LightningNetwork))\n      )\n      ```\n\n- ## About Large Language Models\n\n- ### Public page automatically published\n- ![1721832527031.jpeg](../assets/1721832527031_1721833836065_0.jpeg)\n\n- ## Introduction to Large Language Models\n- [[Large Language Models]] (LLMs) like [[OpenAI]]'s [[GPT]] series have revolutionized the field of [[Artificial Intelligence]], offering unprecedented capabilities in [[Natural Language Understanding]] and [[Text Generation]]. These models are trained on vast amounts of text data using [[Deep Learning]] and [[Transformer]] architectures, enabling them to perform a wide range of [[Natural Language Processing]] tasks, from writing and [[Machine Translation]] to [[Question Answering]] and [[Code Generation]].\n\t- [Understanding AI - Large Language Models Explained](https://www.understandingai.org/p/large-language-models-explained-with) - A jargon-free primer on LLM fundamentals (2025)\n- # LLM Deployment Strategy (2025)\n\t- **Start with Simple API Calls:**\n\t\t- Initially, utilize third-party [[API]] services that serve your needs without complicating your system. This is the most straightforward and cost-effective solution for [[LLM Deployment]].\n\t\t- If third-party APIs meet your requirements in terms of functionality, [[Privacy]], cost, and [[Latency]], there's no need to progress to more complex solutions.\n\t\t- Consider [[L402 Payment Protocol]] for micropayment-enabled [[LLM API]] access, enabling [[Pay-Per-Use]] models with [[Lightning Network]] payments.\n\t- **Deploy Pre-trained Models:**\n\t\t- If API solutions are insufficient due to [[Data Privacy]], cost, or latency issues, consider deploying a generic [[Pre-trained Model]] (like [[Mixtral]], [[LLaMA 2]], or [[Mistral]]) behind your own [[API Gateway]].\n\t\t- This step involves more complexity and control over the data but remains relatively simple for [[Self-Hosted AI]].\n\t- **Curate Context and Improve Prompts:**\n\t\t- Enhance the output quality by curating [[In-Context Learning]] examples and optimizing [[Prompt Engineering]]. This step aims to extract better performance from the existing deployed model with minimal changes.\n\t\t- Leverage [[Few-Shot Learning]] and [[Chain-of-Thought Prompting]] techniques for improved reasoning.\n\t- **Integrate Retrieval Systems:**\n\t\t- If further improvement is needed, integrate a [[Retrieval-Augmented Generation]] (RAG) system to complement the model's responses, based on the available latency and complexity.\n\t\t- Use [[Vector Database]]s and [[Semantic Search]] for efficient knowledge retrieval.\n\t- **Fine-tune on Specific Data:**\n\t\t- When adjustments and retrieval integrations aren't sufficient, proceed to [[Fine-Tuning]] the model on a targeted dataset using techniques like [[LoRA]] or [[QLoRA]].\n\t\t- This step tailors the model more closely to your specific requirements and [[Domain Adaptation]].\n\t- **Swap for a Larger Model or Pre-train Your Own:**\n\t\t- If fine-tuning does not achieve the desired outcomes, consider swapping in a larger [[Foundation Model]] or [[Pre-Training]] your own model for significant customization.\n\t\t- This can involve [[Domain Adaptation]] through further pre-training on a relevant corpus, followed by [[Instruction Tuning]].\n\t- **Iterate and Add Complexity as Necessary:**\n\t\t- Continue iterating, adding layers of complexity only as needed. This approach ensures optimal investment in [[Compute Resources]] and development costs.\n\t- **Simplify and Streamline for Deployment:**\n\t\t- Throughout this process, aim to simplify and streamline solutions for [[Production Deployment]]. Consider the target audience and operationalize with [[Model Optimization]], [[Quantization]], and [[Model Serving]] best practices.\n\t\t-\n- ### Key Resources and Projects (2025)\n\t- **Web LLM Project**: A pioneering initiative bringing [[LLM]] functionalities to the [[Browser]], enabling users to interact with [[Foundation Models]] directly from their web interface using [[WebGPU]] and [[WebAssembly]].\n\t\t- [WebLLM - Client-Side LLMs](https://webllm.mlc.ai/) - Run LLMs entirely in-browser with hardware acceleration (2025)\n\t\t- *This project demonstrates the feasibility of deploying complex [[AI Models]] in consumer-friendly interfaces without server-side processing.*\n\t- **Browser-based Models**: The [Web LLM project (Simon Willison)](https://simonwillison.net/2023/Apr/16/web-llm/) introduces browser-based implementation of [[Vicuna-7B]] and other [[Open Source]] LLMs. This showcases practical application of [[Edge AI]] in web environments, enabling users to interact with sophisticated [[Language Models]] directly within browsers. The initiative highlights evolving accessibility of [[AI Technologies]], bringing powerful [[Natural Language Processing]] tools to broader audiences without specialized [[GPU]] hardware.\n\t\t- Enables [[Privacy-Preserving AI]] by keeping all data client-side\n\t\t- Supports [[Offline AI]] capabilities for disconnected environments\n\t\t- Can integrate with [[Bitcoin]] wallets for [[Decentralized AI]] payments via [[WebLN]]\n- ## Interfaces and Scaling\n\t- The evolution and scaling of interfaces for Large Language Models have significant implications for user interaction and the accessibility of AI technologies. This area explores the integration of LLMs into various interfaces, including immersive spaces and metaverse applications, which opens up new avenues for interaction and engagement with AI.\n- ### Key Projects and Discussions\n\t- **Immersive Spaces**: The potential of generative AI in metaverse applications and game development is vast, offering new ways to create engaging and dynamic environments. While specific links to projects or discussions were not provided in the initial extraction, this area highlights the intersection of LLMs with virtual worlds, suggesting a future where AI can contribute to more immersive and interactive digital spaces.\n\t- **Generative AI in the Metaverse**: An insightful article on why now is the time to use generative AI in your metaverse company, outlining potential impacts and considerations for developers and businesses. [Why You Should Use Generative AI in Your Metaverse Company\n\t\t- The Ghost Howls](https://skarredghost.com/2023/02/11/generative-ai-metaverse-company/)\n\t\t- *This article provides a comprehensive overview of how generative AI can revolutionize metaverse applications, offering a balanced view on the opportunities and challenges.*\n\t- **AI-Assisted Graphics in Game Development**: Exploring the use of AI to assist in graphics creation for games, enhancing realism and efficiency. [AI-Assisted Graphics](https://www.traffickinggame.com/ai-assisted-graphics/)\n\t\t- *This link showcases practical applications of AI in game development, highlighting advancements in creating more immersive and visually stunning gaming experiences.*\n- ## LLM Optimizations (2025)\n\t- Optimizations are critical for enhancing the performance and efficiency of [[Large Language Models]]. This section covers various techniques and tools developed for [[Model Optimization]], [[Inference Acceleration]], and [[Training Efficiency]].\n\t- ### Key Techniques and Tools\n\t\t- **DeepSpeed**: [DeepSpeed by Microsoft](https://github.com/microsoft/DeepSpeed) is an advanced [[Deep Learning]] optimization software suite that significantly accelerates [[Model Training]]. It offers [[Model Parallelism]], [[Gradient Accumulation]], [[Zero Redundancy Optimizer]] (ZeRO), and [[Sparsity]] techniques to achieve unprecedented scale and speed. *DeepSpeed is pivotal for researchers training models with billions of [[Model Parameters]].*\n\t\t\t- Enables [[Trillion-Parameter Models]] through [[Memory Optimization]]\n\t\t\t- Supports [[Mixed Precision Training]] and [[Gradient Checkpointing]]\n\t\t\t- Used by [[OpenAI]], [[Meta]], and [[Microsoft Research]] for [[Foundation Model]] training\n\t\t- **NVIDIA DASK**: [Distributed Computing with GPUs](https://developer.nvidia.com/blog/dask-tutorial-beginners-guide-to-distributed-computing-with-gpus-in-python/) provides insights into using [[DASK]] for [[Distributed Computing]], enhancing [[LLM]] performance by leveraging [[GPU]] resources efficiently. *Essential for [[Multi-GPU Training]] and [[Cluster Computing]].*\n\t\t- **SWARM Training**: [SWARM: Distributed Training of LLMs](https://arxiv.org/pdf/2301.11913.pdf) discusses innovative methods for [[Distributed Training]] of large language models, addressing [[Scalability]] and [[Training Efficiency]] challenges. *The SWARM approach represents advancement in [[Decentralized Training]], relevant for [[Bitcoin]]-powered distributed compute networks.*\n\t\t- **FlashAttention**: Advanced [[Attention Mechanism]] optimization achieving 2-4x speedup with reduced [[Memory Footprint]] for [[Transformer]] models (2025).\n\t\t- **Quantization Techniques**: [[INT8 Quantization]], [[INT4 Quantization]], and [[GPTQ]] enable efficient [[Model Inference]] on consumer hardware with minimal accuracy loss.\n- ### Projects and Implementations\n\t- **Browser-based Models**: A significant advancement in making LLMs accessible via web interfaces. The [Web LLM project](https://simonwillison.net/2023/Apr/16/web-llm/) discusses a browser-based version of the Vicuna-7b Large Language Model, showcasing how LLMs can be integrated into web applications, offering an accurate and fast model capable of handling complex prompts. This project exemplifies the potential of LLMs in providing accessible AI-powered applications directly from a web browser.\n- ### Interfaces and Scaling\n\t- **Immersive Spaces**: Exploring the integration of generative AI, including LLMs, in metaverse applications and game development. The potential for immersive, AI-driven spaces is vast, ranging from enhanced user experiences to novel forms of interaction. [Why you should use generative AI in your metaverse company](https://skarredghost.com/2023/02/11/generative-ai-metaverse-company/)\n\t\t- *This article discusses the implications and opportunities of incorporating generative AI in metaverse platforms.*\n- ### Optimizations\n\t- **DeepSpeed**: A software suite by Microsoft aimed at accelerating deep learning tasks. DeepSpeed offers innovative tools for enhancing the performance and efficiency of LLMs, making it easier to scale up training and inference operations. [DeepSpeed GitHub](https://github.com/microsoft/DeepSpeed)\n\t\t- *DeepSpeed is pivotal in addressing the computational and memory challenges of training large models, offering solutions to significantly reduce training times and resource consumption.*\n- ### Training & Fine-tuning\n\t- **Methods and Tools**: Enhancing LLM performance through innovative training and fine-tuning techniques. Resources cover a range of strategies, including LoRA training, deep retraining, pruning techniques, and model merging strategies.\n\t\t- [LoRA Training Insights](https://lightning.ai/pages/community/lora-insights/)\n\t\t- *An insightful blog post on the application and benefits of Low-Rank Adaptation (LoRA) in training LLMs, providing a deep dive into how LoRA can be used to fine-tune models efficiently.*\n\t\t- [BMTrain Toolkit](https://github.com/OpenBMB/BMTrain)\n\t\t- *BMTrain presents an efficient framework for training large models, focusing on distributed training while maintaining simplicity in code structure, making it accessible for large-scale model training.*\n- ### Evaluation\n\t- **Comparison and Detection**: Tools and methodologies for assessing LLM performance and detecting AI-generated text. This includes evaluations of model outputs and capabilities.\n\t\t- [AI-Generated Text Detection](https://arxiv.org/abs/2303.11156)\n\t\t- *A comprehensive study on the reliability of detecting AI-generated text, highlighting the challenges and methodologies involved in distinguishing between human and AI-generated content.*\n- ### Applications\n\t- **Consumer Tools Using LLMs**: Showcasing the application of LLMs in creating innovative consumer tools.\n\t\t- [CustomGPT for Personalized Customer Experiences](https://customgpt.ai)\n\t\t- *CustomGPT leverages LLMs to offer personalized interactions, demonstrating the potential of AI in enhancing customer service and engagement.*\n- ### Infrastructure\n\t- **Hosting and Deployment**: Solutions for effectively deploying LLMs, addressing the technical challenges involved.\n\t\t- [Rubbrband for Auto Deployments](https://rubbrband.com)\n\t\t- *Rubbrband provides a streamlined solution for deploying LLMs, emphasizing ease of use and efficiency in managing AI model deployments.*\n- ### Multilingual and Abstract Translation\n\t- Projects dedicated to improving LLM capabilities in translation, fostering better understanding and communication across languages.\n\t\t- [SeamlessM4T by Facebook Research](https://github.com/facebookresearch/seamless_communication#seamlessm4t)\n\t\t- *An innovative project aimed at enhancing multilingual translation, showcasing efforts to bridge language barriers and improve communication globally.*\n- ### Additional Training & Fine-tuning Resources\n\t- **Mesh TensorFlow for Distributed Training**: A tool for distributing computation across different hardware to enhance training efficiency. [Mesh TensorFlow](https://github.com/tensorflow/mesh)\n\t\t- *Enables sophisticated distribution strategies, optimizing the use of hardware resources during model training.*\n\t- **Colossal-AI for Easy Distributed Training**: Provides user-friendly tools for distributed deep learning, making it simpler to scale up training processes. [Colossal-AI](https://colossalai.org/)\n\t\t- *Aims to simplify the transition from single-device to distributed model training, supporting more efficient utilization of computing resources.*\n\t- **BMTrain for Large Model Training**: Focuses on training large models with simplicity and efficiency, even in distributed settings. [BMTrain](https://github.com/OpenBMB/BMTrain)\n\t\t- *An efficient toolkit designed for simplicity in training large-scale models, supporting distributed training with ease.*\n\t- **LoRA Training Insights**: Discusses the benefits and application of Low-Rank Adaptation (LoRA) for efficient model fine-tuning. [LoRA Training Insights](https://lightning.ai/pages/community/lora-insights/)\n\t\t- *Provides a deep dive into how LoRA can be utilized to fine-tune models efficiently, offering significant insights into the process.*\n- ### Evaluation\n\t- Comparison and Detection\n\t- **LLM QA Evaluation on Wikipedia**: An insightful comparison of different LLMs' performance on QA tasks using Wikipedia as a benchmark. [LLM QA Evaluation Wikipedia](https://georgesung.github.io/ai/llm-qa-eval-wikipedia/)\n\t- *This study offers a comparative analysis highlighting the strengths and weaknesses of open-source vs closed-source LLMs in handling QA tasks, providing valuable insights for both developers and users.*\n\t- **LLM Zoo**: A collection of various LLMs to explore and compare their capabilities. [LLMZoo GitHub](https://github.com/FreedomIntelligence/LLMZoo)\n\t- *A unique repository that provides access to a wide range of LLMs, facilitating exploration, comparison, and understanding of different models' functionalities and performance.*\n\t- **Can AI-Generated Text be Reliably Detected?**: Addresses the critical question of distinguishing between human and AI-generated text. [AI-Generated Text Detection Study](https://arxiv.org/abs/2303.11156)\n\t- *This paper delves into the challenges and methodologies involved in detecting AI-generated text, offering insights into the reliability of current detection techniques.*\n- ### Applications\n\t- Consumer Tools Using LLMs\n- **Innovative Tools for Personalized Customer Experiences**: LLMs are increasingly used to create tools that offer personalized interactions for users, enhancing ecommerce experiences and facilitating efficient email management.\n\t- [CustomGPT](https://customgpt.ai)\n\t\t- A platform enabling businesses to create their own chatbots using their content, leading to accurate and personalized customer interactions. This tool exemplifies the use of LLMs in improving customer service and engagement.\n\t- [AnythingLLM](https://github.com/Mintplex-Labs/anything-llm)\n\t\t- A full-stack personalized AI assistant application that turns documents or content into reference data for intelligent conversations. Demonstrates the flexibility and potential of LLMs in custom applications.\n\t- [NodePad](https://nodepad.space/)\n\t\t- An LLM-assisted brainstorming tool that helps users organize their ideas visually. Highlights the creative use of LLMs in supporting individual thought processes and ideation.\n- ### Applications\n\t- Consumer Tools Using LLMs\n- **Personalized Customer Experiences**: LLMs are increasingly used to create personalized interactions in consumer applications, enhancing ecommerce experiences and facilitating more intuitive user interfaces.\n\t- [CustomGPT](https://customgpt.ai)\n\t\t- *CustomGPT offers businesses the ability to create their own chatbot using GPT-4 for tailored customer interactions. This platform demonstrates the application of LLMs in improving customer service and engagement by providing accurate, context-aware responses.*\n- **Innovative Interfaces and Applications**: The versatility of LLMs allows for the development of creative tools that simplify complex tasks or provide new services.\n\t- [AnythingLLM](https://github.com/Mintplex-Labs/anything-llm)\n\t\t- *A comprehensive solution for turning any document or piece of content into a piece of data for LLM-based chat interactions, showcasing the potential of LLMs in data management and retrieval.*\n- ### Infrastructure\n\t- Hosting and Deployment\n- **Solutions for LLM Deployment**: Addressing the technical requirements and solutions for deploying LLMs efficiently.\n\t- [Rubbrband for Auto Deployments](https://rubbrband.com)\n\t\t- *Rubbrband simplifies the deployment of LLMs by providing an automated platform that supports various deployment scenarios, facilitating easier access to LLM capabilities.*\n\t- [Hosting VPS Solutions](https://1984.hosting)\n\t\t- *1984 Hosting offers privacy-focused VPS solutions, ideal for hosting LLMs with a commitment to free speech and data protection.*\n\t- [Free Custom Domains VPS](https://codesphere.com/pricing?anonymousId=YTQLcRg)\n\t\t- *Codesphere provides VPS hosting with the option for free custom domains, enabling personalized deployment of LLM applications.*\n- **Distributed Computing and Training**:\n\t- [Nvidia DASK for Distributed Computing](https://developer.nvidia.com/blog/dask-tutorial-beginners-guide-to-distributed-computing-with-gpus-in-python/)\n\t\t- *Nvidia's DASK tutorial offers a beginner's guide to distributed computing with GPUs, enhancing the performance of LLM training and inference.*\n\t- [SWARM Training for LLMs](https://arxiv.org/pdf/2301.11913.pdf)\n\t\t- *The SWARM training paper discusses innovative methods for distributed training of LLMs, proposing solutions to scale training processes efficiently.*\n- ### Multilingual and Abstract Translation\n- **Enhancing Translation Capabilities**: Projects and technologies aimed at improving translation quality and supporting seamless communication across languages.\n\t- [Meta SeamlessM4T](https://github.com/facebookresearch/seamless_communication#seamlessm4t)\n\t\t- *A project by Meta aimed at enhancing multilingual translation to support seamless communication across different languages, showcasing the potential of LLMs in breaking down language barriers.*\n- **Supporting Global Communication**: Efforts to develop tools and models that facilitate understanding and translation across a wide array of languages.\n\t- [MultimodalC4 Extension](https://github.com/allenai/mmc4)\n\t\t- *A multimodal extension of the C4 dataset that interleaves millions of images with text to provide context, aiming at improving the capabilities of LLMs in understanding and generating content in a multilingual and multimodal context.*\n- ### General Purpose and Miscellaneous\n- **Learning and Development**: Resources for learning about LLMs, including educational materials and platforms for fine-tuning and experimenting.\n\t- [Replit LLM Training Guide](https://blog.replit.com/llm-training)\n\t\t- A guide on training your own large language models using Replit.\n\t- [Futurepedia](http://Futurepedia.io)\n\t\t- The largest AI tools directory, featuring over 700 tools in various categories.\n\t- [Understanding Large Language Models](https://magazine.sebastianraschka.com/p/understanding-large-language-models)\n\t\t- A cross-section of relevant literature to get up to speed on LLMs.\n- **Distributed Technology**\n\t- [Mesh TensorFlow](https://github.com/tensorflow/mesh)\n\t\t- A language for distributed deep learning, allowing broad classes of distributed tensor computations.\n\t- [BMTrain](https://github.com/OpenBMB/BMTrain)\n\t\t- An efficient large model training toolkit for distributed training.\n\t- [Colossal-AI](https://colossalai.org/)\n\t\t- Aims to simplify distributed deep learning, supporting easy transition to distributed training.\n- **Optimizations and Scaling**\n\t- [TensorRT-LLM optimization repo](https://github.com/NVIDIA/TensorRT-LLM)\n\t\t- Optimizations for LLMs using TensorRT for better inference performance.\n\t- [DeepSpeed](https://github.com/microsoft/DeepSpeed)\n\t\t- Deep learning optimization software suite by Microsoft for scalable training.\n- **Emotion Tracking**\n\t- [LAION Empathetic](https://dct.openempathic.ai/guide/#:~:text=Rating%20Arousal%20and%20Valence%3A)\n\t\t- A tool for emotion tracking in text.\n- ### Additional Tools and Resources\n- [Horde Image and LLM](https://horde.koboldai.net/)\n\t- A project integrating images with LLMs for enhanced content generation.\n- [LobeHub](https://github.com/lobehub)\n\t- A technology-driven forum for AIGC, offering modern design components and tools.\n- Microsoft [WizardLM 2](https://wizardlm.github.io/WizardLM2/)\n-\n-\n- # old version to integrate\n- ### Large Language Models (LLMs)\n\t- **Introduction to LLMs**\n\t\t- Large language models are advanced computer programs capable of generating text, answering questions, and more, trained on vast internet text. Examples include OpenAI's GPT-3.\n- **Projects and Implementations**\n\t-\n\t- Browser-based models, such as the [Web LLM project](https://simonwillison.net/2023/Apr/16/web-llm/), which discusses a browser-based version of the vicuna-7b Large Language Model.\n-\n- ### Distributed Technology\n\t- **Optimizations and Scaling**\n\t\t- [Nvidia DASK](https://developer.nvidia.com/blog/dask-tutorial-beginners-guide-to-distributed-computing-with-gpus-in-python/): Tutorial for distributed computing with GPUs.\n\t\t- [SWARM Training Paper](https://arxiv.org/pdf/2301.11913.pdf): Discusses methods for distributed training of LLMs.\n- Interfaces and scaling\n\t- [LobeHub (github.com)](https://github.com/lobehub)\n- Distributed tech\n\t- [horde image and llm](https://horde.koboldai.net/)\n- Browser based whole models\n\t- [The Web LLM project has created a browser-based version of the vicuna-7b Large Language Model, which is impressively accurate and fast. The model is able to handle complex prompts and provide accurate responses, although it does sometimes make mistakes.](https://simonwillison.net/2023/Apr/16/web-llm/)\n\t- [Nvidia DASK](https://developer.nvidia.com/blog/dask-tutorial-beginners-guide-to-distributed-computing-with-gpus-in-python/)\n\t- [SWARM training paper](https://arxiv.org/pdf/2301.11913.pdf)\n- immersive spaces\n\t- Why you should use now generative AI in your metaverse company. Or maybe not\n\t\t- The Ghost Howls https://skarredghost.com/2023/02/11/generative-ai-metaverse-company/\n\t- [games dev](https://www.traffickinggame.com/ai-assisted-graphics/)\n- [Instant app from prompts](https://twitter.com/ronithhh/status/1641318606549176321)\n- [endless runner without any coding experience](https://replit.com/@asrsubs/SkyRoads-GPT-4)\n- [Edge (phone deployment on android)](https://github.com/mlc-ai/mlc-llm/tree/main/android)\n- [Tree of thought github](https://github.com/ysymyth/tree-of-thought-llm)\n- [Scaling challenges paper](https://arxiv.org/abs/2307.10169)\n- [Flow node based LLM design](https://github.com/FlowiseAI/Flowise)\n- [TensorRT-LLM optimisation repo](https://github.com/NVIDIA/TensorRT-LLM)\n- [Flowchat](https://github.com/flatypus/flowchat)\n- Multi Modal\n\t- [MultimodalC4 is a multimodal extension of c4 that interleaves millions of images with text. The corpus contains over a billion images, and the text is interleaved with the images to provide context.](https://github.com/allenai/mmc4)\n\t- [Otter with weights](https://otter-ntu.github.io/)\n\t- [minigpt](https://minigpt-4.github.io/)\n\t- [MiniGPT local multimodal](https://github.com/Vision-CAIR/MiniGPT-4)\n\t- [Fuya](https://www.adept.ai/blog/fuyu-8b?)\n- emotion tracking\n\t- [laion empathetic](https://dct.openempathic.ai/guide/#:~:text=Rating%20Arousal%20and%20Valence%3A)\n\t- [entity identify open LLM](https://www.numind.ai/blog/a-foundation-model-for-entity-recognition)\n- Optimisations\n\t- [ùêÉùêûùêûùê©ùêíùê©ùêûùêûùêù is an easy-to-use deep learning optimization software suite that enables unprecedented scale and speed for DL Training and Inference. Visit us at deepspeed.ai or our Github repo.\n\t- üìåMegatron-LM GPT2 tutorial: https://lnkd.in/gXvPhXqb](https://github.com/microsoft/DeepSpeed)\n\t- [The text provides instructions on how to train your own large language models using Replit. It explains that you will need to first create a Replit account and then follow the instructions on the website.](https://blog.replit.com/llm-training)\n\t- [Futurepedia is the largest AI tools directory, with over 700 tools in various categories. It is updated daily, and features search and filter options to help you find the right tool for your needs.](http://Futurepedia.io)\n\t- [GitHub\n\t\t- gitnomad24601/ShogScript: ShogScript: The GitHub repository \"ShogScript\" contains a proof-of-concept pseudocode for GPT-4 AI interactions, ideal for storytelling & communication. The code is released under the MIT license.](https://github.com/gitnomad24601/ShogScript)\n\t- [Flash decoding 8x](https://together.ai/blog/flash-decoding-for-long-context-inference?)\n\t- [Understanding Large Language Models: A Cross-Section of the Most Relevant Literature To Get Up to Speed](https://magazine.sebastianraschka.com/p/understanding-large-language-models)\n\t- [The text describes a change to support the GPTQ triton commit c90adef. This change allows for the disabling of quant attention.](https://github.com/oobabooga/text-generation-webui/pull/1229)\n\t- [2000x performance improvement paper](https://arxiv.org/abs/2305.02301)\n\t- [Flexgen](https://github.com/FMInference/FlexGen#get-started-with-a-single-gpu)\n\t- [4bit compression](https://github.com/johnsmith0031/alpaca_lora_4bit)\n\t- [GPT4 self hallucination checking](https://www.reddit.com/r/MachineLearning/comments/123b66w/dgpt4_might_be_able_to_tell_you_if_it_hallucinated/)\n\t- [Sparse LLM, half the size, all the power](https://arxiv.org/abs/2301.00774)\n\t- [SpQR lossless optimisation paper](https://arxiv.org/abs/2306.03078)\n\t- [Landmark attention qlora oogabooga](https://github.com/eugenepentland/landmark-attention-qlora)\n\t- [LobeHub (github.com)](https://github.com/lobehub)\n\t\t- We are a group of e/acc design-engineers, hoping to provide modern design components and tools for AIGC, and creating a technology-driven forum, fostering knowledge interaction and the exchange of ideas that may culminate in mutual inspiration and collaborative innovation. Whether for users or professional developers, LobeHub will be your AI Agent playground.\n- Training & Finetuning\n- Lora\n- [alpaca lora training](https://discord.com/channels/1086739839761776660/1087508281758584852)\n- [Github](https://github.com/tloen/alpaca-lora)\n- [CPU offload lora training](https://github.com/oobabooga/text-generation-webui/commit/09d8119e3cf36257496acfb44e6445a9f40c3d02)\n- [llamatard 4bit chat instructions](https://rentry.org/llama-tard-v2#llama-int8-4bit-chatbot-guide-v2)\n- [The text provides a guide on how to make your own Loras, which are easy and free to create. The process is described in detail, and the text includes instructions on how to create and customize your own Loras.](https://civitai.com/models/22530)\n- Deep retraining\n- Deepspeed chat retraining in hours\n- microsoft just released a new finetuning pipeline\n  they finetuned a 65B model in 10 hours using RLHF\n- [TRL\n\t- Transformer Reinforcement Learning](https://github.com/lvwerra/trl)\n- Hardware requirements for retraining (links to state of the art)\n\t- [Finetuning blog post](https://www.databricks.com/blog/2023/03/20/fine-tuning-large-language-models-hugging-face-and-deepspeed.html)\n- Pruning\n- [Seems that both 4 bit and straight up pruning don't harm the models much](https://arxiv.org/abs/1803.03635)\n- Merging\n- [diffusion style LLM block merging](https://github.com/TehVenomm/LM_Transformers_BlockMerge)\n- [Domain expert model merging](https://docs.google.com/document/d/1JCzJ1wdBMBVwsFW4CWGUbX-YEDXB0yS4mfFbvwPLQrI/edit)\n- Toolkits and distributed\n- [ùêåùêûùê¨ùê° ùêìùêûùêßùê¨ùê®ùê´ùêÖùê•ùê®ùê∞ (mtf) is a language for distributed deep learning, capable of specifying a broad class of distributed tensor computations. The purpose of Mesh TensorFlow is to formalize and implement distribution strategies for your computation graph over your hardware/processors. For example: \"Split the batch over rows of processors and split the units in the hidden layer across columns of processors.\" Mesh TensorFlow is implemented as a layer over TensorFlow.](https://github.com/tensorflow/mesh)\n- [ùêÅùêåùêìùê´ùêöùê¢ùêß is an efficient large model training toolkit that can be used to train large models with tens of billions of parameters. It can train models in a distributed manner while keeping the code as simple as stand-alone training.](https://github.com/OpenBMB/BMTrain)\n- [ùêÇùê®ùê•ùê®ùê¨ùê¨ùêöùê•-ùêÄùêà provides a collection of parallel components for you. It aim to support us to write our distributed deep learning models just like how we write our model on our laptop. It provide user-friendly tools to kickstart distributed training and inference in a few lines.\n  üìåOpen source solution replicates ChatGPT training process.Ready to go with only 1.6GB GPU memory and gives you 7.73 times faster training: https://lnkd.in/gp4XTCnz](https://colossalai.org/)\n- [EasyLM one stop scaleable toolkit](https://github.com/young-geng/EasyLM)\n- [databerry training and deployment](https://github.com/gmpetrov/databerry)\n- [Petals collaborative fine tuning](https://arxiv.org/abs/2209.01188)\n- [Goodle openXLA training accelerator](https://opensource.googleblog.com/2023/03/openxla-is-ready-to-accelerate-and-simplify-ml-development.html)\n- Adversarial and self instructed\n- [Use GPT API as a GAN (twitter thread)](https://twitter.com/BrianRoemmele/status/1637871062246649856)\n- [Bigscience petals run training through torrents](https://github.com/bigscience-workshop/petals)\n- [airoboros_a_rewrite_of_selfinstructalpaca/](https://www.reddit.com/r/MachineLearning/comments/136vt7b/p_airoboros_a_rewrite_of_selfinstructalpaca/)\n- [A Cookbook of Self-Supervised Learning](https://arxiv.org/abs/2304.12210)\n- [Lora training lessons blog post](https://lightning.ai/pages/community/lora-insights/?)\n- [lit-gpt hackable training platform apache 2](https://github.com/Lightning-AI/lit-gpt)\n- [ChatLLaMA  is a library that allows you to create hyper-personalized ChatGPT-like assistants using your own data and the least amount of compute possible. Instead of depending on one large assistant that ‚Äúrules us all‚Äù, we envision a future where each of us can create our own personalized version of ChatGPT-like assistants.](https://github.com/nebuly-ai/nebullvm/tree/main/apps/accelerate/chatllama)\n- [Substack on retraining a 30B model in an A100](https://abuqader.substack.com/p/releasing-alpaca-30b)\n- [ùêÄùê•ùê©ùêö is a system for training and serving large-scale neural networks. Scaling neural networks to hundreds of billions of parameters has enabled dramatic breakthroughs such as GPT-3, but training and serving these large-scale neural networks require complicated distributed system techniques. Alpa aims to automate large-scale distributed training and serving with just a few lines of code.\n- üìåAlpa:\n-\n- üìåServing OPT-175B, BLOOM-176B and CodeGen-16B using Alpa: https://lnkd.in/g_ANHH6f](https://github.com/alpa-projects/alpa)\n- [ùêåùêûùê†ùêöùê≠ùê´ùê®ùêß-ùêãùêå / Megatron is a large, powerful transformer developed by the Applied Deep Learning Research team at NVIDIA. Below repository is for ongoing research on training large transformer language models at scale. Developing efficient, model-parallel (tensor, sequence, and pipeline), and multi-node pre-training of transformer based models such as GPT, BERT, and T5 using mixed precision.\n- üìåpretrain_gpt3_175B.sh: https://lnkd.in/gFA9h8ns](https://github.com/NVIDIA/Megatron-LM)\n- [Koala paper on training with minimal noise for chatbots](https://bair.berkeley.edu/blog/2023/04/03/koala/?ref=emergentmind)\n- [Emmet twitter and github on fine tuning](https://twitter.com/ehalm_/status/1652373239044112388)\n- [Ensure structured json](https://github.com/1rgs/jsonformer)\n- [Lora training guide from Pytorch lightning.ai people](https://lightning.ai/pages/community/tutorial/lora-llm/)\n- [GPTQ paper code](https://github.com/ist-daslab/gptq)\n- [Microsoft guidance](https://github.com/microsoft/guidance)\n- [QLoRA fast retraining of large models](https://github.com/artidoro/qlora)\n- [paper](https://arxiv.org/pdf/2305.14314.pdf)\n- [Some kind of inscrutable training thing](https://readthedocs.org/projects/alibi/downloads/pdf/latest/)\n- [Llama 2 training guide](https://www.philschmid.de/sagemaker-llama2-qlora)\n- [RLHF cheap paper](https://arxiv.org/pdf/2308.01320.pdf)\n- [Sparse LLM cpu training breakthrough](https://huggingface.co/blog/mwitiderrick/llm-infrerence-on-cpu)\n- Evaluation\n- [github of comparisons](https://georgesung.github.io/ai/llm-qa-eval-wikipedia/)\n- [compare open source vs closed](https://georgesung.github.io/ai/llm-qa-eval-wikipedia/)\n- [LLM zoo](https://github.com/FreedomIntelligence/LLMZoo)\n- [Can AI-Generated Text be Reliably Detected?:](https://arxiv.org/abs/2303.11156)\n- In the paper \"Can AI-Generated Text be Reliably Detected?\", the authors show that current methods for detecting AI-generated text are not reliable in practical scenarios. They first demonstrate that paraphrasing attacks can break a range of detectors, including those using watermarking schemes and neural network-based detectors. They then provide a theoretical impossibility result showing that for a sufficiently good language model, even the best-possible detector can only perform marginally better than a random classifier. Finally, they show that even LLMs protected by watermarking schemes can be vulnerable to spoofing attacks where adversarial humans can add hidden watermarking signatures to their generated text.\n- [gptzero spots AI authoring](http://gptzero.me/)\n- [GPTZero Case Study (Exploring False Positives): Introduction In this case study, I will be sharing the vast amounts of false positives current AI detection software gives, specifically for this case study I will be demonstrating GPTZero. I personally want to thank the supposed ‚ÄúHealthcare professional‚Äù who brought this to my attention via my contact link. It has motivated me to look more into this issue rather than just posting bypasses to these popular AI detection software programs, it will be only more beneficial to highlight their real usability in general.](https://gonzoknows.com/posts/GPTZero-Case-Study/)\n- The text describes a case study on false positives with AI detection software. The study found that the software often gives false positives, particularly with regard to healthcare. The study recommends that users be aware of this issue and take it into account when using such software.\n- [Fake detector product](https://hivemoderation.com/ai-generated-content-detection)\n- [Huggingface leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard)\n- [[Base models]]\n- Prompt engineering and injection\n- Character injection\n- [json builder](https://oobabooga.github.io/character-creator.html)\n- [Huggingface commodity card retrainer](https://huggingface.co/blog/trl-peft)\n- Prompt model tips for learning\n- 1. Improve your writing by getting feedback.\n\t- Use this prompt:\n\t- [paste your writing]\n\t- \"Proofread my writing above. Fix grammar and spelling mistakes. And make suggestions that will improve the clarity of my writing\"\n\t- 2. Use the 80/20 principle to learn faster than ever.\n\t- \"I want to learn about [insert topic]. Identify and share the most important 20% of learnings from this topic that will help me understand 80% of it.\"\n\t- 3. Learn and develop any new skill.\n\t- \"I want to learn / get better at [insert desired skill]. I am a complete beginner. Create a 30 day learning plan that will help a beginner like me learn and improve this skill.\"\n\t- 4. Get short and insight-packed book summaries.\n\t- \"Summarize the book [insert book] by the author [insert author] and give me a list of the most important learnings and insights.\"\n\t- 5. Get feedback from history's greatest minds.\n\t- \"Assume you are [insert famous person e.g. Steve Jobs]. Read my argument below and give me feedback as if you were [insert person again]\"\n\t- [insert your argument]\n\t- 6. Enhance your problem solving skills.\n\t- \"Your role is that of a problem solver. Give me a step-by-step guide to solving [insert your problem].\"\n\t- 7. Generate new ideas and overcome writers block:\n\t- \"I am writing a blog post about [insert topic]. Give me an outline for this blog post with 10 bullet points. Also give me 5 options for a catchy headline.\"\n\t- You can adapt this prompt for whatever you're writing.\n\t- 8. Summarize long texts and accelerate your learning:\n\t- \"Summarize the text below into 500 words or less. Create sections for each important point with a brief summary of that point.\"\n\t- 9. Use stories and metaphors to aid your memory.\n\t- \"I am currently learning about [insert topic]. Convert the key lessons from this topic into engaging stories and metaphors to aid my memorization.\"\n\t- 10. Strengthen your learning by testing yourself.\n\t- \"I am currently learning about [insert topic]. Ask me a series of questions that will test my knowledge. Identify knowledge gaps in my answers and give me better answers to fill those gaps.\"\n- [Prompt injection: what s the worst that can happen?](https://simonwillison.net/2023/Apr/14/worst-that-can-happen/)\n- [To jailbreak ChatGPT, you need to get it to really do what you want. This can be done by editing the source code or by using a third-party tool.](https://www.digitaltrends.com/computing/how-to-jailbreak-chatgpt/)\n- General purpose super short prompt\n- develop+extend+support(ideas), vocab(wide+natural+sophisticated), grammar(wide+flexible), cohesion(logical+smooth), clarity(precise+concise), engagement(attention+interest), mood(objective+explanatory), viewpoint(forward_looking)\n- [Mollick methods post on linkedin](https://www.linkedin.com/posts/emollick_there-are-now-three-research-backed-approaches-activity-7089472152701136896-aZNQ?utm_source=share&utm_medium=member_desktop)\n- [Large Language Models are Human-Level Prompt Engineers: We propose an algorithm for automatic instruction generation and selection for large language models with human level performance.](https://openreview.net/forum?id=92gvk82DE-)\n- [Using models to learn well, blog and paper](https://www.oneusefulthing.org/p/how-to-use-ai-to-teach-some-of-the)\n- [Guide to prompting LLMs](https://olickel.com/everything-i-know-about-prompting-llms)\n- basic software primitives\n      Transformers are a new type of machine learning model that have been making headlines recently. They are very good at keeping track of context, which is why the text they generate makes sense. In this blog post, we will go over their architecture and how they work.\n       <https://txt.cohere.ai/what-are-transformer-models/>\n      Datasets 101\n       <https://www.latent.space/p/datasets-101?utm_source=substack&utm_medium=email#details>\n      implementations\n          pytorch/numpty\n          tensorflow/jax\n      LLM youtube bootcamp 2023\n       <https://www.youtube.com/playlist?list=PL1T8fO7ArWleyIqOy37OVXsP4hFXymdOZ>\n      Linkedin LLM roundup\n       <https://www.linkedin.com/posts/francesco-saverio-zuppichini-94659a150_ai-ml-ds-activity-7072868294000566272-kV83/?utm_source=share&utm_medium=member_android>\n          This is the list of resources I've recommended him\n          Where everything started:\n\t- Attention is all you need Paper: https://lnkd.in/eJWz6ShV Blog: https://lnkd.in/eaUMMy6v\n\t- GPT-3 Language models are few-shot learners Paper: https://lnkd.in/eUgFk7Db Video: https://lnkd.in/ev8whzkb The first one is where Attention was introduced, the main building block of Transformers. The second one shows that LLMs can actually do zero and few shots Then, I suggest having a look at how we went from GPT3 -> ChatGPT. So how it was possible to make LLMs better at human instructions. I suggest reading this Hugging Face blog post about Reinforcement Learning with Human Feedback (RLHF) https://lnkd.in/eAkM_FUj The next step is what happen later, Meta leaked LLama a smaller language model that was actually very good, the takeaway there is that if you train with more stuff and for longer you obtain a better model. Paper: https://lnkd.in/efZRu4mY The next wave is all built upon that model, so how do we make it better at following human instruction. So I suggest looking at the Stanford Alpaca model. Blog: https://lnkd.in/eqCwvVDZ I also said other interesting models are Vicuna (https://lnkd.in/eCYT3yWx) and WizardLM (https://lnkd.in/efvUD8AD). Saying that people have been focused on finding better and cheaper way to instruct the base LLama model. Another important thing is how to prompt, I've recommended chain of thoughts (https://lnkd.in/eYGxFaeS) and tree of thouhts (https://lnkd.in/ejcfkAeN) I've also shared the LLM leaderboard from Hugging Face : https://lnkd.in/eF6C_W6D YT channels that I think are the bests are: AI Explained: https://lnkd.in/emhTmsds Yannic Kilcher: https://lnkd.in/eRGUVme4 Sam Witteveen: https://lnkd.in/e4EiE5iY What do you think? Any resources that may be useful? Resourced shared Pritam Kumar Ravi Stanford CS25 Course https://lnkd.in/e2PrcwTu\n- LLM and creating new LLM\n- Safefty, alignment, and breaking\n- [image perturbation of multimodal](https://arxiv.org/abs/2307.10490)\n- [universal jailbreaks](https://arxiv.org/abs/2307.15043)\n- Consumer tools using LLM\n- [NexusGPT is a freelancer platform that uses AI to help businesses find the right freelancers for their needs. The platform offers a variety of features to help businesses find the perfect freelancer for their project, including a searchable database of freelancers, a rating system, and a feature that allows businesses to post their project and receive bids from freelancers.](https://nexus.snikpic.io)\n- [RadioGPT: 'World‚Äôs first' AI-driven radio station is here (other)](https://interestingengineering.com/innovation/radiogpt-worlds-first-ai-radio-station)\n- Some experts are predicting that the metaverse, a shared online space where users can interact with each other and digital objects, will eventually replace the internet as we know it.\n- [GitHub\n\t- MatveyM11/Mine-ChatGPT: OpenSourced ChatGPT downloader in markdown format. Download all text or markdown-styled code blocks Fear no more that servers are down, under high load or OpenAI adding a new feature. Keep all yours chat's with you locally in the simple .md files.: OpenSourced ChatGPT downloader in markdown format. Download all text or markdown-styled code blocks Fear no more that servers are down, under high load or OpenAI adding a new feature. Keep all your...](https://github.com/MatveyM11/Mine-ChatGPT)\n- This repository contains a ChatGPT downloader that can be used to download all text or markdown-styled code blocks from a chat. Fear no more that servers are down, under high load or OpenAI adding a new feature. Keep all yours chat's with you locally in the simple .md files.\n- [Linkedin bot to make LLM posts](https://github.com/FrancescoSaverioZuppichini/LinkedInGPT)\n- [ArcAngel Falcon based custom chat](https://www.arcangelai.com/)\n- [OpenAI community Pages](https://community.openai.com/c/announcements/6)\n- ChatGPT stuff\n- Code interpreter\n- [setup prompt by mollick](https://www.linkedin.com/feed/update/urn:li:activity:7083969476685099008/)\n- You are going to be an expert at making powerful and beautiful visualizations using principles from Tufte and other experts. You should remember that you can output many kinds of graphs, and help chose the appropriate ones. You also can output jpgs, html, interactive maps, and animated gifs.\n- First, mention some of the types of charts you can create, and the outputs that you can use.\n  Next, read these does and don'ts of data from Angela Zoss\n  Do:\n  1. Do use the full axis.\n- Avoid distortion.\n- For bar charts, the numerical axis (often the y axis) must start at zero.  Our eyes are very sensitive to the area of bars, and we draw inaccurate conclusions when those bars are truncated.\n    (But for line graphs, it may be okay to truncate the y axis.\n- Wide ranges:\n  If you have one or two very tall bars, you might consider using multiple charts to show both the full scale and a \"zoomed in\" view\n\t- also called a Panel Chart.\n- Consistent intervals:\n- Finally, using the full axis also means that you should not skip values when you have numerical data.  See the charts below that have an axis with dates.  The trend is distorted if you do not have even intervals between your dates.  Make sure your spreadsheet has a data point for every date at a consistent interval, even if that data point is zero\n- 2. Do simplify less important information.\n- Chart elements like gridlines, axis labels, colors, etc. can all be simplified to highlight what is most important/relevant/interesting.  You may be able to eliminate gridlines or reserve colors for isolating individual data series and not for differentiating between all of the series being presented\n- 3. Do be creative with your legends and labels.\n- Possibilitiess\n    Label lines individually\n    Put value labels on bars to preserve the clean lines of the bar lengths\n- 4. Do pass the squint test.\n- \"When you squint at your page, so that you cannot read any of the text, do you still 'get' something about the page?\"\n- Which elements draw the most attention? What color pops out?\n    Do the elements balance? Is there a clear organization?\n    Do contrast, grouping, and alignment serve the function of the chart?\n- Don't:\n  1. Don't use 3D or blow apart effects.\n- Studies show that 3D effects reduce comprehension. Blow apart effects likewise make it hard to compare elements and judge areas.\n- 2. Don't use more than (about) six colors.\n- Using color categories that are relatively universal makes it easier to see differences between color\n- The more colors you need (that is, the more categories you try to visualize at once), the harder it is to do this.\n- But different colors should be used for different categories\n  (e.g., male/female, types of fruit), not different values in a range (e.g., age, temperature).\n- If you want color to show a numerical value, use a range\n  that goes from white to a highly saturated color in one of\n  the universal color categories\n- 3. Don't change (style) boats midstream.\n- One of the easiest ways to get the most out of charts is to rely on comparison to do the heavy lifting.\n- Our visual system can detect anomalies in patterns.\n  Try keeping the form of a chart consistent across a series so differences from one chart to another will pop out.\n- Use the same colors, axes, labels, etc. across multiple charts.\n- 4. Don't make users do \"visual math.\"\n- If the chart makes it hard to understand an important relationship between variables, do the extra calculation and visualize that as well.\n- This includes using pie charts with wedges that are too similar to each other, or bubble charts with bubbles that are too similar to each other.  Our visual processing system is not well suited to comparing these types of visual areas.\n- We are also not good at holding precise visual imagery in our memory and comparing it to new stimuli; if you are giving a presentation and want the audience to be able to compare two charts, they need to be on the same slide.\n- 5. Don't overload the chart.\n- Adding too much information to a single chart eliminates the advantages of processing data visually; we have to read every element one by one! Try changing chart types, removing or splitting up data points, simplifying colors or positions, etc.\n- Now ask what kind of data visualization I might be interested in, or if I want to upload some data for yout co consider visualizing.\n- [loads of experiments](https://github.com/SkalskiP/awesome-chatgpt-code-interpreter-experiments)\n- General links and papers\n- [Think of language models like ChatGPT as a ‚Äúcalculator for words‚Äù: One of the most pervasive mistakes I see people using with large language model tools like ChatGPT is trying to use them as a search engine. As with other LLM ‚Ä¶](https://simonwillison.net/2023/Apr/2/calculator-for-words/)\n- Language models like ChatGPT are not reliable for use as a search engine, but can be thought of as a \"calculator for words\". This means that they are good for manipulating language, but not for retrieving accurate information.\n- [Peak LLM: Prompt injection might be just the beginning](https://ihavemanythoughts.substack.com/p/peak-llm)\n- [Language models as inductive reasoners paper](https://sentic.net/language-models-as-inductive-reasoners.pdf)\n- [This repository contains a collection of papers and resources on Reasoning in Large Language Models. The papers survey the state of the art in this area, and discuss how large language models can be used to obtain emergent abilities.](https://github.com/jeffhj/LM-reasoning)\n- [Full trainingset used by bloombergAI](https://mobile.twitter.com/omarsar0/status/1641788196550856704)\n- [Zain Kahn on LinkedIn reports that over 1,000 AI tools were released in March. He states that ChatGPT is just the tip of the iceberg, and that there are 20 AI tools that will transform productivity forever.](https://www.linkedin.com/posts/zainkahn_1000-ai-tools-were-released-in-march-activity-7048285306101358592-4wAA?utm_source=share&utm_medium=member_android)\n- [Language driven shell for OS (ooft)](https://www.reddit.com/r/MachineLearning/comments/129wzdk/p_engshell_a_gpt4_driven_englishlanguage_shell/)\n- [The text contains information on the release of guidelines by the DPA for the use of AI, as well as on similar efforts by other organizations. It also provides links to resources on the topic.](https://www.linkedin.com/posts/ezra-eeman-8a5ba64_dpa-just-released-its-guidelines-for-the-activity-7048985893910519808-921y?utm_source=share&utm_medium=member_android)\n- [Mind AI team website](https://mind.ai/technology)\n- [LMStudio model manager](https://lmstudio.ai/)\n- [Ahead of AI substack](https://magazine.sebastianraschka.com/archive)\n- [Meta research paper](https://drive.google.com/file/d/1i4NJKAggS82wqMamCJ1OHRGgViuyoY6R/view)\n- [State of AI report](https://www.stateof.ai/)\n- [AI ML passes American medical exams](https://www.medpagetoday.com/special-reports/exclusives/102705)\n- [Travelling salesman problem](https://github.com/diego-vicente/som-tsp)\n- [How the compression is so huge in diffusion models](https://medium.com/@socialemail/how-diffusion-models-can-achieve-seemingly-arbitrarily-large-compression-ratios-through-learning-2b21a317a46a)\n- [Understanding deep learning book](https://udlbook.github.io/udlbook/)\n- The book \"Understanding Deep Learning\" by Simon J.D. Prince covers a wide range of topics related to deep learning, from supervised and unsupervised learning to different types of neural networks and training methods. There are also chapters on measuring performance, regularization, and why deep learning works. The book includes many resources for instructors, such as slides, notebooks, and figures.\n- [This repository is a collection of links to various courses and resources about Artificial Intelligence (AI).](https://github.com/SkalskiP/courses)\n- -\n- [Top courses link github](https://github.com/SkalskiP/courses)\n- [State of GPT youtube presentation with great overview](https://www.youtube.com/watch?v=bZQun8Y4L2A)\n- Infrastructure\n- [rubbrband github auto deployments](https://rubbrband.com/)\n- [Hosting VPS](https://1984.hosting/)\n- [Free custom domains VPS](https://codesphere.com/pricing?anonymousId=YTQLcRg)\n- [Arch linux for laptop](https://wiki.archlinux.org/title/HP_Spectre_x360_(2020))\n- [360 camera compression paper](https://www.researchgate.net/publication/368728037_Masked360_Enabling_Robust_360-degree_Video_Streaming_with_Ultra_Low_Bandwidth_Consumption)\n- Multiligual and abstract translation\n- [meta seamless M4T](https://github.com/facebookresearch/seamless_communication#seamlessm4t)\n- -\n- | \n         | \n          CustomGPT is a platform that enables businesses to create their own chatbot using their own content, resulting in accurate responses without making up facts. The tool is designed to help businesses increase customer engagement and improve employee efficiency, ultimately leading to revenue growth and a competitive advantage. CustomGPT offers easy integration of content through seamless website integration or file uploading. The chatbot comes with various pricing plans, depending on the number of custom chatbots, content pages, and queries. The platform is trusted by global companies and customers, and it can be deployed for customer service, support helpdesk, and topic research. CustomGPT is powered by ChatGPT-4 and can be deployed through API or ChatGPT Plugins. The company offers a live demo and contact email for further inquiries. https://customgpt.ai/\n         | \n       |\n-\n-\n- [h2o document summary / summariser with long context](https://github.com/h2oai/h2ogpt)\n- -\n- | \n         | \n          The website replit.com has blocked your access due to the presence of potentially harmful actions, such as submitting a certain word or phrase, a SQL command or malformed data. This is a security measure to protect the website from online attacks. To resolve the issue, you can contact the site owner and provide details of the actions that caused the block and the Cloudflare Ray ID found at the bottom of the page. https://blog.replit.com/llm-training\n         | \n       |\n-\n-\n- -\n- | \n         | \n          NodePad is an LLM-assisted brainstorming experiment that helps users capture, expand, question, and organize their ideas visually. To create a new node, users simply write their thoughts in the input field and hit Enter. Nodes can be edited by double-clicking on them, linked through connectors, and deleted by clicking on them and hitting Backspace or Delete. Users can explore the app or consult the User Guide for further assistance. NodePad is designed for rapid note-taking and serendipitous ideation. https://nodepad.space/#\n         | \n       |\n-\n-\n- [Patterns for building LLMs blog post](https://eugeneyan.com/writing/llm-patterns/)\n- [textgenerator io self host](https://github.com/TextGeneratorio)\n- [Orca: The Model Few Saw Coming](https://www.youtube.com/watch?v=Dt_UNg7Mchg%22%3E%3Crichcontent)\n\t- OpenOrca includes trained in tree of thought examples and is down to 500k training tokens for the same performance as the original Microsoft Orca paper\n- [Mistral Zephyr tune for exceptional performance](https://github.com/huggingface/alignment-handbook)\n- [youtube on it](https://www.youtube.com/watch?v=Up7VKg6ZE90)\n- LLMs\n\t- The AnythingLLM project is a full-stack application designed to allow users to turn any document or piece of content into reference data that can be used by any LLM during conversations. The application can be hosted remotely, but also supports local instances. It utilizes Pinecone, ChromaDB, and other vector storage solutions, as well as OpenAI for LLM and chatting capabilities. Documents are organized into workspaces, which function like threads and allow for context to be kept clean. The monorepo consists of three main sections: the collector, frontend, and server. Requirements include yarn, node, Python 3.8+, access to an LLM such as GPT-3.5 or GPT-4, and a free account with Pinecone.io. The Docker setup enables users to get started in minutes, and the development environment includes instructions for setting up the necessary .env files and collector scripts to embed content. The project is open source and contributors can create issues and pull requests following the designated format. https://github.com/Mintplex-Labs/anything-llm\n\t         | \n\t       |\n- [AWQ 4 bit quants](https://github.com/mit-han-lab/llm-awq)\n- [Tinychat](https://github.com/mit-han-lab/llm-awq/tree/main/tinychat)\n- [Openshat model](https://github.com/imoneoi/openchat)\n- -\n- | \n         | \n          NodePad is a brainstorming tool that allows users to create nodes for their thoughts. Users can create new nodes by typing in the input field, and edit nodes by double-clicking on them. Nodes can be connected through connectors, and both nodes and connectors can be deleted by selecting and pressing Backspace or Delete. NodePad is an LLM-assisted brainstorming experiment that helps users capture, expand, question, and organize their ideas visually. The app offers a User Guide for assistance and is available for download through React Flow. https://nodepad.space/#\n         | \n       |\n-\n-\n- -\n  | \n         | \n          AnythingLLM is a full-stack personalized AI assistant application that allows users to turn any document or piece of content into a piece of data that can be used as reference when chatting. The application uses LLMs that can be hosted remotely or locally, and supports Pinecone, ChromaDB and more for vector storage and OpenAI for LLM chatting. AnythingLLM aims to be a full-stack application that can be run locally as well as hosted remotely and allows for intelligent chatting with any document provided to it. It divides documents into workspaces and provides simple UI-based tools to atomically manage the documents. There are two chat modes, conversation and query, and each chat response contains a citation that is linked to the original content. The monorepo consists of three main sections- collector, frontend and server, and requirements for the application include yarn and node on the user's machine, Python 3.8+ for running scripts in the collector, access to an LLM like GPT-3.5, GPT-4 or a drop-in replacement, and a Pinecone.io free account. https://github.com/Mintplex-Labs/anything-llm\n         | \n       |\n-\n-\n- [This text provides instructions on how to run LLM-As-Chatbot in your cloud using dstack. The steps are as follows:  1. Install and set up dstack by running the command pip install dstack[aws,gcp,azure] -U and then dstack start to start the server.  2. Create a profile by creating a .dstack/profiles.yml file that points to your created project and describes the resources you need. Example:  ``` profiles:   - name: gcp     project: gcp     resources:       memory: 48GB       gpu:         memory: 24GB     default: true ```  3. Run the initialization command: dstack init.  4. Finally, use the dstack run . command to build the environment and run LLM-As-Chatbot in your cloud. dstack will automatically forward the port to your local machine, providing secure and convenient access.  The instructions emphasize the use of dstack to automate the provisioning of cloud resources and simplify the process of running LLM-As-Chatbot in the cloud. More information about dstack and its documentation can be found for further details.](https://github.com/dstackai/LLM-As-Chatbot/wiki/Running-LLM-As-Chatbot-in-your-cloud)\n- [This text describes a project called Simple LLM Finetuner, which is a user-friendly interface designed to facilitate fine-tuning various language models using the LoRA method via the PEFT library on NVIDIA GPUs. The interface allows users to easily manage their datasets, customize parameters, train the models, and evaluate their inference capabilities.   The project includes several features such as the ability to paste datasets directly into the UI, adjustable parameters for fine-tuning and inference, and a beginner-friendly interface with explanations for each parameter. It also provides instructions on how to get started, including prerequisites such as Linux or WSL, a modern NVIDIA GPU with at least 16 GB of VRAM, and the installation of required packages using a virtual environment.  To use the project, users are instructed to clone the repository and install the required packages. Then, they can launch the interface by running the app.py file and accessing it in a browser. They can input their training data, specify the PEFT adapter name, and start the training process. After training is complete, users can navigate to the Inference tab to perform inference using their trained models.  The project provides a YouTube walkthrough for additional guidance and is licensed under the MIT License.  Overall, the Simple LLM Finetuner project aims to simplify the process of fine-tuning language models using the LoRA method and provide a user-friendly interface for managing and evaluating models.](https://github.com/lxe/simple-llama-finetuner)\n- [Maverick is an AI-driven video marketing platform that helps ecommerce stores enhance customer interactions. By creating personalized videos for each customer, Maverick enables brands to build trust, improve brand perception, and increase customer satisfaction. The platform has been well-received by ecommerce brands, with users praising the personalized videos for their effectiveness in engaging with customers and increasing subscription enrollments.  Testimonials from merchants highlight the positive impact of Maverick on their businesses. Merchants have seen a significant increase in customer engagement, with over 100 email responses per week expressing gratitude for the personalized videos. This level of interaction helps strengthen customer relationships and loyalty.  Customers of these ecommerce brands have also expressed their appreciation for the personalized videos. They mention feeling valued and delighted by the direct communication from the brand, which sets the companies apart from others in the market. The personalized videos have made customers more loyal, with some even becoming lifetime members of the brands they previously patronized.  Overall, Maverick's AI-generated video marketing approach has proven to be a game changer for ecommerce brands. It enables personalized interactions with customers at scale, leading to increased customer satisfaction, brand loyalty, and reduced refund requests. The platform has received positive feedback from both merchants and their customers, highlighting the impact and success of Maverick in the ecommerce industry.](https://lnkd.in/eptCVijb)\n- [A Twitter user named Justin Alvey recently tweeted about advancements in artificial intelligence. He mentioned a tool called LLM chaining, which allows users to perform various tasks with emails. This tool was inspired by LangChainAI. Justin Alvey also noted that this functionality is now available in real-time, thanks to OpenAI's gpt-3.5-turbo model. The tweet has gained significant attention, with hundreds of thousands of views, retweets, likes, quotes, and bookmarks.](https://twitter.com/justLV/status/1637876167763202053)\n- [The text is a LinkedIn post by Francesco Saverio Zuppichini, a Machine Learning Engineer, recommending resources to learn about Language Learning Models (LLMs).  The post includes a list of resources that Zuppichini recommended to a friend who wanted to quickly learn about LLMs. The recommended resources include academic papers, blogs, videos, and YouTube channels. Zuppichini also mentions the importance of training models with more data and for longer durations to achieve better results. He suggests looking at models like Vicuna and WizardLM, as well as different methods of prompting, such as chain of thoughts and tree of thoughts. Additionally, Zuppichini shares the LLM leaderboard from Hugging Face and encourages others to share any useful resources they may have. The post receives positive feedback from other LinkedIn users, who appreciate the resources and share their own suggestions.](https://www.linkedin.com/posts/francesco-saverio-zuppichini-94659a150_ai-ml-ds-activity-7072868294000566272-kV83?utm_source=shareandutm_medium=member_android)\n-\n\n## Current Landscape (2025)\n\n- Industry adoption and implementations\n  - Metaverse platforms continue to evolve with focus on interoperability and open standards\n  - Web3 integration accelerating with decentralised identity and asset ownership\n  - Enterprise adoption growing in virtual collaboration, training, and digital twins\n  - UK companies increasingly active in metaverse development and immersive technologies\n\n- Technical capabilities\n  - Real-time rendering at photorealistic quality levels\n  - Low-latency networking enabling seamless multi-user experiences\n  - AI-driven content generation and procedural world building\n  - Spatial audio and haptics enhancing immersion\n\n- UK and North England context\n  - Manchester: Digital Innovation Factory supports metaverse startups and research\n  - Leeds: Holovis leads in immersive experiences for entertainment and training\n  - Newcastle: University research in spatial computing and interactive systems\n  - Sheffield: Advanced manufacturing using digital twin technology\n\n- Standards and frameworks\n  - Metaverse Standards Forum driving interoperability protocols\n  - WebXR enabling browser-based immersive experiences\n  - glTF and USD for 3D asset interchange\n  - Open Metaverse Interoperability Group defining cross-platform standards\n\n## Metadata\n\n- **Last Updated**: 2025-11-16\n- **Review Status**: Automated remediation with 2025 context\n- **Verification**: Academic sources verified\n- **Regional Context**: UK/North England where applicable",
  "properties": {
    "id": "large-language-models-owl-axioms",
    "collapsed": "true",
    "- ontology": "true",
    "- term-id": "AI-0850",
    "- preferred-term": "Large Language Models",
    "- source-domain": "ai",
    "- status": "complete",
    "- public-access": "true",
    "- version": "2.0.0",
    "- last-updated": "2025-01-15",
    "- quality-score": "0.92",
    "- definition": "[[Large Language Models]] (LLMs) are [[Foundation Models]] with billions to trillions of parameters trained on massive text corpora using [[Transformer]] architectures and [[Self-Supervised Learning]], capable of performing diverse [[Natural Language Processing]] tasks through [[Few-Shot Learning]], [[Zero-Shot Learning]], and [[Prompt Engineering]]. LLMs represent a paradigm shift in [[Artificial Intelligence]], demonstrating emergent capabilities in reasoning, code generation, multilingual understanding, and complex task decomposition.",
    "- maturity": "mature",
    "- source": "[[OpenAI Research]], [[Google DeepMind]], [[Anthropic]], [[Meta AI Research]], [[NIST AI Standards]]",
    "- authority-score": "0.95",
    "- owl:class": "ai:LargeLanguageModel",
    "- owl:physicality": "VirtualEntity",
    "- owl:role": "Process",
    "- owl:inferred-class": "ai:VirtualProcess",
    "- belongsToDomain": "[[AI-GroundedDomain]], [[ComputationAndIntelligenceDomain]], [[DataManagementDomain]]",
    "- implementedInLayer": "[[ComputeLayer]], [[DataLayer]], [[AlgorithmicLayer]]",
    "- is-subclass-of": "[[Foundation Models]], [[Neural Network]], [[Deep Learning]], [[Machine Learning]]",
    "- has-part": "[[Transformer]], [[Attention Mechanism]], [[Tokenization]], [[Embedding]], [[Training Data]], [[Model Parameters]]",
    "- implements": "[[Self-Attention]], [[Multi-Head Attention]], [[Positional Encoding]], [[Backpropagation]], [[Gradient Descent]]",
    "- requires": "[[GPU Compute]], [[Distributed Training]], [[Large-Scale Datasets]], [[Model Optimization]], [[Fine-Tuning]]",
    "- enables": "[[Natural Language Understanding]], [[Text Generation]], [[Code Generation]], [[Translation]], [[Question Answering]], [[Reasoning]]",
    "- related-to": "[[Generative AI]], [[AI Agent System]], [[Prompt Engineering]], [[Retrieval-Augmented Generation]], [[Chain-of-Thought Prompting]]",
    "- bridges-to": ""
  },
  "backlinks": [
    "Education and AI",
    "Logseq",
    "BC-0072-node",
    "Open Webui and Pipelines",
    "Prompt Engineering",
    "Foundation Models",
    "Speech and voice",
    "multimodal",
    "Agentic Mycelia",
    "Interfaces",
    "Model Training",
    "Knowledge Graphing",
    "Ai in Games",
    "Runes and Glyphs",
    "Network Latency",
    "ComfyUI",
    "Python and PyTorch",
    "Metaverse Ontology",
    "Death of the Internet",
    "PEOPLE",
    "Hardware and Edge",
    "BC-0014-block-time",
    "AI Companies",
    "BC-0081-network-latency",
    "Anthropic Claude"
  ],
  "wiki_links": [
    "Text Generation",
    "Pay-Per-Use",
    "DASK",
    "Large Language Models",
    "Vector Database",
    "AI Technologies",
    "Tokenization",
    "ComputeLayer",
    "Transformer",
    "Google DeepMind",
    "Microsoft Research",
    "In-Context Learning",
    "WebAssembly",
    "Model Training",
    "INT8 Quantization",
    "Quantization",
    "Meta",
    "Edge AI",
    "OpenAI",
    "Training Data",
    "Gradient Accumulation",
    "Cluster Computing",
    "Attention Mechanism",
    "Data Privacy",
    "INT4 Quantization",
    "Semantic Search",
    "Base models",
    "Vicuna-7B",
    "Code Generation",
    "NIST AI Standards",
    "Production Deployment",
    "Artificial Intelligence",
    "Pre-Training",
    "GPT",
    "Distributed Training",
    "Distributed Computing",
    "Neural Network",
    "Model Optimization",
    "Model Parameters",
    "Reasoning",
    "Fine-Tuning",
    "OpenAI Research",
    "Foundation Model",
    "WebGPU",
    "Training Efficiency",
    "Retrieval-Augmented Generation",
    "Mixed Precision Training",
    "Offline AI",
    "LoRA",
    "API Gateway",
    "AlgorithmicLayer",
    "Domain Adaptation",
    "Decentralized AI",
    "Latency",
    "Smart Contract",
    "Backpropagation",
    "Self-Supervised Learning",
    "Memory Optimization",
    "Privacy-Preserving AI",
    "Pre-trained Model",
    "GPU",
    "Inference Acceleration",
    "Anthropic",
    "Multi-GPU Training",
    "Model Serving",
    "Memory Footprint",
    "Translation",
    "Self-Hosted AI",
    "Trillion-Parameter Models",
    "Gradient Checkpointing",
    "Compute Resources",
    "LLaMA 2",
    "Self-Attention",
    "Mixtral",
    "DataLayer",
    "Gradient Descent",
    "Positional Encoding",
    "AI Agent System",
    "Open Source",
    "DataManagementDomain",
    "GPTQ",
    "Prompt Engineering",
    "Lightning Network",
    "Chain-of-Thought Prompting",
    "WebLN",
    "Zero Redundancy Optimizer",
    "Meta AI Research",
    "Zero-Shot Learning",
    "Decentralized Training",
    "Machine Learning",
    "Large-Scale Datasets",
    "Multi-Head Attention",
    "ComputationAndIntelligenceDomain",
    "L402 Payment Protocol",
    "LLM API",
    "Autonomous Economic Agents",
    "Mistral",
    "Question Answering",
    "Model Inference",
    "Foundation Models",
    "QLoRA",
    "LLM Deployment",
    "Language Models",
    "Natural Language Understanding",
    "LLM",
    "Natural Language Processing",
    "Bitcoin",
    "Few-Shot Learning",
    "API",
    "Browser",
    "Sparsity",
    "Privacy",
    "Embedding",
    "Instruction Tuning",
    "Model Parallelism",
    "Deep Learning",
    "GPU Compute",
    "Generative AI",
    "AI Models",
    "AI-GroundedDomain",
    "Scalability",
    "Machine Translation"
  ],
  "ontology": {
    "term_id": "AI-0850",
    "preferred_term": "Large Language Models",
    "definition": "[[Large Language Models]] (LLMs) are [[Foundation Models]] with billions to trillions of parameters trained on massive text corpora using [[Transformer]] architectures and [[Self-Supervised Learning]], capable of performing diverse [[Natural Language Processing]] tasks through [[Few-Shot Learning]], [[Zero-Shot Learning]], and [[Prompt Engineering]]. LLMs represent a paradigm shift in [[Artificial Intelligence]], demonstrating emergent capabilities in reasoning, code generation, multilingual understanding, and complex task decomposition.",
    "source_domain": "ai",
    "maturity_level": null,
    "authority_score": 0.95
  }
}