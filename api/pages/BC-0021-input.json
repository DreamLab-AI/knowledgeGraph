{
  "title": "Input",
  "content": "- ### OntologyBlock\n  id:: input-ontology\n  collapsed:: true\n\n  - **Identification**\n    - public-access:: true\n    - ontology:: true\n    - term-id:: BC-0021\n    - preferred-term:: Input\n    - source-domain:: blockchain\n    - status:: complete\n    - version:: 1.0.0\n    - last-updated:: 2025-10-28\n\n  - **Definition**\n    - definition:: Transaction funding source within blockchain systems, providing essential functionality for distributed ledger technology operations and properties.\n    - maturity:: mature\n    - source:: [[ISO/IEC 23257:2021]], [[IEEE 2418.1]], [[NIST NISTIR]]\n    - authority-score:: 0.95\n\n  - **Semantic Classification**\n    - owl:class:: bc:Input\n    - owl:physicality:: VirtualEntity\n    - owl:role:: Object\n    - owl:inferred-class:: bc:VirtualObject\n    - belongsToDomain:: [[BlockchainDomain]]\n    - implementedInLayer:: [[ConceptualLayer]]\n\n  - #### Relationships\n    id:: input-relationships\n    - is-subclass-of:: [[Blockchain Entity]], [[DistributedDataStructure]]\n\n  - #### OWL Axioms\n    id:: input-owl-axioms\n    collapsed:: true\n    - ```clojure\n      Prefix(:=<http://metaverse-ontology.org/blockchain#>)\nPrefix(owl:=<http://www.w3.org/2002/07/owl#>)\nPrefix(rdf:=<http://www.w3.org/1999/02/22-rdf-syntax-ns#>)\nPrefix(xml:=<http://www.w3.org/XML/1998/namespace>)\nPrefix(xsd:=<http://www.w3.org/2001/XMLSchema#>)\nPrefix(rdfs:=<http://www.w3.org/2000/01/rdf-schema#>)\nPrefix(dct:=<http://purl.org/dc/terms/>)\n\nOntology(<http://metaverse-ontology.org/blockchain/BC-0021>\n  Import(<http://metaverse-ontology.org/blockchain/core>)\n\n  ## Class Declaration\n  Declaration(Class(:Input))\n\n  ## Subclass Relationships\n  SubClassOf(:Input :DistributedDataStructure)\n  SubClassOf(:Input :BlockchainEntity)\n\n  ## Essential Properties\n  SubClassOf(:Input\n    (ObjectSomeValuesFrom :partOf :Blockchain))\n\n  SubClassOf(:Input\n    (ObjectSomeValuesFrom :hasProperty :Property))\n\n  ## Data Properties\n  DataPropertyAssertion(:hasIdentifier :Input \"BC-0021\"^^xsd:string)\n  DataPropertyAssertion(:hasAuthorityScore :Input \"1.0\"^^xsd:decimal)\n  DataPropertyAssertion(:isFoundational :Input \"true\"^^xsd:boolean)\n\n  ## Object Properties\n  ObjectPropertyAssertion(:enablesFeature :Input :BlockchainFeature)\n  ObjectPropertyAssertion(:relatesTo :Input :RelatedConcept)\n\n  ## Annotations\n  AnnotationAssertion(rdfs:label :Input \"Input\"@en)\n  AnnotationAssertion(rdfs:comment :Input\n    \"Transaction funding source\"@en)\n  AnnotationAssertion(dct:description :Input\n    \"Foundational blockchain concept with formal ontological definition\"@en)\n  AnnotationAssertion(:termID :Input \"BC-0021\")\n  AnnotationAssertion(:priority :Input \"1\"^^xsd:integer)\n  AnnotationAssertion(:category :Input \"blockchain-fundamentals\"@en)\n)\n      ```\n\n- ## About Input\n  id:: input-about\n\n  - Transaction funding source within blockchain systems, providing essential functionality for distributed ledger technology operations and properties.\n  -\n  - ### Key Characteristics\n    id:: input-characteristics\n    - 1. **Definitional Property**: Core defining characteristic\n    - 2. **Functional Property**: Operational behavior\n    - 3. **Structural Property**: Compositional elements\n    - 4. **Security Property**: Security guarantees provided\n    - 5. **Performance Property**: Efficiency considerations\n  -\n  - ### Technical Components\n    id:: input-components\n    - **Implementation**: How concept is realized technically\n    - **Verification**: Methods for validating correctness\n    - **Interaction**: Relationships with other components\n    - **Constraints**: Technical limitations and requirements\n  -\n  - ### Use Cases\n    id:: input-use-cases\n    - **1. Core Blockchain Operation**\n    - **Application**: Fundamental blockchain functionality\n    - **Example**: Practical implementation in major blockchains\n    - **Requirements**: Technical prerequisites\n    - **Benefits**: Value provided to blockchain systems\n  -\n  - ### Standards & References\n    id:: input-standards\n    - [[ISO/IEC 23257:2021]] - Blockchain and distributed ledger technologies\n    - [[IEEE 2418.1]] - Blockchain and distributed ledger technologies\n    - [[NIST NISTIR]] - Blockchain and distributed ledger technologies\n  -\n\n\t- ### **Instant Meshes**\n\t\t- [Instant Meshes GitHub](https://github.com/wjakob/instant-meshes) - Instant Meshes is a utility designed to automatically generate quadrilateral or triangle meshes from 3D input meshes.\n\n- It simplifies the process of producing clean, manifold meshes suitable for tasks like simulation, sculpting and texturing.\n\n- The software provides algorithms for remeshing, meaning it reorganises the mesh connectivity while aiming to preserve the original surface.\n\n- It can handle meshes of arbitrary genus (with holes or handles), offering flexibility in the type of geometry it can process.\n\n- Instant Meshes allows for controlling various aspects of the remeshing process, such as target edge length and alignment to feature lines.\n\n- The tool supports importing and exporting meshes in common 3D file formats like OBJ and PLY.\n\n- It uses a command-line interface, enabling batch processing and integration into automated workflows.\n\n- The software is available as [[open source]], allowing for modifications and redistribution under the terms of its licence.\n\n- It prioritises quality and robustness, striving to produce meshes that are both aesthetically pleasing and technically sound.\n\n- A key function is to align quad meshes to principal curvature directions, resulting in anisotropic meshes that follow the underlying shape.\n\n\t\t- ### Defining Agents\n\t\t\t- An agent is an open-ended AI system that can:\n\t\t\t\t- Generate plans dynamically based on input\n\t\t\t\t- Adapt its approach to varying scenarios\n\t\t\t\t- Make interpretive decisions case-by-case\n\t\t\t\t- Access and use multiple tools flexibly\n\t\t\t\t- Agents excel at replacing entire job functions or handling creative, investigative work where the path to completion isn't predetermined.\n\n\t\t\t- #### Sending Ecash\n\t\t\t\t- Create functionality to allow users to send ecash by generating a transaction (nut) with the unblinded signature and a specified amount.\n\t\t\t\t- Include input fields for the recipient's address and the amount to be sent.\n\npublic:: true\n\n- #Public page\n\t - automatically published\n- ControlNet\n\t- Exhastive guide to controlnet [The Ultimate Guide to ControlNet (Part 1) (civitai.com)](https://education.civitai.com/civitai-guide-to-controlnet/#show-me-examples)\n\t- [OpenPoses](https://openposes.com/)\n- [(1) ControlNet Support for Multi-Input and IP-Adapter-FaceID-portrait in A1111 : StableDiffusion (reddit.com)](https://www.reddit.com/r/StableDiffusion/comments/19cwrxh/controlnet_support_for_multiinput_and/)\n-\n- Multidiffusion Spatial Controls\n\t - a Hugging Face Space by weizmannscience\n       <https://huggingface.co/spaces/weizmannscience/multidiffusion-region-based>\n      Testing ControlNet on Unreal Engine 5 : r/StableDiffusion\n       <https://www.reddit.com/r/StableDiffusion/comments/11fpcb1/testing_controlnet_on_unreal_engine_5/>\n      LineArt to PhotoReal : r/StableDiffusion\n       <https://www.reddit.com/r/StableDiffusion/comments/11mzdxm/lineart_to_photoreal/>\n      Convert Any Image To Lineart Using ControlNet! : r/StableDiffusion\n       <https://www.reddit.com/r/StableDiffusion/comments/11mwzsz/convert_any_image_to_lineart_using_controlnet/>\n      How to use Controlnet to make INCREDIBLE fully customizable Txt2Img templates : r/StableDiffusion\n       <https://www.reddit.com/r/StableDiffusion/comments/11ah3nv/how_to_use_controlnet_to_make_incredible_fully/>\n      regional prompting tutorial\n       <https://www.youtube.com/watch?v=vZ3W62dxuXI>\n      *  Tencent AI just release their method and code very similar to ControlNet : r/StableDiffusion\n       <https://www.reddit.com/r/StableDiffusion/comments/1148x38/tencent_ai_just_release_their_method_and_code/>\n      GitHub\n\t - lllyasviel/ControlNet: Let us control diffusion models!\n          GitHub\n\t - lllyasviel/ControlNet: Let us control diffusion models!: Let us control diffusion models! Contribute to lllyasviel/ControlNet development by creating an account on GitHub.\n           <https://github.com/lllyasviel/ControlNet>\n              The ControlNet project provides a way to control diffusion models. It includes a number of features to help with this, including the ability to automatically download annotators and the ability to shift the guess mode to UC disconnect in order to save memory.\n      Controlnet and character posing reddit post\n       <https://www.reddit.com/r/StableDiffusion/comments/11owo31/something_that_might_help_ppl_with_posing/>\n      *  Tencent AI just release their method and code very similar to ControlNet : r/StableDiffusion\n       <https://www.reddit.com/r/StableDiffusion/comments/1148x38/tencent_ai_just_release_their_method_and_code/>\n      *  ControlNet Character Design Workflow (links in comment) : r/StableDiffusion\n       <https://www.reddit.com/r/StableDiffusion/comments/11rfol4/controlnet_character_design_workflow_links_in/>\n      *  ControlNet Character Design Workflow (links in comment) : r/StableDiffusion\n       <https://www.reddit.com/r/StableDiffusion/comments/11rfol4/controlnet_character_design_workflow_links_in/>\n      Twitter thread on consistency settings\n       <https://twitter.com/TomLikesRobots/status/1628100062910857217>\n      Smooth animation with controlnet\n       <https://www.reddit.com/r/StableDiffusion/comments/125m56z/smooth_animation_with_controlnet_and_regional/>\n          This code snippet sets up Reddit's Sentry error monitoring, which includes a function to check for various types of errors and report them accordingly. Additionally, it sets up a fetch() wrapper to add a header specifying that Sentry should always be used in \"sticky canary\" mode.\n      Multi scene videos using automatic1111\n       <https://www.reddit.com/r/StableDiffusion/comments/127wub7/to_make_a_video_with_multiple_scenes_using_only/>\n          1 go to Automatic1111 Deforum in interpolation mode and generate several pics regarding the prompt theme. Deforum interpolation is not good for animation, but it is good for generating lot of pics about the same subject.\n          2 select the better images and put them in Deforum Init section. Then generate the animations in 2D or 3D. For this test I used only 10 steps, so graphics are not stellar. Repeat until you have several animations, each one on its directory.\n          3 select the good animations. Pick the frames and put them in a directory. Then go to Deforum Output and select Pictures interpolation , put the frames here and interpolate with value 2. With this you generate the video.\n          Note: I interpolated 693 frames. Tried bigger quantities and the interpolator did not work. So this method is pretty limited.\n      Controlnet 1.1\n       <https://www.reddit.com/r/StableDiffusion/comments/12o8qm3/finally_installed_the_newer_controlnet_models_a/>\n      Tencent controlnet\n       <https://civitai.com/models/136070?modelVersionId=155332>\n      Anime fight workflow\n       <https://www.reddit.com/r/StableDiffusion/comments/12z6rh5/half_real_converting_cowboy_bebop_spike_vs/>\n      reference only workflow\n       <https://www.reddit.com/r/StableDiffusion/comments/1408l40/a_simple_4step_workflow_with_reference_only/>\n      *\n      ControlNet v1.1: A complete guide\n\t - Stable Diffusion Art\n       <https://stable-diffusion-art.com/controlnet/>\n      The Twitter conversation includes an AI-generated video created by TomLikesRobots using canny edge detection and EbSynth technology to make a hobbit and Dumbledore contending with Sauron's will. Many people were impressed with the video and asked questions about the software and hardware used to create it. Finally, TomLikesRobots answered the questions by telling them he was using Windows 11, an RTX 3080 with 10GB VRAM and that the img2img part with ControlNet can be run online, but EbSynth might be trickier. https://twitter.com/TomLikesRobots/status/1627073211656732676\n      The text is a collection of video tutorials by Albert Bozesan, demonstrating how to use AI to create various types of digital art and design, including movie and game titles, vector graphics, pixel art, 3D assets, fantasy maps, motion graphics assets, and seamless textures. Bozesan uses a software called Stable Diffusion, which is free and runs on NVIDIA GPUs. He also mentions other AI tools such as ControlNet and Dream Textures for specific tasks. The videos show step-by-step instructions on how to install and use the software, as well as practical tips and techniques for achieving different visual effects. In addition, there are videos covering the latest AI research by NVIDIA and Google, as well as challenges with other artists using AI. The text also includes a note about the use of cookies and data on YouTube, as well as options for managing privacy settings. https://m.youtube.com/watch?v=sNcEhR65pw0&feature=youtu.be\n       <https://m.youtube.com/watch?v=sNcEhR65pw0&amp;feature=youtu.be>\n      This is a conversation between TomLikesRobots about an experiment with \"Ebsynth and Controlnet Img2img\". TomLikesRobots has tried different noise percentage values and blends. The experiment involves switching out Elizabeth Moss for Penelope Cruz, and despite some difficulties, the team is making progress towards high-detail photorealism. https://twitter.com/TomLikesRobots/status/1628104009146826763?s=20\n      The paper proposes a novel approach to generate new video content by combining zero-shot text-to-video generation with ControlNet. The method takes multiple sketched frames as input and generates video output that matches the flow of these frames. By incorporating ControlNet to enable additional input conditions, the approach leverages the benefits of both zero-shot text-to-video generation and the robust control provided by ControlNet. Experiments demonstrate that the method excels at producing high-quality and remarkably consistent video content that accurately aligns with the user’s intended motion for the subject within the video. A demo of the approach is also available for users to try out. https://sketchingthefuture.github.io/\n-\n\n\t- ### Audio-Driven Avatar: Animating Avatars with the Power of Voice\n\t\t- This section explores projects that leverage audio input to drive avatar animation, creating a more immersive and responsive experience.\n\t\t  \n\t\t  * [audio2photoreal (Facebook Research)](https://github.com/facebookresearch/audio2photoreal): A project from Facebook Research focused on generating photorealistic avatars driven by audio.\n\n\t- # Overview\n\t  \n\t   - Voice and text to parametric CAD primitives and larger digital twin models, in shared virtual spaces, is an essential feature for the commercial metaverse. This integration is already evident in current virtual reality and augmented reality applications, enabling users to manipulate and interact with 3D objects in real-time, bridging the gap between the physical and digital worlds.\n\t  \n\t  As the metaverse continues to evolve, the integration of voice and text input will play a crucial role in enhancing the overall user experience. For instance, users can verbally command a virtual design software to create specific CAD primitives or modify existing models. Additionally, the ability to add text annotations or descriptions directly within the virtual space can facilitate collaboration and communication among users.\n\t  \n\t  The expansion of corporate metaverse platforms like [[NVIDIA Omniverse]] will make shared virtual spaces increasingly complex and vast, accommodating a multitude of digital twin models. This means users will be able to explore and interact with realistic replicas of real-world objects and environments, such as buildings, vehicles, or even entire cities.\n\t  \n\t  By incorporating voice and text input functionalities, developers can empower users to manipulate and navigate these digital twin models more intuitively. Whether it's adjusting the dimensions of a virtual prototype or performing intricate measurements, the metaverse's ability to recognize and respond to voice and text commands will revolutionize the way we design, simulate, and experience virtual environments.\n\n\t- ## Future Developments\n\t  \n\t  The integration of voice and text input into the metaverse will continue to evolve with advancements in AI, NLP, and digital twin technology. As platforms like [[NVIDIA Omniverse]] expand, users will have more intuitive tools to manipulate and interact with digital twin models, enhancing collaboration and innovation in various sectors including gaming, education, retail, and real estate.\n\n- # Overview\n\t- Voice and text to parametric CAD primitives and larger digital twin models, in shared virtual spaces is an important and necessary feature for commercial metaverse. We can already see examples and hints of this in current virtual reality and augmented reality applications.\n\t- These technologies enable users to manipulate and interact with 3D objects in real-time, bridging the gap between the physical and digital worlds. As the metaverse continues to evolve, the integration of voice and text input will undoubtedly play a crucial role in enhancing the overall user experience.\n\t- Imagine being able to verbally command a virtual design software to create specific CAD primitives or modify existing models. Additionally, the ability to add text annotations or descriptions directly within the virtual space can facilitate collaboration and communication among users.\n\t- Furthermore, as corporate metaverse like [[NVIDIA Omniverse]] expands, the shared virtual spaces will become increasingly complex and vast, accommodating a multitude of digital twin models. This means that users will be able to explore and interact with realistic replicas of real-world objects and environments, such as buildings, vehicles, or even entire cities.\n\t- By incorporating voice and text input functionalities, developers can empower users to manipulate and navigate these digital twin models more intuitively. Whether it's adjusting the dimensions of a virtual prototype or performing intricate measurements, the metaverse's ability to recognize and respond to voice and text commands will revolutionize the way we design, simulate, and experience virtual environments.\n- [Table Of Contents — bd_warehouse \"0.1.0\" # Uncomment this for the next release? documentation (bd-warehouse.readthedocs.io)](https://bd-warehouse.readthedocs.io/en/latest/)\n- [Latest General topics\n\t- neThing.xyz Community Forum](https://forum.nething.xyz/c/general/4)\n-\n\n- # OpenAI ChatGPT-4o (omni)\n  id:: 66446c0e-93be-431d-93d4-1e5fa36848c5\n\t- Free to use, for everyone! Not private by default.\n\t- True multi modality across video, images, and audio.\n\t- The first of the true publicly accessible models trained without compromise for multi-modality.\n\t- Multi-lingual across 50 languages, supporting image input and output, real time video input, text to 3D.\n\t- Empathetic voice to voice with very low latency.\n\t- [Min Choi on X: \"I used GPT-4o to create STL file for 3D model in ~ 20 seconds on my phone. Pretty remarkable what you can generate with AI and simple prompt now. https://t.co/2fbObrpPol\" / X (twitter.com)](https://twitter.com/minchoi/status/1790396782200987662)\n\t- {{twitter https://twitter.com/minchoi/status/1790396782200987662}}\n\n\t- ### **Instant Meshes**\n\t\t- [Instant Meshes GitHub](https://github.com/wjakob/instant-meshes) - Instant Meshes is a utility designed to automatically generate quadrilateral or triangle meshes from 3D input meshes.\n\n- It simplifies the process of producing clean, manifold meshes suitable for tasks like simulation, sculpting and texturing.\n\n- The software provides algorithms for remeshing, meaning it reorganises the mesh connectivity while aiming to preserve the original surface.\n\n- It can handle meshes of arbitrary genus (with holes or handles), offering flexibility in the type of geometry it can process.\n\n- Instant Meshes allows for controlling various aspects of the remeshing process, such as target edge length and alignment to feature lines.\n\n- The tool supports importing and exporting meshes in common 3D file formats like OBJ and PLY.\n\n- It uses a command-line interface, enabling batch processing and integration into automated workflows.\n\n- The software is available as [[open source]], allowing for modifications and redistribution under the terms of its licence.\n\n- It prioritises quality and robustness, striving to produce meshes that are both aesthetically pleasing and technically sound.\n\n- A key function is to align quad meshes to principal curvature directions, resulting in anisotropic meshes that follow the underlying shape.\n\n\t\t- ### Defining Agents\n\t\t\t- An agent is an open-ended AI system that can:\n\t\t\t\t- Generate plans dynamically based on input\n\t\t\t\t- Adapt its approach to varying scenarios\n\t\t\t\t- Make interpretive decisions case-by-case\n\t\t\t\t- Access and use multiple tools flexibly\n\t\t\t\t- Agents excel at replacing entire job functions or handling creative, investigative work where the path to completion isn't predetermined.\n\n\t\t\t- #### Sending Ecash\n\t\t\t\t- Create functionality to allow users to send ecash by generating a transaction (nut) with the unblinded signature and a specified amount.\n\t\t\t\t- Include input fields for the recipient's address and the amount to be sent.\n\npublic:: true\n\n- #Public page\n\t - automatically published\n- ControlNet\n\t- Exhastive guide to controlnet [The Ultimate Guide to ControlNet (Part 1) (civitai.com)](https://education.civitai.com/civitai-guide-to-controlnet/#show-me-examples)\n\t- [OpenPoses](https://openposes.com/)\n- [(1) ControlNet Support for Multi-Input and IP-Adapter-FaceID-portrait in A1111 : StableDiffusion (reddit.com)](https://www.reddit.com/r/StableDiffusion/comments/19cwrxh/controlnet_support_for_multiinput_and/)\n-\n- Multidiffusion Spatial Controls\n\t - a Hugging Face Space by weizmannscience\n       <https://huggingface.co/spaces/weizmannscience/multidiffusion-region-based>\n      Testing ControlNet on Unreal Engine 5 : r/StableDiffusion\n       <https://www.reddit.com/r/StableDiffusion/comments/11fpcb1/testing_controlnet_on_unreal_engine_5/>\n      LineArt to PhotoReal : r/StableDiffusion\n       <https://www.reddit.com/r/StableDiffusion/comments/11mzdxm/lineart_to_photoreal/>\n      Convert Any Image To Lineart Using ControlNet! : r/StableDiffusion\n       <https://www.reddit.com/r/StableDiffusion/comments/11mwzsz/convert_any_image_to_lineart_using_controlnet/>\n      How to use Controlnet to make INCREDIBLE fully customizable Txt2Img templates : r/StableDiffusion\n       <https://www.reddit.com/r/StableDiffusion/comments/11ah3nv/how_to_use_controlnet_to_make_incredible_fully/>\n      regional prompting tutorial\n       <https://www.youtube.com/watch?v=vZ3W62dxuXI>\n      *  Tencent AI just release their method and code very similar to ControlNet : r/StableDiffusion\n       <https://www.reddit.com/r/StableDiffusion/comments/1148x38/tencent_ai_just_release_their_method_and_code/>\n      GitHub\n\t - lllyasviel/ControlNet: Let us control diffusion models!\n          GitHub\n\t - lllyasviel/ControlNet: Let us control diffusion models!: Let us control diffusion models! Contribute to lllyasviel/ControlNet development by creating an account on GitHub.\n           <https://github.com/lllyasviel/ControlNet>\n              The ControlNet project provides a way to control diffusion models. It includes a number of features to help with this, including the ability to automatically download annotators and the ability to shift the guess mode to UC disconnect in order to save memory.\n      Controlnet and character posing reddit post\n       <https://www.reddit.com/r/StableDiffusion/comments/11owo31/something_that_might_help_ppl_with_posing/>\n      *  Tencent AI just release their method and code very similar to ControlNet : r/StableDiffusion\n       <https://www.reddit.com/r/StableDiffusion/comments/1148x38/tencent_ai_just_release_their_method_and_code/>\n      *  ControlNet Character Design Workflow (links in comment) : r/StableDiffusion\n       <https://www.reddit.com/r/StableDiffusion/comments/11rfol4/controlnet_character_design_workflow_links_in/>\n      *  ControlNet Character Design Workflow (links in comment) : r/StableDiffusion\n       <https://www.reddit.com/r/StableDiffusion/comments/11rfol4/controlnet_character_design_workflow_links_in/>\n      Twitter thread on consistency settings\n       <https://twitter.com/TomLikesRobots/status/1628100062910857217>\n      Smooth animation with controlnet\n       <https://www.reddit.com/r/StableDiffusion/comments/125m56z/smooth_animation_with_controlnet_and_regional/>\n          This code snippet sets up Reddit's Sentry error monitoring, which includes a function to check for various types of errors and report them accordingly. Additionally, it sets up a fetch() wrapper to add a header specifying that Sentry should always be used in \"sticky canary\" mode.\n      Multi scene videos using automatic1111\n       <https://www.reddit.com/r/StableDiffusion/comments/127wub7/to_make_a_video_with_multiple_scenes_using_only/>\n          1 go to Automatic1111 Deforum in interpolation mode and generate several pics regarding the prompt theme. Deforum interpolation is not good for animation, but it is good for generating lot of pics about the same subject.\n          2 select the better images and put them in Deforum Init section. Then generate the animations in 2D or 3D. For this test I used only 10 steps, so graphics are not stellar. Repeat until you have several animations, each one on its directory.\n          3 select the good animations. Pick the frames and put them in a directory. Then go to Deforum Output and select Pictures interpolation , put the frames here and interpolate with value 2. With this you generate the video.\n          Note: I interpolated 693 frames. Tried bigger quantities and the interpolator did not work. So this method is pretty limited.\n      Controlnet 1.1\n       <https://www.reddit.com/r/StableDiffusion/comments/12o8qm3/finally_installed_the_newer_controlnet_models_a/>\n      Tencent controlnet\n       <https://civitai.com/models/136070?modelVersionId=155332>\n      Anime fight workflow\n       <https://www.reddit.com/r/StableDiffusion/comments/12z6rh5/half_real_converting_cowboy_bebop_spike_vs/>\n      reference only workflow\n       <https://www.reddit.com/r/StableDiffusion/comments/1408l40/a_simple_4step_workflow_with_reference_only/>\n      *\n      ControlNet v1.1: A complete guide\n\t - Stable Diffusion Art\n       <https://stable-diffusion-art.com/controlnet/>\n      The Twitter conversation includes an AI-generated video created by TomLikesRobots using canny edge detection and EbSynth technology to make a hobbit and Dumbledore contending with Sauron's will. Many people were impressed with the video and asked questions about the software and hardware used to create it. Finally, TomLikesRobots answered the questions by telling them he was using Windows 11, an RTX 3080 with 10GB VRAM and that the img2img part with ControlNet can be run online, but EbSynth might be trickier. https://twitter.com/TomLikesRobots/status/1627073211656732676\n      The text is a collection of video tutorials by Albert Bozesan, demonstrating how to use AI to create various types of digital art and design, including movie and game titles, vector graphics, pixel art, 3D assets, fantasy maps, motion graphics assets, and seamless textures. Bozesan uses a software called Stable Diffusion, which is free and runs on NVIDIA GPUs. He also mentions other AI tools such as ControlNet and Dream Textures for specific tasks. The videos show step-by-step instructions on how to install and use the software, as well as practical tips and techniques for achieving different visual effects. In addition, there are videos covering the latest AI research by NVIDIA and Google, as well as challenges with other artists using AI. The text also includes a note about the use of cookies and data on YouTube, as well as options for managing privacy settings. https://m.youtube.com/watch?v=sNcEhR65pw0&feature=youtu.be\n       <https://m.youtube.com/watch?v=sNcEhR65pw0&amp;feature=youtu.be>\n      This is a conversation between TomLikesRobots about an experiment with \"Ebsynth and Controlnet Img2img\". TomLikesRobots has tried different noise percentage values and blends. The experiment involves switching out Elizabeth Moss for Penelope Cruz, and despite some difficulties, the team is making progress towards high-detail photorealism. https://twitter.com/TomLikesRobots/status/1628104009146826763?s=20\n      The paper proposes a novel approach to generate new video content by combining zero-shot text-to-video generation with ControlNet. The method takes multiple sketched frames as input and generates video output that matches the flow of these frames. By incorporating ControlNet to enable additional input conditions, the approach leverages the benefits of both zero-shot text-to-video generation and the robust control provided by ControlNet. Experiments demonstrate that the method excels at producing high-quality and remarkably consistent video content that accurately aligns with the user’s intended motion for the subject within the video. A demo of the approach is also available for users to try out. https://sketchingthefuture.github.io/\n-\n\n\t- ### Audio-Driven Avatar: Animating Avatars with the Power of Voice\n\t\t- This section explores projects that leverage audio input to drive avatar animation, creating a more immersive and responsive experience.\n\t\t  \n\t\t  * [audio2photoreal (Facebook Research)](https://github.com/facebookresearch/audio2photoreal): A project from Facebook Research focused on generating photorealistic avatars driven by audio.\n\n\t- # Overview\n\t  \n\t   - Voice and text to parametric CAD primitives and larger digital twin models, in shared virtual spaces, is an essential feature for the commercial metaverse. This integration is already evident in current virtual reality and augmented reality applications, enabling users to manipulate and interact with 3D objects in real-time, bridging the gap between the physical and digital worlds.\n\t  \n\t  As the metaverse continues to evolve, the integration of voice and text input will play a crucial role in enhancing the overall user experience. For instance, users can verbally command a virtual design software to create specific CAD primitives or modify existing models. Additionally, the ability to add text annotations or descriptions directly within the virtual space can facilitate collaboration and communication among users.\n\t  \n\t  The expansion of corporate metaverse platforms like [[NVIDIA Omniverse]] will make shared virtual spaces increasingly complex and vast, accommodating a multitude of digital twin models. This means users will be able to explore and interact with realistic replicas of real-world objects and environments, such as buildings, vehicles, or even entire cities.\n\t  \n\t  By incorporating voice and text input functionalities, developers can empower users to manipulate and navigate these digital twin models more intuitively. Whether it's adjusting the dimensions of a virtual prototype or performing intricate measurements, the metaverse's ability to recognize and respond to voice and text commands will revolutionize the way we design, simulate, and experience virtual environments.\n\n\t- ## Future Developments\n\t  \n\t  The integration of voice and text input into the metaverse will continue to evolve with advancements in AI, NLP, and digital twin technology. As platforms like [[NVIDIA Omniverse]] expand, users will have more intuitive tools to manipulate and interact with digital twin models, enhancing collaboration and innovation in various sectors including gaming, education, retail, and real estate.\n\n- # Overview\n\t- Voice and text to parametric CAD primitives and larger digital twin models, in shared virtual spaces is an important and necessary feature for commercial metaverse. We can already see examples and hints of this in current virtual reality and augmented reality applications.\n\t- These technologies enable users to manipulate and interact with 3D objects in real-time, bridging the gap between the physical and digital worlds. As the metaverse continues to evolve, the integration of voice and text input will undoubtedly play a crucial role in enhancing the overall user experience.\n\t- Imagine being able to verbally command a virtual design software to create specific CAD primitives or modify existing models. Additionally, the ability to add text annotations or descriptions directly within the virtual space can facilitate collaboration and communication among users.\n\t- Furthermore, as corporate metaverse like [[NVIDIA Omniverse]] expands, the shared virtual spaces will become increasingly complex and vast, accommodating a multitude of digital twin models. This means that users will be able to explore and interact with realistic replicas of real-world objects and environments, such as buildings, vehicles, or even entire cities.\n\t- By incorporating voice and text input functionalities, developers can empower users to manipulate and navigate these digital twin models more intuitively. Whether it's adjusting the dimensions of a virtual prototype or performing intricate measurements, the metaverse's ability to recognize and respond to voice and text commands will revolutionize the way we design, simulate, and experience virtual environments.\n- [Table Of Contents — bd_warehouse \"0.1.0\" # Uncomment this for the next release? documentation (bd-warehouse.readthedocs.io)](https://bd-warehouse.readthedocs.io/en/latest/)\n- [Latest General topics\n\t- neThing.xyz Community Forum](https://forum.nething.xyz/c/general/4)\n-\n\n- # OpenAI ChatGPT-4o (omni)\n  id:: 66446c0e-93be-431d-93d4-1e5fa36848c5\n\t- Free to use, for everyone! Not private by default.\n\t- True multi modality across video, images, and audio.\n\t- The first of the true publicly accessible models trained without compromise for multi-modality.\n\t- Multi-lingual across 50 languages, supporting image input and output, real time video input, text to 3D.\n\t- Empathetic voice to voice with very low latency.\n\t- [Min Choi on X: \"I used GPT-4o to create STL file for 3D model in ~ 20 seconds on my phone. Pretty remarkable what you can generate with AI and simple prompt now. https://t.co/2fbObrpPol\" / X (twitter.com)](https://twitter.com/minchoi/status/1790396782200987662)\n\t- {{twitter https://twitter.com/minchoi/status/1790396782200987662}}\n\n\t\t- ### Defining Agents\n\t\t\t- An agent is an open-ended AI system that can:\n\t\t\t\t- Generate plans dynamically based on input\n\t\t\t\t- Adapt its approach to varying scenarios\n\t\t\t\t- Make interpretive decisions case-by-case\n\t\t\t\t- Access and use multiple tools flexibly\n\t\t\t\t- Agents excel at replacing entire job functions or handling creative, investigative work where the path to completion isn't predetermined.\n\n\t- ### Audio-Driven Avatar: Animating Avatars with the Power of Voice\n\t\t- This section explores projects that leverage audio input to drive avatar animation, creating a more immersive and responsive experience.\n\t\t  \n\t\t  * [audio2photoreal (Facebook Research)](https://github.com/facebookresearch/audio2photoreal): A project from Facebook Research focused on generating photorealistic avatars driven by audio.\n\n- # Overview\n\t- Voice and text to parametric CAD primitives and larger digital twin models, in shared virtual spaces is an important and necessary feature for commercial metaverse. We can already see examples and hints of this in current virtual reality and augmented reality applications.\n\t- These technologies enable users to manipulate and interact with 3D objects in real-time, bridging the gap between the physical and digital worlds. As the metaverse continues to evolve, the integration of voice and text input will undoubtedly play a crucial role in enhancing the overall user experience.\n\t- Imagine being able to verbally command a virtual design software to create specific CAD primitives or modify existing models. Additionally, the ability to add text annotations or descriptions directly within the virtual space can facilitate collaboration and communication among users.\n\t- Furthermore, as corporate metaverse like [[NVIDIA Omniverse]] expands, the shared virtual spaces will become increasingly complex and vast, accommodating a multitude of digital twin models. This means that users will be able to explore and interact with realistic replicas of real-world objects and environments, such as buildings, vehicles, or even entire cities.\n\t- By incorporating voice and text input functionalities, developers can empower users to manipulate and navigate these digital twin models more intuitively. Whether it's adjusting the dimensions of a virtual prototype or performing intricate measurements, the metaverse's ability to recognize and respond to voice and text commands will revolutionize the way we design, simulate, and experience virtual environments.\n- [Table Of Contents — bd_warehouse \"0.1.0\" # Uncomment this for the next release? documentation (bd-warehouse.readthedocs.io)](https://bd-warehouse.readthedocs.io/en/latest/)\n- [Latest General topics\n\t- neThing.xyz Community Forum](https://forum.nething.xyz/c/general/4)\n-\n\n\t- ### Avatar Generation: Creating Digital Beings from Scratch\n\t\t- This section explores projects that leverage audio input to drive avatar animation, creating a more immersive and responsive experience.\n\t\t  \n\t\t  * [audio2photoreal (Facebook Research)](https://github.com/facebookresearch/audio2photoreal): A project from Facebook Research focused on generating photorealistic avatars driven by audio.\n\n\t\t- ### Shift-Left Approach\n\t\t    * Use checklists and design systems to ensure accessibility is considered from the outset. This includes setting appropriate colour contrasts, designing for keyboard navigation, and considering alternative input methods.\n\t\t    * **Tools and Best Practices:**\n\n- ## Overview\n\t- Three large projection screens display AI-generated visuals that illustrate the unfolding story\n\t- Kiosks allow participants to input their own story elements, which are integrated into the narrative within minutes\n\n- ## Key Features\n\t- Three large projection screens display AI-generated visuals that illustrate the unfolding story\n\t- Kiosks allow participants to input their own story elements, which are integrated into the narrative within minutes\n\t- The installation adapts to each location, incorporating local history, artists, and cultural elements into the generated stories\n\t- ChatGPT 3.5 and 4 models are used for text generation, with the ability to request specific models for different parts of the system\n\t- Text-to-speech services are run locally to ensure reliability and reduce dependence on cloud services\n\t- The user-generated content is then integrated into the ongoing narrative, appearing on the projection screens within minutes\n\t- Participants are encouraged to input multiple story elements and then sit back and watch how their contributions shape the unfolding tale\n\t- The AI-generated visuals and audio narration create an immersive, mesmerizing experience that adapts to each participant's input\n\n- ## Participant Experience\n\t- Participants approach kiosks where they are prompted to answer questions or provide story elements within a 60-second time limit\n\t- The user-generated content is then integrated into the ongoing narrative, appearing on the projection screens within minutes\n\t- Participants are encouraged to input multiple story elements and then sit back and watch how their contributions shape the unfolding tale\n\t- The AI-generated visuals and audio narration create an immersive, mesmerizing experience that adapts to each participant's input\n\n- ## Key Features\n\t- Three large projection screens display AI-generated visuals that illustrate the unfolding story\n\t- Kiosks allow participants to input their own story elements, which are integrated into the narrative within minutes\n\t- The AI system draws from a database of story motifs, archetypes, and folktales to create a cohesive, ever-evolving narrative\n\t- The installation adapts to each location, incorporating local history, artists, and cultural elements into the generated stories\n\n- ## Participant Experience\n\t- Participants approach kiosks where they are prompted to answer questions or provide story elements within a 60-second time limit\n\t- The user-generated content is then integrated into the ongoing narrative, appearing on the projection screens within minutes\n\t- Participants are encouraged to input multiple story elements and then sit back and watch how their contributions shape the unfolding tale\n\t- The AI-generated visuals and audio narration create an immersive, mesmerizing experience that adapts to each participant's input\n\n- ## Key Features\n\t- Three large projection screens display AI-generated visuals that illustrate the unfolding story\n\t- Kiosks allow participants to input their own story elements, which are integrated into the narrative within minutes\n\t- The AI system draws from a database of story motifs, archetypes, and folktales to create a cohesive, ever-evolving narrative\n\t- The installation adapts to each location, incorporating local history, artists, and cultural elements into the generated stories",
  "properties": {
    "id": "66446c0e-93be-431d-93d4-1e5fa36848c5",
    "collapsed": "true",
    "- public-access": "true",
    "- ontology": "true",
    "- term-id": "BC-0021",
    "- preferred-term": "Input",
    "- source-domain": "blockchain",
    "- status": "complete",
    "- version": "1.0.0",
    "- last-updated": "2025-10-28",
    "- definition": "Transaction funding source within blockchain systems, providing essential functionality for distributed ledger technology operations and properties.",
    "- maturity": "mature",
    "- source": "[[ISO/IEC 23257:2021]], [[IEEE 2418.1]], [[NIST NISTIR]]",
    "- authority-score": "0.95",
    "- owl:class": "bc:Input",
    "- owl:physicality": "VirtualEntity",
    "- owl:role": "Object",
    "- owl:inferred-class": "bc:VirtualObject",
    "- belongsToDomain": "[[BlockchainDomain]]",
    "- implementedInLayer": "[[ConceptualLayer]]",
    "- is-subclass-of": "[[Blockchain Entity]], [[DistributedDataStructure]]",
    "public": "true"
  },
  "backlinks": [],
  "wiki_links": [
    "DistributedDataStructure",
    "NVIDIA Omniverse",
    "ISO/IEC 23257:2021",
    "IEEE 2418.1",
    "Blockchain Entity",
    "BlockchainDomain",
    "open source",
    "NIST NISTIR",
    "ConceptualLayer"
  ],
  "ontology": {
    "term_id": "BC-0021",
    "preferred_term": "Input",
    "definition": "Transaction funding source within blockchain systems, providing essential functionality for distributed ledger technology operations and properties.",
    "source_domain": "blockchain",
    "maturity_level": null,
    "authority_score": 0.95
  }
}