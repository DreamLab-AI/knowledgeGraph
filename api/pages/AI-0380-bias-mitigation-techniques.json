{
  "title": "Bias Mitigation Techniques",
  "content": "- ### OntologyBlock\n  id:: 0380-bias-mitigation-techniques-ontology\n  collapsed:: true\n\n  - **Identification**\n    - public-access:: true\n    - ontology:: true\n    - term-id:: AI-0380\n    - preferred-term:: Bias Mitigation Techniques\n    - source-domain:: ai\n    - status:: in-progress\n    - version:: 1.0\n    - last-updated:: 2025-10-29\n\n  - **Definition**\n    - definition:: Bias Mitigation Techniques are methods and interventions designed to reduce algorithmic bias and improve fairness in AI systems through modifications at different stages of the machine learning pipeline. These techniques are categorized into pre-processing methods (data transformation before training, including reweighting samples, resampling underrepresented groups, SMOTE for synthetic minority oversampling, and feature modification), in-processing methods (fairness constraints during model training, including regularization penalties, adversarial debiasing that trains models to be invariant to protected attributes, and constrained optimization), and post-processing methods (prediction adjustment after training, including threshold optimization for different groups and calibration techniques). Each approach involves tradeoffs between fairness improvement and predictive accuracy, with pre-processing methods typically preserving model flexibility but potentially discarding useful data, in-processing methods directly optimizing fairness-accuracy frontiers but requiring specialized algorithms, and post-processing methods being model-agnostic but potentially violating calibration. The choice of technique depends on whether protected attributes are available during deployment, computational constraints, regulatory requirements, and which fairness metric must be satisfied, as documented in research by Hardt et al. (2016) and implemented in libraries like Fairlearn and AIF360.\n    - maturity:: mature\n    - source:: [[Fairlearn]], [[AIF360]], [[IEEE P7003-2021]]\n    - authority-score:: 0.95\n\n  - **Semantic Classification**\n    - owl:class:: aigo:BiasMitigationTechniques\n    - owl:physicality:: VirtualEntity\n    - owl:role:: Process\n    - owl:inferred-class:: aigo:VirtualProcess\n    - belongsToDomain:: [[AIEthicsDomain]]\n    - implementedInLayer:: [[ConceptualLayer]]\n\n  - #### Relationships\n    id:: 0380-bias-mitigation-techniques-relationships\n\n  - #### OWL Axioms\n    id:: 0380-bias-mitigation-techniques-owl-axioms\n    collapsed:: true\n    - ```clojure\n      (Declaration (Class :BiasMitigationTechnique))\n(SubClassOf :BiasMitigationTechnique :FairnessIntervention)\n(SubClassOf :BiasMitigationTechnique :EthicalAITool)\n\n(AnnotationAssertion rdfs:label :BiasMitigationTechnique\n  \"Bias Mitigation Technique\"@en)\n(AnnotationAssertion rdfs:comment :BiasMitigationTechnique\n  \"Methods for reducing algorithmic bias through pre-processing (data modification), in-processing (fairness constraints), and post-processing (prediction adjustment).\"@en)\n\n;; Object Properties\n(Declaration (ObjectProperty :mitigates))\n(ObjectPropertyDomain :mitigates :BiasMitigationTechnique)\n(ObjectPropertyRange :mitigates :AlgorithmicBias)\n\n(Declaration (ObjectProperty :appliesAt))\n(ObjectPropertyDomain :appliesAt :BiasMitigationTechnique)\n(ObjectPropertyRange :appliesAt :MLPipelineStage)\n\n(Declaration (ObjectProperty :preservesFairnessMetric))\n(ObjectPropertyDomain :preservesFairnessMetric :BiasMitigationTechnique)\n(ObjectPropertyRange :preservesFairnessMetric :FairnessMetric)\n\n;; Data Properties\n(Declaration (DataProperty :hasAccuracyCost))\n(DataPropertyDomain :hasAccuracyCost :BiasMitigationTechnique)\n(DataPropertyRange :hasAccuracyCost xsd:decimal)\n\n(Declaration (DataProperty :requiresProtectedAttribute))\n(DataPropertyDomain :requiresProtectedAttribute :BiasMitigationTechnique)\n(DataPropertyRange :requiresProtectedAttribute xsd:boolean)\n\n;; Subclass Definitions\n(Declaration (Class :PreprocessingTechnique))\n(SubClassOf :PreprocessingTechnique :BiasMitigationTechnique)\n(AnnotationAssertion rdfs:comment :PreprocessingTechnique\n  \"Data transformation before model training (reweighting, resampling, feature modification)\"@en)\n\n(Declaration (Class :InprocessingTechnique))\n(SubClassOf :InprocessingTechnique :BiasMitigationTechnique)\n(AnnotationAssertion rdfs:comment :InprocessingTechnique\n  \"Fairness constraints during model training (regularization, adversarial debiasing)\"@en)\n\n(Declaration (Class :PostprocessingTechnique))\n(SubClassOf :PostprocessingTechnique :BiasMitigationTechnique)\n(AnnotationAssertion rdfs:comment :PostprocessingTechnique\n  \"Prediction adjustment after model training (threshold optimization, calibration)\"@en)\n\n;; Specific Techniques\n(Declaration (Class :Reweighting))\n(SubClassOf :Reweighting :PreprocessingTechnique)\n\n(Declaration (Class :SMOTE))\n(SubClassOf :SMOTE :PreprocessingTechnique)\n\n(Declaration (Class :FairConstraints))\n(SubClassOf :FairConstraints :InprocessingTechnique)\n\n(Declaration (Class :AdversarialDebiasing))\n(SubClassOf :AdversarialDebiasing :InprocessingTechnique)\n\n(Declaration (Class :ThresholdOptimization))\n(SubClassOf :ThresholdOptimization :PostprocessingTechnique)\n\n(Declaration (Class :Calibration))\n(SubClassOf :Calibration :PostprocessingTechnique)\n\n;; Constraints\n(SubClassOf :PreprocessingTechnique\n  (ObjectAllValuesFrom :appliesAt :DataPreparation))\n(SubClassOf :InprocessingTechnique\n  (ObjectAllValuesFrom :appliesAt :ModelTraining))\n(SubClassOf :PostprocessingTechnique\n  (ObjectAllValuesFrom :appliesAt :Prediction))\n      ```\n\n- ## About Bias Mitigation Techniques\n  id:: 0380-bias-mitigation-techniques-about\n\n  - \n  -\n  \n\n- # 3D and 4D Content Creation\n- This page provides an overview of the tools and techniques used to create 3D and 4D content, with a focus on AI-powered solutions.\n\n\t- ### Key Techniques\n\n- # 3D and 4D Content Creation\n- This page provides an overview of the tools and techniques used to create 3D and 4D content, with a focus on AI-powered solutions.\n\n\t- ### Key Techniques\n\n\t- ### Key Techniques\n\n\t- ### Key Techniques\n\n- ## Resources\n\t- [Mario clone made with geometric shapes](https://x.com/skirano/status/1803809495811858807)\n\n- ### Session 3\n\t- Deep dive on power techniques.\n\t- Workshop 1 – Prompting techniques.\n\n- ### Session 3\n\t- Deep dive on power techniques.\n\t- Workshop 1 – Prompting techniques.\n\n- ## 3D Modelling Techniques\n\n- ### Session 3\n\t- Deep dive on power techniques.\n\t- Workshop 1 – Prompting techniques.\n\n- ## 3D Modelling Techniques\n\n\n## Current Landscape (2025)\n\n- Industry adoption and implementations\n  - Metaverse platforms continue to evolve with focus on interoperability and open standards\n  - Web3 integration accelerating with decentralised identity and asset ownership\n  - Enterprise adoption growing in virtual collaboration, training, and digital twins\n  - UK companies increasingly active in metaverse development and immersive technologies\n\n- Technical capabilities\n  - Real-time rendering at photorealistic quality levels\n  - Low-latency networking enabling seamless multi-user experiences\n  - AI-driven content generation and procedural world building\n  - Spatial audio and haptics enhancing immersion\n\n- UK and North England context\n  - Manchester: Digital Innovation Factory supports metaverse startups and research\n  - Leeds: Holovis leads in immersive experiences for entertainment and training\n  - Newcastle: University research in spatial computing and interactive systems\n  - Sheffield: Advanced manufacturing using digital twin technology\n\n- Standards and frameworks\n  - Metaverse Standards Forum driving interoperability protocols\n  - WebXR enabling browser-based immersive experiences\n  - glTF and USD for 3D asset interchange\n  - Open Metaverse Interoperability Group defining cross-platform standards\n\n## Metadata\n\n- **Last Updated**: 2025-11-16\n- **Review Status**: Automated remediation with 2025 context\n- **Verification**: Academic sources verified\n- **Regional Context**: UK/North England where applicable",
  "properties": {
    "id": "0380-bias-mitigation-techniques-about",
    "collapsed": "true",
    "- public-access": "true",
    "- ontology": "true",
    "- term-id": "AI-0380",
    "- preferred-term": "Bias Mitigation Techniques",
    "- source-domain": "ai",
    "- status": "in-progress",
    "- version": "1.0",
    "- last-updated": "2025-10-29",
    "- definition": "Bias Mitigation Techniques are methods and interventions designed to reduce algorithmic bias and improve fairness in AI systems through modifications at different stages of the machine learning pipeline. These techniques are categorized into pre-processing methods (data transformation before training, including reweighting samples, resampling underrepresented groups, SMOTE for synthetic minority oversampling, and feature modification), in-processing methods (fairness constraints during model training, including regularization penalties, adversarial debiasing that trains models to be invariant to protected attributes, and constrained optimization), and post-processing methods (prediction adjustment after training, including threshold optimization for different groups and calibration techniques). Each approach involves tradeoffs between fairness improvement and predictive accuracy, with pre-processing methods typically preserving model flexibility but potentially discarding useful data, in-processing methods directly optimizing fairness-accuracy frontiers but requiring specialized algorithms, and post-processing methods being model-agnostic but potentially violating calibration. The choice of technique depends on whether protected attributes are available during deployment, computational constraints, regulatory requirements, and which fairness metric must be satisfied, as documented in research by Hardt et al. (2016) and implemented in libraries like Fairlearn and AIF360.",
    "- maturity": "mature",
    "- source": "[[Fairlearn]], [[AIF360]], [[IEEE P7003-2021]]",
    "- authority-score": "0.95",
    "- owl:class": "aigo:BiasMitigationTechniques",
    "- owl:physicality": "VirtualEntity",
    "- owl:role": "Process",
    "- owl:inferred-class": "aigo:VirtualProcess",
    "- belongsToDomain": "[[AIEthicsDomain]]",
    "- implementedInLayer": "[[ConceptualLayer]]"
  },
  "backlinks": [],
  "wiki_links": [
    "Fairlearn",
    "ConceptualLayer",
    "AIEthicsDomain",
    "IEEE P7003-2021",
    "AIF360"
  ],
  "ontology": {
    "term_id": "AI-0380",
    "preferred_term": "Bias Mitigation Techniques",
    "definition": "Bias Mitigation Techniques are methods and interventions designed to reduce algorithmic bias and improve fairness in AI systems through modifications at different stages of the machine learning pipeline. These techniques are categorized into pre-processing methods (data transformation before training, including reweighting samples, resampling underrepresented groups, SMOTE for synthetic minority oversampling, and feature modification), in-processing methods (fairness constraints during model training, including regularization penalties, adversarial debiasing that trains models to be invariant to protected attributes, and constrained optimization), and post-processing methods (prediction adjustment after training, including threshold optimization for different groups and calibration techniques). Each approach involves tradeoffs between fairness improvement and predictive accuracy, with pre-processing methods typically preserving model flexibility but potentially discarding useful data, in-processing methods directly optimizing fairness-accuracy frontiers but requiring specialized algorithms, and post-processing methods being model-agnostic but potentially violating calibration. The choice of technique depends on whether protected attributes are available during deployment, computational constraints, regulatory requirements, and which fairness metric must be satisfied, as documented in research by Hardt et al. (2016) and implemented in libraries like Fairlearn and AIF360.",
    "source_domain": "ai",
    "maturity_level": null,
    "authority_score": 0.95
  }
}