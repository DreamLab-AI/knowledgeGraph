{
  "title": "Apple",
  "content": "- ### OntologyBlock\n  id:: apple-ontology\n  collapsed:: true\n\t- ontology:: true\n\t- term-id:: mv-573493248142\n\t- preferred-term:: Apple\n\t- source-domain:: metaverse\n\t- status:: active\n\t- public-access:: true\n\t- definition:: A component of the metaverse ecosystem focusing on apple.\n\t- maturity:: mature\n\t- authority-score:: 0.85\n\t- owl:class:: mv:Apple\n\t- owl:physicality:: ConceptualEntity\n\t- owl:role:: Concept\n\t- belongsToDomain:: [[MetaverseDomain]]\n\t- #### Relationships\n\t  id:: apple-relationships\n\t\t- enables:: [[ImmersiveExperience]], [[Presence]], [[SpatialComputing]]\n\t\t- requires:: [[DisplayTechnology]], [[TrackingSystem]], [[RenderingEngine]]\n\t\t- bridges-to:: [[HumanComputerInteraction]], [[ComputerVision]], [[Robotics]]\n\n\t- #### OWL Axioms\n\t  id:: apple-owl-axioms\n\t  collapsed:: true\n\t\t- ```clojure\n\t\t  Declaration(Class(mv:Apple))\n\n\t\t  # Classification\n\t\t  SubClassOf(mv:Apple mv:ConceptualEntity)\n\t\t  SubClassOf(mv:Apple mv:Concept)\n\n\t\t  # Domain classification\n\t\t  SubClassOf(mv:Apple\n\t\t    ObjectSomeValuesFrom(mv:belongsToDomain mv:MetaverseDomain)\n\t\t  )\n\n\t\t  # Annotations\n\t\t  AnnotationAssertion(rdfs:label mv:Apple \"Apple\"@en)\n\t\t  AnnotationAssertion(rdfs:comment mv:Apple \"A component of the metaverse ecosystem focusing on apple.\"@en)\n\t\t  AnnotationAssertion(dcterms:identifier mv:Apple \"mv-573493248142\"^^xsd:string)\n\t\t  ```\n\npublic:: true\n\n- #Public page automatically published\n- # Partnership with OpenAI, and Siri\n\t- Apple is focusing on \"AI for the rest of us\" - making AI capabilities accessible and useful for everyday tasks rather than flashy frontier use cases. The emphasis is on small but significant time-saving wins.\n\t- Siri is the centerpiece, with expanded natural language understanding, ability to maintain context, and both voice and text input. Siri can now take actions across Apple and third-party apps.\n\t- Apple has partnered with OpenAI to integrate ChatGPT into Siri and other Apple experiences later this year. Siri can tap into ChatGPT when needed to expand its capabilities.\n\t- Apple argues that great product experience matters more than just having state-of-the-art AI models. They are willing to reduce user choice to create a simpler experience, as seen with the limited options in their Image Playground feature.\n\t- Apple is trying to balance leveraging personal context and data with strong privacy protections through on-device processing and a new \"private cloud compute\" capability.\n\t- Apple is taking an approach of deeply integrating AI assistants and capabilities across the OS in a frictionless way to help users with everyday tasks, while maintaining their emphasis on privacy and a carefully designed user experience over flashy demos. The partnership with OpenAI expands what's possible while keeping the Apple experience at the forefront.\n\t- ### IOS18 Security and Privacy\n\t\t- [Thread by Matthew Green](https://threadreaderapp.com/thread/1800291897245835616.html?)\n\t\t\t- Apple, unlike most other mobile providers, has traditionally done a lot of processing on-device. For example, all of the machine learning and OCR text recognition on Photos is done right on your device. \n\t\t\t  The problem is that while modern phone “neural” hardware is improving, it’s not improving fast enough to take advantage of all the crazy features Silicon Valley wants from modern AI, including generative AI and its ilk. This fundamentally requires servers.\n\t\t\t- But if you send your tasks out to servers in “the cloud” (god using quotes makes me feel 80), this means sending incredibly private data off your phone and out over the Internet. That exposes you to spying, hacking, and the data hungry business model of Silicon Valley.\n\t\t\t- The solution Apple has come up with is to try to build secure and trustworthy hardware in their own data centers. Your phone can then “outsource” heavy tasks to this hardware. Seems easy, right? Well: here’s the blog post.\n\t\t\t- [**Blog**](https://security.apple.com/blog/private-cloud-compute/) Private Cloud Compute: A new frontier for AI privacy in the cloud - Apple Security Research**Secure and private AI processing in the cloud poses a formidable new challenge. To support advanced features of Apple Intelligence with larger foundation models, we created Private Cloud Compute (PCC)\n\t\t\t- TL;DR: it is not easy. Building trustworthy computers is literally the hardest problem in computer security. Honestly it’s almost the only problem in computer security. But while it remains a challenging problem, we’ve made a lot of advances. Apple is using almost all of them.\n\t\t\t- The first thing Apple is doing is using all the advances they’ve made in building secure phones and PCs in their new servers. This involves using Secure Boot and a Secure Enclave Processor (SEP) to hold keys. They’ve presumably turned on all the processor security features.\n\t\t\t- Then they’re throwing all kinds of processes at the server hardware to make sure the hardware isn’t tampered with. I can’t tell if this prevents hardware attacks, but it seems like a start.\n\t\t\t- They also use a bunch of protections to ensure that software is legitimate. One is that the software is “stateless” and allegedly doesn’t keep information between user requests. To help ensure this, each server/node reboot re-keys and wipes all storage.\n\t\t\t- A second protection is that the operating system can “attest” to the software image it’s running. Specifically, it signs a hash of the software and shares this with every phone/client. If you trust this infrastructure, you’ll know it’s running a specific piece of software.\n\t\t\t- Of course, knowing that the phone is running a specific piece of software doesn’t help you if you don’t trust the software. So Apple plans to put each binary image into a “transparency log” and publish the software.\n\t\t\t- Security researchers will get *some code* and a VM they can use to run the software. They’ll then have to reverse-engineer the binaries to see if they’re doing unexpected things. It’s a little suboptimal.\n\t\t\t- When your phone wants to outsource a task, it will contact Apple and obtain a list of servers/nodes and their keys. It will then encrypt its request to all servers, and one will process it. They’re even using fancy anonymous credentials and a third part relay to hide your IP.\n\t\t\t- Ok there are probably half a dozen more technical details in the blog post. It’s a very thoughtful design. Indeed, if you gave an excellent team a huge pile of money and told them to build the best “private” cloud in the world, it would probably look like this.\n\t\t\t- But now the tough questions. Is it a good idea? And is it as secure as what Apple does today? And most importantly:\n\t\t\t- I admit that as I learned about this feature, it made me kind of sad. The thought that was going through my head was: this is going to be too much of a temptation. Once you can “safely” outsource tasks to the cloud, why bother doing them locally. Outsource everything!\n\t\t\t- As best I can tell, Apple does not have explicit plans to announce when your data is going off-device for to Private Compute. You won’t opt into this, you won’t necessarily even be told it’s happening. It will just happen. Magically. I don’t love that part.\n\t\t\t- Finally, there are so many invisible sharp edges that could exist in a system like this. Hardware flaws. Issues with the cryptographic attestation framework. Clever software exploits. Many of these will be hard for security researchers to detect. That worries me too. 18/\n\t\t\t- Wrapping up on a more positive note: it’s worth keeping in mind that sometimes the perfect is the enemy of the really good.\n\t\t\t- In practice the alternative to on-device is: ship private data to OpenAI or someplace sketchier, where who knows what might happen to it.\n\t\t\t- And of course, keep in mind that super-spies aren’t your biggest adversary. For many people your biggest adversary is the company who sold you your device/software. This PCC system represents a real commitment by Apple not to “peek” at your data. That’s a big deal.\n\t\t\t- In any case, this is the world we’re moving to. Your phone might seem to be in your pocket, but a part of it lives 2,000 miles away in a data center. As security folks we probably need to get used to that fact, and do the best we can to make sure all parts are secure.\n- They are sitting on a huge cash war chest and can effectively buy their way through and out of the coming battles around ip like the NYT court case.\n- Biding their time waiting for local inferencing that leverages strong legacy media buy in might be a great play. Only the cost to their mind share of talent might be an issue.\n- Apple are innovating in core ML research to support large language models.\n- They are developing new techniques for data management between flash memory and DRAM, crucial for running larger models on devices with limited memory.\n- The research also reveals significant speed improvements, with 4-5 times faster processing on CPUs and 20-25 times on GPUs for models up to twice the size of the available DRAM. These advancements could lead to a wider adoption of these technologies,\n\t- [Paper page\n\t\t- LLM in a flash: Efficient Large Language Model Inference with Limited Memory (huggingface.co)](https://huggingface.co/papers/2312.11514) [[Hardware and Edge]]\n- [Apple wants AI to run directly on its hardware instead of in the cloud | Ars Technica](https://arstechnica.com/apple/2023/12/apple-wants-ai-to-run-directly-on-its-hardware-instead-of-in-the-cloud/) [[Hardware and Edge]]\n- HUGS: Human [[Gaussian splatting and Similar]]\n\t- [Apple Machine Learning Research](https://machinelearning.apple.com/research/hugs)\n- Apple presents [Paper page\n\t- Speculative Streaming: Fast LLM Inference without Auxiliary Models (huggingface.co)](https://huggingface.co/papers/2402.11131):\n\t- Speculative decoding is a prominent technique to speed up the inference of a large target language model based on predictions of an auxiliary draft model. While effective, in application-specific settings, it often involves fine-tuning both draft and target models to achieve high acceptance rates. As the number of downstream tasks grows, these draft models add significant complexity to inference systems. We propose Speculative Streaming, a single-model speculative decoding method that fuses drafting into the target model by changing the fine-tuning objective from next token prediction to future n-gram prediction. Speculative Streaming speeds up decoding by 1.8\n\t- 3.1X in a diverse set of tasks, such as Summarization, Structured Queries, and Meaning Representation, without sacrificing generation quality. Additionally, Speculative Streaming is parameter-efficient. It achieves on-par/higher speed-ups than Medusa-style architectures while using ~10000X fewer extra parameters, making it well-suited for resource-constrained devices.\n\n## Current Landscape (2025)\n\n- Industry adoption and implementations\n  - Metaverse platforms continue to evolve with focus on interoperability and open standards\n  - Web3 integration accelerating with decentralised identity and asset ownership\n  - Enterprise adoption growing in virtual collaboration, training, and digital twins\n  - UK companies increasingly active in metaverse development and immersive technologies\n\n- Technical capabilities\n  - Real-time rendering at photorealistic quality levels\n  - Low-latency networking enabling seamless multi-user experiences\n  - AI-driven content generation and procedural world building\n  - Spatial audio and haptics enhancing immersion\n\n- UK and North England context\n  - Manchester: Digital Innovation Factory supports metaverse startups and research\n  - Leeds: Holovis leads in immersive experiences for entertainment and training\n  - Newcastle: University research in spatial computing and interactive systems\n  - Sheffield: Advanced manufacturing using digital twin technology\n\n- Standards and frameworks\n  - Metaverse Standards Forum driving interoperability protocols\n  - WebXR enabling browser-based immersive experiences\n  - glTF and USD for 3D asset interchange\n  - Open Metaverse Interoperability Group defining cross-platform standards\n\n## Metadata\n\n- **Last Updated**: 2025-11-16\n- **Review Status**: Automated remediation with 2025 context\n- **Verification**: Academic sources verified\n- **Regional Context**: UK/North England where applicable",
  "properties": {
    "id": "apple-owl-axioms",
    "collapsed": "true",
    "- ontology": "true",
    "- term-id": "mv-573493248142",
    "- preferred-term": "Apple",
    "- source-domain": "metaverse",
    "- status": "active",
    "- public-access": "true",
    "- definition": "A component of the metaverse ecosystem focusing on apple.",
    "- maturity": "mature",
    "- authority-score": "0.85",
    "- owl:class": "mv:Apple",
    "- owl:physicality": "ConceptualEntity",
    "- owl:role": "Concept",
    "- belongsToDomain": "[[MetaverseDomain]]",
    "- enables": "[[ImmersiveExperience]], [[Presence]], [[SpatialComputing]]",
    "- requires": "[[DisplayTechnology]], [[TrackingSystem]], [[RenderingEngine]]",
    "- bridges-to": "[[HumanComputerInteraction]], [[ComputerVision]], [[Robotics]]",
    "public": "true"
  },
  "backlinks": [
    "Definitions and frameworks for Metaverse",
    "Patent",
    "Mixed reality",
    "BC-0072-node",
    "Prompt Engineering",
    "multimodal",
    "ComfyUI",
    "relighting",
    "BC-0074-light-node"
  ],
  "wiki_links": [
    "ComputerVision",
    "Presence",
    "MetaverseDomain",
    "TrackingSystem",
    "DisplayTechnology",
    "RenderingEngine",
    "ImmersiveExperience",
    "Robotics",
    "Gaussian splatting and Similar",
    "HumanComputerInteraction",
    "Hardware and Edge",
    "SpatialComputing"
  ],
  "ontology": {
    "term_id": "mv-573493248142",
    "preferred_term": "Apple",
    "definition": "A component of the metaverse ecosystem focusing on apple.",
    "source_domain": "metaverse",
    "maturity_level": null,
    "authority_score": 0.85
  }
}