{
  "title": "Scene Capture and Reconstruction",
  "content": "- ### OntologyBlock\n  id:: scene-capture-and-reconstruction-ontology\n  collapsed:: true\n\t- ontology:: true\n\t- term-id:: mv-61811463877\n\t- preferred-term:: Scene Capture and Reconstruction\n\t- source-domain:: metaverse\n\t- status:: active\n\t- public-access:: true\n\t- definition:: A component of the metaverse ecosystem focusing on scene capture and reconstruction.\n\t- maturity:: mature\n\t- authority-score:: 0.85\n\t- owl:class:: mv:SceneCaptureAndReconstruction\n\t- owl:physicality:: VirtualEntity\n\t- owl:role:: Object\n\t- belongsToDomain:: [[MetaverseDomain]]\n\t- #### Relationships\n\t  id:: scene-capture-and-reconstruction-relationships\n\t\t- is-part-of:: [[VirtualWorld]], [[MetaversePlatform]]\n\t\t- requires:: [[DigitalIdentity]], [[AuthenticationService]]\n\t\t- enables:: [[SocialInteraction]], [[Presence]], [[UserRepresentation]]\n\t\t- has-property:: [[Appearance]], [[Customization]], [[Animation]]\n\n\t- #### OWL Axioms\n\t  id:: scene-capture-and-reconstruction-owl-axioms\n\t  collapsed:: true\n\t\t- ```clojure\n\t\t  Declaration(Class(mv:SceneCaptureAndReconstruction))\n\n\t\t  # Classification\n\t\t  SubClassOf(mv:SceneCaptureAndReconstruction mv:VirtualEntity)\n\t\t  SubClassOf(mv:SceneCaptureAndReconstruction mv:Object)\n\n\t\t  # Domain classification\n\t\t  SubClassOf(mv:SceneCaptureAndReconstruction\n\t\t    ObjectSomeValuesFrom(mv:belongsToDomain mv:MetaverseDomain)\n\t\t  )\n\n\t\t  # Annotations\n\t\t  AnnotationAssertion(rdfs:label mv:SceneCaptureAndReconstruction \"Scene Capture and Reconstruction\"@en)\n\t\t  AnnotationAssertion(rdfs:comment mv:SceneCaptureAndReconstruction \"A component of the metaverse ecosystem focusing on scene capture and reconstruction.\"@en)\n\t\t  AnnotationAssertion(dcterms:identifier mv:SceneCaptureAndReconstruction \"mv-61811463877\"^^xsd:string)\n\t\t  ```\n\npublic:: true\n\n- #Public page automatically published\n- ## NeRFs\n\t- **Early Foundations of NeRF:**\n\t\t- **Early Photography and Photosculpture (ca 1850):** Pioneers in photography began experimenting with aerial photogrammetry and photosculptures, creating 3D representations from multiple 2D photographs, laying groundwork for future 3D capture technologies.\n\t\t\t- [More on early photography](https://hackaday.com/2022/10/02/in-a-way-3d-scanning-is-over-a-century-old/)\n\t- **Plenoptic Function and Light Fields (1908 & 1936):**\n\t\t- Gabriel Lippmann introduces the concept of the **plenoptic function** which evolves into the **light field** concept, simplifying the capture and representation of light as it travels through space.\n\t- **Advances in Computing and Graphics (1960-1980s):**\n\t\t- The development of lasers, holograms, and computer graphics sets the stage for more advanced 3D representations and the capture of light fields.\n\t- **Lightfield Camera Arrays and Image-Based Rendering (2000):**\n\t\t- The introduction of lightfield camera arrays and image-based rendering techniques brings light fields into practical use, although it remains complex and niche.\n\t\t\t- [Stanford light field camera array 2004](http://graphics.stanford.edu/projects/lightfield/)\n\t- **Generative AI and Inverse Rendering Birth of NeRF (2020):**\n\t\t- **Neural Radiance Fields (NeRF):** A breakthrough in 2020 with the introduction of NeRF, offering an efficient way to recreate 3D scenes from 2D images using neural networks.\n\t\t\t- [First NeRF paper, 2020](https://www.matthewtancik.com/nerf)\n\t- **Realtime NeRF and Commercial Applications (2022):**\n\t\t- **NVIDIA's Instant-NGP:** Accelerates NeRF processing, enabling real-time rendering on consumer-grade hardware and facilitating wider adoption and practical applications in various fields.\n\t\t\t- [NVIDIA instant-ngp 2022](https://github.com/NVlabs/instant-ngp)\n\t- **The Future of NeRF:**\n\t\t- The continuous development of NeRF is expected to integrate more seamlessly with web browsers, game engines, and potentially transform large-scale mapping, video conferencing, and real-time interactive applications.\n\t\t\t- [Google's Project Starline](https://blog.google/technology/research/project-starline-expands-testing/)\n- <iframe src=\"https://mohamad-shahbazi.github.io/inserf/\" style=\"width: 100%; height: 600px\"></iframe>\n- **Links and Image Sources:**\n\t- [SceneScript: Reconstructing Scenes With An Autoregressive Structured Language Model | Research AI at Meta](https://ai.meta.com/research/publications/scenescript-reconstructing-scenes-with-an-autoregressive-structured-language-model/)\n\t\t- ![Screenshot 2024-03-22 192251.png](../assets/Screenshot_2024-03-22_192251_1711135500802_0.png)\n\t- [RealityCapture 3D Models from Photos and/or Laser Scans (capturingreality.com)](https://www.capturingreality.com/)  **FREE**\n\t- [A Short 170 Year History Of Neural Radiance Fields (NeRF), Holograms, And Light Fields](radiancefields.com/history-of-neural-radiance-fields/)\n\t- [ProNeRF: Learning Efficient Projection-Aware Ray Sampling for Fine-Grained Implicit Neural Radiance Fields (kaist-viclab.github.io)](https://kaist-viclab.github.io/pronerf-site/)\n\t- [Zip-NeRF: Anti-Aliased Grid-Based Neural Radiance Fields](https://jonbarron.info/zipnerf)\n\t- [Home](https://github.com/3a1b2c3/seeingSpace/wiki/Hands-on:-Getting-started-and-Nerf-frameworks)\n\t- [InseRF: Text-Driven Generative Object Insertion in Neural 3D Scenes (mohamad-shahbazi.github.io)](https://mohamad-shahbazi.github.io/inserf/)\n\t- [NeRFLiX: Increased NeRF Quality And Floater Removal | Neural Radiance Fields](https://neuralradiancefields.io/nerflix-increased-nerf-quality-and-floater-removal)\n\t- [A Short 170 Year History Of Neural Radiance Fields (NeRF), Holograms, And Light Fields | Neural Radiance Fields](https://neuralradiancefields.io/history-of-neural-radiance-fields)\n\t- [Home | MMLab@NTU](https://www.mmlab-ntu.com/project/vtoonify)\n\t- [RecolorNeRF](https://sites.google.com/view/recolornerf)\n\t- [Rob Sloan on LinkedIn: #nerfstudio #nerfstudio #polycam #nerf #nerfacto #polycam #neuralnetworksâ€¦ | 11 comments](https://www.linkedin.com/posts/robcsloan_nerfstudio-nerfstudio-polycam-activity-6999169160379297792-SN4F)\n\t- [MoyGcc/vid2avatar: Vid2Avatar: 3D Avatar Reconstruction from Videos in the Wild via Self-supervised Scene Decomposition (CVPR2023)](https://github.com/MoyGcc/vid2avatar)\n\t- [HumanNeRF: Free-viewpoint Rendering of Moving People from Monocular Video](https://grail.cs.washington.edu/projects/humannerf)\n\t- [Axie Infinity: Infinite Opportunity or Infinite Peril?](https://naavik.co/business-breakdowns/axie-infinity)\n\t- [NVlabs/instant-ngp](https://github.com/NVlabs/instant-ngp)\n\t- [SpecNeRF (limacv.github.io)](https://limacv.github.io/SpecNeRF_web/)\n\t- [UniSDF (fangjinhuawang.github.io)](https://fangjinhuawang.github.io/UniSDF/)\n\t- [SIGNeRF (jdihlmann.com)](https://signerf.jdihlmann.com/) fast nerf scene editing\n\t- [GARField: Group Anything with Radiance Fields](https://www.garfield.studio/)\n\t- [Byplay](https://www.byplay.io/) is camera motion tracking for mobile\n\t-\n- The neural radiance field Wikipedia is very good [Neural radiance field Wikipedia](https://en.wikipedia.org/wiki/Neural_radiance_field)\n- # LIDAR\n\t- [vectr-ucla/direct_lidar_inertial_odometry: [IEEE ICRA'23] A new lightweight LiDAR-inertial odometry algorithm with a novel coarse-to-fine approach in constructing continuous-time trajectories for precise motion correction. (github.com)](https://github.com/vectr-ucla/direct_lidar_inertial_odometry)\n-\n- [Nerfs](https://www.matthewtancik.com/nerf)\n- All of the LIDAR, [[Gaussian splatting and Similar]], [[Gaussian splatting and Similar]] etc are hopefully going to end up in here\n- [History of NeRFs](https://neuralradiancefields.io/history-of-neural-radiance-fields/)\n- waiting on capture\n- use polycam\n\t- try the BTS cam?\n- [viewier](https://github.com/sxyu/volrend)\n- Windows NeRF environment to WebGL\n- [install windows NeRF](https://github.com/bycloudai/instant-ngp-Windows)\n- check out mip nerf 360s\n\t- [Record3D](https://github.com/marek-simonik/record3d_unity_streaming)\n- [github of links](https://github.com/yenchenlin/awesome-NeRF)\n- [nerfs with polycam](https://www.linkedin.com/posts/robcsloan_nerfstudio-nerfstudio-polycam-activity-6999169160379297792-SN4F?utm_source=share&utm_medium=member_desktop)\n- [Polycam developer mode instructions](https://docs.nerf.studio/en/latest/quickstart/custom_dataset.html#polycam-capture)\n- [Nerf to animated people oneshot](https://elicit3d.github.io/)\n- [4K ultra high res nerfs with code](https://paperswithcode.com/paper/4k-nerf-high-fidelity-neural-radiance-fields)\n- [code](https://github.com/frozoul/4K-NeRF)\n- [city modelling](https://www.reddit.com/r/deeplearning/comments/zowgqn/neural_rendering_reconstruct_your_city_in_3d/)\n- [more city modelling](https://waymo.com/research/block-nerf/)\n- [field guide](https://github.com/3a1b2c3/seeingSpace/wiki/Hands-on:-Getting-started-and-Nerf-frameworks)\n- [NeRF SLAM](https://github.com/ToniRV/NeRF-SLAM)\n- [NeuralUDF surface capture](https://www.xxlong.site/NeuralUDF/)\n- [stablisation paper](https://arxiv.org/abs/2102.06205)\n- [nerfs without neural nets](https://alexyu.net/plenoxels/)\n- [NeuS2: Fast Learning of Neural Implicit Surfaces\n  for Multi-view Reconstruction](https://vcai.mpi-inf.mpg.de/projects/NeuS2/)\n- [Original 2020 nerf paper](https://www.matthewtancik.com/nerf)\n- [Recolour NeRF](https://sites.google.com/view/recolornerf?pli=1)\n- [Volinga Nerf into Unreal](https://volinga.ai/)\n- [Text2Nerf4D](https://make-a-video3d.github.io/)\n- [Robust nerfs which deal with occlusion](https://robustnerf.github.io/public/)\n- [Blender integration](https://github.com/JamesPerlman/NeRFRenderCore/blob/main/src/integrations/blender.cuh)\n- [Rapidnerf VR integration with erase](https://github.com/NVlabs/instant-ngp#vr-controls)\n- [Nerf to large scale geom](https://bakedsdf.github.io/)\n- [ELICIT,ELICIT creates free-viewpoint motion videos from a single image by constructing an animatable NeRF representation in one-shot learning. Offcial website of 'One-shot Implicit Animatable Avatars with Model-based Priors'](https://elicit3d.github.io/)\n- [GitHub frozoul/4K-NeRF: Official implementation of arxiv paper   4K-NeRF: High Fidelity Neural Radiance Fields at Ultra High Resolutions   , Official implementation of arxiv paper   4K-NeRF: High Fidelity Neural Radiance Fields at Ultra High Resolutions   - GitHub frozoul/4K-NeRF: Official implementation of arxiv paper   4K-NeRF: High Fidelity Neural Radiance Fields at Ultra High Resolutions](https://github.com/frozoul/4k-nerf)\n- [ClimateNeRF,-](https://climatenerf.github.io/)\n- [GitHub ToniRV/NeRF-SLAM: NeRF-SLAM: Real-Time Dense Monocular SLAM with Neural Radiance Fields.](https://github.com/tonirv/nerf-slam)\n- [HumanNeRF: Free-viewpoint Rendering of Moving People from Monocular Video,HumanNeRF: Free-viewpoint Rendering of Moving People from Monocular Video](https://grail.cs.washington.edu/projects/humannerf/)\n- [editing nerfs with instructions](https://instruct-nerf2nerf.github.io/)\n- [instruct2nerf twitter thread](https://mobile.twitter.com/bilawalsidhu/status/1638919452392583169)\n- [Render without cuda using just pytorch](https://github.com/taichi-dev/taichi-nerfs)\n- [Nerf with free camera trajectory](https://totoro97.github.io/projects/f2-nerf/)\n- [Language embedded nerfs (LERFS)](https://www.lerf.io/)\n- [Splatting paper, go where you like](https://repo-sam.inria.fr/fungraph/3d-gaussian-splatting/)\n- [nerf RPN](https://github.com/lyclyc52/NeRF_RPN)\n- [google indoor reconstruction from nerfs](https://ai.googleblog.com/2023/06/reconstructing-indoor-spaces-with-nerf.html)\n- [focal length for capture](https://neuralradiancefields.io/whats-the-best-focal-length-to-take-a-nerf/)\n- The paper [Zip-NeRF](https://jonbarron.info/zipnerf/): Anti-Aliased Grid-Based Neural Radiance Fields\" proposes a technique that combines ideas from rendering and signal processing to combat aliasing in grid-based representations of neural radiance fields (NeRF). NeRF's learned mapping from spatial coordinates to colors and volumetric density can be accelerated through the use of grid-based representations, but they lack an explicit understanding of scale and often introduce aliasing. The proposed technique combines mip-NeRF 360 and Instant NGP to yield error rates that are 8%-77% lower than either prior technique and trains 24x faster than mip-NeRF 360. The technique uses multisampling to approximate the average NGP feature over a conical frustum, and the method produces prefiltered renderings that do not flicker or shimmer, even as the camera moves laterally. Moreover, their improvements to proposal network supervision result in a prefiltered proposal output that preserves the foreground object for all frames, preventing an artifact called z-aliasing where foreground content alternately appears and disappears as the camera moves towards or away from the scene content. The proposed method shows promising results for accelerating NeRF training while combating aliasing in grid-based representations.\n- [baked nerf mesh paper](https://bakedsdf.github.io/)\n- [Facebook VR nerf](https://neuralradiancefields.io/venturing-beyond-reality-vr-nerf/)\n- RP-Lidar + Raspberry pi + ROS RTAB-MAP\n- [RTAB-Map](http://introlab.github.io/rtabmap/)\n- [Reality Scan](https://www.unrealengine.com/en-US/blog/realityscan-is-now-free-to-download-on-ios)\n- [Drone SLAM](https://www.youtube.com/watch?v=CEC5UwPV9gY)\n- [Adobe substance3d](https://www.substance3d.com/)\n- [3DPresso](https://3dpresso.ai/viewer?seq=mr3.yg5isic8KGJZ1DAjW5VMc)\n- [Apple point cloud rendering](https://machinelearning.apple.com/research/pointersect)\n- [Nvidia NeuralAngelo](https://research.nvidia.com/labs/dir/neuralangelo/)\n- [OmniMotion track all pixels](https://huggingface.co/papers/2306.05422)\n- [Leica handheld scanner](https://leica-geosystems.com/products/laser-scanners/autonomous-reality-capture/leica-blk2go-handheld-imaging-laser-scanner)\n- [Meshroom open source photogrammetry](https://alicevision.org/#meshroom)\n- [Nira.app](https://nira.app/)\n- [Houdini mesh from google earth](https://github.com/xjorma/EarthMeshHoudini?)\n- [DiffusionLight: Light Probes for Free by Painting a Chrome Ball](https://diffusionlight.github.io/index.html)\n- [MocapEvery (jiyewise.github.io)](https://jiyewise.github.io/projects/MocapEvery/)\n\n## Current Landscape (2025)\n\n- Industry adoption and implementations\n  - Metaverse platforms continue to evolve with focus on interoperability and open standards\n  - Web3 integration accelerating with decentralised identity and asset ownership\n  - Enterprise adoption growing in virtual collaboration, training, and digital twins\n  - UK companies increasingly active in metaverse development and immersive technologies\n\n- Technical capabilities\n  - Real-time rendering at photorealistic quality levels\n  - Low-latency networking enabling seamless multi-user experiences\n  - AI-driven content generation and procedural world building\n  - Spatial audio and haptics enhancing immersion\n\n- UK and North England context\n  - Manchester: Digital Innovation Factory supports metaverse startups and research\n  - Leeds: Holovis leads in immersive experiences for entertainment and training\n  - Newcastle: University research in spatial computing and interactive systems\n  - Sheffield: Advanced manufacturing using digital twin technology\n\n- Standards and frameworks\n  - Metaverse Standards Forum driving interoperability protocols\n  - WebXR enabling browser-based immersive experiences\n  - glTF and USD for 3D asset interchange\n  - Open Metaverse Interoperability Group defining cross-platform standards\n\n## Metadata\n\n- **Last Updated**: 2025-11-16\n- **Review Status**: Automated remediation with 2025 context\n- **Verification**: Academic sources verified\n- **Regional Context**: UK/North England where applicable",
  "properties": {
    "id": "scene-capture-and-reconstruction-owl-axioms",
    "collapsed": "true",
    "- ontology": "true",
    "- term-id": "mv-61811463877",
    "- preferred-term": "Scene Capture and Reconstruction",
    "- source-domain": "metaverse",
    "- status": "active",
    "- public-access": "true",
    "- definition": "A component of the metaverse ecosystem focusing on scene capture and reconstruction.",
    "- maturity": "mature",
    "- authority-score": "0.85",
    "- owl:class": "mv:SceneCaptureAndReconstruction",
    "- owl:physicality": "VirtualEntity",
    "- owl:role": "Object",
    "- belongsToDomain": "[[MetaverseDomain]]",
    "- is-part-of": "[[VirtualWorld]], [[MetaversePlatform]]",
    "- requires": "[[DigitalIdentity]], [[AuthenticationService]]",
    "- enables": "[[SocialInteraction]], [[Presence]], [[UserRepresentation]]",
    "- has-property": "[[Appearance]], [[Customization]], [[Animation]]",
    "public": "true"
  },
  "backlinks": [
    "BC-0014-block-time",
    "Gaussian splatting and Similar"
  ],
  "wiki_links": [
    "DigitalIdentity",
    "AuthenticationService",
    "Appearance",
    "SocialInteraction",
    "MetaversePlatform",
    "Animation",
    "Gaussian splatting and Similar",
    "UserRepresentation",
    "Customization",
    "VirtualWorld",
    "MetaverseDomain",
    "Presence"
  ],
  "ontology": {
    "term_id": "mv-61811463877",
    "preferred_term": "Scene Capture and Reconstruction",
    "definition": "A component of the metaverse ecosystem focusing on scene capture and reconstruction.",
    "source_domain": "metaverse",
    "maturity_level": null,
    "authority_score": 0.85
  }
}