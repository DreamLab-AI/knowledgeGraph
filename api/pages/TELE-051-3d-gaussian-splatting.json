{
  "title": "3D Gaussian Splatting",
  "content": "# 3D Gaussian Splatting\n\n### OntologyBlock\nid:: gaussian-splatting-ontology\ncollapsed:: true\n- ontology:: true\n- term-id:: TELE-051\n- preferred-term:: 3D Gaussian Splatting\n- alternate-terms::\n  - Gaussian Splatting\n  - 3DGS\n  - Real-Time Gaussian Rendering\n  - Gaussian Radiance Fields\n- source-domain:: tele\n- status:: active\n- public-access:: true\n- definition:: \"A neural rendering technique that represents 3D scenes as collections of millions of 3D Gaussian primitives with learnable positions, colours, opacities, and covariances, enabling photorealistic real-time rendering at 100+ frames per second through GPU-accelerated rasterisation, revolutionising telepresence and immersive collaboration with unprecedented visual fidelity.\"\n- maturity:: developing\n- authority-score:: 0.91\n- owl:class:: tele:GaussianSplatting\n- owl:physicality:: ConceptualEntity\n- owl:role:: Process\n- belongsToDomain::\n  - [[TELE-0000-telepresence-domain]]\n  - [[NeuralRenderingTelepresence]]\n- bridges-to::\n  - [[AIDomain]]\n  - [[MetaverseDomain]]\n\n#### Relationships\nid:: gaussian-splatting-relationships\n- is-subclass-of:: [[TELE-050-neural-rendering-telepresence]], [[NeuralRendering]], [[3DReconstruction]]\n- enables:: [[PhotorealisticTelepresence]], [[RealTimeRendering]], [[NovelViewSynthesis]]\n- requires:: [[MultiViewImages]], [[GPUAcceleration]], [[GradientDescent]], [[DifferentiableRendering]]\n- related-to:: [[TELE-052-neural-radiance-fields]], [[TELE-053-volumetric-video-conferencing]], [[TELE-060-instant-ngp]]\n\n#### OWL Axioms\nid:: gaussian-splatting-owl-axioms\ncollapsed:: true\n- ```clojure\n  Declaration(Class(tele:GaussianSplatting))\n\n  SubClassOf(tele:GaussianSplatting tele:NeuralRenderingTelepresence)\n  SubClassOf(tele:GaussianSplatting ai:NeuralRendering)\n\n  SubClassOf(tele:GaussianSplatting\n    ObjectSomeValuesFrom(tele:belongsToDomain tele:TelecollaborationDomain)\n  )\n\n  SubClassOf(tele:GaussianSplatting\n    ObjectSomeValuesFrom(tele:enables tele:PhotorealisticTelepresence)\n  )\n\n  SubClassOf(tele:GaussianSplatting\n    ObjectSomeValuesFrom(tele:bridgesTo ai:AIDomain)\n  )\n\n  AnnotationAssertion(rdfs:label tele:GaussianSplatting \"3D Gaussian Splatting\"@en-GB)\n  AnnotationAssertion(rdfs:comment tele:GaussianSplatting \"Real-time photorealistic neural rendering technique\"@en-GB)\n  AnnotationAssertion(dcterms:identifier tele:GaussianSplatting \"TELE-051\"^^xsd:string)\n  AnnotationAssertion(dcterms:created tele:GaussianSplatting \"2025-11-16\"^^xsd:date)\n  ```\n\n## Definition\n\n**3D Gaussian Splatting** is a breakthrough neural rendering method published at SIGGRAPH 2023 by Kerbl et al., representing 3D scenes as explicit collections of anisotropic 3D Gaussian distributions rather than implicit neural networks. Each Gaussian primitive encodes a 3D position (mean), colour, opacity, and 3√ó3 covariance matrix defining its shape and orientation in space. Rendering involves \"splatting\" these Gaussians onto the image plane through differentiable rasterisation, achieving photorealistic quality at 100-300 frames per second on consumer GPUs‚Äîover 100√ó faster than Neural Radiance Fields ([[TELE-052-neural-radiance-fields]]) whilst matching or exceeding visual fidelity.\n\nThe technique trains by optimising Gaussian parameters (positions, colours, covariances) to match input multi-view photographs through gradient descent, starting with sparse 3D point clouds from Structure-from-Motion (SfM). Gaussians are adaptively split, cloned, or pruned during optimisation to capture fine detail (hair strands, foliage) or remove redundancy. The explicit scene representation enables real-time rendering through GPU rasterisation pipelines, unlocking applications in [[TELE-020-virtual-reality-telepresence]], [[TELE-053-volumetric-video-conferencing]], and immersive telepresence where photorealistic environments must render at 90+ FPS for comfortable VR.\n\n## Current Landscape (2025)\n\n3D Gaussian Splatting has rapidly transitioned from academic novelty to production deployment, with major telepresence platforms integrating the technology for photorealistic avatars and environments.\n\n**Adoption Statistics**:\n- 67% of neural rendering research papers (2024-2025) employ Gaussian splatting variants (arXiv analysis)\n- Meta, Apple, Niantic incorporate Gaussian splatting in AR/VR pipelines\n- 14,000+ GitHub stars on official implementation (most-starred graphics paper 2023)\n- Consumer apps (Luma AI, PolyCam) enable smartphone Gaussian capture\n\n**Technology Capabilities (2025)**:\n- **Training Time**: 30 minutes for room-scale scene on RTX 4090 (vs. 24 hours for NeRF)\n- **Rendering Speed**: 150-300 FPS at 1080p resolution\n- **Quality**: PSNR 30-35 dB (comparable to NeRF, exceeding mesh-based methods)\n- **Scene Size**: Millions of Gaussians represent entire buildings\n\n**UK Context**:\n- **Luma AI** (London office): Develops NeRF-to-Gaussian conversion tools\n- **PolyCam** (UK users): Gaussian splatting mode in 3D scanning app\n- **University of Oxford**: Research on dynamic Gaussian splatting for moving objects\n- **Imperial College London**: Compression techniques for streaming Gaussian scenes\n\n## Technical Details\n\n### Scene Representation\n\nEach 3D Gaussian primitive ùí¢·µ¢ defined by:\n- **Mean Œº·µ¢ ‚àà ‚Ñù¬≥**: 3D position in world space\n- **Covariance Œ£·µ¢ ‚àà ‚Ñù¬≥À£¬≥**: Defines ellipsoidal shape/orientation\n- **Colour c·µ¢ ‚àà ‚Ñù¬≥** (or spherical harmonics for view-dependent appearance)\n- **Opacity Œ±·µ¢ ‚àà [0,1]**: Transparency\n\nGaussian function: G(x) = exp(-¬Ω(x-Œº)·µÄŒ£‚Åª¬π(x-Œº))\n\n### Rendering Pipeline\n\n1. **Projection**: Transform 3D Gaussians to 2D image space\n   - Project mean Œº·µ¢ via camera matrix\n   - Approximate 2D covariance via Jacobian of projection\n\n2. **Sorting**: Order Gaussians by depth (painter's algorithm with Œ±-blending)\n\n3. **Rasterisation**: For each pixel, accumulate colours of overlapping Gaussians\n   - Front-to-back traversal with early stopping when opacity saturates\n   - GPU-accelerated parallel processing\n\n4. **Output**: Photorealistic rendered image from novel viewpoint\n\n### Optimisation\n\n**Input**: 50-200 multi-view photographs with camera poses (from SfM)\n\n**Initialisation**: Sparse 3D point cloud ‚Üí one Gaussian per point\n\n**Loss Function**: L1 + SSIM (Structural Similarity Index) between rendered and ground truth images\n\n**Optimisation**:\n- Stochastic gradient descent with Adam optimiser\n- 30,000 iterations (~30 minutes on RTX 4090)\n- Adaptive density control: split under-reconstructed regions, prune low-opacity Gaussians\n\n**Result**: Millions of optimised Gaussians encoding scene\n\n### Advantages Over NeRF\n\n| Aspect | Gaussian Splatting | Neural Radiance Fields ([[TELE-052-neural-radiance-fields]]) |\n|--------|-------------------|--------------------------------------------------------------|\n| **Rendering Speed** | 100-300 FPS | 0.1-1 FPS (real-time variants: 30 FPS) |\n| **Training Time** | 30 minutes | 12-48 hours |\n| **Quality** | Photorealistic (30-35 dB PSNR) | Photorealistic (30-36 dB PSNR) |\n| **Representation** | Explicit (Gaussians) | Implicit (MLP weights) |\n| **Memory** | 100-500 MB per scene | 10-50 MB (more compact) |\n| **Editability** | Easy (move/delete Gaussians) | Difficult (retrain network) |\n\n## Applications in Telepresence\n\n### Photorealistic Virtual Environments ([[TELE-020-virtual-reality-telepresence]])\n- Scan real office spaces with smartphones (50-100 photos)\n- Train Gaussian scene in 30 minutes\n- Render in VR at 90 FPS for telepresence meetings\n- Example: Meta Horizon Workrooms experimenting with Gaussian environments (2025)\n\n### Volumetric Video Conferencing ([[TELE-053-volumetric-video-conferencing]])\n- Capture participant with multi-camera rig (6-12 cameras)\n- Real-time Gaussian optimisation (30 Hz update rate)\n- Stream compressed Gaussians to remote clients\n- Render photorealistic avatar from any angle\n- Example: Microsoft Mesh exploring dynamic Gaussian avatars\n\n### Virtual Tourism\n- Museums digitise exhibits with Gaussian scans\n- Remote visitors navigate photorealistic 3D environments\n- Example: Luma AI captures heritage sites for virtual tours\n\n### Remote Site Inspection\n- Construction sites scanned with drones\n- Engineers inspect progress remotely in photorealistic 3D\n- Example: UK engineering firms use PolyCam for site documentation\n\n## Technical Challenges and Solutions\n\n### Challenge: Large File Sizes\n- **Problem**: Millions of Gaussians ‚Üí 500 MB+ per scene\n- **Solution**: Neural compression, quantisation (reduce to 50-100 MB)\n- **Research**: Compact 3DGS, EAGLES (entropy-aware compression)\n\n### Challenge: Dynamic Scenes\n- **Problem**: Original technique assumes static scenes\n- **Solution**: 4D Gaussian splatting (add time dimension), deformable Gaussians\n- **Research**: Dynamic 3DGS, 4DGaussians (moving people, avatars)\n\n### Challenge: Training Data Requirements\n- **Problem**: Needs 50-200 high-quality photos with accurate poses\n- **Solution**: Structure-from-Motion automation, smartphone capture apps\n- **Tools**: COLMAP (SfM), Luma AI app, PolyCam\n\n### Challenge: Real-Time Streaming\n- **Problem**: 500 MB scenes unsuitable for network streaming\n- **Solution**: Progressive transmission (coarse-to-fine), level-of-detail rendering\n- **Research**: Streamable Gaussians, LoD-GS\n\n## Future Directions\n\n**Near-Term (2025-2027)**:\n- Real-time Gaussian capture from single RGB-D camera (iPhone LiDAR)\n- Compression to <50 MB per scene for mobile deployment\n- Integration into WebXR standard (browser-based Gaussian rendering)\n\n**Medium-Term (2027-2030)**:\n- Photorealistic full-body Gaussian avatars updating at 60 Hz\n- Gaussian-based telepresence as default in Meta/Apple platforms\n- Semantic Gaussians (each primitive labelled: \"table\", \"wall\", etc.)\n\n**Long-Term (2030+)**:\n- Neural codecs compressing Gaussians 100√ó further\n- Light-field displays rendering Gaussians holographically (no headset)\n- Real-time global illumination in Gaussian scenes (ray tracing)\n\n## Related Concepts\n\n- [[TELE-050-neural-rendering-telepresence]]\n- [[TELE-052-neural-radiance-fields]]\n- [[TELE-053-volumetric-video-conferencing]]\n- [[TELE-020-virtual-reality-telepresence]]\n- [[TELE-060-instant-ngp]]\n\n## Academic References\n\n1. Kerbl, B., Kopanas, G., Leimk√ºhler, T., & Drettakis, G. (2023). \"3D Gaussian Splatting for Real-Time Radiance Field Rendering\". *ACM Transactions on Graphics (SIGGRAPH)*, 42(4), 1-14.\n2. Luiten, J., et al. (2023). \"Dynamic 3D Gaussians: Tracking by Persistent Dynamic View Synthesis\". *arXiv preprint*.\n3. Niedermayr, S., et al. (2024). \"Compressed 3D Gaussian Splatting for Accelerated Novel View Synthesis\". *CVPR 2024*.\n\n## Open-Source Implementations\n\n- **Official**: https://github.com/graphdeco-inria/gaussian-splatting\n- **Nerfstudio**: Gaussian splatting module in unified NeRF framework\n- **gsplat**: PyTorch library for differentiable Gaussian rasterisation\n- **WebGL Viewer**: Real-time browser-based Gaussian rendering\n\n## Metadata\n\n- **Term-ID**: TELE-051\n- **Last Updated**: 2025-11-16\n- **Maturity**: Developing\n- **Authority Score**: 0.91\n- **UK Context**: High (Luma AI, university research)\n- **Cross-Domain**: Bridges to AI, Metaverse",
  "properties": {},
  "backlinks": [
    "TELE-052-neural-radiance-fields",
    "TELE-100-ai-avatars",
    "TELE-0000-telepresence-domain",
    "TELE-001-telepresence"
  ],
  "wiki_links": [
    "TELE-052-neural-radiance-fields",
    "TELE-053-volumetric-video-conferencing",
    "NeuralRenderingTelepresence",
    "NeuralRendering",
    "PhotorealisticTelepresence",
    "TELE-050-neural-rendering-telepresence",
    "MultiViewImages",
    "GPUAcceleration",
    "DifferentiableRendering",
    "TELE-060-instant-ngp",
    "NovelViewSynthesis",
    "TELE-0000-telepresence-domain",
    "AIDomain",
    "TELE-020-virtual-reality-telepresence",
    "GradientDescent",
    "MetaverseDomain",
    "RealTimeRendering",
    "3DReconstruction"
  ],
  "ontology": {
    "term_id": "TELE-051",
    "preferred_term": "3D Gaussian Splatting",
    "definition": "\"A neural rendering technique that represents 3D scenes as collections of millions of 3D Gaussian primitives with learnable positions, colours, opacities, and covariances, enabling photorealistic real-time rendering at 100+ frames per second through GPU-accelerated rasterisation, revolutionising telepresence and immersive collaboration with unprecedented visual fidelity.\"",
    "source_domain": "tele",
    "maturity_level": null,
    "authority_score": 0.91
  }
}