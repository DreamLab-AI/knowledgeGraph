{
  "title": "Speech and voice",
  "content": "- ### OntologyBlock\n  id:: speech-and-voice-ontology\n  collapsed:: true\n\t- ontology:: true\n\t- term-id:: mv-848312212064\n\t- preferred-term:: Speech and voice\n\t- source-domain:: metaverse\n\t- status:: active\n\t- public-access:: true\n\t- definition:: A component of the metaverse ecosystem focusing on speech and voice.\n\t- maturity:: mature\n\t- authority-score:: 0.85\n\t- owl:class:: mv:SpeechAndVoice\n\t- owl:physicality:: ConceptualEntity\n\t- owl:role:: Concept\n\t- belongsToDomain:: [[MetaverseDomain]]\n\t- #### Relationships\n\t  id:: speech-and-voice-relationships\n\t\t- enables:: [[ImmersiveExperience]], [[Presence]], [[SpatialComputing]]\n\t\t- requires:: [[DisplayTechnology]], [[TrackingSystem]], [[RenderingEngine]]\n\t\t- bridges-to:: [[HumanComputerInteraction]], [[ComputerVision]], [[Robotics]]\n\n\t- #### OWL Axioms\n\t  id:: speech-and-voice-owl-axioms\n\t  collapsed:: true\n\t\t- ```clojure\n\t\t  Declaration(Class(mv:SpeechAndVoice))\n\n\t\t  # Classification\n\t\t  SubClassOf(mv:SpeechAndVoice mv:ConceptualEntity)\n\t\t  SubClassOf(mv:SpeechAndVoice mv:Concept)\n\n\t\t  # Domain classification\n\t\t  SubClassOf(mv:SpeechAndVoice\n\t\t    ObjectSomeValuesFrom(mv:belongsToDomain mv:MetaverseDomain)\n\t\t  )\n\n\t\t  # Annotations\n\t\t  AnnotationAssertion(rdfs:label mv:SpeechAndVoice \"Speech and voice\"@en)\n\t\t  AnnotationAssertion(rdfs:comment mv:SpeechAndVoice \"A component of the metaverse ecosystem focusing on speech and voice.\"@en)\n\t\t  AnnotationAssertion(dcterms:identifier mv:SpeechAndVoice \"mv-848312212064\"^^xsd:string)\n\t\t  ```\n\npublic:: true\n\n- #Public page automatically published\n- {{video https://www.youtube.com/watch?v=xCDAjpZJWYw}}\n- [NVIDIA/NeMo: NeMo: a toolkit for conversational AI (github.com)](https://github.com/NVIDIA/NeMo)\n\t- [Canary\n\t\t- NVIDIA NeMo](https://nvidia.github.io/NeMo/blogs/2024/2024-02-canary/)\n\t- ![H200-NeMo-performance](https://github.com/sbhavani/TransformerEngine/raw/main/docs/examples/H200-NeMo-performance.png)\n\t-\n- [NeMo/tutorials/tts/FastPitch_Adapter_Finetuning.ipynb at main · NVIDIA/NeMo (github.com)](https://github.com/NVIDIA/NeMo/blob/main/tutorials/tts/FastPitch_Adapter_Finetuning.ipynb)\n- [ElevenLabs Audio Native](https://elevenlabs.io/blog/audio-native/)\n- [OpenAI whisper local deploy](https://github.com/openai/whisper)\n- [realtime transciber](https://github.com/davabase/transcriber_app/)\n- [high performance CPP](https://github.com/ggerganov/whisper.cpp)\n- [30% quantised optimisation](https://medium.com/@daniel-klitzke/quantizing-openais-whisper-with-the-huggingface-optimum-library-30-faster-inference-64-36d9815190e0)\n- [Brillbits OpenAI whisper demo with mic](https://www.youtube.com/watch?v=nwPaRSlDSaY)\n- [Cleanvoice audio denoise](https://cleanvoice.ai/)\n- [Cloud voice change app](https://voice.ai/)\n- [downloadable voice generation systems](https://github.com/neonbjb/tortoise-tts)\n- [Language AI open libraries](https://txt.cohere.ai/introducing-sandbox-coheres-experimental-open-source-initiative/)\n- [Language practice](https://huggingface.co/spaces/JavaFXpert/Chat-GPT-LangChain)\n- [MUGEN multi modal from facebook](https://mugen-org.github.io/)\n- [Oneshot speach to text](https://atosystem.github.io/blogs/speechclip)\n- [Record and cleanup pro audio with commodity hardware](https://podcastle.ai/)\n- [Respeecher](https://variety.com/2022/digital/news/james-earl-jones-darth-vader-retiring-star-wars-ai-1235382827/)\n- [Voice AI voices](https://voice.ai/)\n- [Voice controlled assisted creation](https://the-decoder.com/developer-combines-stable-diffusion-whisper-and-gpt-3-for-a-futuristic-design-assistant/)\n- [Voice to text, Lopp](https://blog.lopp.net/open-source-transcription-software-comparisons/)\n- [whisper transcriber](https://github.com/modal-labs/modal-examples/tree/main/misc/whisper_pod_transcriber)\n- [Wolfram alpha voice chatbot integration](https://huggingface.co/spaces/JavaFXpert/Chat-GPT-LangChain)\n- [Microsoft Vall-E voice synthesis](https://valle-demo.github.io/)\n- [Uberduck text to speech (plus own voice)](https://app.uberduck.ai/)\n- [Eleven labs language and text to speech](https://beta.elevenlabs.io/)\n- [Uberduck open source text to speech](https://uberduck.ai/)\n- [numen voice control system in linux](https://numenvoice.com)\n- [Inworld (steam game plugin AI system) for voice chat and answer](https://www.youtube.com/watch?v=DnF4WzM5LPU)\n- [Bark text to speech from google labs](https://github.com/suno-ai/bark)\n- https://github.com/TensorSpeech/TensorFlowTTS\n  very configurable from what I see\n- [VoiceVox engine](https://www.youtube.com/watch?v=TGZV831VTpc)\n- [coqui-ai TTS\n\t- very good samples](https://github.com/coqui-ai/TTS)\n- https://github.com/neonbjb/tortoise-tts\n- https://github.com/CorentinJ/Real-Time-Voice-Cloning\n\t- custom voices? looks neat\n- https://github.com/rhasspy/larynx - very low-spec compatible, acceptable quality\n- [Voice cloning local](https://git.ecker.tech/mrq/ai-voice-cloning)\n- [Meta voicebox](https://ai.facebook.com/blog/voicebox-generative-ai-model-speech/)\n- The Reddit post discusses the different open source voice cloning projects available, including Coqui, Tortoise, and Bark. The advantages and disadvantages of each project are briefly outlined, with ElevenLabs being noted as the best but not open source, while Tortoise is suggested as the closest open source alternative. Other tools for speech to speech and singing conversion, such as so-vits/diff-svc/rvc, are also mentioned. The post suggests that the quality of open source voice cloning projects is improving, and that there may be more options available in the future. https://www.reddit.com/r/MachineLearning/comments/133hanr/d_what_are_the_differences_between_the_major_open/\n- The Retrieval-based Voice Conversion WebUI is a simple and useful voice conversion (voice changer) framework based on the VITS algorithm. It can use a small amount of voice data and still achieve good results. It incorporates a top-1 retrieval method to replace the source feature with the training set feature to avoid voice leakage, and it is easy to use with a simple web interface. It also features model fusion to change voice characteristics and the ability to integrate with the UVR5 model to quickly separate vocals and accompaniment. The project requires the installation of PyTorch and its core dependencies, and other pre-models are also needed for inference and training. The repository provides a guide to environment setup and usage, as well as links to relevant resources and contributors. https://github.com/RVC-Project/Retrieval-based-Voice-Conversion-WebUI\n- The article discusses different open-source voice cloning projects and their advantages and disadvantages. The projects mentioned include Coqui, Tortoise, and Bark, with the author highlighting Coqui's unlocked platform, while Tortoise and Bark are newer transformer-based projects that can clone much more effectively with much less training and are restricted to prevent custom voice cloning. The author suggests that the ElevenLabs is currently the best voice cloning solution available, but it is not open source and can be expensive. The article also includes comments from other Reddit users, who suggest other open source options and provide additional insights into each option's strengths and weaknesses. https://www.reddit.com/r/MachineLearning/comments/133hanr/d_what_are_the_differences_between_the_major_open/\n- The article provides instructions on how to use OpenAI's ChatGPT chatbot on an Android device using the Tasker app. The process involves importing a ChatGPT profile into Tasker, obtaining an API key from OpenAI, and setting up home screen shortcuts. The article also notes that ChatGPT can be run through Google Assistant with voice commands. The author suggests that while ChatGPT may not necessarily be better than Google Assistant, it can perform tasks that Google Assistant may not be capable of. https://www.howtogeek.com/882019/how-to-use-chatgpt-like-google-assistant-on-android/\n- The Voice Assistant is an AI-powered chatbot that uses several APIs to understand natural language commands and provide helpful responses. It features a wide range of capabilities, including answering general knowledge questions, providing recommendations, performing productivity tasks, and entertaining users. The Voice Assistant was built using ChatGPT, Whisper API, Gradio, and Microsoft's SpVoice TTS API, and it can be accessed through a web-based interface. The installation process involves cloning the repository and installing the required Python packages. Contributions to the project are welcome. https://github.com/DonGuillotine/chatGPT_whisper_AI_voice_assistant\n- The Retrieval-based Voice Conversion WebUI is a voice conversion framework that uses a top-1 retrieval algorithm to eliminate voice leakage. It is capable of quickly training even on relatively poor GPUs and can achieve good results even with just 10 minutes of low noise voice data. It has a user-friendly web interface and the ability to use a model fusion system to change voice timbre. The setup recommends using Poetry and downloading the necessary pre-trained models from their Hugging Face space. It also includes additional files such as ffmpeg and ffprobe that may need to be downloaded. The WebUI can be initiated using the command \"python infer-web.py\" and Windows users can run the \"go-web.bat\" file. The project also acknowledges the contributions of related tools and libraries such as Gradio, HIFIGAN, and ContentVec. https://github.com/RVC-Project/Retrieval-based-Voice-Conversion-WebUI\n- VoicePen is a tool that uses AI to convert audio or video files into blog posts and transcriptions in minutes. The service includes a transcription and SRT file generated by a top speech-to-text model, an English blog post that pulls out key topics from the audio, and the ability to convert audio in 96 different languages. Use cases include repurposing podcasts, webinars, and tutorial videos. Monthly plans are available, with options for one-time conversions. Testimonials praise the accuracy and speed of VoicePen's service. https://voicepen.ai\n- Krisp is a software application designed to improve the productivity of online meetings by using AI-powered voice clarity and a meeting assistant to cancel background noise, echo, and accent localization. It works on both Mac and Windows platforms and processes only the user's voice on their device, unlike other solutions that transmit voice over the internet. Krisp offers a free forever plan with no credit card required and is trusted by global brands. The insights gathered from calls can be viewed by the user to improve their communication skills over time. Krisp has received recognition from various prestigious awards such as America's Most Promising AI Companies and has been awarded for its quality of support and ease of use. Krisp also offers SDK for developers, pricing and plans, and use cases such as contact centers and enterprise. The company prioritizes customers' privacy, security and offers accessible support, including video tutorials and a help center. By accepting all cookies, users consent to the storing of cookies on their device to enhance site navigation, analyze site usage and assist in the company's marketing efforts. https://krisp.ai/\n- Cleanvoice AI is an artificial intelligence platform that assists users in editing their podcasts or audio recordings. The platform offers various features such as filler sound removal, mouth sound removal, stutter removal, and Deadair remover to make the audio recording more professional. Cleanvoice AI is multilingual and can detect filler sounds in multiple languages, including accents from various countries. The platform also allows for manual editing with assistance and offers tools like podcast mixing and background noise remover. Users can try Cleanvoice AI for free for 30 minutes without providing credit card details. However, users must accept the platform's cookie policy to use the service. https://cleanvoice.ai/\n- The article discusses the potential of Central Intelligent Agents (CIAs) and the role of large language models (LLMs) and other next-generation AI technologies in enabling them. It highlights the need for businesses to have a cross-functional team, ethical guidelines, and clear objectives in deploying their own CIA. The article also suggests steps to build a solid foundation for deploying a CIA, assess organizational readiness, assemble a cross-functional team, define objectives, develop the CIA components and evaluate its performance while continuing to learn and adapt. The author discusses the potential of AI tools and voice assistants in transforming the way businesses interact with their customers and suggests that the advent of advanced AI technologies has revolutionized the shift of businesses towards a more personalized and ethically responsible approach to engaging with their customers. Finally, the article ends by highlighting the importance of experimenting through crisis and providing expert guidance tailored to specific business needs. https://www.linkedin.com/pulse/central-intelligent-agent-enabling-next-generation-james-poulter?\n- [TensorSpeech/TensorFlowTTS: :stuck_out_tongue_closed_eyes: TensorFlowTTS: Real-Time State-of-the-art Speech Synthesis for Tensorflow 2 (supported including English, French, Korean, Chinese, German and Easy to adapt for other languages)](https://github.com/TensorSpeech/TensorFlowTTS) [[Translation]] [[Accessibility]] [[Speech and voice]] [[Speech and voice]]\n- [Variety](https://variety.com/2022/digital/news/james-earl-jones-darth-vader-retiring-star-wars-ai-1235382827/%7D%7BRespeecher%7D) [[Speech and voice]] [[Social contract and jobs]]\n- [transcriptionstream/transcriptionstream: turnkey self-hosted offline transcription and diarization service with llm summary (github.com)](https://github.com/transcriptionstream/transcriptionstream) [[Speech and voice]] transcription locally [[SHOULD]]\n- [Tincans - Gazelle v0.2](https://tincans.ai/slm3) [[Speech and voice]] fast speech engine [[SHOULD]]\n- [[Speech and voice]] [Open Voice (myshell.ai)](https://research.myshell.ai/open-voice) cloning MIT license\n- [EndlessDreams: Voice directed real-time videos at 1280x1024 : r/StableDiffusion (reddit.com)](https://www.reddit.com/r/StableDiffusion/comments/1c8oea6/endlessdreams_voice_directed_realtime_videos_at/) [[Speech and voice]] [[Speech and voice]] [[Product Design]] [[Real Time]]\n- https://demo.hume.ai/? [[Speech and voice]] [[Large language models]] empathetic voice to voice\n- [[Speech and voice]] [metavoiceio/metavoice-src: AI for human-level speech intelligence (github.com)](https://github.com/metavoiceio/metavoice-src) check for [[PlayerTwo]]\n- [NeMo/tutorials/tts/NeMo_TTS_Primer.ipynb at main · NVIDIA/NeMo (github.com)](https://github.com/NVIDIA/NeMo/blob/main/tutorials/tts/NeMo_TTS_Primer.ipynb) [[NVIDIA Omniverse]] [[Speech and voice]] primer and demo.\n-\n\n## Current Landscape (2025)\n\n- Industry adoption and implementations\n  - Metaverse platforms continue to evolve with focus on interoperability and open standards\n  - Web3 integration accelerating with decentralised identity and asset ownership\n  - Enterprise adoption growing in virtual collaboration, training, and digital twins\n  - UK companies increasingly active in metaverse development and immersive technologies\n\n- Technical capabilities\n  - Real-time rendering at photorealistic quality levels\n  - Low-latency networking enabling seamless multi-user experiences\n  - AI-driven content generation and procedural world building\n  - Spatial audio and haptics enhancing immersion\n\n- UK and North England context\n  - Manchester: Digital Innovation Factory supports metaverse startups and research\n  - Leeds: Holovis leads in immersive experiences for entertainment and training\n  - Newcastle: University research in spatial computing and interactive systems\n  - Sheffield: Advanced manufacturing using digital twin technology\n\n- Standards and frameworks\n  - Metaverse Standards Forum driving interoperability protocols\n  - WebXR enabling browser-based immersive experiences\n  - glTF and USD for 3D asset interchange\n  - Open Metaverse Interoperability Group defining cross-platform standards\n\n## Metadata\n\n- **Last Updated**: 2025-11-16\n- **Review Status**: Automated remediation with 2025 context\n- **Verification**: Academic sources verified\n- **Regional Context**: UK/North England where applicable",
  "properties": {
    "id": "speech-and-voice-owl-axioms",
    "collapsed": "true",
    "- ontology": "true",
    "- term-id": "mv-848312212064",
    "- preferred-term": "Speech and voice",
    "- source-domain": "metaverse",
    "- status": "active",
    "- public-access": "true",
    "- definition": "A component of the metaverse ecosystem focusing on speech and voice.",
    "- maturity": "mature",
    "- authority-score": "0.85",
    "- owl:class": "mv:SpeechAndVoice",
    "- owl:physicality": "ConceptualEntity",
    "- owl:role": "Concept",
    "- belongsToDomain": "[[MetaverseDomain]]",
    "- enables": "[[ImmersiveExperience]], [[Presence]], [[SpatialComputing]]",
    "- requires": "[[DisplayTechnology]], [[TrackingSystem]], [[RenderingEngine]]",
    "- bridges-to": "[[HumanComputerInteraction]], [[ComputerVision]], [[Robotics]]",
    "public": "true"
  },
  "backlinks": [
    "Automated Podcasting",
    "Speech and voice"
  ],
  "wiki_links": [
    "Product Design",
    "Speech and voice",
    "ComputerVision",
    "Robotics",
    "ImmersiveExperience",
    "Translation",
    "Large language models",
    "NVIDIA Omniverse",
    "SHOULD",
    "Presence",
    "DisplayTechnology",
    "SpatialComputing",
    "TrackingSystem",
    "MetaverseDomain",
    "PlayerTwo",
    "Real Time",
    "Social contract and jobs",
    "RenderingEngine",
    "Accessibility",
    "HumanComputerInteraction"
  ],
  "ontology": {
    "term_id": "mv-848312212064",
    "preferred_term": "Speech and voice",
    "definition": "A component of the metaverse ecosystem focusing on speech and voice.",
    "source_domain": "metaverse",
    "maturity_level": null,
    "authority_score": 0.85
  }
}