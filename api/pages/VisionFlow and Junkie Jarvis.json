{
  "title": "VisionFlow and Junkie Jarvis",
  "content": "- ### OntologyBlock\n  id:: visionflow-and-junkie-jarvis-ontology\n  collapsed:: true\n\t- ontology:: true\n\t- term-id:: mv-682173246137\n\t- preferred-term:: VisionFlow and Junkie Jarvis\n\t- source-domain:: metaverse\n\t- status:: active\n\t- public-access:: true\n\t- definition:: A component of the metaverse ecosystem focusing on visionflow and junkie jarvis.\n\t- maturity:: mature\n\t- authority-score:: 0.85\n\t- owl:class:: mv:VisionflowAndJunkieJarvis\n\t- owl:physicality:: ConceptualEntity\n\t- owl:role:: Concept\n\t- belongsToDomain:: [[MetaverseDomain]]\n\t- #### Relationships\n\t  id:: visionflow-and-junkie-jarvis-relationships\n\t\t- enables:: [[ImmersiveExperience]], [[Presence]], [[SpatialComputing]]\n\t\t- requires:: [[DisplayTechnology]], [[TrackingSystem]], [[RenderingEngine]]\n\t\t- bridges-to:: [[HumanComputerInteraction]], [[ComputerVision]], [[Robotics]]\n\n\t- #### OWL Axioms\n\t  id:: visionflow-and-junkie-jarvis-owl-axioms\n\t  collapsed:: true\n\t\t- ```clojure\n\t\t  Declaration(Class(mv:VisionflowAndJunkieJarvis))\n\n\t\t  # Classification\n\t\t  SubClassOf(mv:VisionflowAndJunkieJarvis mv:ConceptualEntity)\n\t\t  SubClassOf(mv:VisionflowAndJunkieJarvis mv:Concept)\n\n\t\t  # Domain classification\n\t\t  SubClassOf(mv:VisionflowAndJunkieJarvis\n\t\t    ObjectSomeValuesFrom(mv:belongsToDomain mv:MetaverseDomain)\n\t\t  )\n\n\t\t  # Annotations\n\t\t  AnnotationAssertion(rdfs:label mv:VisionflowAndJunkieJarvis \"VisionFlow and Junkie Jarvis\"@en)\n\t\t  AnnotationAssertion(rdfs:comment mv:VisionflowAndJunkieJarvis \"A component of the metaverse ecosystem focusing on visionflow and junkie jarvis.\"@en)\n\t\t  AnnotationAssertion(dcterms:identifier mv:VisionflowAndJunkieJarvis \"mv-682173246137\"^^xsd:string)\n\t\t  ```\n\npublic:: true\n\n- [[VisionFlow Client]]\n- # Uses Open Source Knowledge Management (LogSeq)\n\t- <iframe src=\"https://narrativegoldmine.com//#/graph\" style=\"width: 100%; height: 600px\"></iframe>\n- # Distributed Human-AI Collaboration on Private Datasets\n\t- ## Introduction\n\t\t- VisionFlow is a cutting-edge platform designed for **real-time, collaborative knowledge management and AI-driven problem-solving**. At its core, VisionFlow enables **distributed human experts*  spread across teams, organizations, or even global network  to interact seamlessly with **powerful AI agents** in a shared, immersive 3D environment. By grounding AI capabilities in **private, secure datasets**, it transforms abstract data into actionable insights, fostering innovation without compromising privacy or security.\n\t\t- Whether you're a software engineer debugging complex systems, a researcher analyzing interconnected knowledge graphs, or a team collaborating on strategic initiatives, VisionFlow acts as a **pliable general-purpose platform**. It bridges human expertise with AI orchestration, allowing users to visualize, manipulate, and evolve ideas in real time while maintaining full control over sensitive data.\n\t\t- Built on a robust Rust backend with GPU-accelerated physics and React/Three.js frontend, VisionFlow is extensible, scalable, and production-ready. It supports everything from simple note-taking to enterprise-scale AI multi-agent workflows, making it versatile for individuals, teams, and organizations.\n\t- ## The Utility of VisionFlow\n\t\t- In today's data-driven world, knowledge is often scattered across documents, databases, and tools, making it hard for experts to collaborate effectively, specially when integrating AI. VisionFlow solves this by providing a **unified canvas** where humans and AI agents work together on **grounded private datasets**. Here's why it's a game-changer:\n\t\t\t- ### 1. **Real-Time Human-AI Symbiosis**\n\t\t\t  VisionFlow isn't just a visualization tool; it's a **collaborative workspace** where humans and AI agents interact in real time. Experts can:\n\t\t\t\t- **Pose questions** to AI agents powered by Claude Flow's MCP (Model Context Protocol).\n\t\t\t\t- **Spawn specialized agents** (e.g., researchers, coders, analysts) to tackle subtasks.\n\t\t\t\t- **Observe live progress** as agents update the 3D graph, showing evolving insights.\n\t\t\t\t- **Intervene dynamically**—drag nodes, adjust parameters, or redirect agents—all in a shared virtual space.\n\t\t\t- This pliability allows distributed teams (e.g., remote engineers or global research groups) to co-create solutions, with AI handling computation-heavy tasks while humans provide domain expertise.\n\t\t\t- ### 2. **Grounded in Private Datasets**\n\t\t\t- Unlike public AI tools, VisionFlow keeps your data **private and secure**:\n\t\t\t\t- **Local-First Processing**: Metadata and graph structures are processed server-side on your infrastructure.\n\t\t\t\t- **Nostr Authentication**: Decentralized, key-based auth ensures user control without centralized identity providers.\n\t\t\t\t- **End-to-End Encryption**: Sensitive data (e.g., API keys, user sessions) is encrypted at rest and in transit.\n\t\t\t\t- **Zero-Trust Model**: Continuous verification—no implicit trust, even for internal requests.\n\t\t\t\t- **On-Premise Deployment**: Run entirely on your hardware or private cloud, with optional GPU acceleration.\n\t\t\t- You can import private datasets (e.g., internal docs, proprietary graphs) and have AI agents analyze them without data leaving your environment, ensuring compliance with regulations like GDPR or HIPAA.\n\t\t\t- ### 3. **Pliability for General-Purpose Use**\n\t\t\t- VisionFlow is designed as a **general-purpose platform**, adaptable to diverse domains:\n\t\t\t\t- **Research & Analysis**: Visualize literature or datasets; spawn researcher agents for insights.\n\t\t\t\t- **Software Engineering**: Map codebases; use coder agents for refactoring, tester agents for validation.\n\t\t\t\t- **Business Intelligence**: Model organizational knowledge; analysts can query and refine in real time.\n\t\t\t\t- **Creative Collaboration**: Teams co-edit graphs; AI agents generate ideas, documentation, or prototypes.\n\t\t\t\t- **Education**: Interactive 3D mind maps; students collaborate with AI tutors on private notes.\n\t\t\t- Its modular architecture allows custom extensions, making it pliable for any problem requiring human-AI synergy on private data.\n\t- ## Key Features\n\t\t- VisionFlow's features emphasize real-time interaction, privacy, and extensibility.\n\t\t\t- ### Real-Time 3D Graph Visualization\n\t\t\t\t- **GPU-Accelerated Rendering**: Powered by Three.js and React Three Fiber, rendering up to 100,000+ nodes at 60 FPS.\n\t\t\t\t- **Force-Directed Layout**: Nodes attract/repel based on semantic similarity and connections, creating intuitive clusters.\n\t\t\t\t- **Dual-Graph Support**: Separate physics for knowledge graphs (stable, blue theme) and agent graphs (dynamic, green theme).\n\t\t\t\t- **Interactive Controls**: Drag nodes, zoom, orbit, and apply forces in real time.\n\t\t\t- ### AI Multi-Agent Orchestration\n\t\t\t\t- **Claude Flow MCP Integration**: Spawn and coordinate AI agents (e.g., researchers, coders, analysts) via TCP protocol.\n\t\t\t\t- **Dynamic Topologies**: Mesh (collaborative), hierarchical (command chain), ring (sequential), star (centralized).\n\t\t\t\t- **Agent Specialisation**: 12+ agent types with domain expertise (e.g., architect for design, tester for validation).\n\t\t\t\t- **Real-Time Telemetry**: Live updates on agent status, task progress, and performance metrics.\n\t\t\t- ### Semantic Analysis & Constraints\n\t\t\t\t- **Multi-Modal Pipeline**: Extracts features (topics, domains, structure) to generate dynamic edges and forces.\n\t\t\t\t- **Ontology Validation**: Maps graph to OWL/RDF for logical consistency checks and inference.\n\t\t\t\t- **Grounded Insights**: AI agents operate on your private data, ensuring privacy while providing context-aware responses.\n\t\t\t- ### Secure, Private Data Handling\n\t\t\t\t- **Nostr Authentication**: Decentralized identity without centralized providers; users control their keys.\n\t\t\t\t- **Private Datasets**: All processing happens on your infrastructure; no data leaves your environment.\n\t\t\t\t- **End-to-End Encryption**: AES-256 for sensitive data; TLS 1.3 for all communications.\n\t\t\t\t- **Zero-Trust Model**: Continuous verification;\n- shadertoy\n  collapsed:: true\n\t- [Geodesic tiling](https://www.shadertoy.com/view/llVXRd)\n\t- ```\n\t  #define MODEL_ROTATION vec2(.3, .25)\n\t  #define CAMERA_ROTATION vec2(.5, .5)\n\t  \n\t  // 0: Defaults\n\t  // 1: Model\n\t  // 2: Camera\n\t  #define MOUSE_CONTROL 1\n\t  \n\t  //#define DEBUG\n\t  \n\t  // 1, 2, or 3\n\t  //#define LOOP 1\n\t  \n\t  \n\t  // --------------------------------------------------------\n\t  // HG_SDF\n\t  // https://www.shadertoy.com/view/Xs3GRB\n\t  // --------------------------------------------------------\n\t  \n\t  void pR(inout vec2 p, float a) {\n\t      p = cos(a)*p + sin(a)*vec2(p.y, -p.x);\n\t  }\n\t  \n\t  float pReflect(inout vec3 p, vec3 planeNormal, float offset) {\n\t      float t = dot(p, planeNormal)+offset;\n\t      if (t < 0.) {\n\t          p = p - (2.*t)*planeNormal;\n\t      }\n\t      return sign(t);\n\t  }\n\t  \n\t  float smax(float a, float b, float r) {\n\t      float m = max(a, b);\n\t      if ((-a < r) && (-b < r)) {\n\t          return max(m, -(r - sqrt((r+a)*(r+a) + (r+b)*(r+b))));\n\t      } else {\n\t          return m;\n\t      }\n\t  }\n\t  \n\t  \n\t  // --------------------------------------------------------\n\t  // Icosahedron domain mirroring\n\t  // Adapted from knighty https://www.shadertoy.com/view/MsKGzw\n\t  // --------------------------------------------------------\n\t  \n\t  #define PI 3.14159265359\n\t  \n\t  vec3 facePlane;\n\t  vec3 uPlane;\n\t  vec3 vPlane;\n\t  \n\t  int Type=5;\n\t  vec3 nc;\n\t  vec3 pab;\n\t  vec3 pbc;\n\t  vec3 pca;\n\t  \n\t  void initIcosahedron() {//setup folding planes and vertex\n\t      float cospin=cos(PI/float(Type)), scospin=sqrt(0.75-cospin*cospin);\n\t      nc=vec3(-0.5,-cospin,scospin);//3rd folding plane. The two others are xz and yz planes\n\t      pbc=vec3(scospin,0.,0.5);//No normalization in order to have 'barycentric' coordinates work evenly\n\t      pca=vec3(0.,scospin,cospin);\n\t      pbc=normalize(pbc); pca=normalize(pca);//for slightly better DE. In reality it's not necesary to apply normalization :)\n\t  \tpab=vec3(0,0,1);\n\t      \n\t      facePlane = pca;\n\t      uPlane = cross(vec3(1,0,0), facePlane);\n\t      vPlane = vec3(1,0,0);\n\t  }\n\t  \n\t  void pModIcosahedron(inout vec3 p) {\n\t      p = abs(p);\n\t      pReflect(p, nc, 0.);\n\t      p.xy = abs(p.xy);\n\t      pReflect(p, nc, 0.);\n\t      p.xy = abs(p.xy);\n\t      pReflect(p, nc, 0.);\n\t  }\n\t  \n\t  \n\t  // --------------------------------------------------------\n\t  // Triangle tiling\n\t  // Adapted from mattz https://www.shadertoy.com/view/4d2GzV\n\t  // --------------------------------------------------------\n\t  \n\t  const float sqrt3 = 1.7320508075688772;\n\t  const float i3 = 0.5773502691896258;\n\t  \n\t  const mat2 cart2hex = mat2(1, 0, i3, 2. * i3);\n\t  const mat2 hex2cart = mat2(1, 0, -.5, .5 * sqrt3);\n\t  \n\t  #define PHI (1.618033988749895)\n\t  #define TAU 6.283185307179586\n\t  \n\t  struct TriPoints {\n\t  \tvec2 a;\n\t      vec2 b;\n\t      vec2 c;\n\t      vec2 center;\n\t      vec2 ab;\n\t      vec2 bc;\n\t      vec2 ca;\n\t  };\n\t  \n\t  TriPoints closestTriPoints(vec2 p) {    \n\t      vec2 pTri = cart2hex * p;\n\t      vec2 pi = floor(pTri);\n\t      vec2 pf = fract(pTri);\n\t      \n\t      float split1 = step(pf.y, pf.x);\n\t      float split2 = step(pf.x, pf.y);\n\t      \n\t      vec2 a = vec2(split1, 1);\n\t      vec2 b = vec2(1, split2);\n\t      vec2 c = vec2(0, 0);\n\t  \n\t      a += pi;\n\t      b += pi;\n\t      c += pi;\n\t  \n\t      a = hex2cart * a;\n\t      b = hex2cart * b;\n\t      c = hex2cart * c;\n\t      \n\t      vec2 center = (a + b + c) / 3.;\n\t      \n\t  \tvec2 ab = (a + b) / 2.;\n\t      vec2 bc = (b + c) / 2.;\n\t      vec2 ca = (c + a) / 2.;\n\t  \n\t      return TriPoints(a, b, c, center, ab, bc, ca);\n\t  }\n\t  \n\t  \n\t  // --------------------------------------------------------\n\t  // Geodesic tiling\n\t  // --------------------------------------------------------\n\t  \n\t  struct TriPoints3D {\n\t  \tvec3 a;\n\t      vec3 b;\n\t      vec3 c;\n\t  \tvec3 center;\n\t      vec3 ab;\n\t      vec3 bc;\n\t      vec3 ca;\n\t  };\n\t  \n\t  vec3 intersection(vec3 n, vec3 planeNormal, float planeOffset) {\n\t      float denominator = dot(planeNormal, n);\n\t      float t = (dot(vec3(0), planeNormal ) + planeOffset) / -denominator;\n\t      return n * t;\n\t  }\n\t  \n\t  //// Edge length of an icosahedron with an inscribed sphere of radius of 1\n\t  //float edgeLength = 1. / ((sqrt(3.) / 12.) * (3. + sqrt(5.)));\n\t  //// Inner radius of the icosahedron's face\n\t  //float faceRadius = (1./6.) * sqrt(3.) * edgeLength;\n\t  float faceRadius = 0.3819660112501051;\n\t  \n\t  // 2D coordinates on the icosahedron face\n\t  vec2 icosahedronFaceCoordinates(vec3 p) {\n\t      vec3 pn = normalize(p);\n\t      vec3 i = intersection(pn, facePlane, -1.);\n\t      return vec2(dot(i, uPlane), dot(i, vPlane));\n\t  }\n\t  \n\t  // Project 2D icosahedron face coordinates onto a sphere\n\t  vec3 faceToSphere(vec2 facePoint) {\n\t  \treturn normalize(facePlane + (uPlane * facePoint.x) + (vPlane * facePoint.y));\n\t  }\n\t  \n\t  TriPoints3D geodesicTriPoints(vec3 p, float subdivisions) {\n\t      // Get 2D cartesian coordiantes on that face\n\t      vec2 uv = icosahedronFaceCoordinates(p);\n\t      \n\t      // Get points on the nearest triangle tile\n\t  \tfloat uvScale = subdivisions / faceRadius / 2.;\n\t      TriPoints points = closestTriPoints(uv * uvScale);\n\t      \n\t      // Project 2D triangle coordinates onto a sphere \n\t      vec3 a = faceToSphere(points.a / uvScale);\n\t      vec3 b = faceToSphere(points.b / uvScale);\n\t      vec3 c = faceToSphere(points.c / uvScale);\n\t      vec3 center = faceToSphere(points.center / uvScale);\n\t      vec3 ab = faceToSphere(points.ab / uvScale);\n\t      vec3 bc = faceToSphere(points.bc / uvScale);\n\t      vec3 ca = faceToSphere(points.ca / uvScale);\n\t      \n\t      return TriPoints3D(a, b, c, center, ab, bc, ca);\n\t  }\n\t  \n\t  \n\t  // --------------------------------------------------------\n\t  // Spectrum colour palette\n\t  // IQ https://www.shadertoy.com/view/ll2GD3\n\t  // --------------------------------------------------------\n\t  \n\t  vec3 pal( in float t, in vec3 a, in vec3 b, in vec3 c, in vec3 d ) {\n\t      return a + b*cos( 6.28318*(c*t+d) );\n\t  }\n\t  \n\t  vec3 spectrum(float n) {\n\t      return pal( n, vec3(0.5,0.5,0.5),vec3(0.5,0.5,0.5),vec3(1.0,1.0,1.0),vec3(0.0,0.33,0.67) );\n\t  }\n\t  \n\t  \n\t  // --------------------------------------------------------\n\t  // Model/Camera Rotation\n\t  // --------------------------------------------------------\n\t  \n\t  mat3 sphericalMatrix(float theta, float phi) {\n\t      float cx = cos(theta);\n\t      float cy = cos(phi);\n\t      float sx = sin(theta);\n\t      float sy = sin(phi);\n\t      return mat3(\n\t          cy, -sy * -sx, -sy * cx,\n\t          0, cx, sx,\n\t          sy, cy * -sx, cy * cx\n\t      );\n\t  }\n\t  \n\t  mat3 mouseRotation(bool enable, vec2 xy) {\n\t      if (enable) {\n\t          vec2 mouse = iMouse.xy / iResolution.xy;\n\t  \n\t          if (mouse.x != 0. && mouse.y != 0.) {\n\t              xy.x = mouse.x;\n\t              xy.y = mouse.y;\n\t          }\n\t      }\n\t      float rx, ry;\n\t      \n\t      rx = (xy.y + .5) * PI;\n\t      ry = (-xy.x) * 2. * PI;\n\t      \n\t      return sphericalMatrix(rx, ry);\n\t  }\n\t  \n\t  mat3 modelRotation() {\n\t      mat3 m = mouseRotation(MOUSE_CONTROL==1, MODEL_ROTATION);\n\t      return m;\n\t  }\n\t  \n\t  mat3 cameraRotation() {\n\t      mat3 m = mouseRotation(MOUSE_CONTROL==2, CAMERA_ROTATION);\n\t      return m;\n\t  }\n\t  \n\t  \n\t  // --------------------------------------------------------\n\t  // Animation \n\t  // --------------------------------------------------------\n\t  \n\t  const float SCENE_DURATION = 6.;\n\t  const float CROSSFADE_DURATION = 2.;\n\t  \n\t  float time;\n\t  \n\t  struct HexSpec {\n\t      float roundTop;\n\t      float roundCorner;\n\t  \tfloat height;\n\t      float thickness;\n\t      float gap;    \n\t  };\n\t      \n\t  HexSpec newHexSpec(float subdivisions) {\n\t  \treturn HexSpec(\n\t          .05 / subdivisions,\n\t          .1 / subdivisions,\n\t          2.,\n\t          2.,\n\t          .005\n\t      );\n\t  }\n\t      \n\t  // Animation 1\n\t      \n\t  float animSubdivisions1() {\n\t  \treturn mix(2.4, 3.4, cos(time * PI) * .5 + .5);\n\t  }\n\t  \n\t  HexSpec animHex1(vec3 hexCenter, float subdivisions) {\n\t      HexSpec spec = newHexSpec(subdivisions);\n\t      \n\t      float offset = time * 3. * PI;\n\t      offset -= subdivisions;\n\t      float blend = dot(hexCenter, pca);\n\t      blend = cos(blend * 30. + offset) * .5 + .5;\n\t      spec.height = mix(1.75, 2., blend);\n\t  \n\t      spec.thickness = spec.height;\n\t  \n\t      return spec;\n\t  }\n\t  \n\t  // Animation 2\n\t  \n\t  float animSubdivisions2() {\n\t      return mix(1., 2.3, sin(time * PI/2.) * .5 + .5);\n\t  }\n\t  \n\t  HexSpec animHex2(vec3 hexCenter, float subdivisions) {\n\t      HexSpec spec = newHexSpec(subdivisions);\n\t      \n\t      float blend = hexCenter.y;\n\t      spec.height = mix(1.6, 2., sin(blend * 10. + time * PI) * .5 + .5);\n\t      \n\t      spec.roundTop = .02 / subdivisions;\n\t      spec.roundCorner = .09 / subdivisions;\n\t      spec.thickness = spec.roundTop * 4.;\n\t      spec.gap = .01;\n\t  \n\t      return spec;\n\t  }\n\t  \n\t  // Animation 3\n\t  \n\t  float animSubdivisions3() {\n\t  \treturn 5.;\n\t  }\n\t  \n\t  HexSpec animHex3(vec3 hexCenter, float subdivisions) {\n\t      HexSpec spec = newHexSpec(subdivisions);\n\t      \n\t      float blend = acos(dot(hexCenter, pab)) * 10.;\n\t      blend = cos(blend + time * PI) * .5 + .5;\n\t      spec.gap = mix(.01, .4, blend) / subdivisions;\n\t  \n\t      spec.thickness = spec.roundTop * 2.;\n\t  \n\t  \treturn spec;\n\t  }\n\t  \n\t  // Transition between animations\n\t  \n\t  float sineInOut(float t) {\n\t    return -0.5 * (cos(PI * t) - 1.0);\n\t  }\n\t  \n\t  float transitionValues(float a, float b, float c) {\n\t      #ifdef LOOP\n\t          #if LOOP == 1\n\t              return a;\n\t          #endif\n\t          #if LOOP == 2\n\t              return b;\n\t          #endif\n\t          #if LOOP == 3\n\t              return c;\n\t          #endif\n\t      #endif\n\t      float t = time / SCENE_DURATION;\n\t      float scene = floor(mod(t, 3.));\n\t      float blend = fract(t);\n\t      float delay = (SCENE_DURATION - CROSSFADE_DURATION) / SCENE_DURATION;\n\t      blend = max(blend - delay, 0.) / (1. - delay);\n\t      blend = sineInOut(blend);\n\t      float ab = mix(a, b, blend);\n\t      float bc = mix(b, c, blend);\n\t      float cd = mix(c, a, blend);\n\t      float result = mix(ab, bc, min(scene, 1.));\n\t      result = mix(result, cd, max(scene - 1., 0.));\n\t      return result;\n\t  }\n\t   \n\t  HexSpec transitionHexSpecs(HexSpec a, HexSpec b, HexSpec c) {\n\t      float roundTop = transitionValues(a.roundTop, b.roundTop, c.roundTop);\n\t      float roundCorner = transitionValues(a.roundCorner, b.roundCorner, c.roundCorner);\n\t  \tfloat height = transitionValues(a.height, b.height, c.height);\n\t      float thickness = transitionValues(a.thickness, b.thickness, c.thickness);\n\t      float gap = transitionValues(a.gap, b.gap, c.gap);\n\t  \treturn HexSpec(roundTop, roundCorner, height, thickness, gap);\n\t  }\n\t  \n\t  \n\t  // --------------------------------------------------------\n\t  // Modelling \n\t  // --------------------------------------------------------\n\t  \n\t  const vec3 FACE_COLOR = vec3(.9,.9,1.);\n\t  const vec3 BACK_COLOR = vec3(.1,.1,.15);\n\t  const vec3 BACKGROUND_COLOR = vec3(.0, .005, .03);\n\t  \n\t  struct Model {\n\t      float dist;\n\t      vec3 albedo;\n\t      float glow;\n\t  };\n\t  \n\t  Model hexModel(\n\t      vec3 p,\n\t      vec3 hexCenter,\n\t      vec3 edgeA,\n\t      vec3 edgeB,\n\t      HexSpec spec\n\t  ) {\n\t      float d;\n\t  \n\t      float edgeADist = dot(p, edgeA) + spec.gap;\n\t      float edgeBDist = dot(p, edgeB) - spec.gap;\n\t      float edgeDist = smax(edgeADist, -edgeBDist, spec.roundCorner);\n\t  \n\t      float outerDist = length(p) - spec.height;\n\t      d = smax(edgeDist, outerDist, spec.roundTop);\n\t  \n\t      float innerDist = length(p) - spec.height + spec.thickness;\n\t      d = smax(d, -innerDist, spec.roundTop);\n\t      \n\t      vec3 color;\n\t  \n\t      float faceBlend = (spec.height - length(p)) / spec.thickness;\n\t      faceBlend = clamp(faceBlend, 0., 1.);\n\t      color = mix(FACE_COLOR, BACK_COLOR, step(.5, faceBlend));\n\t      \n\t      vec3 edgeColor = spectrum(dot(hexCenter, pca) * 5. + length(p) + .8);    \n\t  \tfloat edgeBlend = smoothstep(-.04, -.005, edgeDist);\n\t      color = mix(color, edgeColor, edgeBlend); \n\t  \n\t      return Model(d, color, edgeBlend);\n\t  }\n\t  \n\t  // checks to see which intersection is closer\n\t  Model opU( Model m1, Model m2 ){\n\t      if (m1.dist < m2.dist) {\n\t          return m1;\n\t      } else {\n\t          return m2;\n\t      }\n\t  }\n\t  \n\t  Model geodesicModel(vec3 p) {\n\t  \n\t      pModIcosahedron(p);\n\t      \n\t      float subdivisions = transitionValues(\n\t          animSubdivisions1(),\n\t          animSubdivisions2(),\n\t          animSubdivisions3()\n\t     \t);\n\t  \tTriPoints3D points = geodesicTriPoints(p, subdivisions);\n\t          \n\t  \tvec3 edgeAB = normalize(cross(points.center, points.ab));\n\t  \tvec3 edgeBC = normalize(cross(points.center, points.bc));\n\t      vec3 edgeCA = normalize(cross(points.center, points.ca));\n\t      \n\t      Model model, part;\n\t      HexSpec spec;\n\t  \n\t  \tspec = transitionHexSpecs(\n\t          animHex1(points.b, subdivisions),\n\t          animHex2(points.b, subdivisions),\n\t          animHex3(points.b, subdivisions)\n\t      );\n\t      part = hexModel(p, points.b, edgeAB, edgeBC, spec);\n\t      model = part;\n\t  \n\t  \tspec = transitionHexSpecs(\n\t          animHex1(points.c, subdivisions),\n\t          animHex2(points.c, subdivisions),\n\t          animHex3(points.c, subdivisions)\n\t      );\n\t      part = hexModel(p, points.c, edgeBC, edgeCA, spec);\n\t      model = opU(model, part);\n\t      \n\t  \tspec = transitionHexSpecs(\n\t          animHex1(points.a, subdivisions),\n\t          animHex2(points.a, subdivisions),\n\t          animHex3(points.a, subdivisions)\n\t      );\n\t      part = hexModel(p, points.a, edgeCA, edgeAB, spec);\n\t      model = opU(model, part);\n\t      \n\t  \treturn model;\n\t  }\n\t  \n\t  Model map( vec3 p ){\n\t      mat3 m = modelRotation();\n\t      p *= m;  \n\t      #ifndef LOOP\n\t      \tpR(p.xz, time * PI/16.);\n\t      #endif\n\t      Model model = geodesicModel(p);\n\t      return model;\n\t  }\n\t  \n\t  // --------------------------------------------------------\n\t  // LIGHTING\n\t  // Adapted from IQ https://www.shadertoy.com/view/Xds3zN\n\t  // --------------------------------------------------------\n\t  \n\t  vec3 doLighting(Model model, vec3 pos, vec3 nor, vec3 ref, vec3 rd) {\n\t      vec3 lightPos = normalize(vec3(.5,.5,-1.));\n\t      vec3 backLightPos = normalize(vec3(-.5,-.3,1));\n\t      vec3 ambientPos = vec3(0,1,0);\n\t      \n\t      vec3  lig = lightPos;\n\t      float amb = clamp((dot(nor, ambientPos) + 1.) / 2., 0., 1.);\n\t      float dif = clamp( dot( nor, lig ), 0.0, 1.0 );\n\t      float bac = pow(clamp(dot(nor, backLightPos), 0., 1.), 1.5);\n\t      float fre = pow( clamp(1.0+dot(nor,rd),0.0,1.0), 2.0 );\n\t      \n\t      vec3 lin = vec3(0.0);\n\t      lin += 1.20 * dif * vec3(.9);\n\t      lin += 0.80 * amb * vec3(.5, .7, .8);\n\t      lin += 0.30 * bac * vec3(.25);\n\t      lin += 0.20 * fre * vec3(1);\n\t      \n\t      vec3 albedo = model.albedo;\n\t      vec3 col = mix(albedo * lin, albedo, model.glow);    \n\t  \n\t      return col;\n\t  }   \n\t  \n\t  \n\t  // --------------------------------------------------------\n\t  // Ray Marching\n\t  // Adapted from cabbibo https://www.shadertoy.com/view/Xl2XWt\n\t  // --------------------------------------------------------\n\t  \n\t  const float MAX_TRACE_DISTANCE = 8.; // max trace distance\n\t  const float INTERSECTION_PRECISION = .001; // precision of the intersection\n\t  const int NUM_OF_TRACE_STEPS = 100;\n\t  const float FUDGE_FACTOR = .9; // Default is 1, reduce to fix overshoots\n\t  \n\t  struct CastRay {\n\t      vec3 origin;\n\t      vec3 direction;\n\t  };\n\t  \n\t  struct Ray {\n\t      vec3 origin;\n\t      vec3 direction;\n\t      float len;\n\t  };\n\t  \n\t  struct Hit {\n\t      Ray ray;\n\t      Model model;\n\t      vec3 pos;\n\t      bool isBackground;\n\t      vec3 normal;\n\t      vec3 color;\n\t  };\n\t  \n\t  vec3 calcNormal( in vec3 pos ){\n\t      vec3 eps = vec3( 0.001, 0.0, 0.0 );\n\t      vec3 nor = vec3(\n\t          map(pos+eps.xyy).dist - map(pos-eps.xyy).dist,\n\t          map(pos+eps.yxy).dist - map(pos-eps.yxy).dist,\n\t          map(pos+eps.yyx).dist - map(pos-eps.yyx).dist );\n\t      return normalize(nor);\n\t  }\n\t      \n\t  Hit raymarch(CastRay castRay){\n\t  \n\t      float currentDist = INTERSECTION_PRECISION * 2.0;\n\t      Model model;\n\t      \n\t      Ray ray = Ray(castRay.origin, castRay.direction, 0.);\n\t  \n\t      for( int i=0; i< NUM_OF_TRACE_STEPS ; i++ ){\n\t          if (currentDist < INTERSECTION_PRECISION || ray.len > MAX_TRACE_DISTANCE) {\n\t              break;\n\t          }\n\t          model = map(ray.origin + ray.direction * ray.len);\n\t          currentDist = model.dist;\n\t          ray.len += currentDist * FUDGE_FACTOR;\n\t      }\n\t      \n\t      bool isBackground = false;\n\t      vec3 pos = vec3(0);\n\t      vec3 normal = vec3(0);\n\t      vec3 color = vec3(0);\n\t      \n\t      if (ray.len > MAX_TRACE_DISTANCE) {\n\t          isBackground = true;\n\t      } else {\n\t          pos = ray.origin + ray.direction * ray.len;\n\t          normal = calcNormal(pos);\n\t      }\n\t  \n\t      return Hit(ray, model, pos, isBackground, normal, color);\n\t  }\n\t  \n\t  \n\t  // --------------------------------------------------------\n\t  // Rendering\n\t  // --------------------------------------------------------\n\t  \n\t  void shadeSurface(inout Hit hit){\n\t      \n\t      vec3 color = BACKGROUND_COLOR;\n\t      \n\t      if (hit.isBackground) {\n\t          hit.color = color;\n\t          return;\n\t      }\n\t  \n\t      vec3 ref = reflect(hit.ray.direction, hit.normal);\n\t  \n\t      #ifdef DEBUG\n\t          color = hit.normal * 0.5 + 0.5;\n\t      #else \n\t          color = doLighting(\n\t              hit.model,\n\t              hit.pos,\n\t              hit.normal,\n\t              ref,\n\t              hit.ray.direction\n\t          );\n\t      #endif\n\t  \n\t      hit.color = color;\n\t  }\n\t  \n\t  vec3 render(Hit hit){\n\t      shadeSurface(hit);\n\t  \treturn hit.color;\n\t  }\n\t  \n\t  \n\t  // --------------------------------------------------------\n\t  // Camera\n\t  // https://www.shadertoy.com/view/Xl2XWt\n\t  // --------------------------------------------------------\n\t  \n\t  mat3 calcLookAtMatrix( in vec3 ro, in vec3 ta, in float roll )\n\t  {\n\t      vec3 ww = normalize( ta - ro );\n\t      vec3 uu = normalize( cross(ww,vec3(sin(roll),cos(roll),0.0) ) );\n\t      vec3 vv = normalize( cross(uu,ww));\n\t      return mat3( uu, vv, ww );\n\t  }\n\t  \n\t  void doCamera(out vec3 camPos, out vec3 camTar, out float camRoll, in float time, in vec2 mouse) {\n\t      float dist = 5.5;\n\t      camRoll = 0.;\n\t      camTar = vec3(0,0,0);\n\t      camPos = vec3(0,0,-dist);\n\t      camPos *= cameraRotation();\n\t      camPos += camTar;\n\t  }\n\t  \n\t  \n\t  // --------------------------------------------------------\n\t  // Gamma\n\t  // https://www.shadertoy.com/view/Xds3zN\n\t  // --------------------------------------------------------\n\t  \n\t  const float GAMMA = 2.2;\n\t  \n\t  vec3 gamma(vec3 color, float g) {\n\t      return pow(color, vec3(g));\n\t  }\n\t  \n\t  vec3 linearToScreen(vec3 linearRGB) {\n\t      return gamma(linearRGB, 1.0 / GAMMA);\n\t  }\n\t  \n\t  void mainImage( out vec4 fragColor, in vec2 fragCoord )\n\t  {\n\t      time = iTime;\n\t  \n\t      #ifdef LOOP\n\t          #if LOOP == 1\n\t              time = mod(time, 2.);   \n\t          #endif\n\t          #if LOOP == 2\n\t              time = mod(time, 4.);   \n\t          #endif\n\t          #if LOOP == 3\n\t              time = mod(time, 2.);\n\t      \t#endif\n\t      #endif\n\t      \n\t      initIcosahedron();\n\t      \n\t      vec2 p = (-iResolution.xy + 2.0*fragCoord.xy)/iResolution.y;\n\t      vec2 m = iMouse.xy / iResolution.xy;\n\t  \n\t      vec3 camPos = vec3( 0., 0., 2.);\n\t      vec3 camTar = vec3( 0. , 0. , 0. );\n\t      float camRoll = 0.;\n\t      \n\t      // camera movement\n\t      doCamera(camPos, camTar, camRoll, iTime, m);\n\t      \n\t      // camera matrix\n\t      mat3 camMat = calcLookAtMatrix( camPos, camTar, camRoll );  // 0.0 is the camera roll\n\t      \n\t      // create view ray\n\t      vec3 rd = normalize( camMat * vec3(p.xy,2.0) ); // 2.0 is the lens length\n\t      \n\t      Hit hit = raymarch(CastRay(camPos, rd));\n\t  \n\t      vec3 color = render(hit);\n\t      \n\t      #ifndef DEBUG\n\t          color = linearToScreen(color);\n\t      #endif\n\t  \n\t      fragColor = vec4(color,1.0);\n\t  }\n\t  ```\n\n## Current Landscape (2025)\n\n- Industry adoption and implementations\n  - Metaverse platforms continue to evolve with focus on interoperability and open standards\n  - Web3 integration accelerating with decentralised identity and asset ownership\n  - Enterprise adoption growing in virtual collaboration, training, and digital twins\n  - UK companies increasingly active in metaverse development and immersive technologies\n\n- Technical capabilities\n  - Real-time rendering at photorealistic quality levels\n  - Low-latency networking enabling seamless multi-user experiences\n  - AI-driven content generation and procedural world building\n  - Spatial audio and haptics enhancing immersion\n\n- UK and North England context\n  - Manchester: Digital Innovation Factory supports metaverse startups and research\n  - Leeds: Holovis leads in immersive experiences for entertainment and training\n  - Newcastle: University research in spatial computing and interactive systems\n  - Sheffield: Advanced manufacturing using digital twin technology\n\n- Standards and frameworks\n  - Metaverse Standards Forum driving interoperability protocols\n  - WebXR enabling browser-based immersive experiences\n  - glTF and USD for 3D asset interchange\n  - Open Metaverse Interoperability Group defining cross-platform standards\n\n## Metadata\n\n- **Last Updated**: 2025-11-16\n- **Review Status**: Automated remediation with 2025 context\n- **Verification**: Academic sources verified\n- **Regional Context**: UK/North England where applicable",
  "properties": {
    "id": "visionflow-and-junkie-jarvis-owl-axioms",
    "collapsed": "true",
    "- ontology": "true",
    "- term-id": "mv-682173246137",
    "- preferred-term": "VisionFlow and Junkie Jarvis",
    "- source-domain": "metaverse",
    "- status": "active",
    "- public-access": "true",
    "- definition": "A component of the metaverse ecosystem focusing on visionflow and junkie jarvis.",
    "- maturity": "mature",
    "- authority-score": "0.85",
    "- owl:class": "mv:VisionflowAndJunkieJarvis",
    "- owl:physicality": "ConceptualEntity",
    "- owl:role": "Concept",
    "- belongsToDomain": "[[MetaverseDomain]]",
    "- enables": "[[ImmersiveExperience]], [[Presence]], [[SpatialComputing]]",
    "- requires": "[[DisplayTechnology]], [[TrackingSystem]], [[RenderingEngine]]",
    "- bridges-to": "[[HumanComputerInteraction]], [[ComputerVision]], [[Robotics]]",
    "public": "true"
  },
  "backlinks": [
    "AICafev6",
    "BC-0037-public-key"
  ],
  "wiki_links": [
    "VisionFlow Client",
    "TrackingSystem",
    "ImmersiveExperience",
    "ComputerVision",
    "RenderingEngine",
    "SpatialComputing",
    "Robotics",
    "HumanComputerInteraction",
    "MetaverseDomain",
    "DisplayTechnology",
    "Presence"
  ],
  "ontology": {
    "term_id": "mv-682173246137",
    "preferred_term": "VisionFlow and Junkie Jarvis",
    "definition": "A component of the metaverse ecosystem focusing on visionflow and junkie jarvis.",
    "source_domain": "metaverse",
    "maturity_level": null,
    "authority_score": 0.85
  }
}