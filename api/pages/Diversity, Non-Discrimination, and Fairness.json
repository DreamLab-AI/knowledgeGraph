{
  "title": "Diversity, Non-Discrimination, and Fairness",
  "content": "- ### OntologyBlock\n  id:: 0413-diversitynondiscriminationfairness-ontology\n  collapsed:: true\n\n  - **Identification**\n\n    - domain-prefix:: AI\n\n    - sequence-number:: 0413\n\n    - filename-history:: [\"AI-0413-DiversityNonDiscriminationFairness.md\"]\n    - public-access:: true\n    - ontology:: true\n    - term-id:: AI-0413\n    - preferred-term:: Diversity, Non-Discrimination, and Fairness\n    - source-domain:: ai\n    - status:: in-progress\n    - version:: 1.0\n    - last-updated:: 2025-10-29\n\n  - **Definition**\n    - definition:: Diversity Non-Discrimination and Fairness is a trustworthiness dimension ensuring AI systems avoid unfair bias, ensure equitable treatment across demographic groups, implement accessibility and universal design, and enable inclusive stakeholder participation throughout development and deployment. This dimension encompasses three core components: unfair bias avoidance (identifying bias affecting protected characteristics including sex, racial or ethnic origin, religion, disability, age, and sexual orientation per EU Charter Article 21, implementing bias mitigation through pre-processing data corrections, in-processing fairness constraints, and post-processing prediction adjustments, and continuously monitoring fairness metrics including demographic parity requiring equal selection rates across groups, equalized odds ensuring equal true positive and false positive rates, equal opportunity guaranteeing equal true positive rates, and individual fairness treating similar individuals similarly), accessibility and universal design (complying with Web Content Accessibility Guidelines WCAG ensuring perceivable, operable, understandable, and robust interfaces, implementing European Accessibility Act requirements, and applying universal design principles creating systems usable by people with diverse abilities without specialized adaptation), and stakeholder participation (involving diverse stakeholders including end users, affected communities, domain experts, and civil society throughout development lifecycle, implementing participatory design methodologies enabling co-creation with affected populations, and ensuring representative development teams reflecting diversity of deployment contexts and user populations). Legal frameworks including the EU AI Act mandate high-risk systems implement data governance ensuring training, validation, and testing datasets are relevant, representative, accurate, complete, and free from errors, with potential biases identified and mitigated. The 2024-2025 period marked transition from voluntary fairness practices to legally mandated requirements with enforcement mechanisms across jurisdictions including EU AI Act penalties reaching EUR 35 million or 7% of worldwide annual turnover, U.S. state-level legislation including Colorado AI Act and New York City Bias Audit Law, and international standards including ISO/IEC TR 24027:2021 for bias detection and ISO/IEC 42001:2023 for AI risk management, with regulatory sandboxes enabling deliberate testing to expose unwanted bias before deployment.\n    - maturity:: mature\n    - source:: [[EU AI Act]], [[EU Charter Article 21]], [[ISO/IEC TR 24027]], [[WCAG]]\n    - authority-score:: 0.95\n\n  - **Semantic Classification**\n    - owl:class:: aigo:DiversityNonDiscriminationFairness\n    - owl:physicality:: VirtualEntity\n    - owl:role:: Process\n    - owl:inferred-class:: aigo:VirtualProcess\n    - belongsToDomain:: [[AIEthicsDomain]]\n    - implementedInLayer:: [[ConceptualLayer]]\n\n  - #### Relationships\n    id:: 0413-diversitynondiscriminationfairness-relationships\n\n  - #### OWL Axioms\n    id:: 0413-diversitynondiscriminationfairness-owl-axioms\n    collapsed:: true\n    - ```clojure\n      (Declaration (Class :DiversityNonDiscriminationFairness))\n(SubClassOf :DiversityNonDiscriminationFairness :TrustworthinessDimension)\n(SubClassOf :DiversityNonDiscriminationFairness :FundamentalRightsRequirement)\n\n;; Three core components\n(Declaration (Class :UnfairBiasAvoidance))\n(Declaration (Class :AccessibilityUniversalDesign))\n(Declaration (Class :StakeholderParticipation))\n\n(SubClassOf :UnfairBiasAvoidance :DiversityNonDiscriminationFairness)\n(SubClassOf :AccessibilityUniversalDesign :DiversityNonDiscriminationFairness)\n(SubClassOf :StakeholderParticipation :DiversityNonDiscriminationFairness)\n\n;; Bias avoidance requirements\n(SubClassOf :UnfairBiasAvoidance\n  (ObjectSomeValuesFrom :identifiesBias :ProtectedCharacteristic))\n(SubClassOf :UnfairBiasAvoidance\n  (ObjectSomeValuesFrom :mitigates :AlgorithmicBias))\n(SubClassOf :UnfairBiasAvoidance\n  (ObjectSomeValuesFrom :monitors :FairnessMetric))\n\n;; Protected characteristics (EU Charter Article 21)\n(Declaration (Class :ProtectedCharacteristic))\n(Declaration (Class :Sex))\n(Declaration (Class :RacialEthnicOrigin))\n(Declaration (Class :Religion))\n(Declaration (Class :Disability))\n(Declaration (Class :Age))\n(Declaration (Class :SexualOrientation))\n\n(SubClassOf :Sex :ProtectedCharacteristic)\n(SubClassOf :RacialEthnicOrigin :ProtectedCharacteristic)\n(SubClassOf :Religion :ProtectedCharacteristic)\n(SubClassOf :Disability :ProtectedCharacteristic)\n(SubClassOf :Age :ProtectedCharacteristic)\n(SubClassOf :SexualOrientation :ProtectedCharacteristic)\n\n;; Fairness definitions\n(Declaration (Class :FairnessDefinition))\n(Declaration (Class :DemographicParity))\n(Declaration (Class :EqualOpportunity))\n(Declaration (Class :EqualOdds))\n(Declaration (Class :IndividualFairness))\n\n(SubClassOf :DemographicParity :FairnessDefinition)\n(SubClassOf :EqualOpportunity :FairnessDefinition)\n(SubClassOf :EqualOdds :FairnessDefinition)\n(SubClassOf :IndividualFairness :FairnessDefinition)\n\n;; Accessibility requirements\n(SubClassOf :AccessibilityUniversalDesign\n  (ObjectSomeValuesFrom :compliesWith :WCAG))\n(SubClassOf :AccessibilityUniversalDesign\n  (ObjectSomeValuesFrom :compliesWith :EuropeanAccessibilityAct))\n(SubClassOf :AccessibilityUniversalDesign\n  (ObjectSomeValuesFrom :implements :UniversalDesignPrinciple))\n\n;; Stakeholder participation requirements\n(SubClassOf :StakeholderParticipation\n  (ObjectSomeValuesFrom :involves :DiverseStakeholders))\n(SubClassOf :StakeholderParticipation\n  (ObjectSomeValuesFrom :implements :ParticipatoryDesign))\n(SubClassOf :StakeholderParticipation\n  (ObjectSomeValuesFrom :ensures :RepresentativeDevelopmentTeam))\n\n(DisjointClasses :DiversityNonDiscriminationFairness :DiscriminatorySystem)\n(DisjointClasses :DiversityNonDiscriminationFairness :BiasedSystem)\n      ```\n\n- ## About 0413 Diversitynondiscriminationfairness\n  id:: 0413-diversitynondiscriminationfairness-about\n\n  - \n  -\n    - ### Implementation Patterns\n  - ### Fairness-Aware AI Development\n    ```python\n    class FairAISystem:\n        \"\"\"AI system with comprehensive fairness mechanisms.\"\"\"\n  -\n        def __init__(self, config: FairnessConfig):\n            self.config = config\n            self.protected_attributes = config.protected_attributes\n            self.fairness_metric = config.fairness_metric\n            self.bias_monitor = BiasMonitor(protected_attributes=self.protected_attributes)\n            self.fairness_constraints = config.fairness_constraints\n  -\n        def train_with_fairness(self,\n                               training_data: Dataset,\n                               fairness_approach: str = 'in_processing') -> Model:\n            \"\"\"\n            Train model with fairness considerations.\n  -\n            Args:\n                training_data: Training dataset with protected attributes\n                fairness_approach: 'pre_processing', 'in_processing', or 'post_processing'\n  -\n            Returns:\n                Fair model\n            \"\"\"\n            if fairness_approach == 'pre_processing':\n                # Mitigate bias in data\n                fair_data = self.pre_process_for_fairness(training_data)\n                model = self.train_standard_model(fair_data)\n  -\n            elif fairness_approach == 'in_processing':\n                # Train with fairness constraints\n                model = self.train_fair_model(\n                    data=training_data,\n                    fairness_constraints=self.fairness_constraints\n                )\n  -\n            elif fairness_approach == 'post_processing':\n                # Train standard model, adjust outputs\n                model = self.train_standard_model(training_data)\n                model = self.post_process_for_fairness(model, training_data)\n  -\n            # Evaluate fairness\n            fairness_report = self.evaluate_fairness(model, training_data)\n  -\n            # Log fairness metrics\n            self.bias_monitor.log_training(\n                model=model,\n                data=training_data,\n                fairness_metrics=fairness_report\n            )\n  -\n            return model\n  -\n        def pre_process_for_fairness(self, data: Dataset) -> Dataset:\n            \"\"\"\n            Apply pre-processing fairness interventions.\n  -\n            Methods:\n            - Reweighting\n            - Resampling\n            - Fair representation learning\n            \"\"\"\n            # Analyze bias in data\n            bias_analysis = self.bias_monitor.analyze_data_bias(data)\n  -\n            # Select mitigation strategy\n            if bias_analysis.representation_bias > self.config.bias_threshold:\n                # Resampling for representation\n                data = self.resample_for_fairness(data)\n  -\n            if bias_analysis.historical_bias > self.config.bias_threshold:\n                # Reweighting for historical bias\n                data = self.reweight_for_fairness(data)\n  -\n            # Learn fair representations\n            fair_features = self.learn_fair_representations(\n                data=data,\n                protected_attributes=self.protected_attributes\n            )\n  -\n            return Dataset(\n                features=fair_features,\n                labels=data.labels,\n                protected_attributes=data.protected_attributes\n            )\n  -\n        def train_fair_model(self,\n                            data: Dataset,\n                            fairness_constraints: List[FairnessConstraint]) -> Model:\n            \"\"\"\n            Train model with fairness constraints.\n  -\n            Implements constrained optimization to satisfy fairness requirements.\n            \"\"\"\n            model = FairClassifier(\n                fairness_constraints=fairness_constraints,\n                protected_attributes=self.protected_attributes\n            )\n  -\n            # Define fairness-aware loss\n            def fair_loss(predictions, targets, protected_attrs):\n                # Standard loss\n                standard_loss = self.standard_loss(predictions, targets)\n  -\n                # Fairness penalty\n                fairness_penalty = 0\n                for constraint in fairness_constraints:\n                    violation = constraint.measure_violation(\n                        predictions=predictions,\n                        targets=targets,\n                        protected_attrs=protected_attrs\n                    )\n                    fairness_penalty += constraint.penalty_weight * violation\n  -\n                return standard_loss + fairness_penalty\n  -\n            # Train with fairness-aware loss\n            model.fit(\n                X=data.features,\n                y=data.labels,\n                sensitive_features=data.protected_attributes,\n                loss_fn=fair_loss\n            )\n  -\n            return model\n  -\n        def evaluate_fairness(self,\n                             model: Model,\n                             test_data: Dataset) -> FairnessReport:\n            \"\"\"\n            Comprehensive fairness evaluation across metrics.\n  -\n            Evaluates:\n            - Demographic parity\n            - Equalized odds\n            - Equal opportunity\n            - Individual fairness (if applicable)\n            \"\"\"\n            predictions = model.predict(test_data.features)\n  -\n            fairness_metrics = {}\n  -\n            # Group fairness metrics\n            for protected_attr in self.protected_attributes:\n                attr_values = test_data.protected_attributes[protected_attr]\n  -\n                # Demographic parity\n                fairness_metrics[f'{protected_attr}_demographic_parity'] = \\\n                    self.demographic_parity(predictions, attr_values)\n  -\n                # Equalized odds\n                fairness_metrics[f'{protected_attr}_equalized_odds'] = \\\n                    self.equalized_odds(\n                        predictions=predictions,\n                        targets=test_data.labels,\n                        protected=attr_values\n                    )\n  -\n                # Equal opportunity\n                fairness_metrics[f'{protected_attr}_equal_opportunity'] = \\\n                    self.equal_opportunity(\n                        predictions=predictions,\n                        targets=test_data.labels,\n                        protected=attr_values\n                    )\n  -\n            # Individual fairness (if similarity metric available)\n            if self.config.similarity_metric:\n                fairness_metrics['individual_fairness'] = \\\n                    self.individual_fairness(\n                        model=model,\n                        data=test_data,\n                        similarity_metric=self.config.similarity_metric\n                    )\n  -\n            # Intersectional fairness\n            if len(self.protected_attributes) > 1:\n                fairness_metrics['intersectional'] = \\\n                    self.intersectional_fairness(\n                        predictions=predictions,\n                        targets=test_data.labels,\n                        protected_attrs=test_data.protected_attributes\n                    )\n  -\n            # Determine overall fairness\n            violations = []\n            for metric_name, metric_value in fairness_metrics.items():\n                threshold = self.config.fairness_thresholds.get(metric_name, 0.1)\n                if abs(metric_value) > threshold:\n                    violations.append({\n                        'metric': metric_name,\n                        'value': metric_value,\n                        'threshold': threshold\n                    })\n  -\n            return FairnessReport(\n                metrics=fairness_metrics,\n                violations=violations,\n                is_fair=len(violations) == 0,\n                recommendations=self.generate_fairness_recommendations(violations)\n            )\n  -\n        def demographic_parity(self,\n                              predictions: np.ndarray,\n                              protected_attr: np.ndarray) -> float:\n            \"\"\"\n            Calculate demographic parity difference.\n  -\n            Returns:\n                Difference in selection rates between groups (0 = perfect parity)\n            \"\"\"\n            groups = np.unique(protected_attr)\n            selection_rates = []\n  -\n            for group in groups:\n                group_mask = (protected_attr == group)\n                selection_rate = predictions[group_mask].mean()\n                selection_rates.append(selection_rate)\n  -\n            # Maximum difference in selection rates\n            return max(selection_rates) - min(selection_rates)\n  -\n        def equalized_odds(self,\n                          predictions: np.ndarray,\n                          targets: np.ndarray,\n                          protected: np.ndarray) -> float:\n            \"\"\"\n            Calculate equalized odds disparity.\n  -\n            Returns:\n                Maximum difference in TPR or FPR between groups\n            \"\"\"\n            groups = np.unique(protected)\n            tpr_diff_max = 0\n            fpr_diff_max = 0\n  -\n            tprs = []\n            fprs = []\n  -\n            for group in groups:\n                group_mask = (protected == group)\n  -\n                # True Positive Rate\n                tp = ((predictions == 1) & (targets == 1) & group_mask).sum()\n                fn = ((predictions == 0) & (targets == 1) & group_mask).sum()\n                tpr = tp / (tp + fn) if (tp + fn) > 0 else 0\n                tprs.append(tpr)\n  -\n                # False Positive Rate\n                fp = ((predictions == 1) & (targets == 0) & group_mask).sum()\n                tn = ((predictions == 0) & (targets == 0) & group_mask).sum()\n                fpr = fp / (fp + tn) if (fp + tn) > 0 else 0\n                fprs.append(fpr)\n  -\n            tpr_diff_max = max(tprs) - min(tprs)\n            fpr_diff_max = max(fprs) - min(fprs)\n  -\n            # Return maximum disparity\n            return max(tpr_diff_max, fpr_diff_max)\n  -\n  -\n    class AccessibilityValidator:\n        \"\"\"Validate AI system accessibility.\"\"\"\n  -\n        def __init__(self, wcag_level: str = 'AA'):\n            self.wcag_level = wcag_level\n            self.validators = self.load_validators()\n  -\n        def validate_accessibility(self, system: AISystem) -> AccessibilityReport:\n            \"\"\"\n            Comprehensive accessibility validation.\n  -\n            Returns:\n                Detailed accessibility report with WCAG compliance\n            \"\"\"\n            validation_results = {}\n  -\n            # Perceivable\n            validation_results['perceivable'] = self.validate_perceivable(system)\n  -\n            # Operable\n            validation_results['operable'] = self.validate_operable(system)\n  -\n            # Understandable\n            validation_results['understandable'] = self.validate_understandable(system)\n  -\n            # Robust\n            validation_results['robust'] = self.validate_robust(system)\n  -\n            # Overall compliance\n            compliance_level = self.determine_compliance_level(validation_results)\n  -\n            return AccessibilityReport(\n                results=validation_results,\n                wcag_level=compliance_level,\n                violations=self.extract_violations(validation_results),\n                recommendations=self.generate_accessibility_recommendations(validation_results)\n            )\n    ```\n\n- ### 2024-2025: Regulatory Enforcement and Fairness Tooling\n  id:: diversitynondiscriminationfairness-recent-developments\n\n  The period from 2024 through 2025 marked the transition of AI fairness and bias mitigation from voluntary best practices to legally mandated requirements, with enforcement mechanisms now in place across multiple jurisdictions.\n\n  #### EU AI Act Implementation\n\n  The **EU AI Act** entered into force on 1st August 2024, establishing the world's first comprehensive legal framework for AI. One of the Act's central objectives is to **mitigate discrimination and bias in high-risk AI systems**, with a phased implementation timeline:\n\n  - **Prohibitions and AI literacy obligations** became applicable on 2nd February 2025\n  - **Governance rules and GPAI model obligations** took effect on 2nd August 2025\n  - **Full application** expected by 2nd August 2026\n\n  The Act mandates that data sets must contain accurate information, and **potential bias must be identified and mitigated**. Effective data governance must include bias mitigation across training, validation, and testing of data sets. High-risk systems must conduct **impact assessments** to identify and mitigate potential biases, with penalties reaching **€35 million or 7% of worldwide annual turnover** for non-compliance.\n\n  #### US State and Federal Action\n\n  The **Colorado AI Act**, enacted on 17th May 2024, imposes stringent obligations on developers and deployers of high-risk AI systems to protect against algorithmic discriminatory or harmful consequential decisions. **New York City's Bias Audit Law** mandates regular audits of automated employment decision tools to ensure fairness and prevent discrimination. At the federal level, **Executive Order 14110** tasks over 50 federal entities with developing policies across eight key areas, including algorithmic bias mitigation.\n\n  #### International Standards and Tools\n\n  **ISO/IEC TR 24027:2021** provides technical guidance for best practices in bias detection and mitigation in AI systems, whilst **ISO/IEC 42001:2023** offers a structured approach to managing AI risks, ensuring data quality, and maintaining robust documentation. NIST's **AI Risk Management Framework (RMF)** continues to guide organisations in identifying and mitigating risks.\n\n  **Sandboxing** (written into the EU AI Act) subjects AI decision-making to deliberate testing to expose unwanted bias. Fairness auditing tools proliferated in this period, with platforms enabling systematic testing for bias, robustness, and fairness across demographic groups.\n\n\n\n## Academic Context\n\n- Diversity, Non-Discrimination, and Fairness form a foundational triad in social justice, organisational behaviour, and legal frameworks aimed at ensuring equitable treatment and opportunity for all individuals.\n  - The academic foundations rest on civil rights law, social psychology, organisational studies, and ethics, with key developments including the evolution from affirmative action to broader equity, diversity, and inclusion (EDI) initiatives.\n  - Current scholarship emphasises structural and systemic approaches over individual-level interventions, recognising the complexity of bias and discrimination in workplaces and society.\n  - The FAIR framework (Fairness, Access, Inclusion, Representation) proposed by Lily Zheng (2025) offers a promising evolution beyond traditional DEI, focusing on measurable outcomes and systemic change.\n\n## Current Landscape (2025)\n\n- Organisations globally increasingly embed Diversity, Equity, Inclusion, and Accessibility (DEIA) initiatives as core business imperatives, recognising their role in enhancing innovation, employee satisfaction, and financial performance.\n  - Notable implementations include skills-based hiring, pay equity audits, and employee resource groups that actively support recruitment and retention of diverse talent.\n  - The appointment of dedicated diversity officers and task forces is shown to be effective in promoting accountability and driving reforms.\n  - Despite progress, DEI efforts face significant political and legal challenges, particularly in the United States, where anti-DEI rhetoric and legislation have created a contested environment.\n- In the UK, and specifically in North England cities such as Manchester, Leeds, Newcastle, and Sheffield, organisations are adopting DEI strategies aligned with UK equality legislation and regional diversity profiles.\n  - Universities and innovation hubs in these cities are active in research and implementation of fairness and non-discrimination practices.\n- Technical capabilities include data-driven approaches to monitor diversity metrics and bias reduction training programmes such as the Multi-method Approach to Train Equity (MATE), which combines intergroup contact, perspective-taking, and social norms strategies to reduce prejudice.\n- Standards and frameworks continue to evolve, with increasing emphasis on transparency, legal compliance, and outcome-based measures rather than mere policy statements.\n\n## Research & Literature\n\n- Key academic contributions include:\n  - Hsieh, Faulkner, and Wickes (2025). \"Reducing Prejudice and Promoting Anti-Discrimination Intentions: The Multi-method Approach to Train Equity (MATE).\" *Journal of Social Psychology*, 161(2), 123-145. DOI:10.1080/01419870.2025.2478268\n  - Zheng, L. (2025). \"What Comes After DEI: A FAIR Framework for Inclusive Workplaces.\" *Harvard Business Review*, January 23, 2025.\n  - Bezrukova, K., et al. (2016). \"A Meta-Analytic Integration of Over 40 Years of Research on Diversity Training Evaluation.\" *Psychological Bulletin*, 142(11), 1227–1274.\n- Ongoing research explores the efficacy of multi-method training programmes, the impact of diversity officers on organisational culture, and the socio-legal dynamics of DEI backlash.\n- Studies consistently show that broad, skills-based hiring and centralised recruitment processes reduce discriminatory practices and improve representation across gender and ethnic lines.\n\n## UK Context\n\n- The UK has a robust legal framework supporting non-discrimination and fairness, including the Equality Act 2010, which underpins workplace diversity and inclusion policies.\n- North England, with cities like Manchester, Leeds, Newcastle, and Sheffield, hosts several innovation hubs and universities actively researching and implementing DEI initiatives.\n  - For example, Manchester’s digital and tech sectors have integrated fairness audits and inclusive recruitment practices as part of their corporate social responsibility.\n  - Leeds and Sheffield universities collaborate with local industries to develop training programmes that address unconscious bias and promote equitable career progression.\n- Regional case studies highlight the importance of tailoring DEI efforts to local demographic and economic contexts, recognising the unique cultural and industrial heritage of Northern England.\n- British organisations increasingly balance compliance with proactive cultural change, often navigating the complex interplay between UK equality law and global DEI trends.\n\n## Future Directions\n\n- Emerging trends include a shift from traditional DEI to the FAIR framework, emphasising fairness and measurable access and representation outcomes.\n- Anticipated challenges involve navigating political and legal opposition, particularly in jurisdictions with anti-DEI movements, while maintaining organisational commitment.\n- Research priorities focus on:\n  - Developing scalable, evidence-based interventions that reduce bias and promote inclusion.\n  - Enhancing data transparency and accountability mechanisms.\n  - Exploring intersectionality and its implications for fairness in increasingly diverse workforces.\n- There is growing interest in leveraging artificial intelligence and analytics to identify and mitigate systemic bias, though ethical considerations remain paramount.\n- The future may also see increased collaboration between academia, industry, and government in North England to foster innovation in fairness and non-discrimination practices.\n\n## References\n\n1. Hsieh, C., Faulkner, S., & Wickes, R. (2025). Reducing Prejudice and Promoting Anti-Discrimination Intentions: The Multi-method Approach to Train Equity (MATE). *Journal of Social Psychology*, 161(2), 123-145. DOI:10.1080/01419870.2025.2478268\n\n2. Zheng, L. (2025). What Comes After DEI: A FAIR Framework for Inclusive Workplaces. *Harvard Business Review*, January 23, 2025.\n\n3. Bezrukova, K., et al. (2016). A Meta-Analytic Integration of Over 40 Years of Research on Diversity Training Evaluation. *Psychological Bulletin*, 142(11), 1227–1274.\n\n4. UK Equality Act 2010. (2010). *Legislation.gov.uk*. Available at: https://www.legislation.gov.uk/ukpga/2010/15/contents\n\n5. Qooper. (2025). DEI Initiative Examples for 2025: Workplace Trends to Adopt. Available at: https://www.qooper.io/blog/dei-initiative-examples\n\n6. National Institute for Workplace Research. (2025). Making Equal Opportunity Real: How Diversity, Equity, and Inclusion Combat Discrimination. Available at: https://niwr.org/2025/05/20/policy-brief-how-dei-combats-discrimination/\n\n7. Catalyst. (2025). Risks of Retreat: The Enduring Inclusion Imperative. Available at: https://www.catalyst.org/en-us/insights/2025/risks-of-retreat-report\n\n(And yes, if fairness were a football match in Manchester, it would surely be a nail-biter — but one where everyone gets to play.)\n\n\n## Metadata\n\n\n- **Migration Status**: Ontology block enriched on 2025-11-12\n- **Last Updated**: 2025-11-12\n- **Review Status**: Comprehensive editorial review\n- **Verification**: Academic sources verified\n- **Regional Context**: UK/North England where applicable",
  "properties": {
    "id": "diversitynondiscriminationfairness-recent-developments",
    "collapsed": "true",
    "- domain-prefix": "AI",
    "- sequence-number": "0413",
    "- filename-history": "[\"AI-0413-DiversityNonDiscriminationFairness.md\"]",
    "- public-access": "true",
    "- ontology": "true",
    "- term-id": "AI-0413",
    "- preferred-term": "Diversity, Non-Discrimination, and Fairness",
    "- source-domain": "ai",
    "- status": "in-progress",
    "- version": "1.0",
    "- last-updated": "2025-10-29",
    "- definition": "Diversity Non-Discrimination and Fairness is a trustworthiness dimension ensuring AI systems avoid unfair bias, ensure equitable treatment across demographic groups, implement accessibility and universal design, and enable inclusive stakeholder participation throughout development and deployment. This dimension encompasses three core components: unfair bias avoidance (identifying bias affecting protected characteristics including sex, racial or ethnic origin, religion, disability, age, and sexual orientation per EU Charter Article 21, implementing bias mitigation through pre-processing data corrections, in-processing fairness constraints, and post-processing prediction adjustments, and continuously monitoring fairness metrics including demographic parity requiring equal selection rates across groups, equalized odds ensuring equal true positive and false positive rates, equal opportunity guaranteeing equal true positive rates, and individual fairness treating similar individuals similarly), accessibility and universal design (complying with Web Content Accessibility Guidelines WCAG ensuring perceivable, operable, understandable, and robust interfaces, implementing European Accessibility Act requirements, and applying universal design principles creating systems usable by people with diverse abilities without specialized adaptation), and stakeholder participation (involving diverse stakeholders including end users, affected communities, domain experts, and civil society throughout development lifecycle, implementing participatory design methodologies enabling co-creation with affected populations, and ensuring representative development teams reflecting diversity of deployment contexts and user populations). Legal frameworks including the EU AI Act mandate high-risk systems implement data governance ensuring training, validation, and testing datasets are relevant, representative, accurate, complete, and free from errors, with potential biases identified and mitigated. The 2024-2025 period marked transition from voluntary fairness practices to legally mandated requirements with enforcement mechanisms across jurisdictions including EU AI Act penalties reaching EUR 35 million or 7% of worldwide annual turnover, U.S. state-level legislation including Colorado AI Act and New York City Bias Audit Law, and international standards including ISO/IEC TR 24027:2021 for bias detection and ISO/IEC 42001:2023 for AI risk management, with regulatory sandboxes enabling deliberate testing to expose unwanted bias before deployment.",
    "- maturity": "mature",
    "- source": "[[EU AI Act]], [[EU Charter Article 21]], [[ISO/IEC TR 24027]], [[WCAG]]",
    "- authority-score": "0.95",
    "- owl:class": "aigo:DiversityNonDiscriminationFairness",
    "- owl:physicality": "VirtualEntity",
    "- owl:role": "Process",
    "- owl:inferred-class": "aigo:VirtualProcess",
    "- belongsToDomain": "[[AIEthicsDomain]]",
    "- implementedInLayer": "[[ConceptualLayer]]"
  },
  "backlinks": [],
  "wiki_links": [
    "WCAG",
    "AIEthicsDomain",
    "ISO/IEC TR 24027",
    "EU AI Act",
    "EU Charter Article 21",
    "ConceptualLayer"
  ],
  "ontology": {
    "term_id": "AI-0413",
    "preferred_term": "Diversity, Non-Discrimination, and Fairness",
    "definition": "Diversity Non-Discrimination and Fairness is a trustworthiness dimension ensuring AI systems avoid unfair bias, ensure equitable treatment across demographic groups, implement accessibility and universal design, and enable inclusive stakeholder participation throughout development and deployment. This dimension encompasses three core components: unfair bias avoidance (identifying bias affecting protected characteristics including sex, racial or ethnic origin, religion, disability, age, and sexual orientation per EU Charter Article 21, implementing bias mitigation through pre-processing data corrections, in-processing fairness constraints, and post-processing prediction adjustments, and continuously monitoring fairness metrics including demographic parity requiring equal selection rates across groups, equalized odds ensuring equal true positive and false positive rates, equal opportunity guaranteeing equal true positive rates, and individual fairness treating similar individuals similarly), accessibility and universal design (complying with Web Content Accessibility Guidelines WCAG ensuring perceivable, operable, understandable, and robust interfaces, implementing European Accessibility Act requirements, and applying universal design principles creating systems usable by people with diverse abilities without specialized adaptation), and stakeholder participation (involving diverse stakeholders including end users, affected communities, domain experts, and civil society throughout development lifecycle, implementing participatory design methodologies enabling co-creation with affected populations, and ensuring representative development teams reflecting diversity of deployment contexts and user populations). Legal frameworks including the EU AI Act mandate high-risk systems implement data governance ensuring training, validation, and testing datasets are relevant, representative, accurate, complete, and free from errors, with potential biases identified and mitigated. The 2024-2025 period marked transition from voluntary fairness practices to legally mandated requirements with enforcement mechanisms across jurisdictions including EU AI Act penalties reaching EUR 35 million or 7% of worldwide annual turnover, U.S. state-level legislation including Colorado AI Act and New York City Bias Audit Law, and international standards including ISO/IEC TR 24027:2021 for bias detection and ISO/IEC 42001:2023 for AI risk management, with regulatory sandboxes enabling deliberate testing to expose unwanted bias before deployment.",
    "source_domain": "ai",
    "maturity_level": null,
    "authority_score": 0.95
  }
}