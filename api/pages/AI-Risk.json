{
  "title": "AI Risk",
  "content": "- ### OntologyBlock\n  id:: ai-risk-ontology\n  collapsed:: true\n\t- ontology:: true\n\t- term-id:: AI-0076\n\t- preferred-term:: AI Risk\n\t- source-domain:: ai\n\t- status:: draft\n\t- public-access:: true\n\t- definition:: The potential for AI systems to cause adverse effects on individuals, groups, organizations, communities, or society, arising from technical failures, security vulnerabilities, biased outcomes, privacy violations, or unintended consequences of system design, deployment, or operation.\n\t- maturity:: draft\n\t- owl:class:: mv:AIRisk\n\t- owl:physicality:: ConceptualEntity\n\t- owl:role:: Concept\n\t- belongsToDomain:: [[MetaverseDomain]]\n\n- ## Definition and Scope\n\nArtificial Intelligence Risk refers to the potential for AI systems to cause adverse effects on individuals, groups, organizations, communities, or society at large. These risks arise from technical failures, security vulnerabilities, biased outcomes, privacy violations, or unintended consequences during AI system design, deployment, or operation.\n\nThe academic foundations of AI risk management draw from computer science, ethics, cybersecurity, and social sciences, emphasizing interdisciplinary approaches to understand and mitigate harm. According to the NIST AI Risk Management Framework (AI RMF 1.0, January 2023), AI risk is defined as the composite measure of an event's probability of occurring and the magnitude or degree of the consequences of the corresponding event.\n\n- ## Formal Specification\n\n### Ontological Structure\n- AI Risk is a subclass of general Risk with specific characteristics unique to artificial intelligence systems\n- Core components include likelihood (probability of risk occurring), impact (severity of consequences), and context (domain and stakeholders affected)\n- Risk types encompass technical, ethical, social, legal, security, and operational categories\n- Standards alignment includes ISO/IEC 23894:2023, NIST AI RMF 1.0, EU AI Act (Regulation 2024/1689)\n\n### Risk Properties\n- AI risks require assessment of posesRiskTo, hasLikelihood, hasImpact, managedBy, and manifestsAs relationships\n- Risk scoring ranges from 0.0 (no risk) to 1.0 (critical risk) based on composite assessment\n- Risk levels categorized as unacceptable, high, limited, or minimal per EU AI Act framework\n\n- ## Authoritative References\n\n### Primary Standards and Frameworks\n\n#### ISO/IEC 23894:2023 - AI Risk Management\nPublished February 2023, this comprehensive international standard for [[AI Risk Management]] adapts traditional risk management practices (ISO 31000) to AI's unique characteristics including opacity, complexity, autonomy, and data dependency. The standard gained widespread adoption throughout 2024-2025 as organizations sought structured risk management methodologies. Implementation requires cross-functional collaboration between data scientists, security professionals, legal counsel, and business leaders.\n\n#### NIST AI Risk Management Framework (AI RMF 1.0)\nReleased January 2023, updated with Generative AI Profile (NIST-AI-600-1) on July 26, 2024. The Profile added over 200 specific actions addressing unique risks including CBRN information risks, confabulation (hallucinations), dangerous or hateful content generation, data privacy violations, information integrity issues (deepfakes), intellectual property infringement, and obscene content generation. This framework became essential for organizations deploying [[Large Language Models]], image generators, and multimodal systems.\n\n#### EU AI Act (Regulation 2024/1689)\nFinalized June 2024, entered into force August 1, 2024. The risk-based regulatory framework categorizes AI systems as unacceptable, high, limited, or minimal risk, establishing the global template for risk-proportionate regulation. **[Updated 2025]** Implementation proceeds on schedule with phased obligations. Prohibited practices became effective February 2, 2025. Major provisions including GPAI models, governance, and penalties apply from August 2, 2025. Full high-risk AI system requirements take effect August 2, 2026. Non-compliance penalties reach up to â‚¬35 million or 7% of worldwide annual turnover, whichever is higher.\n\n- ## See Also\n- [[AI Risk Management]]\n- [[NIST AI RMF]]\n- [[EU AI Act]]\n- [[ISO/IEC 23894]]\n- [[Algorithmic Bias]]\n- [[AI Ethics]]\n- [[AI Governance]]\n\n\n## Current Landscape (2025)\n\n- Industry adoption and implementations\n  - Metaverse platforms continue to evolve with focus on interoperability and open standards\n  - Web3 integration accelerating with decentralised identity and asset ownership\n  - Enterprise adoption growing in virtual collaboration, training, and digital twins\n  - UK companies increasingly active in metaverse development and immersive technologies\n\n- Technical capabilities\n  - Real-time rendering at photorealistic quality levels\n  - Low-latency networking enabling seamless multi-user experiences\n  - AI-driven content generation and procedural world building\n  - Spatial audio and haptics enhancing immersion\n\n- UK and North England context\n  - Manchester: Digital Innovation Factory supports metaverse startups and research\n  - Leeds: Holovis leads in immersive experiences for entertainment and training\n  - Newcastle: University research in spatial computing and interactive systems\n  - Sheffield: Advanced manufacturing using digital twin technology\n\n- Standards and frameworks\n  - Metaverse Standards Forum driving interoperability protocols\n  - WebXR enabling browser-based immersive experiences\n  - glTF and USD for 3D asset interchange\n  - Open Metaverse Interoperability Group defining cross-platform standards\n\n## Metadata\n\n- **Last Updated**: 2025-11-16\n- **Review Status**: Automated remediation with 2025 context\n- **Verification**: Academic sources verified\n- **Regional Context**: UK/North England where applicable",
  "properties": {
    "id": "ai-risk-ontology",
    "collapsed": "true",
    "- ontology": "true",
    "- term-id": "AI-0076",
    "- preferred-term": "AI Risk",
    "- source-domain": "ai",
    "- status": "draft",
    "- public-access": "true",
    "- definition": "The potential for AI systems to cause adverse effects on individuals, groups, organizations, communities, or society, arising from technical failures, security vulnerabilities, biased outcomes, privacy violations, or unintended consequences of system design, deployment, or operation.",
    "- maturity": "draft",
    "- owl:class": "mv:AIRisk",
    "- owl:physicality": "ConceptualEntity",
    "- owl:role": "Concept",
    "- belongsToDomain": "[[MetaverseDomain]]"
  },
  "backlinks": [],
  "wiki_links": [
    "AI Risk Management",
    "EU AI Act",
    "MetaverseDomain",
    "Large Language Models",
    "ISO/IEC 23894",
    "NIST AI RMF",
    "Algorithmic Bias",
    "AI Ethics",
    "AI Governance"
  ],
  "ontology": {
    "term_id": "AI-0076",
    "preferred_term": "AI Risk",
    "definition": "The potential for AI systems to cause adverse effects on individuals, groups, organizations, communities, or society, arising from technical failures, security vulnerabilities, biased outcomes, privacy violations, or unintended consequences of system design, deployment, or operation.",
    "source_domain": "ai",
    "maturity_level": null,
    "authority_score": null
  }
}