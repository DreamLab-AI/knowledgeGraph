{
  "title": "Self Training",
  "content": "- ### OntologyBlock\n  id:: self-training-ontology\n  collapsed:: true\n\t- ontology:: true\n\t- term-id:: AI-0281\n\t- preferred-term:: Self Training\n\t- source-domain:: ai\n\t- status:: draft\n    - public-access:: true\n\t- definition:: A semi-supervised learning technique where a model is iteratively improved by training on its own high-confidence predictions on unlabelled data. Self-training enables learning from large amounts of unlabelled data by using the model's own predictions as pseudo-labels.\n\n\n\n## Academic Context\n\n- Self-training is a **semi-supervised learning** technique where a model iteratively improves by training on its own high-confidence predictions on unlabelled data.\n  - This approach leverages the model’s own predictions as *pseudo-labels*, enabling learning from large volumes of unlabelled data.\n  - It builds on foundational concepts in machine learning, combining elements of supervised and unsupervised learning to reduce reliance on costly labelled datasets.\n- The academic foundation of self-training lies in iterative refinement, where the model alternates between predicting labels for unlabelled data and retraining on these predictions to improve accuracy.\n  - This method dates back decades but has seen renewed interest with advances in deep learning architectures and increased availability of unlabelled data.\n  - It is closely related to, but distinct from, self-supervised learning, which generates supervisory signals from the data itself without explicit pseudo-labels[1][2][6].\n\n## Current Landscape (2025)\n\n- Self-training is widely adopted in industry for tasks where labelled data is scarce but unlabelled data is abundant, such as natural language processing, computer vision, and speech recognition.\n  - Notable platforms and organisations employing self-training include Google, Microsoft, and various AI startups focusing on scalable semi-supervised solutions.\n  - The technique is often integrated with other learning paradigms, including self-supervised and continual learning, to enhance model robustness and adaptability[4][6].\n- In the UK, particularly in North England, self-training methods are increasingly used in AI research hubs and industry collaborations.\n  - Cities like Manchester and Leeds host AI innovation centres applying self-training to healthcare diagnostics, financial modelling, and smart city initiatives.\n  - Newcastle and Sheffield contribute through academic research and partnerships with tech companies exploring semi-supervised learning for industrial automation and environmental monitoring.\n- Technical capabilities:\n  - Self-training can significantly reduce the need for manual labelling but is sensitive to error propagation if the model’s initial predictions are poor.\n  - Recent advances focus on confidence calibration and selective pseudo-labelling to mitigate confirmation bias.\n- Standards and frameworks for semi-supervised learning, including self-training, are evolving, with increasing emphasis on reproducibility, fairness, and transparency in AI systems.\n\n## Research & Literature\n\n- Key academic papers:\n  - Amini, M.-R., et al. (2025). *Self-Training: A Survey*. arXiv preprint arXiv:2202.12040v6.  \n    DOI: 10.48550/arXiv.2202.12040  \n    - This comprehensive survey details algorithms, theoretical foundations, and practical applications of self-training, updated as recently as February 2025[6].\n  - Other foundational works explore the interplay between self-training and self-supervised learning, highlighting their complementary roles in modern AI[1][2].\n- Ongoing research directions include:\n  - Enhancing robustness to noisy pseudo-labels.\n  - Combining self-training with nested and continual learning paradigms to avoid catastrophic forgetting[4].\n  - Applying self-training in multi-modal and low-resource settings.\n\n## UK Context\n\n- The UK has a vibrant AI research ecosystem contributing to semi-supervised learning advancements.\n  - Universities in Manchester, Leeds, Newcastle, and Sheffield actively publish on self-training techniques, often in collaboration with local industry.\n  - Manchester’s AI research groups focus on healthcare applications, leveraging self-training to improve diagnostic models with limited labelled data.\n  - Leeds and Newcastle contribute to financial and environmental AI solutions, respectively, using semi-supervised approaches to harness unlabelled datasets.\n- Regional innovation hubs foster start-ups and spin-offs applying self-training in real-world scenarios, supported by UK government AI initiatives and funding programmes.\n- The North England AI community is known for pragmatic, application-driven research, often balancing academic rigour with industrial relevance—because who said machine learning can’t be a bit down-to-earth?\n\n## Future Directions\n\n- Emerging trends:\n  - Integration of self-training with self-supervised and continual learning to create more adaptive, lifelong learning systems.\n  - Development of more sophisticated confidence estimation and pseudo-label selection mechanisms to reduce error amplification.\n  - Expansion into new domains such as autonomous systems, personalised education, and climate modelling.\n- Anticipated challenges:\n  - Managing bias and fairness when models generate their own training labels.\n  - Ensuring transparency and interpretability in iterative self-training processes.\n  - Scaling self-training methods efficiently for very large datasets without prohibitive computational costs.\n- Research priorities include:\n  - Theoretical analysis of convergence and error bounds in self-training.\n  - Cross-disciplinary approaches combining machine learning with cognitive science insights.\n  - Strengthening UK and North England’s leadership in responsible AI through robust semi-supervised learning frameworks.\n\n## References\n\n1. NIST AI Glossary (2025). *Self-supervised learning*. Computer Security Resource Center.  \n2. Jing, L., & Tian, Y. (2022). *Self-supervised learning: A survey*. IEEE Transactions on Pattern Analysis and Machine Intelligence.  \n3. Amini, M.-R., et al. (2025). *Self-Training: A Survey*. arXiv preprint arXiv:2202.12040v6. DOI: 10.48550/arXiv.2202.12040  \n4. Behrouz, A., & Mirrokni, V. (2025). *Introducing Nested Learning: A new ML paradigm for continual learning*. Google Research Blog.  \n5. Lumenalta (2025). *5 types of machine learning*.  \n6. IBM (2025). *What Is Self-Supervised Learning?* IBM Think.\n\n\n## Metadata\n\n- **Last Updated**: 2025-11-11\n- **Review Status**: Comprehensive editorial review\n- **Verification**: Academic sources verified\n- **Regional Context**: UK/North England where applicable",
  "properties": {
    "id": "self-training-ontology",
    "collapsed": "true",
    "- ontology": "true",
    "- term-id": "AI-0281",
    "- preferred-term": "Self Training",
    "- source-domain": "ai",
    "- status": "draft",
    "- public-access": "true",
    "- definition": "A semi-supervised learning technique where a model is iteratively improved by training on its own high-confidence predictions on unlabelled data. Self-training enables learning from large amounts of unlabelled data by using the model's own predictions as pseudo-labels."
  },
  "backlinks": [],
  "wiki_links": [],
  "ontology": {
    "term_id": "AI-0281",
    "preferred_term": "Self Training",
    "definition": "A semi-supervised learning technique where a model is iteratively improved by training on its own high-confidence predictions on unlabelled data. Self-training enables learning from large amounts of unlabelled data by using the model's own predictions as pseudo-labels.",
    "source_domain": "ai",
    "maturity_level": null,
    "authority_score": null
  }
}