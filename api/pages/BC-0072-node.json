{
  "title": "Node",
  "content": "- ### OntologyBlock\n  id:: node-ontology\n  collapsed:: true\n\n  - **Identification**\n    - ontology:: true\n    - term-id:: BC-0072\n    - preferred-term:: Node\n    - source-domain:: blockchain\n    - status:: complete\n    - public-access:: true\n    - version:: 1.0.0\n    - last-updated:: 2025-10-28\n\n  - **Definition**\n    - definition:: Network participant computer within blockchain systems, providing essential functionality for distributed ledger technology operations and properties.\n    - maturity:: mature\n    - source:: [[ISO/IEC 23257:2021]], [[IEEE 2418.1]], [[NIST NISTIR]]\n    - authority-score:: 0.95\n\n  - **Semantic Classification**\n    - owl:class:: bc:Node\n    - owl:physicality:: VirtualEntity\n    - owl:role:: Object\n    - owl:inferred-class:: bc:VirtualObject\n    - belongsToDomain:: [[CryptographicDomain]]\n    - implementedInLayer:: [[SecurityLayer]]\n\n  - #### Relationships\n    id:: node-relationships\n    - is-subclass-of:: [[Blockchain Entity]], [[NetworkComponent]]\n\n  - #### OWL Axioms\n    id:: node-owl-axioms\n    collapsed:: true\n    - ```clojure\n      Prefix(:=<http://metaverse-ontology.org/blockchain#>)\nPrefix(owl:=<http://www.w3.org/2002/07/owl#>)\nPrefix(rdf:=<http://www.w3.org/1999/02/22-rdf-syntax-ns#>)\nPrefix(xml:=<http://www.w3.org/XML/1998/namespace>)\nPrefix(xsd:=<http://www.w3.org/2001/XMLSchema#>)\nPrefix(rdfs:=<http://www.w3.org/2000/01/rdf-schema#>)\nPrefix(dct:=<http://purl.org/dc/terms/>)\n\nOntology(<http://metaverse-ontology.org/blockchain/BC-0072>\n  Import(<http://metaverse-ontology.org/blockchain/core>)\n\n  ## Class Declaration\n  Declaration(Class(:Node))\n\n  ## Subclass Relationships\n  SubClassOf(:Node :NetworkComponent)\n  SubClassOf(:Node :BlockchainEntity)\n\n  ## Essential Properties\n  SubClassOf(:Node\n    (ObjectSomeValuesFrom :partOf :Blockchain))\n\n  SubClassOf(:Node\n    (ObjectSomeValuesFrom :hasProperty :Property))\n\n  ## Data Properties\n  DataPropertyAssertion(:hasIdentifier :Node \"BC-0072\"^^xsd:string)\n  DataPropertyAssertion(:hasAuthorityScore :Node \"1.0\"^^xsd:decimal)\n  DataPropertyAssertion(:isFoundational :Node \"true\"^^xsd:boolean)\n\n  ## Object Properties\n  ObjectPropertyAssertion(:enablesFeature :Node :BlockchainFeature)\n  ObjectPropertyAssertion(:relatesTo :Node :RelatedConcept)\n\n  ## Annotations\n  AnnotationAssertion(rdfs:label :Node \"Node\"@en)\n  AnnotationAssertion(rdfs:comment :Node\n    \"Network participant computer\"@en)\n  AnnotationAssertion(dct:description :Node\n    \"Foundational blockchain concept with formal ontological definition\"@en)\n  AnnotationAssertion(:termID :Node \"BC-0072\")\n  AnnotationAssertion(:priority :Node \"1\"^^xsd:integer)\n  AnnotationAssertion(:category :Node \"network-security\"@en)\n)\n      ```\n\n- ## About Node\n  id:: node-about\n\n  - Network participant computer within blockchain systems, providing essential functionality for distributed ledger technology operations and properties.\n  -\n  - ### Key Characteristics\n    id:: node-characteristics\n    - 1. **Definitional Property**: Core defining characteristic\n    - 2. **Functional Property**: Operational behavior\n    - 3. **Structural Property**: Compositional elements\n    - 4. **Security Property**: Security guarantees provided\n    - 5. **Performance Property**: Efficiency considerations\n  -\n  - ### Technical Components\n    id:: node-components\n    - **Implementation**: How concept is realized technically\n    - **Verification**: Methods for validating correctness\n    - **Interaction**: Relationships with other components\n    - **Constraints**: Technical limitations and requirements\n  -\n  - ### Use Cases\n    id:: node-use-cases\n    - **1. Core Blockchain Operation**\n    - **Application**: Fundamental blockchain functionality\n    - **Example**: Practical implementation in major blockchains\n    - **Requirements**: Technical prerequisites\n    - **Benefits**: Value provided to blockchain systems\n  -\n  - ### 2024-2025: Node Infrastructure Evolution\n    id:: node-recent-developments\n\n    The blockchain node ecosystem experienced significant maturation from 2024 through 2025, with focus shifting from pure decentralisation metrics to practical operational infrastructure supporting Lightning Network payments, L402 micropayments for AI services, and integrated identity systems.\n\n    #### Bitcoin Node Landscape\n    The Bitcoin network maintained approximately **15,000 publicly-reachable nodes** distributed globally, representing a triumvirate of economic actors: holders/users trading and speculating; miners securing the network through Proof of Work validation; and node operators enforcing consensus rules and trust minimisation. Since IT engineer Stadicus released the RaspiBolt guide in 2017, small-scale Bitcoin and Lightning node operators proliferated—around 30,000 Raspberry Pi Lightning nodes (which are also by definition Bitcoin nodes) ran open-source distributions, democratising network participation.\n\n    #### Lightning Network Infrastructure\n    Lightning node management software matured substantially, with tools like **Torq** providing workflow automation for large Lightning Network nodes, enabling institutional-grade operations. **L402** emerged as a critical innovation: a Lightning-native reverse proxy standard enabling micropayment-gated API access, particularly relevant for AI services requiring per-query payment without traditional subscription models. This infrastructure positioned Lightning as the native payment rail for machine-to-machine commerce.\n\n    #### Bitcoin-Dollar Interoperability\n    **Stablesats** and similar technologies introduced algorithmic dollar-stable value using exclusively Lightning Bitcoin, providing price stability without centralised stablecoin issuers. More significantly, Strike's opening of public Lightning network access enabled individuals with self-hosted wallets or nodes to pay directly for goods across America, with instant settlement in dollars using Bitcoin at zero cost. This bridged the Lightning network with dollar-denominated metaverse stores, creating unprecedented interoperability between cryptocurrency infrastructure and traditional commerce.\n\n    #### Distributed Identity and Data\n    Emerging protocols integrated with Bitcoin's node infrastructure: **Nostr** provided decentralised identity and messaging layers, whilst **SOLID-lite data pods** enabled private data storage with cryptographic access control. These systems layered on top of Bitcoin's settlement layer, creating a full-stack decentralised internet infrastructure where nodes served not just financial settlement but also identity, communication, and data storage—a vision of truly peer-to-peer digital infrastructure.\n\n    The Cleveland Federal Reserve published analysis confirming Lightning Network's potential to transform Bitcoin from speculative asset to functional money, validating technical infrastructure developments with academic economic analysis. By late 2025, node operation transitioned from ideological hobby to practical infrastructure requirement for individuals and businesses seeking financial sovereignty and censorship-resistant commerce capabilities.\n\n    #### Institutional Node Infrastructure and the Centralisation Paradox\n\n    Whilst retail Bitcoin node operators democratised network participation through Raspberry Pi deployments, the broader multi-chain blockchain ecosystem witnessed **dramatic consolidation** of node infrastructure into institutional providers servicing hundreds of thousands of developers and applications:\n\n    **Dominant Institutional Providers:**\n    - **Alchemy**: Emerged as the preeminent blockchain infrastructure provider with support for Ethereum, Polygon, Solana, Arbitrum, Optimism, and 15+ chains, powering some of the largest DeFi protocols, NFT marketplaces, and Web3 applications. Alchemy's robust APIs, exceptional developer experience, and enterprise-grade reliability (99.9% uptime SLAs) attracted major projects including OpenSea, Axie Infinity, and numerous institutional financial applications. By 2025, Alchemy served over **500,000 developers**, representing an estimated **70-80% of Ethereum dApp traffic** routing through Alchemy infrastructure\n\n    - **Infura** (ConsenSys subsidiary): Operated as the second-largest Ethereum Virtual Machine (EVM)-compatible blockchain infrastructure provider, supporting Ethereum, Polygon, Arbitrum, Optimism, and other EVM chains. With over **400,000 developers** registered on the platform by 2025, Infura provided the critical RPC (remote procedure call) infrastructure enabling wallets (MetaMask), block explorers (Etherscan), and dApps to interact with blockchains without operating full nodes. Infura's integration with **MetaMask**—the dominant Web3 wallet with 30+ million monthly active users—created **single point of failure** concerns: when Infura experienced outages (March 2020, November 2020, January 2023), vast swaths of Ethereum's user-facing applications became temporarily inaccessible\n\n    - **Blockdaemon**: Focused on **institutional-grade blockchain infrastructure** and validator-as-a-service, trusted by over **300 financial institutions** and managing **100,000+ nodes** across 50+ blockchain networks by 2025. Blockdaemon's enterprise positioning—offering dedicated infrastructure, compliance reporting, and institutional SLAs—attracted banks, asset managers, and hedge funds deploying blockchain strategies. Major clients included Deutsche Bank, BNY Mellon, and Citi, signalling traditional finance's reliance on third-party node infrastructure rather than self-hosted infrastructure\n\n    - **InfStones**: Specialised in **institutional staking infrastructure** managing over **$10 billion** in staked assets across Ethereum, Solana, Polkadot, Cosmos, and 30+ proof-of-stake networks by 2025. InfStones provided white-label staking services enabling exchanges (Crypto.com, OKX) and custodians to offer staking yields to customers without operating validator infrastructure internally\n\n    - **Figment**: Advanced staking provider supporting **40+ blockchain networks**, managing over **$15 billion** in staked assets and serving institutional clients including asset managers, hedge funds, and crypto-native companies. Figment's institutional focus and compliance capabilities (SOC 2 Type II audits, GDPR compliance) positioned it as the preferred provider for regulated entities entering staking markets\n\n    **Centralisation Concerns and Decentralisation Theatre:**\n\n    The consolidation of node infrastructure into handful of institutional providers created **fundamental tensions** with blockchain's decentralisation ethos:\n\n    - **RPC centralisation**: Despite Ethereum's **400,000+ validator nodes** (post-Merge) creating robust consensus decentralisation, the vast majority of users and applications accessed the network through **centralised RPC providers** (Alchemy, Infura, QuickNode). This created a **centralisation paradox**: consensus was decentralised, but **access** was centralised, enabling potential censorship at the infrastructure layer (RPC providers could theoretically filter transactions, censor addresses, or deny service to specific applications)\n\n    - **Validator centralisation risks**: In proof-of-stake networks, institutional staking providers (Coinbase, Figment, InfStones, Blockdaemon) controlling **30-40% of total staked assets** across multiple networks created **single-entity risks**: if major provider experienced technical failure, compromise, or regulatory pressure (e.g., government order to censor transactions), significant portion of network's validation capacity could be affected simultaneously\n\n    - **Geographic concentration**: Major node providers operated data centres predominantly in **United States and Europe** (AWS, Google Cloud, Microsoft Azure regions), creating geographic centralisation where **50-60% of blockchain validator nodes** ran in U.S.-controlled cloud infrastructure by 2025. This exposed networks to **jurisdictional risk**: U.S. regulatory actions could potentially affect majority of validation infrastructure simultaneously\n\n    - **Cloud infrastructure dependency**: Estimated **60-70% of Ethereum validators** ran on **Amazon Web Services (AWS)** infrastructure, **15-20% on Google Cloud**, and **5-10% on Microsoft Azure**, with only approximately **10-15% on dedicated/bare-metal servers**. This created **catastrophic failure scenarios**: AWS outages (December 2021, December 2022) caused widespread disruptions to blockchain networks despite consensus layer remaining operational—users and applications couldn't access networks due to infrastructure-layer failures\n\n    **Regulatory Compliance and Censorship Risks:**\n\n    The August 2022 **Tornado Cash sanctions** by U.S. Office of Foreign Assets Control (OFAC) exposed infrastructure-layer censorship vulnerabilities: following sanctions, major RPC providers (Alchemy, Infura, QuickNode) **blocked transactions** interacting with Tornado Cash smart contracts, demonstrating that centralised infrastructure created de facto censorship capability even on ostensibly censorship-resistant blockchains.\n\n    By 2024-2025, concern intensified around **MEV-Boost relays** and **block building centralisation**: over **90% of Ethereum blocks** were built by small number of specialised block builders (Flashbots, BloXroute, Eden Network) rather than validators directly, creating **transaction ordering centralisation** and potential censorship points. Following Tornado Cash sanctions, some block builders implemented **OFAC compliance filtering**, excluding sanctioned addresses from blocks—though other builders maintained censorship resistance, creating a **two-tier block building market**.\n\n    **Self-Hosted Node Renaissance and Solutions:**\n\n    Awareness of infrastructure centralisation risks catalysed **self-hosted node movement** through 2024-2025:\n\n    - **Home staking initiatives**: Ethereum Foundation's promotion of **home staking** as \"decentralisation ideal\" encouraged individuals to run validators on consumer hardware (NUC, custom builds, Raspberry Pi 4+ with external SSD). By 2025, an estimated **15-20% of Ethereum validators** ran on home hardware vs. cloud/institutional infrastructure, up from approximately **10-12% in 2022**\n\n    - **Node-in-a-box products**: Plug-and-play node solutions (DAppNode, Avado, RaspiBlitz, Umbrel) reduced technical barriers for self-hosting blockchain nodes, enabling non-technical users to operate full nodes and validators with minimal configuration. These products integrated Bitcoin full nodes, Lightning nodes, Ethereum validators, and IPFS nodes into unified interfaces\n\n    - **Decentralised RPC alternatives**: Protocols like **Pocket Network** (decentralised RPC marketplace), **Ankr** (decentralised node infrastructure), and **The Graph** (decentralised indexing) emerged as alternatives to centralised providers, incentivising distributed node operators through token rewards. By 2025, Pocket Network supported **60+ blockchains** with **70,000+ distributed nodes**, though still represented <5% of total RPC traffic compared to Alchemy/Infura dominance\n\n    - **Client diversity improvements**: Ethereum's push for **client diversity** (running multiple node implementations—Geth, Nethermind, Besu, Erigon—to reduce single-client bugs affecting network) achieved **45-50% Geth dominance** by 2025, down from **70-75% in 2021**, reducing single-implementation risks though still below desired <33% per-client threshold\n\n    **Future Trajectory: Decentralisation Resurgence or Permanent Centralisation?**\n\n    By mid-2025, blockchain node infrastructure existed in a **bifurcated state**: Bitcoin maintained relatively robust node decentralisation (15,000+ public nodes, 30,000+ Lightning nodes operated by individuals), whilst multi-chain ecosystems (Ethereum, Polygon, Solana, Cosmos) exhibited **consensus decentralisation** but **infrastructure centralisation**, with majority of user/developer access mediated by handful of institutional providers.\n\n    Potential trajectories included:\n\n    - **Permanent infrastructure centralisation**: Developer convenience and institutional reliability requirements entrench Alchemy/Infura/Blockdaemon oligopoly, with decentralised consensus layers masked by centralised access layers—\"decentralisation theatre\" where networks are theoretically censorship-resistant but practically vulnerable to infrastructure-layer control\n\n    - **Decentralised infrastructure maturation**: Protocols like Pocket Network, Ankr, and client-side light clients (EIP-4844 enabling trustless light clients) mature to provide comparable performance/reliability to centralised providers, gradually redistributing infrastructure market share to distributed operators and reducing single-entity risks\n\n    - **Regulatory forcing function**: Government intervention targeting major providers (subpoenas for transaction data, mandatory censorship, service bans) creates **regulatory necessity** for decentralised alternatives, accelerating adoption of self-hosted and distributed infrastructure solutions\n\n    The 2024-2025 period crystallised blockchain node infrastructure's **fundamental tension**: technical decentralisation without infrastructure decentralisation created systems that were **theoretically censorship-resistant** yet **practically vulnerable** to centralised control points—a tension likely to define security and resilience debates for the remainder of the decade.\n  -\n  - ### Standards & References\n    id:: node-standards\n    - [[ISO/IEC 23257:2021]] - Blockchain and distributed ledger technologies\n    - [[IEEE 2418.1]] - Blockchain and distributed ledger technologies\n    - [[NIST NISTIR]] - Blockchain and distributed ledger technologies\n  -\n\n- # Lightning\n\t- [Setup lnbits and lightningtipbot](https://www.massmux.com/howto-complete-lightningtipbot-lnbits-setup-vps/)\n\t- [GitHub - ln-vortex/ln-vortex: Lightning and Taproot enabled collaborative transactions (other)](https://github.com/ln-vortex/ln-vortex)\n\t- [This is a node management software for large Lightning Network nodes. It provides a way to automate workflows, manage code changes, and track work progress.](https://github.com/lncapital/torq)\n\t- L402 lightning reverse proxy with LND for AI\n\t\t- [docs](https://docs.lightning.engineering/the-lightning-network/l402)\n\t- [Cleveland bank paper on lighting improving Bitcoin](https://www.clevelandfed.org/en/publications/working-paper/2022/wp-2219-the-lightning-network-turning-bitcoin-into-money)\n\n\t- ##### Bitcoin based FIAT\n\t\t- More interestingly for metaverse applications Mallers has opened this section of the company to interact with the public Lightning network, allowing people with a self hosted wallet or node to pay directly for goods across America, settling immediately in Dollars, using their Bitcoin, at zero cost. **This opens the possibility to buy from US based(Dollar denominated) metaverse stores, using the capabilities of the stack assembled at the end of the book**. The implications globally are unclear at this time.\n\t\t- [Stablesats](https://stablesats.com/) is another approach which uses exclusively lightning bitcoin but makes the value stable against the US dollar using an algorithm. This is a very interesting option and will be explored in detail at some point.\n\n\t\t- ## Bitcoin nodes\n\t - The Bitcoin network can be considered a triumvirate of economic actors,each with different incentives. These are:\n\t\t\t\t- Holders and users of the tokens, including exchanges and market makers, who make money speculating, [arbitraging](https://en.wikipedia.org/wiki/Arbitrage), and providing liquidity into the network. Increasingly this is also real ‘money’ users of BTC, earning and spending in pools of circular economic activity. Perversely Bitcoin as a money is the fringe use case at this time.\n\t\t\t\t- Miners, who profit from creation of new UTXOs, and receive payments for adding transactions to the chain. In return they secure the network by validating the other miners blocks according the rules enforced by the node operators.\n\t\t\t\t- Node operators, [who enforce the consensus](https://www.truthcoin.info/blog/measuring-decentralization/) rule-set, which the miners must abide by in order to propagate new transaction into the network. In return node operators optimise their trust minimisation, and help protect the network from changes which might undermine their speculation and use of the tokens.[[blocksizewars]]\n\t\t\t\t- There are currently around [15,000 Bitcoin nodes](https://bitnodes.io/)distributed across the world. Since IT engineer[Stadicus](https://stadicus.com/) released his [Raspiboltguide](https://raspibolt.org/backstory.html) in 2017 there has been anexplosion of small scale Bitcoin and Lightning node operators. Aroundthirty thousand Raspberry Pi Lightning nodes (which are also bydefinition Bitcoin nodes) run one of a big selection of [open sourcedistributions](https://github.com/bavarianledger/bitcoin-nodes). We willbuild toward our own throughout the book.\n\n- # Lightning\n\t- [Setup lnbits and lightningtipbot](https://www.massmux.com/howto-complete-lightningtipbot-lnbits-setup-vps/)\n\t- [GitHub - ln-vortex/ln-vortex: Lightning and Taproot enabled collaborative transactions (other)](https://github.com/ln-vortex/ln-vortex)\n\t- [This is a node management software for large Lightning Network nodes. It provides a way to automate workflows, manage code changes, and track work progress.](https://github.com/lncapital/torq)\n\t- L402 lightning reverse proxy with LND for AI\n\t\t- [docs](https://docs.lightning.engineering/the-lightning-network/l402)\n\t- [Cleveland bank paper on lighting improving Bitcoin](https://www.clevelandfed.org/en/publications/working-paper/2022/wp-2219-the-lightning-network-turning-bitcoin-into-money)\n\n\t\t- ### Segmentation\n\t\t\t- ![Segmentation for fashion](https://raw.githubusercontent.com/cozymantis/human-parser-comfyui-node/main/assets/lipexample.png)\n\t\t\t-\n\n- # Pre event buildout notes (here be dragons)\n  collapsed:: true\n\t- DONE Infrastructure build\n\t\t- DONE Get Ollama bridge working\n\t\t  collapsed:: true\n\t\t\t- [stavsap/comfyui-ollama (github.com)](https://github.com/stavsap/comfyui-ollama)\n\t\t\t- [MinusZoneAI/ComfyUI-Prompt-MZ: 基于llama.cpp的一些和提示词相关的节点，目前包括美化提示词和类似clip-interrogator的图片反推 | Use llama.cpp to assist in generating some nodes related to prompt words, including beautifying prompt words and image recognition similar to clip-interrogator (github.com)](https://github.com/MinusZoneAI/ComfyUI-Prompt-MZ)\n\t\t\t- [xXAdonesXx/NodeGPT: ComfyUI Extension Nodes for Automated Text Generation. (github.com)](https://github.com/xXAdonesXx/NodeGPT)\n\t\t\t- [stavsap/comfyui-ollama (github.com)](https://github.com/stavsap/comfyui-ollama)\n\t\t\t  id:: 6633f4c0-358f-44cf-bf05-d43c75febe36\n\t\t- DONE Backup the working docker\n\t\t- DONE sort the vpn and port forwarding\n\t\t- DONE Check the security\n\t\t- DONE Install the rest of the feature set\n\t\t- DONE Sort the models and Loras\n\t\t- DONE Fire up 3 instances\n\t\t\t- DONE TripoSR (no point, feature dropped)\n\t\t\t- DONE [Zero123](https://github.com/SUDO-AI-3D/zero123plus) (no point, feature dropped\n\t\t\t- DONE CRM\n\t\t- DONE [yisol/IDM-VTON: IDM-VTON : Improving Diffusion Models for Authentic Virtual Try-on in the Wild (github.com)](https://github.com/yisol/IDM-VTON)\n\t\t  collapsed:: true\n\t\t\t- [TemryL/ComfyUI-IDM-VTON: ComfyUI adaptation of IDM-VTON for virtual try-on. (github.com)](https://github.com/TemryL/ComfyUI-IDM-VTON)\n\t\t- TODO [Lllava 8b?](https://huggingface.co/collections/xtuner/llava-llama-3-8b-662a5f95adbe8d58799d7fdb) for descriptions\n\t\t  :LOGBOOK:\n\t\t  CLOCK: [2024-05-06 Mon 09:23:58]--[2024-05-06 Mon 09:23:58] =>  00:00:00\n\t\t  CLOCK: [2024-05-06 Mon 09:23:59]--[2024-05-06 Mon 09:24:00] =>  00:00:01\n\t\t  :END:\n\t\t- DONE Face swap\n\t\t- DONE NSFW filter\n\t\t- DONE Annotations and instructions\n\t\t  :LOGBOOK:\n\t\t  CLOCK: [2024-05-06 Mon 09:25:33]--[2024-05-06 Mon 12:16:24] =>  02:50:51\n\t\t  :END:\n\t\t- DONE Send to Pete to test\n\t\t- DONE Presentation outlines?\n\t\t- DONE Talk to Marco\n\t\t- DONE Next need to fix insightface [as here](https://github.com/cubiq/ComfyUI_IPAdapter_plus/issues/162) but\n\t\t\t- DONE backup first\n\t\t\t  :LOGBOOK:\n\t\t\t  CLOCK: [2024-05-06 Mon 10:23:31]--[2024-05-06 Mon 10:23:31] =>  00:00:00\n\t\t\t  :END:\n\t\t- TODO New nets and workflows? Pete?\n\t\t- DONE Confirm the compute arriving.\n\t\t- DONE Confirm the TV in time?\n\t\t- DONE Fix the windows laptop for delegates\n\t\t- DONE Condition the mac for delegates (come in monday afternoon)\n\t\t- DONE Charge the Rundiffusion account (talking to Tony this afternoon).\n\t\t- DONE Catering\n\t\t- DONE Talk to Marco\n\t\t- DONE Make a presentation for the day (logseq based for me)\n\t\t  :LOGBOOK:\n\t\t  CLOCK: [2024-05-12 Sun 16:41:41]--[2024-05-14 Tue 22:17:47] =>  53:36:06\n\t\t  :END:\n\t\t- DONE Delegate advance communications\n\t\t  :LOGBOOK:\n\t\t  CLOCK: [2024-05-11 Sat 19:58:36]--[2024-05-12 Sun 16:41:31] =>  20:42:55\n\t\t  CLOCK: [2024-05-12 Sun 16:41:35]--[2024-05-14 Tue 22:17:50] =>  53:36:15\n\t\t  :END:\n\npublic:: true\n\n- #Public page automatically published\n-\n- It's not the easiest way to use Stable Diffusion.\n- https://private-user-images.githubusercontent.com/140084057/368715655-f46f769d-f168-454c-9c7b-ed8bcd727c1d.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MzEwNjY1NTEsIm5iZiI6MTczMTA2NjI1MSwicGF0aCI6Ii8xNDAwODQwNTcvMzY4NzE1NjU1LWY0NmY3NjlkLWYxNjgtNDU0Yy05YzdiLWVkOGJjZDcyN2MxZC5wbmc_WC1BbXotQWxnb3JpdGhtPUFXUzQtSE1BQy1TSEEyNTYmWC1BbXotQ3JlZGVudGlhbD1BS0lBVkNPRFlMU0E1M1BRSzRaQSUyRjIwMjQxMTA4JTJGdXMtZWFzdC0xJTJGczMlMkZhd3M0X3JlcXVlc3QmWC1BbXotRGF0ZT0yMDI0MTEwOFQxMTQ0MTFaJlgtQW16LUV4cGlyZXM9MzAwJlgtQW16LVNpZ25hdHVyZT00MjQzYjkyNDU2MjNjOTRhYjgyM2JkN2E1YzQxMDcxMDAwNTQ5ZjYzZThhMjBhYjM4NGZhYzhiYzg0ZTNhNDYzJlgtQW16LVNpZ25lZEhlYWRlcnM9aG9zdCJ9.eOij_sfuzFqJLnC0tcT-OYnMTJF30B_mnj2xM3qjG-Y\n-\n- The [GitHub repository](https://github.com/ZHO-ZHO-ZHO/[[ComfyUI]]-[[Gemini]]) integrates Gemini into ComfyUI, offering models like Gemini-pro, Genimi-pro-vision, and Gemini 1.5 Pro for text, image, and file processing tasks. Users can apply for their own API Key to access Gemini API. The repository provides workflow examples, installation instructions, and updates. Contact information includes zhozho3965@gmail.com and a QQ Group (839821928). Social media links to '-Zho-', Bilibili, Twitter, Little Red Book, and support on Bilibili and Aifadian. Credits to ComfyUI_Custom_Nodes_AlekPet. - Users need to apply for a Gemini_API_Key to use Gemini nodes, ensure a stable connection to [[Google]] Gemini's services, and update the dependency 'google-generativeai' to version 0.4.1 for Gemini 1.5 Pro. Installation can be done using ComfyUI Manager or manually by cloning the repository from GitHub and installing requirements. Workflow versions include V3.0 with Gemini 1.5 Pro, V2.0 with a chatbot workflow, and V1.1 with workflows for Gemini-pro and Genimi-pro-vision. Updates include Version 3.0 adding Gemini 1.5 Pro, system instructions, and file uploads, Version 2.1 fixing a bug, and Version 2.0 adding context chat nodes. - The [status history](https://api.status-histroy.com/svg?repos=ZHO-ZHO-ZHO/ConfyUI-Gemini&type=Timeline) for the repository is available.\n- The [video](https://www.youtube.com/watch?v=fFdYdTzq7Kg) titled 'Expanding Horizons: Outpainting Mastery in [[ComfyUI]]' explores the artistry of Outpainting with ComfyUI's [[Stable Diffusion]] feature. The video delves into Hyper Expansion, Sketch to Render, and Auto Background Regeneration within the realm of Outpainting. Topics discussed include the use of LORAs, Quick Basic Outpainting, and various techniques for Outpainting with ComfyUI.\n- The [Aerial view of the building](https://civitai.com/models/121728/aerial-view-of-the-building) showcases a [[LoRA DoRA etc]] model for urban bird's-eye views, offering high-definition training sets for cityscapes and buildings. The model, based on SD 1.5, has received very positive reviews and was last updated on Aug 3, 2023.\n- The [GitHub repository](https://github.com/comfyanonymous/[[ComfyUI]]) showcases ComfyUI, a powerful and modular [[Stable Diffusion]] GUI, API, and backend with a graph/nodes interface. The interface allows users to design and execute advanced stable diffusion pipelines without needing to code. ComfyUI supports various features like SD1.x, SD2.x, SDXL, Stable Video Diffusion, and Stable Cascade. Users can - back_hashcash-denial_2002 - - [[experiment]] with complex workflows, embeddings/textual inversion, Loras, hypernetworks, and more. The repository provides detailed installation instructions for Windows, Linux, AMD GPUs, NVIDIA GPUs, Intel GPUs, [[Apple]] Mac silicon, and DirectML for AMD cards on Windows. Additionally, it offers shortcuts for workflow management, high-quality previews, TLS/SSL setup, and support channels for users. The repository is licensed under GPL-3.0 and has garnered 35.3k stars and 3.8k forks.\n- The [video](https://www.youtube.com/watch?v=DROM8vfIYUY) titled 'How 2 Canvas Node' is available on YouTube. The video duration is 14 minutes and 22 seconds.\n- The [video](https://www.youtube.com/watch?v=AU8NDSBIS1U) showcases a plugin that provides realtime AI assistance to - ### OntologyBlock id:: krita-ontology collapsed:: true - ontology:: true - term-id:: mv-523525042165 - [[krita]], a digital painting software. The channel, Nerdy Rodent, offers tutorials on [[Stable Diffusion]], Generative AI, [[Large language models]], and other AI tools, catering to AI enthusiasts and professionals. The plugin enhances various aspects of AI technology, such as voice cloning, text-to-speech, and style transfer, making it a valuable resource for AI enthusiasts and artists alike.\n- The [video](https://www.youtube.com/watch?v=reimr3jZ8lI) titled '[[ComfyUI]] Fundamentals - [[Upscaling]] 1' explores the concept of upscaling in the context of ComfyUI. The video delves into how upscaling works and provides tips to enhance the upscaling process. The presenter discusses the ability to go under 1 on the upscale by node, highlighting key aspects of image upscaling and various upscaling options.\n- The [YouTube video](https://www.youtube.com/watch?v=ppE1W0-LJas) titled '[[ComfyUI]] Modular- Ultimate Starter Workflow Usage' provides insights into using ComfyUI Modular for an ultimate starter workflow.\n- The [YouTube](https://www.youtube.com/@sedetweiler) page provides information about the use of cookies and data for various purposes, including [[Google]] services, audience engagement, and site statistics. By accepting all, users allow for the development of new services, displaying ads, and showing personalised content and ads based on their settings. Non-personalised content and ads are influenced by factors like the viewed content and location. Users can manage privacy settings and find more information by selecting 'Meer opties'.\n- The [GitHub repository](https://github.com/phineas-pta/comfy-trt-test) is an attempt to use TensorRT with [[ComfyUI]], focusing on specific models and workflows. The repository provides instructions for installation and usage, highlighting compatible models like [[Stable Diffusion]] and SDXL. It also mentions limitations and future improvements for user-friendliness and compatibility with ComfyUI-Manager. The repository is a ComfyUI port from the official A1111 extension, with potential maintenance issues addressed by an alternative repository. The README outlines conversion scripts, dependencies, and error messages encountered during the process.\n- The [GitHub repository](https://github.com/gameltb/ComfyUI_stable_fast) showcases experimental usage of stable-fast and TensorRT. It includes speed tests, installation instructions, and features of stable-fast and TensorRT.\n\t- The repository provides guidance on enabling stable-fast nodes and installing TensorRT for testing purposes.\n\t- It highlights the compatibility of stable-fast with [[LoRA DoRA etc]], ControlNet, and other models, along with speed optimizations and node support.\n\t- Speed tests on a GeForce RTX 3060 Mobile show performance metrics for stable-fast and TensorRT implementations.\n\t- The repository also includes a table detailing features, tested nodes, and performance benchmarks for different workflows.\n- The [GitHub repository](https://github.com/Acly/[[krita]]-ai-diffusion?tab=readme-ov-file) provides a streamlined interface for generating images with AI in Krita. Users can inpaint and outpaint with optional text prompts without the need for tweaking. The plugin allows for creating new images from scratch, refining existing content, and live painting. It supports various resolutions, job queues, and history tracking. Customization options are available for advanced users. The technology used includes [[Stable Diffusion]] for image generation, [[ComfyUI]] for the diffusion backend, ControlNet for [[Inpainting]], and IP-Adapter for outpainting. The repository is licensed under GPL-3.0 and has garnered 4.8k stars and 216 forks. Contributors have added features like object selection tools and GPU cloud support.\n- The [[ComfyUI]] [models](https://civitai.com/tag/comfyui) tag on Civitai features 379 models. Civitai offers a range of AI models for various applications and purposes.\n- The [GitHub repository](https://github.com/fictions-ai/sharing-is-caring/tree/main) titled 'sharing-is-caring' is maintained by the fictions.ai team. They focus on collaboration and sharing knowledge related to A.I. generation. The repository contains various workflows, scripts, and tools for A.I. generation. Some key [[contents]] include Comfy Workflows, upscale workflows, and specific workflow requirements. Contact the fictions.ai team for questions or feedback.\n- The [Index](https://wyrde.github.io/ComfyResources/nodes/) page of [[ComfyUI]] Resources provides a comprehensive list of custom nodes and tools for ComfyUI. Various nodes such as CLIP BLIP Node, GPT node ComfyUI, and Vid2vid Node Suite are available for installation. Instructions for installing custom nodes are included on the page.\n- The [Comfy Workflows](https://[[ComfyWorkFlows]].com/) website offers a platform for sharing art and workflows, with a focus on [[ComfyUI]]. Users can explore thousands of workflows created by the community and run them with zero setup using the ComfyUI Launcher. The site features a variety of workflows, images, and videos created by different creators.\n- The [GitHub repository](https://github.com/ZHO-ZHO-ZHO/[[ComfyUI]]-[[Gemini]]) integrates Gemini into ComfyUI, enabling users to generate prompts, describe images, and converse with Gemini. The repository features the latest Gemini 1.5 Pro model with system instruction settings, multi-modal conversations, and file reading capabilities. Users can request their API Key for Gemini API. Various models and nodes are provided for different functionalities, along with installation methods, workflows, changelog, and contact details.\n- The [video](https://www.youtube.com/watch?v=fFdYdTzq7Kg) titled 'Expanding Horizons: Outpainting Mastery in [[ComfyUI]]' on YouTube showcases the mastery of outpainting in ComfyUI. The video explores the revolutionary design aspects, including Vignette Mastery, ComfyUI Magic, and Font Previews Galore.\n- The [paper](https://arxiv.org/abs/2309.11497) titled FreeU: Free Lunch in Diffusion U-Net explores the potential of enhancing generation quality in [[Diffusion Models]] without additional training. The authors propose a method called 'FreeU' that strategically re-weights contributions from U-Net's skip connections and backbone feature maps to improve generation quality. The results show promising outcomes for image and video generation tasks, demonstrating the ease of integration with existing diffusion models.\n- The [GitHub repository](https://github.com/phineas-pta/comfy-trt-test) explores the attempt to use TensorRT with [[ComfyUI]], focusing on compatibility and installation instructions. The repository provides information on supported models like [[Stable Diffusion]] and SDXL, along with a list of working and non-working models. The project aims to make the process more user-friendly and automatic in the future. The README.md file outlines the steps for installing Python dependencies, TensorRT versions, and converting [[Checkpoints]] to TensorRT engines. It also discusses the usage of converted engines in ComfyUI and common error messages. The repository includes links to original implementations and download links for testing with various checkpoints and models.\n- The [AP Workflow 9.0 for [[ComfyUI]]](https://perilli.com/ai/comfyui/#soon) introduces new features such as upscalers, image generation with [[Dall-e]], advanced XYZ plot, face cloner, face analyzer, and training helper for batch captioning. Instructions for troubleshooting custom node installation, switching to SD 1.5 models, using LM Studio for prompt enrichment, securing ComfyUI connection with SSL, and FAQs are provided. Special thanks to custom node creators and a full changelog for version 9.0 are included. The [AP Workflow](https://perilli.com/ai/comfyui/#soon) version 8.0 offers features like bookmark nodes, [[IPAdapter]] plus v2 nodes, uploader function, caption generator function, image evaluators, face analyzer, aesthetic score predictor, image chooser, prompt enricher function, [[LoRA DoRA etc]] info node, face detailer function, and reorganized L2 pipeline layout with removed functions like ReVision and Image Enhancer.\n- The [[Google]] [Spreadsheets](https://docs.google.com/spreadsheets/d/1IYJw4Iv9M_vX507MPbdX4thhVYxOr6-IThbaRjdpVgM/edit#gid=0) document titled 'SDXL Model Compare' provides information on various data models and comparison metrics. The content includes data on different styles, illustrations, and financial aspects. The document also offers support for screen readers and data cleaning suggestions.\n- The [GitHub repository](https://github.com/chaojie/[[ComfyUI]]-MotionCtrl) for ComfyUI-MotionCtrl contains an implementation of MotionCtrl for video generation. The repository includes nodes for loading motion control [[Checkpoints]], motion control conditioning, and motion control [[Sampling]]. Tools for generating motion trajectories and camera points are also provided. Examples of workflows for generating LVDM/VideoCrafter videos and using [[AnimateDiff]] for scribbling are available. The repository is licensed under Apache-2.0 and has received 121 stars and 4 forks.\n- The [YouTube](https://www.youtube.com/@ferniclestix) page provides information about cookies and data usage for [[Google]] services. Users can choose to accept or reject cookies for various purposes, including delivering Google services, tracking outages, measuring audience engagement, and showing personalised content and ads based on user settings.\n- The [video](https://www.youtube.com/watch?v=DROM8vfIYUY) titled 'How 2 Canvas Node' is available on YouTube.\n- The [YouTube](https://www.youtube.com/@NerdyRodent) page provides information about cookies and data usage by [[Google]] services. Users can choose to accept or reject cookies for various purposes, including delivering Google services, measuring audience engagement, and showing personalised content and ads. More options are available for managing privacy settings.\n- The [GitHub repository](https://github.com/Acly/[[krita]]-ai-diffusion?tab=readme-ov-file) provides a streamlined interface for generating images with AI in Krita. It allows users to inpaint and outpaint images with optional text prompts, requiring no tweaking. The plugin supports features like generating images from scratch, refining existing content, live painting, and job queue management. Customization options are available for advanced users. The plugin is open source and free to use. [[krita-plugin]], [[stable-diffusion]], [[Generative AI]]\n- The [YouTube video](https://www.youtube.com/watch?v=AU8NDSBIS1U) showcases a plugin that adds real-time AI assistance to [[krita]]. The video covers topics such as [[Stable Diffusion]], Generative AI, [[Large language models]], AI Animation, Voice Cloning, and more. The channel, Nerdy Rodent, provides tutorials on [[Artificial Intelligence]] in an easy-to-digest format. The AI enthusiast behind the channel recommends specific hardware for the best AI experience at home. Not suitable for children. Please use AI responsibly.\n- The [GitHub repository](https://github.com/comfyanonymous/[[ComfyUI]]) showcases ComfyUI, a powerful and modular [[Stable Diffusion]] GUI, API, and backend with a graph/nodes interface. The interface allows users to design and execute advanced stable diffusion pipelines without needing to code. ComfyUI supports various [[Diffusion Models]], asynchronous queue system, and many optimizations to enhance workflow efficiency. Users can - back_hashcash-denial_2002 - - [[experiment]] with complex workflows, including area composition, [[Inpainting]], controlnet, upscale models, and more. The repository provides detailed installation instructions for Windows, Linux, AMD GPUs, NVIDIA GPUs, Intel GPUs, [[Apple]] Mac silicon, and DirectML for AMD cards on Windows. Additionally, it offers shortcuts for workflow management, high-quality preview setup, TLS/SSL configuration, and support channels for users. The repository is licensed under GPL-3.0 and has garnered significant community engagement with 35.3k stars and 3.8k forks.\n- The [video](https://www.youtube.com/watch?v=OdMtJMzjNLg) titled 'LATENT Tricks - Amazing ways to use [[ComfyUI]]' features Olivio Sarikas, an AI Expert and passionate Artist, showcasing the exciting world of AI art. The video invites viewers to explore AI art and creative visions with live streams. Olivio Sarikas, a professional Designer with a Masters Degree in Fine Arts, shares [[Tips and Tricks]] for using ComfyUI.\n- The [[ComfyUI]][models](https://civitai.com/tag/comfyui) tag on Civitai features 379 models for [[Stable Diffusion]] AI. Users can explore and access these models for various applications. Civitai offers a range of services and resources for creators, including terms of service, privacy policies, and safety guidelines.\n- The [[ComfyUI]]-[extension-tutorials/ComfyUI-Impact-Pack/workflow](https://github.com/ltdrdata/ComfyUI-extension-tutorials/tree/Main/ComfyUI-Impact-Pack/workflow) on GitHub provides a collection of workflow-related files for various image processing tasks. The repository includes files for tasks such as [[Upscaling]], animation, segmentation, and more. Each file represents a specific workflow step or technique, showcasing the versatility and capabilities of the ComfyUI Impact Pack.\n- The [video](https://www.youtube.com/watch?v=KvZ8ucBqyqw) titled '[[ComfyUI]] Impact Pack - Q&A;: Detailer Options' on YouTube provides explanations on the important parameters of the detailer.\n- The [Index](https://wyrde.github.io/ComfyResources/nodes/#index) page of [[ComfyUI]] Resources provides a comprehensive list of custom nodes and tools available for use. The nodes cover a wide range of functionalities, from image processing to AI installation tools. Installation instructions can be found on the respective node pages, with the option to streamline the process using ltdrdata's Comfy Manager.\n- The [YouTube video](https://www.youtube.com/watch?v=reimr3jZ8lI) titled '[[ComfyUI]] Fundamentals - [[Upscaling]] 1' provides insights into the fundamentals of ComfyUI and the concept of upscaling in the context of user interface design.\n- The [YouTube video](https://www.youtube.com/watch?v=ppE1W0-LJas) titled '[[ComfyUI]] Modular- Ultimate Starter Workflow Usage' provides insights into using ComfyUI Modular for an ultimate starter workflow.\n- The [GitHub repository](https://github.com/SHI-Labs/Prompt-Free-Diffusion) for Prompt-Free Diffusion discusses a diffusion model that generates images using only visual inputs, replacing text encoders with a Semantic Context Encoder (SeeCoder). The model is reusable across various T2I models and adaptive layers. The repository includes pretrained models and tools for model conversion. The implementation is based on a research paper presented at arXiv 2023 / CVPR 2024.\n- The [file](https://gist.github.com/comfyanonymous/343e5675f9a2c8281fde0c440df2e2c6#file-workflow-[[json]]) contains Python code for a custom node in the [[ComfyUI]]/custom_nodes directory. The code defines a class for handling model references and latent data. The JSON file included in the gist outlines a workflow with various nodes like VAEDecode, CLIPTextEncode, KSampler, SaveImage, CheckpointLoaderSimple, ImageScale, LoadImage, and ReferenceOnlySimple.\n- The [GitHub repository](https://github.com/itsKaynine/comfy-ui-client) contains a Node.js WebSockets API client for [[ComfyUI]]. The client is based on the WebSockets API example and is licensed under the MIT [[license]]. The repository includes folders and files such as examples, source code, and configuration files. The client allows users to connect to a server, generate images based on prompts, and save the images to a specified directory. Topics related to the project include nodejs, api, stable-diffusion, comfyui, and sdxl.\n- The [YouTube](https://www.youtube.com/@ArchAi3D/videos) page provides information about cookies and data usage by [[Google]] services. Users can choose to accept or reject cookies for various purposes, including personalised content and ads. More options are available for managing privacy settings.\n- The [video](https://www.youtube.com/watch?v=js4JeDF3v4g) showcases ComyUI, a tool for video animation rendering using AI technologies like WAS, Seecoder, Style, and Semantic segmentation. The creator, Amir Ferdos, a seasoned 3D artist and designer, explores the fusion of AI with design processes, offering unique workflows and tutorials on their YouTube channel. The videos aim to educate and inspire designers on the transformative impact of AI in design. For a deeper dive into the creator's work, exclusive tutorials and source files are available on their Patreon page.\n- The [[ComfyUI]] [Community Manual](https://blenderneko.github.io/ComfyUI-docs/) provides documentation for ComfyUI, a [[Stable Diffusion]] GUI and backend. It covers topics such as installation, downloading models, first steps with Comfy, loading other flows, and further support. The manual includes detailed information on interface, core nodes (including advanced, conditioning, experimental, image, latent, loaders, mask, and [[Sampling]]), custom nodes, developing custom nodes, and contributing documentation.\n- The [GitHub repository](https://github.com/pydn/[[ComfyUI]]-to-Python-Extension) provides a powerful tool that translates ComfyUI workflows into executable Python code. The tool bridges the gap between ComfyUI's visual interface and Python's programming environment, streamlining the process for data scientists, software developers, and AI enthusiasts. Use cases include creating lean app deployments, programmatic experiments, and large image generation queues. The v1.0.0 release notes highlight support for custom nodes. To use the tool, clone the repository, enable Dev mode options in ComfyUI, save workflows in API format, and run the script to generate Python code for image generation without launching a server. The repository is primarily focused on topics like pytorch, generative art, image generation, AI art, [[Stable Diffusion]], and ComfyUI.\n- The [website](https://comfy.icu/) offers [[ComfyUI]] Cloud services for running and deploying workflows without the need for downloads or installs. Users can pay only for active GPU usage, avoiding idle time and unnecessary costs. ComfyICU provides ready-to-use creative workflows and a simple, scalable API for production. The platform aims to simplify workflow creation and deployment, offering fast performance and cost-efficiency. Users can access over 5000 happy users' testimonials and FAQs for more information.\n- The [video](https://www.youtube.com/watch?v=SMOM1bIY5yA) titled 'EASY [[Inpainting]] in [[ComfyUI]] with SAM (segment Anything) | Creative Workflow Tutorial' provides a tutorial on using SAM for inpainting in ComfyUI.\n- The [wiki page](https://github.com/chrisgoringe/Comfy-Custom-Node-How-To/wiki) provides a practical and [[collaborative]] guide on developing custom nodes for [[ComfyUI]]. The guide is unofficial and focuses on practicality over formality, encouraging collaboration through Q&A-style discussions. It covers various topics related to custom node development, such as control flow, data types, and UI design.\n- The [Comfy Workflows](https://[[ComfyWorkFlows]].com/) website offers a platform for sharing art and workflows, with features like [[ComfyUI]] Launcher for running workflows with zero setup. Users can explore thousands of workflows created by the community. Trending creators and the latest images and videos are showcased on the site.\n- Custom nodes for interpolating between, well, everything in the [[Stable Diffusion]] [[ComfyUI]]. The [GitHub repository](https://github.com/shockz0rz/ComfyUI_InterpolateEverything) contains functionality to create preprocessed ControlNet OpenPose inputs midway between two images. Future features include line-art interpolation. To install, follow the provided instructions.\n- The [GitHub repository](https://github.com/xXAdonesXx/NodeGPT) contains [[ComfyUI]] Extension Nodes for Automated Text Generation. The repository is under development, with features like autogen, automated task solving, and group chat capabilities. The repository includes various folders and files for different functionalities. To contribute, users can submit pull requests, suggestions, or issue reports. The repository is licensed under AGPL-3.0. The repository has 314 stars, 24 forks, and 4 contributors.\n- The [AutoGen Advanced Tutorial](https://www.youtube.com/watch?v=PUPO2tTyPOo) on YouTube focuses on building incredible AI AGENT teams. The tutorial delves into advanced techniques for creating AI teams and enhancing their capabilities.\n- The [GitHub repository](https://github.com/olegchomp/TDComfyUI) provides a TouchDesigner interface for [[ComfyUI]], offering features like workflow creation and image send/receive. The repository includes installation instructions and resources for using the TDComfyUI component. It also offers guidance on connecting to [[Stable Diffusion]] and utilising ComfyUI settings for optimal performance.\n- The [GitHub repository](https://github.com/NimaNzrii/[[ComfyUI]]-photoshop) showcases the ComfyUI plugin for Photoshop, offering AI-powered image generation features. The plugin enables unlimited generative fill, customizable back-end workflow, and one-click image transformation. System requirements include a minimum of 6GB graphics memory and 12GB RAM. Installation involves downloading the plugin from a provided link or locally via a .CCX file. Additional files are required for specific functionalities. Support and contributions are encouraged through GitHub.\n- The [model](https://civitai.com/models/121728/aerial-view-of-the-building) titled 'Aerial view of the building' offers a high-definition training set for urban bird's-eye views, encompassing a variety of domestic and foreign architectural drawings. The model is based on [[LoRA DoRA etc]] technology and has received positive reviews. The training set is designed for cityscape and building enthusiasts.\n\n\t- #### Veilid\n\t- A Peer-to-Peer Privacy Mesh Project\n- Veilid is an open-source, mobile-first, networked application frameworkfor building decentralized apps with networking, distributed datastorage, and built-in IP privacy without reliance on external services.\n-\n\t- **Platforms**: Runs on Linux, Mac, Windows, Android, iOS, and in browsers via WASM. Bindings available in Rust, Dart, and other languages.\n-\n\t- **Protocols**: Supports UDP, TCP, WebSockets. DNS only used briefly during bootstrap.\n-\n\t- **Encryption**: Uses Ed25519, XChaCha20, BLAKE3 for end-to-end encryption and authentication.\n-\n\t- **Storage**: Distributed hash table for data records close to node keys. Popular data replicated.\n-\n\t- **Routing**: Nodes help each other connect. Routing based on node IDs. Private routing over encrypted hops.\n-\n\t- **Goals**: Enable decentralized apps without reliance on centralized corporate systems.\n- Key features include strong cryptography, ability to run on a variety ofplatforms, distributed and replicated data storage, and private routingto provide IP privacy. The decentralized design aims to avoid issueswith centralized and corporate controlled systems.\n-\n\n- # Node based\n\t- [Rivet (ironcladapp.com)](https://rivet.ironcladapp.com/) is a [[Interfaces]] for [[Infrastructure]] [[Hardware and Edge]] based [[Agents]] using [[Large language models]]\n\t- [[Node based visual interfaces]] is a  [[Interfaces]] for [[Large language models]] which is open source and [locally hosted.](https://github.com/FlowiseAI/Flowise) also see [[Langflow]]\n\t- [[Langflow]] [[Interfaces]] for [[Large language models]] builder, with slightly more features than [[Node based visual interfaces]] which is another one I need to try soon from their [github](https://github.com/logspace-ai/langflow)\n\t- ComfyUI is a [[Interfaces]] for [[Stable Diffusion]]. It allows very high levels of control over Diffusion Models by leveraging open source extensions and a vibrant developer and creator community. The tool's flexibility and efficiency comes from its innovative design philosophy, that prioritizes user customization and interaction.\n\n\t\t- ## Ontological Layer for Metaverse\n\t\t\t- To approach the problem of using Mamba to analyze formal ontological graphs as used by the W3C, we can draw upon several techniques and architectures discussed in the Mamba literature. Here's a proposed approach:\n\t\t\t- Data Preprocessing:\n\t\t\t\t- Graph Normalization:\n\t\t\t\t\t- Ensure consistent formatting and structure of the ontological graphs\n\t\t\t\t\t- Handle missing or inconsistent data\n\t\t\t\t\t- Normalize node and edge labels\n\t\t\t\t- Graph Merging:\n\t\t\t\t\t- Combine multiple ontological graphs into a single unified graph\n\t\t\t\t\t- Resolve conflicts and inconsistencies between different ontologies\n\t\t\t\t\t- Establish mappings between equivalent concepts across ontologies\n\t\t\t\t- Graph Embedding:\n\t\t\t\t\t- Generate low-dimensional vector representations of nodes and edges\n\t\t\t\t\t- Preserve the semantic relationships and structure of the ontological graphs\n\t\t\t\t\t- Use techniques like RDF2Vec, TransE, or Graph Convolutional Networks (GCNs)\n\t\t\t- Mamba Architecture:\n\t\t\t\t- Graph-Mamba:\n\t\t\t\t\t- Adapt the Mamba architecture to handle graph-structured data\n\t\t\t\t\t- Utilize the selective state space mechanism to capture long-range dependencies in the ontological graphs\n\t\t\t\t\t- Achieve efficient memory usage and reduced computational complexity compared to traditional graph neural networks (GNNs)\n\t\t\t\t- Multi-dimensional Sequencing (Mamba-ND):\n\t\t\t\t\t- Treat the ontological graphs as multi-dimensional sequences (e.g., node features, edge types, and graph structure)\n\t\t\t\t\t- Apply Mamba-ND to capture dependencies across all dimensions\n\t\t\t- Hybrid Architectures:\n\t\t\t\t- Combine Graph-Mamba with other graph neural network architectures (e.g., GCNs, GraphSAGE)\n\t\t\t\t- Leverage the strengths of both approaches to capture local and global patterns in the ontological graphs\n\t\t\t\t-\n\t\t\t\t-\n\t\t\t- ```mermaid\n\t\t\t  graph TD\n\t\t\t   A[Ontological Graphs] --> B(Data Preprocessing)\n\t\t\t   B --> C{Mamba Architecture}\n\t\t\t   C --> D[Graph-Mamba]\n\t\t\t   C --> E[Multi-dimensional Sequencing Mamba-ND]\n\t\t\t   C --> F[Hybrid Architectures]\n\t\t\t   D --> G[Comprehensive Graph Representation]\n\t\t\t   E --> G\n\t\t\t   F --> G\n\t\t\t   G --> H[Ontology Alignment]\n\t\t\t   G --> I[Knowledge Graph Completion]\n\t\t\t   G --> J[Semantic Similarity]\n\t\t\t   ```\n\t\t\t\t-\n\t\t\t\t- TODO What are the specific ontologies being used (e.g., RDF, OWL)?\n\t\t\t\t- TODO Are there any domain-specific requirements or constraints to consider?\n\t\t\t\t- TODO What are the desired output tasks (e.g., ontology alignment, knowledge graph completion, semantic similarity)?\n\t\t\t\t- TODO How large and complex are the ontological graphs being analyzed?\n\t\t\t\t- TODO Handling scalability issues for large-scale ontological graphs\n\t\t\t\t- TODO Incorporating domain knowledge and ontology-specific constraints\n\t\t\t\t- TODO Leveraging transfer learning from pre-trained models on similar ontological graphs\n\t\t\t\t- TODO Evaluating the model's performance using appropriate graph-based metrics and validation techniques\n\t\t\t\t- TODO Interpreting and visualizing the learned graph representations for ontology engineers and domain experts\n\n\t- ##### Bitcoin based FIAT\n\t\t- More interestingly for metaverse applications Mallers has opened this section of the company to interact with the public Lightning network, allowing people with a self hosted wallet or node to pay directly for goods across America, settling immediately in Dollars, using their Bitcoin, at zero cost. **This opens the possibility to buy from US based(Dollar denominated) metaverse stores, using the capabilities of the stack assembled at the end of the book**. The implications globally are unclear at this time.\n\t\t- [Stablesats](https://stablesats.com/) is another approach which uses exclusively lightning bitcoin but makes the value stable against the US dollar using an algorithm. This is a very interesting option and will be explored in detail at some point.\n\n\t\t- ## Bitcoin nodes\n\t - The Bitcoin network can be considered a triumvirate of economic actors,each with different incentives. These are:\n\t\t\t\t- Holders and users of the tokens, including exchanges and market makers, who make money speculating, [arbitraging](https://en.wikipedia.org/wiki/Arbitrage), and providing liquidity into the network. Increasingly this is also real ‘money’ users of BTC, earning and spending in pools of circular economic activity. Perversely Bitcoin as a money is the fringe use case at this time.\n\t\t\t\t- Miners, who profit from creation of new UTXOs, and receive payments for adding transactions to the chain. In return they secure the network by validating the other miners blocks according the rules enforced by the node operators.\n\t\t\t\t- Node operators, [who enforce the consensus](https://www.truthcoin.info/blog/measuring-decentralization/) rule-set, which the miners must abide by in order to propagate new transaction into the network. In return node operators optimise their trust minimisation, and help protect the network from changes which might undermine their speculation and use of the tokens.[[blocksizewars]]\n\t\t\t\t- There are currently around [15,000 Bitcoin nodes](https://bitnodes.io/)distributed across the world. Since IT engineer[Stadicus](https://stadicus.com/) released his [Raspiboltguide](https://raspibolt.org/backstory.html) in 2017 there has been anexplosion of small scale Bitcoin and Lightning node operators. Aroundthirty thousand Raspberry Pi Lightning nodes (which are also bydefinition Bitcoin nodes) run one of a big selection of [open sourcedistributions](https://github.com/bavarianledger/bitcoin-nodes). We willbuild toward our own throughout the book.\n\n- # Lightning\n\t- [Setup lnbits and lightningtipbot](https://www.massmux.com/howto-complete-lightningtipbot-lnbits-setup-vps/)\n\t- [GitHub - ln-vortex/ln-vortex: Lightning and Taproot enabled collaborative transactions (other)](https://github.com/ln-vortex/ln-vortex)\n\t- [This is a node management software for large Lightning Network nodes. It provides a way to automate workflows, manage code changes, and track work progress.](https://github.com/lncapital/torq)\n\t- L402 lightning reverse proxy with LND for AI\n\t\t- [docs](https://docs.lightning.engineering/the-lightning-network/l402)\n\t- [Cleveland bank paper on lighting improving Bitcoin](https://www.clevelandfed.org/en/publications/working-paper/2022/wp-2219-the-lightning-network-turning-bitcoin-into-money)\n\n\t\t- ### Segmentation\n\t\t\t- ![Segmentation for fashion](https://raw.githubusercontent.com/cozymantis/human-parser-comfyui-node/main/assets/lipexample.png)\n\t\t\t-\n\n- # Pre event buildout notes (here be dragons)\n  collapsed:: true\n\t- DONE Infrastructure build\n\t\t- DONE Get Ollama bridge working\n\t\t  collapsed:: true\n\t\t\t- [stavsap/comfyui-ollama (github.com)](https://github.com/stavsap/comfyui-ollama)\n\t\t\t- [MinusZoneAI/ComfyUI-Prompt-MZ: 基于llama.cpp的一些和提示词相关的节点，目前包括美化提示词和类似clip-interrogator的图片反推 | Use llama.cpp to assist in generating some nodes related to prompt words, including beautifying prompt words and image recognition similar to clip-interrogator (github.com)](https://github.com/MinusZoneAI/ComfyUI-Prompt-MZ)\n\t\t\t- [xXAdonesXx/NodeGPT: ComfyUI Extension Nodes for Automated Text Generation. (github.com)](https://github.com/xXAdonesXx/NodeGPT)\n\t\t\t- [stavsap/comfyui-ollama (github.com)](https://github.com/stavsap/comfyui-ollama)\n\t\t\t  id:: 6633f4c0-358f-44cf-bf05-d43c75febe36\n\t\t- DONE Backup the working docker\n\t\t- DONE sort the vpn and port forwarding\n\t\t- DONE Check the security\n\t\t- DONE Install the rest of the feature set\n\t\t- DONE Sort the models and Loras\n\t\t- DONE Fire up 3 instances\n\t\t\t- DONE TripoSR (no point, feature dropped)\n\t\t\t- DONE [Zero123](https://github.com/SUDO-AI-3D/zero123plus) (no point, feature dropped\n\t\t\t- DONE CRM\n\t\t- DONE [yisol/IDM-VTON: IDM-VTON : Improving Diffusion Models for Authentic Virtual Try-on in the Wild (github.com)](https://github.com/yisol/IDM-VTON)\n\t\t  collapsed:: true\n\t\t\t- [TemryL/ComfyUI-IDM-VTON: ComfyUI adaptation of IDM-VTON for virtual try-on. (github.com)](https://github.com/TemryL/ComfyUI-IDM-VTON)\n\t\t- TODO [Lllava 8b?](https://huggingface.co/collections/xtuner/llava-llama-3-8b-662a5f95adbe8d58799d7fdb) for descriptions\n\t\t  :LOGBOOK:\n\t\t  CLOCK: [2024-05-06 Mon 09:23:58]--[2024-05-06 Mon 09:23:58] =>  00:00:00\n\t\t  CLOCK: [2024-05-06 Mon 09:23:59]--[2024-05-06 Mon 09:24:00] =>  00:00:01\n\t\t  :END:\n\t\t- DONE Face swap\n\t\t- DONE NSFW filter\n\t\t- DONE Annotations and instructions\n\t\t  :LOGBOOK:\n\t\t  CLOCK: [2024-05-06 Mon 09:25:33]--[2024-05-06 Mon 12:16:24] =>  02:50:51\n\t\t  :END:\n\t\t- DONE Send to Pete to test\n\t\t- DONE Presentation outlines?\n\t\t- DONE Talk to Marco\n\t\t- DONE Next need to fix insightface [as here](https://github.com/cubiq/ComfyUI_IPAdapter_plus/issues/162) but\n\t\t\t- DONE backup first\n\t\t\t  :LOGBOOK:\n\t\t\t  CLOCK: [2024-05-06 Mon 10:23:31]--[2024-05-06 Mon 10:23:31] =>  00:00:00\n\t\t\t  :END:\n\t\t- TODO New nets and workflows? Pete?\n\t\t- DONE Confirm the compute arriving.\n\t\t- DONE Confirm the TV in time?\n\t\t- DONE Fix the windows laptop for delegates\n\t\t- DONE Condition the mac for delegates (come in monday afternoon)\n\t\t- DONE Charge the Rundiffusion account (talking to Tony this afternoon).\n\t\t- DONE Catering\n\t\t- DONE Talk to Marco\n\t\t- DONE Make a presentation for the day (logseq based for me)\n\t\t  :LOGBOOK:\n\t\t  CLOCK: [2024-05-12 Sun 16:41:41]--[2024-05-14 Tue 22:17:47] =>  53:36:06\n\t\t  :END:\n\t\t- DONE Delegate advance communications\n\t\t  :LOGBOOK:\n\t\t  CLOCK: [2024-05-11 Sat 19:58:36]--[2024-05-12 Sun 16:41:31] =>  20:42:55\n\t\t  CLOCK: [2024-05-12 Sun 16:41:35]--[2024-05-14 Tue 22:17:50] =>  53:36:15\n\t\t  :END:\n\npublic:: true\n\n- #Public page automatically published\n-\n- It's not the easiest way to use Stable Diffusion.\n- https://private-user-images.githubusercontent.com/140084057/368715655-f46f769d-f168-454c-9c7b-ed8bcd727c1d.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MzEwNjY1NTEsIm5iZiI6MTczMTA2NjI1MSwicGF0aCI6Ii8xNDAwODQwNTcvMzY4NzE1NjU1LWY0NmY3NjlkLWYxNjgtNDU0Yy05YzdiLWVkOGJjZDcyN2MxZC5wbmc_WC1BbXotQWxnb3JpdGhtPUFXUzQtSE1BQy1TSEEyNTYmWC1BbXotQ3JlZGVudGlhbD1BS0lBVkNPRFlMU0E1M1BRSzRaQSUyRjIwMjQxMTA4JTJGdXMtZWFzdC0xJTJGczMlMkZhd3M0X3JlcXVlc3QmWC1BbXotRGF0ZT0yMDI0MTEwOFQxMTQ0MTFaJlgtQW16LUV4cGlyZXM9MzAwJlgtQW16LVNpZ25hdHVyZT00MjQzYjkyNDU2MjNjOTRhYjgyM2JkN2E1YzQxMDcxMDAwNTQ5ZjYzZThhMjBhYjM4NGZhYzhiYzg0ZTNhNDYzJlgtQW16LVNpZ25lZEhlYWRlcnM9aG9zdCJ9.eOij_sfuzFqJLnC0tcT-OYnMTJF30B_mnj2xM3qjG-Y\n-\n- The [GitHub repository](https://github.com/ZHO-ZHO-ZHO/[[ComfyUI]]-[[Gemini]]) integrates Gemini into ComfyUI, offering models like Gemini-pro, Genimi-pro-vision, and Gemini 1.5 Pro for text, image, and file processing tasks. Users can apply for their own API Key to access Gemini API. The repository provides workflow examples, installation instructions, and updates. Contact information includes zhozho3965@gmail.com and a QQ Group (839821928). Social media links to '-Zho-', Bilibili, Twitter, Little Red Book, and support on Bilibili and Aifadian. Credits to ComfyUI_Custom_Nodes_AlekPet. - Users need to apply for a Gemini_API_Key to use Gemini nodes, ensure a stable connection to [[Google]] Gemini's services, and update the dependency 'google-generativeai' to version 0.4.1 for Gemini 1.5 Pro. Installation can be done using ComfyUI Manager or manually by cloning the repository from GitHub and installing requirements. Workflow versions include V3.0 with Gemini 1.5 Pro, V2.0 with a chatbot workflow, and V1.1 with workflows for Gemini-pro and Genimi-pro-vision. Updates include Version 3.0 adding Gemini 1.5 Pro, system instructions, and file uploads, Version 2.1 fixing a bug, and Version 2.0 adding context chat nodes. - The [status history](https://api.status-histroy.com/svg?repos=ZHO-ZHO-ZHO/ConfyUI-Gemini&type=Timeline) for the repository is available.\n- The [video](https://www.youtube.com/watch?v=fFdYdTzq7Kg) titled 'Expanding Horizons: Outpainting Mastery in [[ComfyUI]]' explores the artistry of Outpainting with ComfyUI's [[Stable Diffusion]] feature. The video delves into Hyper Expansion, Sketch to Render, and Auto Background Regeneration within the realm of Outpainting. Topics discussed include the use of LORAs, Quick Basic Outpainting, and various techniques for Outpainting with ComfyUI.\n- The [Aerial view of the building](https://civitai.com/models/121728/aerial-view-of-the-building) showcases a [[LoRA DoRA etc]] model for urban bird's-eye views, offering high-definition training sets for cityscapes and buildings. The model, based on SD 1.5, has received very positive reviews and was last updated on Aug 3, 2023.\n- The [GitHub repository](https://github.com/comfyanonymous/[[ComfyUI]]) showcases ComfyUI, a powerful and modular [[Stable Diffusion]] GUI, API, and backend with a graph/nodes interface. The interface allows users to design and execute advanced stable diffusion pipelines without needing to code. ComfyUI supports various features like SD1.x, SD2.x, SDXL, Stable Video Diffusion, and Stable Cascade. Users can - back_hashcash-denial_2002 - - [[experiment]] with complex workflows, embeddings/textual inversion, Loras, hypernetworks, and more. The repository provides detailed installation instructions for Windows, Linux, AMD GPUs, NVIDIA GPUs, Intel GPUs, [[Apple]] Mac silicon, and DirectML for AMD cards on Windows. Additionally, it offers shortcuts for workflow management, high-quality previews, TLS/SSL setup, and support channels for users. The repository is licensed under GPL-3.0 and has garnered 35.3k stars and 3.8k forks.\n- The [video](https://www.youtube.com/watch?v=DROM8vfIYUY) titled 'How 2 Canvas Node' is available on YouTube. The video duration is 14 minutes and 22 seconds.\n- The [video](https://www.youtube.com/watch?v=AU8NDSBIS1U) showcases a plugin that provides realtime AI assistance to - ### OntologyBlock id:: krita-ontology collapsed:: true - ontology:: true - term-id:: mv-523525042165 - [[krita]], a digital painting software. The channel, Nerdy Rodent, offers tutorials on [[Stable Diffusion]], Generative AI, [[Large language models]], and other AI tools, catering to AI enthusiasts and professionals. The plugin enhances various aspects of AI technology, such as voice cloning, text-to-speech, and style transfer, making it a valuable resource for AI enthusiasts and artists alike.\n- The [video](https://www.youtube.com/watch?v=reimr3jZ8lI) titled '[[ComfyUI]] Fundamentals - [[Upscaling]] 1' explores the concept of upscaling in the context of ComfyUI. The video delves into how upscaling works and provides tips to enhance the upscaling process. The presenter discusses the ability to go under 1 on the upscale by node, highlighting key aspects of image upscaling and various upscaling options.\n- The [YouTube video](https://www.youtube.com/watch?v=ppE1W0-LJas) titled '[[ComfyUI]] Modular- Ultimate Starter Workflow Usage' provides insights into using ComfyUI Modular for an ultimate starter workflow.\n- The [YouTube](https://www.youtube.com/@sedetweiler) page provides information about the use of cookies and data for various purposes, including [[Google]] services, audience engagement, and site statistics. By accepting all, users allow for the development of new services, displaying ads, and showing personalised content and ads based on their settings. Non-personalised content and ads are influenced by factors like the viewed content and location. Users can manage privacy settings and find more information by selecting 'Meer opties'.\n- The [GitHub repository](https://github.com/phineas-pta/comfy-trt-test) is an attempt to use TensorRT with [[ComfyUI]], focusing on specific models and workflows. The repository provides instructions for installation and usage, highlighting compatible models like [[Stable Diffusion]] and SDXL. It also mentions limitations and future improvements for user-friendliness and compatibility with ComfyUI-Manager. The repository is a ComfyUI port from the official A1111 extension, with potential maintenance issues addressed by an alternative repository. The README outlines conversion scripts, dependencies, and error messages encountered during the process.\n- The [GitHub repository](https://github.com/gameltb/ComfyUI_stable_fast) showcases experimental usage of stable-fast and TensorRT. It includes speed tests, installation instructions, and features of stable-fast and TensorRT.\n\t- The repository provides guidance on enabling stable-fast nodes and installing TensorRT for testing purposes.\n\t- It highlights the compatibility of stable-fast with [[LoRA DoRA etc]], ControlNet, and other models, along with speed optimizations and node support.\n\t- Speed tests on a GeForce RTX 3060 Mobile show performance metrics for stable-fast and TensorRT implementations.\n\t- The repository also includes a table detailing features, tested nodes, and performance benchmarks for different workflows.\n- The [GitHub repository](https://github.com/Acly/[[krita]]-ai-diffusion?tab=readme-ov-file) provides a streamlined interface for generating images with AI in Krita. Users can inpaint and outpaint with optional text prompts without the need for tweaking. The plugin allows for creating new images from scratch, refining existing content, and live painting. It supports various resolutions, job queues, and history tracking. Customization options are available for advanced users. The technology used includes [[Stable Diffusion]] for image generation, [[ComfyUI]] for the diffusion backend, ControlNet for [[Inpainting]], and IP-Adapter for outpainting. The repository is licensed under GPL-3.0 and has garnered 4.8k stars and 216 forks. Contributors have added features like object selection tools and GPU cloud support.\n- The [[ComfyUI]] [models](https://civitai.com/tag/comfyui) tag on Civitai features 379 models. Civitai offers a range of AI models for various applications and purposes.\n- The [GitHub repository](https://github.com/fictions-ai/sharing-is-caring/tree/main) titled 'sharing-is-caring' is maintained by the fictions.ai team. They focus on collaboration and sharing knowledge related to A.I. generation. The repository contains various workflows, scripts, and tools for A.I. generation. Some key [[contents]] include Comfy Workflows, upscale workflows, and specific workflow requirements. Contact the fictions.ai team for questions or feedback.\n- The [Index](https://wyrde.github.io/ComfyResources/nodes/) page of [[ComfyUI]] Resources provides a comprehensive list of custom nodes and tools for ComfyUI. Various nodes such as CLIP BLIP Node, GPT node ComfyUI, and Vid2vid Node Suite are available for installation. Instructions for installing custom nodes are included on the page.\n- The [Comfy Workflows](https://[[ComfyWorkFlows]].com/) website offers a platform for sharing art and workflows, with a focus on [[ComfyUI]]. Users can explore thousands of workflows created by the community and run them with zero setup using the ComfyUI Launcher. The site features a variety of workflows, images, and videos created by different creators.\n- The [GitHub repository](https://github.com/ZHO-ZHO-ZHO/[[ComfyUI]]-[[Gemini]]) integrates Gemini into ComfyUI, enabling users to generate prompts, describe images, and converse with Gemini. The repository features the latest Gemini 1.5 Pro model with system instruction settings, multi-modal conversations, and file reading capabilities. Users can request their API Key for Gemini API. Various models and nodes are provided for different functionalities, along with installation methods, workflows, changelog, and contact details.\n- The [video](https://www.youtube.com/watch?v=fFdYdTzq7Kg) titled 'Expanding Horizons: Outpainting Mastery in [[ComfyUI]]' on YouTube showcases the mastery of outpainting in ComfyUI. The video explores the revolutionary design aspects, including Vignette Mastery, ComfyUI Magic, and Font Previews Galore.\n- The [paper](https://arxiv.org/abs/2309.11497) titled FreeU: Free Lunch in Diffusion U-Net explores the potential of enhancing generation quality in [[Diffusion Models]] without additional training. The authors propose a method called 'FreeU' that strategically re-weights contributions from U-Net's skip connections and backbone feature maps to improve generation quality. The results show promising outcomes for image and video generation tasks, demonstrating the ease of integration with existing diffusion models.\n- The [GitHub repository](https://github.com/phineas-pta/comfy-trt-test) explores the attempt to use TensorRT with [[ComfyUI]], focusing on compatibility and installation instructions. The repository provides information on supported models like [[Stable Diffusion]] and SDXL, along with a list of working and non-working models. The project aims to make the process more user-friendly and automatic in the future. The README.md file outlines the steps for installing Python dependencies, TensorRT versions, and converting [[Checkpoints]] to TensorRT engines. It also discusses the usage of converted engines in ComfyUI and common error messages. The repository includes links to original implementations and download links for testing with various checkpoints and models.\n- The [AP Workflow 9.0 for [[ComfyUI]]](https://perilli.com/ai/comfyui/#soon) introduces new features such as upscalers, image generation with [[Dall-e]], advanced XYZ plot, face cloner, face analyzer, and training helper for batch captioning. Instructions for troubleshooting custom node installation, switching to SD 1.5 models, using LM Studio for prompt enrichment, securing ComfyUI connection with SSL, and FAQs are provided. Special thanks to custom node creators and a full changelog for version 9.0 are included. The [AP Workflow](https://perilli.com/ai/comfyui/#soon) version 8.0 offers features like bookmark nodes, [[IPAdapter]] plus v2 nodes, uploader function, caption generator function, image evaluators, face analyzer, aesthetic score predictor, image chooser, prompt enricher function, [[LoRA DoRA etc]] info node, face detailer function, and reorganized L2 pipeline layout with removed functions like ReVision and Image Enhancer.\n- The [[Google]] [Spreadsheets](https://docs.google.com/spreadsheets/d/1IYJw4Iv9M_vX507MPbdX4thhVYxOr6-IThbaRjdpVgM/edit#gid=0) document titled 'SDXL Model Compare' provides information on various data models and comparison metrics. The content includes data on different styles, illustrations, and financial aspects. The document also offers support for screen readers and data cleaning suggestions.\n- The [GitHub repository](https://github.com/chaojie/[[ComfyUI]]-MotionCtrl) for ComfyUI-MotionCtrl contains an implementation of MotionCtrl for video generation. The repository includes nodes for loading motion control [[Checkpoints]], motion control conditioning, and motion control [[Sampling]]. Tools for generating motion trajectories and camera points are also provided. Examples of workflows for generating LVDM/VideoCrafter videos and using [[AnimateDiff]] for scribbling are available. The repository is licensed under Apache-2.0 and has received 121 stars and 4 forks.\n- The [YouTube](https://www.youtube.com/@ferniclestix) page provides information about cookies and data usage for [[Google]] services. Users can choose to accept or reject cookies for various purposes, including delivering Google services, tracking outages, measuring audience engagement, and showing personalised content and ads based on user settings.\n- The [video](https://www.youtube.com/watch?v=DROM8vfIYUY) titled 'How 2 Canvas Node' is available on YouTube.\n- The [YouTube](https://www.youtube.com/@NerdyRodent) page provides information about cookies and data usage by [[Google]] services. Users can choose to accept or reject cookies for various purposes, including delivering Google services, measuring audience engagement, and showing personalised content and ads. More options are available for managing privacy settings.\n- The [GitHub repository](https://github.com/Acly/[[krita]]-ai-diffusion?tab=readme-ov-file) provides a streamlined interface for generating images with AI in Krita. It allows users to inpaint and outpaint images with optional text prompts, requiring no tweaking. The plugin supports features like generating images from scratch, refining existing content, live painting, and job queue management. Customization options are available for advanced users. The plugin is open source and free to use. [[krita-plugin]], [[stable-diffusion]], [[Generative AI]]\n- The [YouTube video](https://www.youtube.com/watch?v=AU8NDSBIS1U) showcases a plugin that adds real-time AI assistance to [[krita]]. The video covers topics such as [[Stable Diffusion]], Generative AI, [[Large language models]], AI Animation, Voice Cloning, and more. The channel, Nerdy Rodent, provides tutorials on [[Artificial Intelligence]] in an easy-to-digest format. The AI enthusiast behind the channel recommends specific hardware for the best AI experience at home. Not suitable for children. Please use AI responsibly.\n- The [GitHub repository](https://github.com/comfyanonymous/[[ComfyUI]]) showcases ComfyUI, a powerful and modular [[Stable Diffusion]] GUI, API, and backend with a graph/nodes interface. The interface allows users to design and execute advanced stable diffusion pipelines without needing to code. ComfyUI supports various [[Diffusion Models]], asynchronous queue system, and many optimizations to enhance workflow efficiency. Users can - back_hashcash-denial_2002 - - [[experiment]] with complex workflows, including area composition, [[Inpainting]], controlnet, upscale models, and more. The repository provides detailed installation instructions for Windows, Linux, AMD GPUs, NVIDIA GPUs, Intel GPUs, [[Apple]] Mac silicon, and DirectML for AMD cards on Windows. Additionally, it offers shortcuts for workflow management, high-quality preview setup, TLS/SSL configuration, and support channels for users. The repository is licensed under GPL-3.0 and has garnered significant community engagement with 35.3k stars and 3.8k forks.\n- The [video](https://www.youtube.com/watch?v=OdMtJMzjNLg) titled 'LATENT Tricks - Amazing ways to use [[ComfyUI]]' features Olivio Sarikas, an AI Expert and passionate Artist, showcasing the exciting world of AI art. The video invites viewers to explore AI art and creative visions with live streams. Olivio Sarikas, a professional Designer with a Masters Degree in Fine Arts, shares [[Tips and Tricks]] for using ComfyUI.\n- The [[ComfyUI]][models](https://civitai.com/tag/comfyui) tag on Civitai features 379 models for [[Stable Diffusion]] AI. Users can explore and access these models for various applications. Civitai offers a range of services and resources for creators, including terms of service, privacy policies, and safety guidelines.\n- The [[ComfyUI]]-[extension-tutorials/ComfyUI-Impact-Pack/workflow](https://github.com/ltdrdata/ComfyUI-extension-tutorials/tree/Main/ComfyUI-Impact-Pack/workflow) on GitHub provides a collection of workflow-related files for various image processing tasks. The repository includes files for tasks such as [[Upscaling]], animation, segmentation, and more. Each file represents a specific workflow step or technique, showcasing the versatility and capabilities of the ComfyUI Impact Pack.\n- The [video](https://www.youtube.com/watch?v=KvZ8ucBqyqw) titled '[[ComfyUI]] Impact Pack - Q&A;: Detailer Options' on YouTube provides explanations on the important parameters of the detailer.\n- The [Index](https://wyrde.github.io/ComfyResources/nodes/#index) page of [[ComfyUI]] Resources provides a comprehensive list of custom nodes and tools available for use. The nodes cover a wide range of functionalities, from image processing to AI installation tools. Installation instructions can be found on the respective node pages, with the option to streamline the process using ltdrdata's Comfy Manager.\n- The [YouTube video](https://www.youtube.com/watch?v=reimr3jZ8lI) titled '[[ComfyUI]] Fundamentals - [[Upscaling]] 1' provides insights into the fundamentals of ComfyUI and the concept of upscaling in the context of user interface design.\n- The [YouTube video](https://www.youtube.com/watch?v=ppE1W0-LJas) titled '[[ComfyUI]] Modular- Ultimate Starter Workflow Usage' provides insights into using ComfyUI Modular for an ultimate starter workflow.\n- The [GitHub repository](https://github.com/SHI-Labs/Prompt-Free-Diffusion) for Prompt-Free Diffusion discusses a diffusion model that generates images using only visual inputs, replacing text encoders with a Semantic Context Encoder (SeeCoder). The model is reusable across various T2I models and adaptive layers. The repository includes pretrained models and tools for model conversion. The implementation is based on a research paper presented at arXiv 2023 / CVPR 2024.\n- The [file](https://gist.github.com/comfyanonymous/343e5675f9a2c8281fde0c440df2e2c6#file-workflow-[[json]]) contains Python code for a custom node in the [[ComfyUI]]/custom_nodes directory. The code defines a class for handling model references and latent data. The JSON file included in the gist outlines a workflow with various nodes like VAEDecode, CLIPTextEncode, KSampler, SaveImage, CheckpointLoaderSimple, ImageScale, LoadImage, and ReferenceOnlySimple.\n- The [GitHub repository](https://github.com/itsKaynine/comfy-ui-client) contains a Node.js WebSockets API client for [[ComfyUI]]. The client is based on the WebSockets API example and is licensed under the MIT [[license]]. The repository includes folders and files such as examples, source code, and configuration files. The client allows users to connect to a server, generate images based on prompts, and save the images to a specified directory. Topics related to the project include nodejs, api, stable-diffusion, comfyui, and sdxl.\n- The [YouTube](https://www.youtube.com/@ArchAi3D/videos) page provides information about cookies and data usage by [[Google]] services. Users can choose to accept or reject cookies for various purposes, including personalised content and ads. More options are available for managing privacy settings.\n- The [video](https://www.youtube.com/watch?v=js4JeDF3v4g) showcases ComyUI, a tool for video animation rendering using AI technologies like WAS, Seecoder, Style, and Semantic segmentation. The creator, Amir Ferdos, a seasoned 3D artist and designer, explores the fusion of AI with design processes, offering unique workflows and tutorials on their YouTube channel. The videos aim to educate and inspire designers on the transformative impact of AI in design. For a deeper dive into the creator's work, exclusive tutorials and source files are available on their Patreon page.\n- The [[ComfyUI]] [Community Manual](https://blenderneko.github.io/ComfyUI-docs/) provides documentation for ComfyUI, a [[Stable Diffusion]] GUI and backend. It covers topics such as installation, downloading models, first steps with Comfy, loading other flows, and further support. The manual includes detailed information on interface, core nodes (including advanced, conditioning, experimental, image, latent, loaders, mask, and [[Sampling]]), custom nodes, developing custom nodes, and contributing documentation.\n- The [GitHub repository](https://github.com/pydn/[[ComfyUI]]-to-Python-Extension) provides a powerful tool that translates ComfyUI workflows into executable Python code. The tool bridges the gap between ComfyUI's visual interface and Python's programming environment, streamlining the process for data scientists, software developers, and AI enthusiasts. Use cases include creating lean app deployments, programmatic experiments, and large image generation queues. The v1.0.0 release notes highlight support for custom nodes. To use the tool, clone the repository, enable Dev mode options in ComfyUI, save workflows in API format, and run the script to generate Python code for image generation without launching a server. The repository is primarily focused on topics like pytorch, generative art, image generation, AI art, [[Stable Diffusion]], and ComfyUI.\n- The [website](https://comfy.icu/) offers [[ComfyUI]] Cloud services for running and deploying workflows without the need for downloads or installs. Users can pay only for active GPU usage, avoiding idle time and unnecessary costs. ComfyICU provides ready-to-use creative workflows and a simple, scalable API for production. The platform aims to simplify workflow creation and deployment, offering fast performance and cost-efficiency. Users can access over 5000 happy users' testimonials and FAQs for more information.\n- The [video](https://www.youtube.com/watch?v=SMOM1bIY5yA) titled 'EASY [[Inpainting]] in [[ComfyUI]] with SAM (segment Anything) | Creative Workflow Tutorial' provides a tutorial on using SAM for inpainting in ComfyUI.\n- The [wiki page](https://github.com/chrisgoringe/Comfy-Custom-Node-How-To/wiki) provides a practical and [[collaborative]] guide on developing custom nodes for [[ComfyUI]]. The guide is unofficial and focuses on practicality over formality, encouraging collaboration through Q&A-style discussions. It covers various topics related to custom node development, such as control flow, data types, and UI design.\n- The [Comfy Workflows](https://[[ComfyWorkFlows]].com/) website offers a platform for sharing art and workflows, with features like [[ComfyUI]] Launcher for running workflows with zero setup. Users can explore thousands of workflows created by the community. Trending creators and the latest images and videos are showcased on the site.\n- Custom nodes for interpolating between, well, everything in the [[Stable Diffusion]] [[ComfyUI]]. The [GitHub repository](https://github.com/shockz0rz/ComfyUI_InterpolateEverything) contains functionality to create preprocessed ControlNet OpenPose inputs midway between two images. Future features include line-art interpolation. To install, follow the provided instructions.\n- The [GitHub repository](https://github.com/xXAdonesXx/NodeGPT) contains [[ComfyUI]] Extension Nodes for Automated Text Generation. The repository is under development, with features like autogen, automated task solving, and group chat capabilities. The repository includes various folders and files for different functionalities. To contribute, users can submit pull requests, suggestions, or issue reports. The repository is licensed under AGPL-3.0. The repository has 314 stars, 24 forks, and 4 contributors.\n- The [AutoGen Advanced Tutorial](https://www.youtube.com/watch?v=PUPO2tTyPOo) on YouTube focuses on building incredible AI AGENT teams. The tutorial delves into advanced techniques for creating AI teams and enhancing their capabilities.\n- The [GitHub repository](https://github.com/olegchomp/TDComfyUI) provides a TouchDesigner interface for [[ComfyUI]], offering features like workflow creation and image send/receive. The repository includes installation instructions and resources for using the TDComfyUI component. It also offers guidance on connecting to [[Stable Diffusion]] and utilising ComfyUI settings for optimal performance.\n- The [GitHub repository](https://github.com/NimaNzrii/[[ComfyUI]]-photoshop) showcases the ComfyUI plugin for Photoshop, offering AI-powered image generation features. The plugin enables unlimited generative fill, customizable back-end workflow, and one-click image transformation. System requirements include a minimum of 6GB graphics memory and 12GB RAM. Installation involves downloading the plugin from a provided link or locally via a .CCX file. Additional files are required for specific functionalities. Support and contributions are encouraged through GitHub.\n- The [model](https://civitai.com/models/121728/aerial-view-of-the-building) titled 'Aerial view of the building' offers a high-definition training set for urban bird's-eye views, encompassing a variety of domestic and foreign architectural drawings. The model is based on [[LoRA DoRA etc]] technology and has received positive reviews. The training set is designed for cityscape and building enthusiasts.\n\n\t- #### Veilid\n\t- A Peer-to-Peer Privacy Mesh Project\n- Veilid is an open-source, mobile-first, networked application frameworkfor building decentralized apps with networking, distributed datastorage, and built-in IP privacy without reliance on external services.\n-\n\t- **Platforms**: Runs on Linux, Mac, Windows, Android, iOS, and in browsers via WASM. Bindings available in Rust, Dart, and other languages.\n-\n\t- **Protocols**: Supports UDP, TCP, WebSockets. DNS only used briefly during bootstrap.\n-\n\t- **Encryption**: Uses Ed25519, XChaCha20, BLAKE3 for end-to-end encryption and authentication.\n-\n\t- **Storage**: Distributed hash table for data records close to node keys. Popular data replicated.\n-\n\t- **Routing**: Nodes help each other connect. Routing based on node IDs. Private routing over encrypted hops.\n-\n\t- **Goals**: Enable decentralized apps without reliance on centralized corporate systems.\n- Key features include strong cryptography, ability to run on a variety ofplatforms, distributed and replicated data storage, and private routingto provide IP privacy. The decentralized design aims to avoid issueswith centralized and corporate controlled systems.\n-\n\n- # Node based\n\t- [Rivet (ironcladapp.com)](https://rivet.ironcladapp.com/) is a [[Interfaces]] for [[Infrastructure]] [[Hardware and Edge]] based [[Agents]] using [[Large language models]]\n\t- [[Node based visual interfaces]] is a  [[Interfaces]] for [[Large language models]] which is open source and [locally hosted.](https://github.com/FlowiseAI/Flowise) also see [[Langflow]]\n\t- [[Langflow]] [[Interfaces]] for [[Large language models]] builder, with slightly more features than [[Node based visual interfaces]] which is another one I need to try soon from their [github](https://github.com/logspace-ai/langflow)\n\t- ComfyUI is a [[Interfaces]] for [[Stable Diffusion]]. It allows very high levels of control over Diffusion Models by leveraging open source extensions and a vibrant developer and creator community. The tool's flexibility and efficiency comes from its innovative design philosophy, that prioritizes user customization and interaction.\n\n\t\t- ## Ontological Layer for Metaverse\n\t\t\t- To approach the problem of using Mamba to analyze formal ontological graphs as used by the W3C, we can draw upon several techniques and architectures discussed in the Mamba literature. Here's a proposed approach:\n\t\t\t- Data Preprocessing:\n\t\t\t\t- Graph Normalization:\n\t\t\t\t\t- Ensure consistent formatting and structure of the ontological graphs\n\t\t\t\t\t- Handle missing or inconsistent data\n\t\t\t\t\t- Normalize node and edge labels\n\t\t\t\t- Graph Merging:\n\t\t\t\t\t- Combine multiple ontological graphs into a single unified graph\n\t\t\t\t\t- Resolve conflicts and inconsistencies between different ontologies\n\t\t\t\t\t- Establish mappings between equivalent concepts across ontologies\n\t\t\t\t- Graph Embedding:\n\t\t\t\t\t- Generate low-dimensional vector representations of nodes and edges\n\t\t\t\t\t- Preserve the semantic relationships and structure of the ontological graphs\n\t\t\t\t\t- Use techniques like RDF2Vec, TransE, or Graph Convolutional Networks (GCNs)\n\t\t\t- Mamba Architecture:\n\t\t\t\t- Graph-Mamba:\n\t\t\t\t\t- Adapt the Mamba architecture to handle graph-structured data\n\t\t\t\t\t- Utilize the selective state space mechanism to capture long-range dependencies in the ontological graphs\n\t\t\t\t\t- Achieve efficient memory usage and reduced computational complexity compared to traditional graph neural networks (GNNs)\n\t\t\t\t- Multi-dimensional Sequencing (Mamba-ND):\n\t\t\t\t\t- Treat the ontological graphs as multi-dimensional sequences (e.g., node features, edge types, and graph structure)\n\t\t\t\t\t- Apply Mamba-ND to capture dependencies across all dimensions\n\t\t\t- Hybrid Architectures:\n\t\t\t\t- Combine Graph-Mamba with other graph neural network architectures (e.g., GCNs, GraphSAGE)\n\t\t\t\t- Leverage the strengths of both approaches to capture local and global patterns in the ontological graphs\n\t\t\t\t-\n\t\t\t\t-\n\t\t\t- ```mermaid\n\t\t\t  graph TD\n\t\t\t   A[Ontological Graphs] --> B(Data Preprocessing)\n\t\t\t   B --> C{Mamba Architecture}\n\t\t\t   C --> D[Graph-Mamba]\n\t\t\t   C --> E[Multi-dimensional Sequencing Mamba-ND]\n\t\t\t   C --> F[Hybrid Architectures]\n\t\t\t   D --> G[Comprehensive Graph Representation]\n\t\t\t   E --> G\n\t\t\t   F --> G\n\t\t\t   G --> H[Ontology Alignment]\n\t\t\t   G --> I[Knowledge Graph Completion]\n\t\t\t   G --> J[Semantic Similarity]\n\t\t\t   ```\n\t\t\t\t-\n\t\t\t\t- TODO What are the specific ontologies being used (e.g., RDF, OWL)?\n\t\t\t\t- TODO Are there any domain-specific requirements or constraints to consider?\n\t\t\t\t- TODO What are the desired output tasks (e.g., ontology alignment, knowledge graph completion, semantic similarity)?\n\t\t\t\t- TODO How large and complex are the ontological graphs being analyzed?\n\t\t\t\t- TODO Handling scalability issues for large-scale ontological graphs\n\t\t\t\t- TODO Incorporating domain knowledge and ontology-specific constraints\n\t\t\t\t- TODO Leveraging transfer learning from pre-trained models on similar ontological graphs\n\t\t\t\t- TODO Evaluating the model's performance using appropriate graph-based metrics and validation techniques\n\t\t\t\t- TODO Interpreting and visualizing the learned graph representations for ontology engineers and domain experts\n\n\t\t- ## Difficulty adjustment\n\t\t\t\t- Node operators, [who enforce the consensus](https://www.truthcoin.info/blog/measuring-decentralization/) rule-set, which the miners must abide by in order to propagate new transaction into the network. In return node operators optimise their trust minimisation, and help protect the network from changes which might undermine their speculation and use of the tokens.[[blocksizewars]]\n\t\t\t\t- There are currently around [15,000 Bitcoin nodes](https://bitnodes.io/)distributed across the world. Since IT engineer[Stadicus](https://stadicus.com/) released his [Raspiboltguide](https://raspibolt.org/backstory.html) in 2017 there has been anexplosion of small scale Bitcoin and Lightning node operators. Aroundthirty thousand Raspberry Pi Lightning nodes (which are also bydefinition Bitcoin nodes) run one of a big selection of [open sourcedistributions](https://github.com/bavarianledger/bitcoin-nodes). We willbuild toward our own throughout the book.\n\n\t\t- ### Segmentation\n\t\t\t- ![Segmentation for fashion](https://raw.githubusercontent.com/cozymantis/human-parser-comfyui-node/main/assets/lipexample.png)\n\t\t\t-\n\n- # Node based\n\t- [Rivet (ironcladapp.com)](https://rivet.ironcladapp.com/) is a [[Interfaces]] for [[Infrastructure]] [[Hardware and Edge]] based [[Agents]] using [[Large language models]]\n\n\t\t- ### Scalability and Performance\n\t\t- Key features of USD include:\n\t\t- Hierarchical scene composition\n\t\t- Layered overrides and customization\n\t\t- Instancing and referencing\n\t\t- Key aspects of Omniverse Variations include:\n\t\t- Variant sets: Collections of related variants for a specific purpose (e.g., material variations, level-of-detail variants)\n\t\t- Variant selection: Specifying which variant from each variant set should be active at any given time\n\t\t- Variant authoring: Creating and modifying variants using USD editing tools or supported 3D software applications\n\t\t- USD import and export: Blender can read and write USD files, enabling seamless exchange of 3D data with other USD-compatible applications\n\t\t- Omniverse Connector: A plugin that enables Blender to connect to Omniverse servers for real-time collaboration and synchronization\n\t\t- USD schema support: Blender supports various USD schemas, such as UsdGeom, UsdShade, and UsdSkel, allowing for the preservation of important 3D data when exchanging files\n\t\t- Node-based scene composition: Users can create and connect nodes representing 3D objects, materials, lights, and other scene elements\n\t\t- Attribute editing: Modify the properties of 3D objects and scene elements using the node interface\n\t\t- Variant management: Create and switch between different variations of objects and scenes\n\n\t\t- ## Difficulty adjustment\n\t\t\t\t- Node operators, [who enforce the consensus](https://www.truthcoin.info/blog/measuring-decentralization/) rule-set, which the miners must abide by in order to propagate new transaction into the network. In return node operators optimise their trust minimisation, and help protect the network from changes which might undermine their speculation and use of the tokens.[[blocksizewars]]\n\t\t\t\t- There are currently around [15,000 Bitcoin nodes](https://bitnodes.io/)distributed across the world. Since IT engineer[Stadicus](https://stadicus.com/) released his [Raspiboltguide](https://raspibolt.org/backstory.html) in 2017 there has been anexplosion of small scale Bitcoin and Lightning node operators. Aroundthirty thousand Raspberry Pi Lightning nodes (which are also bydefinition Bitcoin nodes) run one of a big selection of [open sourcedistributions](https://github.com/bavarianledger/bitcoin-nodes). We willbuild toward our own throughout the book.\n\n\t\t- ## Bitcoin script and miniscript\n\t\t\t\t- There are currently around [15,000 Bitcoin nodes](https://bitnodes.io/)distributed across the world. Since IT engineer[Stadicus](https://stadicus.com/) released his [Raspiboltguide](https://raspibolt.org/backstory.html) in 2017 there has been anexplosion of small scale Bitcoin and Lightning node operators. Aroundthirty thousand Raspberry Pi Lightning nodes (which are also bydefinition Bitcoin nodes) run one of a big selection of [open sourcedistributions](https://github.com/bavarianledger/bitcoin-nodes). We willbuild toward our own throughout the book.\n\n- ## USD Composer\n\t- USD Composer is a powerful visual programming tool provided by Omniverse that enables users to create and modify USD scenes using a node-based interface. It allows for the composition, layout, and editing of 3D scenes without the need for manual coding.\n\t\t- Key features of USD Composer include:\n\t\t- Node-based scene composition: Users can create and connect nodes representing 3D objects, materials, lights, and other scene elements\n\t\t- Attribute editing: Modify the properties of 3D objects and scene elements using the node interface\n\t\t- Variant management: Create and switch between different variations of objects and scenes\n\t\t- Extensibility: Custom nodes and extensions can be developed to extend the functionality of USD Composer\n\n- ##### 2.4.1 Veilid\n\t- A Peer-to-Peer Privacy Mesh Project\n\t  Veilid is an open-source, mobile-first, networked application framework for building decentralized apps with networking, distributed data storage, and built-in IP privacy without reliance on external services.\n\t  [Platforms] : Runs on Linux, Mac, Windows,     Android, iOS, and in browsers via WASM. Bindings available in Rust,     Dart, and other languages.     [Protocols] : Supports UDP, TCP, WebSockets.     DNS only used briefly during bootstrap.     [Encryption] : Uses Ed25519, XChaCha20, BLAKE3     for end-to-end encryption and authentication.     [Storage] : Distributed hash table for data     records close to node keys. Popular data replicated.     [Routing] : Nodes help each other connect.     Routing based on node IDs. Private routing over encrypted hops.     [Goals] : Enable decentralized apps without     reliance on centralized corporate systems.\n\t  Key features include strong cryptography, ability to run on a variety of platforms, distributed and replicated data storage, and private routing to provide IP privacy. The decentralized design aims to avoid issues with centralized and corporate controlled systems.\n\n- ## Unsorted links\n\t- [ZHO-ZHO-ZHO/ComfyUI-Gemini: Using Gemini in ComfyUI (github.com)](https://github.com/ZHO-ZHO-ZHO/ComfyUI-Gemini)\n\t- A GitHub repository that provides instructions on using Gemini in ComfyUI.\n\t- [Expanding Horizons: Outpainting Mastery in ComfyUI (youtube.com)](https://www.youtube.com/watch?v=fFdYdTzq7Kg)\n\t- A YouTube video tutorial that demonstrates how to master outpainting in ComfyUI.\n\t- [Chaoses-Ib/ComfyScript: A Python front end for ComfyUI (github.com)](https://github.com/Chaoses-Ib/ComfyScript)\n\t- This GitHub repository contains a Python front end for ComfyUI, known as ComfyScript.\n\t- [Aerial view of the building（建筑鸟瞰图）\n\t- v1.0 | Stable Diffusion LoRA | Civitai](https://civitai.com/models/121728/aerial-view-of-the-building)\n\t- Civitai provides a stable diffusion LoRA model for generating an aerial view of a building using ComfyUI.\n\t- [ComfyUI nodes based](https://github.com/comfyanonymous/ComfyUI)\n\t- A GitHub repository that provides nodes-based examples and workflows for ComfyUI.\n\t- 🎬\n\t\t- [How 2 Canvas Node YouTube](https://www.youtube.com/watch?v=DROM8vfIYUY)\n\t\t- A YouTube video tutorial that explains how to use the Canvas Node in ComfyUI.\n\t\t- [This One Simple Plugin Adds Realtime AI Assistance to Krita\n\t\t- YouTube](https://www.youtube.com/watch?v=AU8NDSBIS1U)\n\t\t- A YouTube video that introduces a plugin for Krita that adds realtime AI assistance using ComfyUI.\n\t\t- [Tutorials from first principles](https://www.youtube.com/watch?v=reimr3jZ8lI)\n\t\t- A YouTube video tutorial series that covers ComfyUI from the basic principles.\n\t\t- [modular workflow](https://www.youtube.com/watch?v=ppE1W0-LJas)\n\t\t- A YouTube video that showcases a modular workflow in ComfyUI.\n\t\t- [Detailed youtube tutorials](https://www.youtube.com/@sedetweiler)\n\t\t- A YouTube channel with detailed tutorials on using ComfyUI.\n\t\t- 🔧\n\t\t\t- [phineas-pta/comfy-trt-test: attempt to use TensorRT with ComfyUI (github.com)](https://github.com/phineas-pta/comfy-trt-test)\n\t\t\t- A GitHub repository that attempts to use TensorRT with ComfyUI.\n\t\t\t- [gameltb/ComfyUI_stable_fast: Experimental usage of stable-fast and TensorRT. (github.com)](https://github.com/gameltb/ComfyUI_stable_fast)\n\t\t\t- This GitHub repository provides an experimental usage of stable-fast and TensorRT in ComfyUI.\n\t\t\t- [Acly/krita-ai-diffusion: Streamlined interface for generating images with AI in Krita. Inpaint and outpaint with optional text prompt, no tweaking required. (github.com)](https://github.com/Acly/krita-ai-diffusion?tab=readme-ov-file)\n\t\t\t- A GitHub repository that offers a streamlined interface for generating images with AI in Krita using ComfyUI.\n\t\t\t- [Sytan workflow (contains js!)](https://github.com/SytanSD/Sytan-SDXL-ComfyUI)\n\t\t\t- This GitHub repository contains a workflow for ComfyUI that includes JavaScript files.\n\t\t- 🌐\n\t\t\t- [(2) Plush-for-ComfyUI style_prompt, can now use ChatGPT to create prompts from images : comfyui (reddit.com)](https://www.reddit.com/r/comfyui/comments/18uincm/plushforcomfyui_style_prompt_can_now_use_chatgpt/)\n\t\t\t- A Reddit post that discusses the Plush-for-ComfyUI style_prompt and its capability to create prompts from images using ChatGPT.\n\t\t\t- [(1) AP Workflow 6.0 for ComfyUI\n\t\t\t- Now with support for SD 1.5 and HiRes Fix, IPAdapter, Prompt Enricher via local LLMs (and OpenAI), and a new Object Swapper + Face Swapper, FreeU v2, XY Plot, ControlNet and ControlLoRAs, SDXL Base + Refiner, Hand Detailer, Face Detailer, Upscalers, ReVision, etc. : StableDiffusion (reddit.com)](https://www.reddit.com/r/StableDiffusion/comments/17v0bo3/ap_workflow_60_for_comfyui_now_with_support_for/)\n\t\t\t- A Reddit post about the AP Workflow 6.0 for ComfyUI, which includes various features and enhancements.\n\t\t\t- [UI node packs on civitai](https://civitai.com/tag/comfyui)\n\t\t\t- Civitai provides UI node packs for ComfyUI.\n\t\t- 📑\n\t\t\t- [fictions-ai/sharing-is-caring (github.com)](https://github.com/fictions-ai/sharing-is-caring/tree/main)\n\t\t\t- This GitHub repository contains various resources related to ComfyUI and its applications.\n\t\t\t- [Wiki full of links](https://wyrde.github.io/ComfyResources/nodes/)\n\t\t\t- A comprehensive wiki filled with links to resources, tutorials, and examples related to ComfyUI.\n\t\t\t- [Images / workflows](https://comfyworkflows.com/)\n\t\t\t- ComfyWorkflows offers a collection of images and workflows created using ComfyUI.\n\t\t- Using LLMs in ComfyUI\n\t\t\t- [ZHO-ZHO-ZHO/ComfyUI-Gemini: Using Gemini in ComfyUI (github.com)](https://github.com/ZHO-ZHO-ZHO/ComfyUI-Gemini)\n\t\t-\n\t\t- [Expanding Horizons: Outpainting Mastery in ComfyUI (youtube.com)](https://www.youtube.com/watch?v=fFdYdTzq7Kg)\n\t\t- https://arxiv.org/abs/2309.11497\n\t\t- TensorRT converter [phineas-pta/comfy-trt-test: attempt to use TensorRT with ComfyUI (github.com)](https://github.com/phineas-pta/comfy-trt-test)\n\t\t- [gameltb/ComfyUI_stable_fast: Experimental usage of stable-fast and TensorRT. (github.com)](https://github.com/gameltb/ComfyUI_stable_fast)\n\t\t- https://perilli.com/ai/comfyui/#soon\n\t\t-\n\t\t- [Chaoses-Ib/ComfyScript: A Python front end for ComfyUI (github.com)](https://github.com/Chaoses-Ib/ComfyScript)\n\t\t- Model leaderboard  [SDXL Model Compare\n\t\t\t- Google Sheets](https://docs.google.com/spreadsheets/d/1IYJw4Iv9M_vX507MPbdX4thhVYxOr6-IThbaRjdpVgM/edit#gid=0)\n\t\t- [fictions-ai/sharing-is-caring (github.com)](https://github.com/fictions-ai/sharing-is-caring/tree/main)\n\t\t- [chaojie/ComfyUI-MotionCtrl (github.com)](https://github.com/chaojie/ComfyUI-MotionCtrl)\n\t\t- [(2) Plush-for-ComfyUI style_prompt, can now use ChatGPT to create prompts from images : comfyui (reddit.com)](https://www.reddit.com/r/comfyui/comments/18uincm/plushforcomfyui_style_prompt_can_now_use_chatgpt/)\n\t\t- [Ferniclestix](https://www.youtube.com/@ferniclestix)\n\t\t\t- [How 2 Canvas Node YouTube](https://www.youtube.com/watch?v=DROM8vfIYUY)\n\t\t- [Nerdy Rodent YouTube](https://www.youtube.com/@NerdyRodent)\n\t\t- [Acly/krita-ai-diffusion: Streamlined interface for generating images with AI in [[Krita]]. Inpaint and outpaint with optional text prompt, no tweaking required. (github.com)](https://github.com/Acly/krita-ai-diffusion?tab=readme-ov-file)\n\t\t\t- TODO this needs the live view debugging\n\t\t\t- [This One Simple Plugin Adds Realtime AI Assistance to Krita YouTube](https://www.youtube.com/watch?v=AU8NDSBIS1U) [[Courses and Training]]\n\t\t- [ComfyUI nodes based](https://github.com/comfyanonymous/ComfyUI)\n\t\t- [Controlnet auto installer](https://github.com/Fannovel16/comfy_controlnet_preprocessors)\n\t\t- [LATENT Tricks Amazing ways to use ComfyUI](https://www.youtube.com/watch?v=OdMtJMzjNLg)\n\t\t- [latent consistency model](https://github.com/0xbitches/ComfyUI-LCM#img2img--vid2vid)\n\t\t- [UI node packs on civitai](https://civitai.com/tag/comfyui)\n\t\t- [(1) AP Workflow 6.0 for ComfyUI\n\t\t\t- Now with support for SD 1.5 and HiRes Fix, IPAdapter, Prompt Enricher via local LLMs (and OpenAI), and a new Object Swapper + Face Swapper, FreeU v2, XY Plot, ControlNet and ControlLoRAs, SDXL Base + Refiner, Hand Detailer, Face Detailer, Upscalers, ReVision, etc. : StableDiffusion (reddit.com)](https://www.reddit.com/r/StableDiffusion/comments/17v0bo3/ap_workflow_60_for_comfyui_now_with_support_for/)\n\t\t- [Workflows that can be loaded](https://github.com/comfyanonymous/ComfyUI_examples)\n\t\t- [Sytan workflow (contains js!)](https://github.com/SytanSD/Sytan-SDXL-ComfyUI)\n\t\t- [Impact pack and youtube](https://github.com/ltdrdata/ComfyUI-extension-tutorials/tree/Main/ComfyUI-Impact-Pack/workflow)\n\t\t- [youtube](https://www.youtube.com/watch?v=KvZ8ucBqyqw)\n\t\t- [Wiki full of links](https://wyrde.github.io/ComfyResources/nodes/)\n\t\t- [Tutorials from first principles](https://www.youtube.com/watch?v=reimr3jZ8lI)\n\t\t- [modular workflow](https://www.youtube.com/watch?v=ppE1W0-LJas)\n\t\t- [Detailed youtube tutorials](https://www.youtube.com/@sedetweiler)\n\t\t- [Prompt free diffusion](https://github.com/SHI-Labs/Prompt-Free-Diffusion)\n\t\t- Motion brush [chaojie/ComfyUI-DragNUWA (github.com)](https://github.com/chaojie/ComfyUI-DragNUWA)\n\t\t- [reference_only controlnet](https://gist.github.com/comfyanonymous/343e5675f9a2c8281fde0c440df2e2c6#file-workflow-json)\n\t\t- [Citivia autoprompt](https://civitai.com/models/123358/sdvn-comfyui-workflow-autoprompt-sdxl)\n\t\t- [Typescript client for comfyui](https://github.com/itsKaynine/comfy-ui-client)\n\t\t- [Animation workflow](https://www.reddit.com/r/comfyui/comments/15s6lpr/short_animation_img2img_in_comfyui_with/)\n\t\t- [Complex workflow tutorials](https://www.youtube.com/@ArchAi3D/videos)\n\t\t- [animation](https://www.youtube.com/watch?v=js4JeDF3v4g)\n\t\t- [Manual](https://blenderneko.github.io/ComfyUI-docs/)\n\t\t- [Turn comfyui to python](https://github.com/pydn/ComfyUI-to-Python-Extension)\n\t\t- [Share workflows](https://comfy.icu/)\n\t\t- [consistent character creation](https://www.reddit.com/r/comfyui/comments/16ceh10/i_succeeded_to_adapt_the_tutorial_character/)\n\t\t- [Edit in another tab](https://www.reddit.com/r/comfyui/comments/16d0wtx/workflow_using_15_scribble_controlnet_to_feed/)\n\t\t- [semi automated inpainting](https://www.youtube.com/watch?v=SMOM1bIY5yA)\n\t\t- [Canvas editor with layers](https://github.com/Lerc/canvas_tab)\n\t\t- [Build custom nodes howto](https://github.com/chrisgoringe/Comfy-Custom-Node-How-To/wiki)\n\t\t- [Images / workflows](https://comfyworkflows.com/)\n\t\t- [Interpolate everything (openpose)](https://github.com/shockz0rz/ComfyUI_InterpolateEverything)\n\t\t- [Autogen inside comfyui](https://github.com/xXAdonesXx/NodeGPT)\n\t\t- [autogen tutorial](https://www.youtube.com/watch?v=PUPO2tTyPOo)\n\t\t- [lcm consistency lora](https://github.com/0xbitches/ComfyUI-LCM)\n\t\t- [touch designer](https://github.com/olegchomp/TDComfyUI)\n\t\t- [NimaNzrii/comfyui-photoshop:](github.com)](https://github.com/NimaNzrii/comfyui-photoshop) [[Photoshop]] node inside of ComfyUi, send and get data from Photoshop\n\t\t\t- Reddit post on the matter  [(2) NEW AI NEWS! Photoshop to Comfyui V1 is Finally Released! : comfyui (reddit.com)](https://www.reddit.com/r/comfyui/comments/18jygtn/new_ai_news_photoshop_to_comfyui_v1_is_finally/)\n\t\t- [Aerial view of the building（建筑鸟瞰图）\n\t\t\t- v1.0 | Stable Diffusion LoRA | Civitai](https://civitai.com/models/121728/aerial-view-of-the-building)\n-\n\n- ## USD Composer\n\t- USD Composer is a powerful visual programming tool provided by Omniverse that enables users to create and modify USD scenes using a node-based interface. It allows for the composition, layout, and editing of 3D scenes without the need for manual coding.\n\t\t- Key features of USD Composer include:\n\t\t- Node-based scene composition: Users can create and connect nodes representing 3D objects, materials, lights, and other scene elements\n\t\t- Attribute editing: Modify the properties of 3D objects and scene elements using the node interface\n\t\t- Variant management: Create and switch between different variations of objects and scenes\n\t\t- Extensibility: Custom nodes and extensions can be developed to extend the functionality of USD Composer\n\n- ##### 2.4.1 Veilid\n\t- A Peer-to-Peer Privacy Mesh Project\n\t  Veilid is an open-source, mobile-first, networked application framework for building decentralized apps with networking, distributed data storage, and built-in IP privacy without reliance on external services.\n\t  [Platforms] : Runs on Linux, Mac, Windows,     Android, iOS, and in browsers via WASM. Bindings available in Rust,     Dart, and other languages.     [Protocols] : Supports UDP, TCP, WebSockets.     DNS only used briefly during bootstrap.     [Encryption] : Uses Ed25519, XChaCha20, BLAKE3     for end-to-end encryption and authentication.     [Storage] : Distributed hash table for data     records close to node keys. Popular data replicated.     [Routing] : Nodes help each other connect.     Routing based on node IDs. Private routing over encrypted hops.     [Goals] : Enable decentralized apps without     reliance on centralized corporate systems.\n\t  Key features include strong cryptography, ability to run on a variety of platforms, distributed and replicated data storage, and private routing to provide IP privacy. The decentralized design aims to avoid issues with centralized and corporate controlled systems.\n\n- ## Unsorted links\n\t- [ZHO-ZHO-ZHO/ComfyUI-Gemini: Using Gemini in ComfyUI (github.com)](https://github.com/ZHO-ZHO-ZHO/ComfyUI-Gemini)\n\t- A GitHub repository that provides instructions on using Gemini in ComfyUI.\n\t- [Expanding Horizons: Outpainting Mastery in ComfyUI (youtube.com)](https://www.youtube.com/watch?v=fFdYdTzq7Kg)\n\t- A YouTube video tutorial that demonstrates how to master outpainting in ComfyUI.\n\t- [Chaoses-Ib/ComfyScript: A Python front end for ComfyUI (github.com)](https://github.com/Chaoses-Ib/ComfyScript)\n\t- This GitHub repository contains a Python front end for ComfyUI, known as ComfyScript.\n\t- [Aerial view of the building（建筑鸟瞰图）\n\t- v1.0 | Stable Diffusion LoRA | Civitai](https://civitai.com/models/121728/aerial-view-of-the-building)\n\t- Civitai provides a stable diffusion LoRA model for generating an aerial view of a building using ComfyUI.\n\t- [ComfyUI nodes based](https://github.com/comfyanonymous/ComfyUI)\n\t- A GitHub repository that provides nodes-based examples and workflows for ComfyUI.\n\t- 🎬\n\t\t- [How 2 Canvas Node YouTube](https://www.youtube.com/watch?v=DROM8vfIYUY)\n\t\t- A YouTube video tutorial that explains how to use the Canvas Node in ComfyUI.\n\t\t- [This One Simple Plugin Adds Realtime AI Assistance to Krita\n\t\t- YouTube](https://www.youtube.com/watch?v=AU8NDSBIS1U)\n\t\t- A YouTube video that introduces a plugin for Krita that adds realtime AI assistance using ComfyUI.\n\t\t- [Tutorials from first principles](https://www.youtube.com/watch?v=reimr3jZ8lI)\n\t\t- A YouTube video tutorial series that covers ComfyUI from the basic principles.\n\t\t- [modular workflow](https://www.youtube.com/watch?v=ppE1W0-LJas)\n\t\t- A YouTube video that showcases a modular workflow in ComfyUI.\n\t\t- [Detailed youtube tutorials](https://www.youtube.com/@sedetweiler)\n\t\t- A YouTube channel with detailed tutorials on using ComfyUI.\n\t\t- 🔧\n\t\t\t- [phineas-pta/comfy-trt-test: attempt to use TensorRT with ComfyUI (github.com)](https://github.com/phineas-pta/comfy-trt-test)\n\t\t\t- A GitHub repository that attempts to use TensorRT with ComfyUI.\n\t\t\t- [gameltb/ComfyUI_stable_fast: Experimental usage of stable-fast and TensorRT. (github.com)](https://github.com/gameltb/ComfyUI_stable_fast)\n\t\t\t- This GitHub repository provides an experimental usage of stable-fast and TensorRT in ComfyUI.\n\t\t\t- [Acly/krita-ai-diffusion: Streamlined interface for generating images with AI in Krita. Inpaint and outpaint with optional text prompt, no tweaking required. (github.com)](https://github.com/Acly/krita-ai-diffusion?tab=readme-ov-file)\n\t\t\t- A GitHub repository that offers a streamlined interface for generating images with AI in Krita using ComfyUI.\n\t\t\t- [Sytan workflow (contains js!)](https://github.com/SytanSD/Sytan-SDXL-ComfyUI)\n\t\t\t- This GitHub repository contains a workflow for ComfyUI that includes JavaScript files.\n\t\t- 🌐\n\t\t\t- [(2) Plush-for-ComfyUI style_prompt, can now use ChatGPT to create prompts from images : comfyui (reddit.com)](https://www.reddit.com/r/comfyui/comments/18uincm/plushforcomfyui_style_prompt_can_now_use_chatgpt/)\n\t\t\t- A Reddit post that discusses the Plush-for-ComfyUI style_prompt and its capability to create prompts from images using ChatGPT.\n\t\t\t- [(1) AP Workflow 6.0 for ComfyUI\n\t\t\t- Now with support for SD 1.5 and HiRes Fix, IPAdapter, Prompt Enricher via local LLMs (and OpenAI), and a new Object Swapper + Face Swapper, FreeU v2, XY Plot, ControlNet and ControlLoRAs, SDXL Base + Refiner, Hand Detailer, Face Detailer, Upscalers, ReVision, etc. : StableDiffusion (reddit.com)](https://www.reddit.com/r/StableDiffusion/comments/17v0bo3/ap_workflow_60_for_comfyui_now_with_support_for/)\n\t\t\t- A Reddit post about the AP Workflow 6.0 for ComfyUI, which includes various features and enhancements.\n\t\t\t- [UI node packs on civitai](https://civitai.com/tag/comfyui)\n\t\t\t- Civitai provides UI node packs for ComfyUI.\n\t\t- 📑\n\t\t\t- [fictions-ai/sharing-is-caring (github.com)](https://github.com/fictions-ai/sharing-is-caring/tree/main)\n\t\t\t- This GitHub repository contains various resources related to ComfyUI and its applications.\n\t\t\t- [Wiki full of links](https://wyrde.github.io/ComfyResources/nodes/)\n\t\t\t- A comprehensive wiki filled with links to resources, tutorials, and examples related to ComfyUI.\n\t\t\t- [Images / workflows](https://comfyworkflows.com/)\n\t\t\t- ComfyWorkflows offers a collection of images and workflows created using ComfyUI.\n\t\t- Using LLMs in ComfyUI\n\t\t\t- [ZHO-ZHO-ZHO/ComfyUI-Gemini: Using Gemini in ComfyUI (github.com)](https://github.com/ZHO-ZHO-ZHO/ComfyUI-Gemini)\n\t\t-\n\t\t- [Expanding Horizons: Outpainting Mastery in ComfyUI (youtube.com)](https://www.youtube.com/watch?v=fFdYdTzq7Kg)\n\t\t- https://arxiv.org/abs/2309.11497\n\t\t- TensorRT converter [phineas-pta/comfy-trt-test: attempt to use TensorRT with ComfyUI (github.com)](https://github.com/phineas-pta/comfy-trt-test)\n\t\t- [gameltb/ComfyUI_stable_fast: Experimental usage of stable-fast and TensorRT. (github.com)](https://github.com/gameltb/ComfyUI_stable_fast)\n\t\t- https://perilli.com/ai/comfyui/#soon\n\t\t-\n\t\t- [Chaoses-Ib/ComfyScript: A Python front end for ComfyUI (github.com)](https://github.com/Chaoses-Ib/ComfyScript)\n\t\t- Model leaderboard  [SDXL Model Compare\n\t\t\t- Google Sheets](https://docs.google.com/spreadsheets/d/1IYJw4Iv9M_vX507MPbdX4thhVYxOr6-IThbaRjdpVgM/edit#gid=0)\n\t\t- [fictions-ai/sharing-is-caring (github.com)](https://github.com/fictions-ai/sharing-is-caring/tree/main)\n\t\t- [chaojie/ComfyUI-MotionCtrl (github.com)](https://github.com/chaojie/ComfyUI-MotionCtrl)\n\t\t- [(2) Plush-for-ComfyUI style_prompt, can now use ChatGPT to create prompts from images : comfyui (reddit.com)](https://www.reddit.com/r/comfyui/comments/18uincm/plushforcomfyui_style_prompt_can_now_use_chatgpt/)\n\t\t- [Ferniclestix](https://www.youtube.com/@ferniclestix)\n\t\t\t- [How 2 Canvas Node YouTube](https://www.youtube.com/watch?v=DROM8vfIYUY)\n\t\t- [Nerdy Rodent YouTube](https://www.youtube.com/@NerdyRodent)\n\t\t- [Acly/krita-ai-diffusion: Streamlined interface for generating images with AI in [[Krita]]. Inpaint and outpaint with optional text prompt, no tweaking required. (github.com)](https://github.com/Acly/krita-ai-diffusion?tab=readme-ov-file)\n\t\t\t- TODO this needs the live view debugging\n\t\t\t- [This One Simple Plugin Adds Realtime AI Assistance to Krita YouTube](https://www.youtube.com/watch?v=AU8NDSBIS1U) [[Courses and Training]]\n\t\t- [ComfyUI nodes based](https://github.com/comfyanonymous/ComfyUI)\n\t\t- [Controlnet auto installer](https://github.com/Fannovel16/comfy_controlnet_preprocessors)\n\t\t- [LATENT Tricks Amazing ways to use ComfyUI](https://www.youtube.com/watch?v=OdMtJMzjNLg)\n\t\t- [latent consistency model](https://github.com/0xbitches/ComfyUI-LCM#img2img--vid2vid)\n\t\t- [UI node packs on civitai](https://civitai.com/tag/comfyui)\n\t\t- [(1) AP Workflow 6.0 for ComfyUI\n\t\t\t- Now with support for SD 1.5 and HiRes Fix, IPAdapter, Prompt Enricher via local LLMs (and OpenAI), and a new Object Swapper + Face Swapper, FreeU v2, XY Plot, ControlNet and ControlLoRAs, SDXL Base + Refiner, Hand Detailer, Face Detailer, Upscalers, ReVision, etc. : StableDiffusion (reddit.com)](https://www.reddit.com/r/StableDiffusion/comments/17v0bo3/ap_workflow_60_for_comfyui_now_with_support_for/)\n\t\t- [Workflows that can be loaded](https://github.com/comfyanonymous/ComfyUI_examples)\n\t\t- [Sytan workflow (contains js!)](https://github.com/SytanSD/Sytan-SDXL-ComfyUI)\n\t\t- [Impact pack and youtube](https://github.com/ltdrdata/ComfyUI-extension-tutorials/tree/Main/ComfyUI-Impact-Pack/workflow)\n\t\t- [youtube](https://www.youtube.com/watch?v=KvZ8ucBqyqw)\n\t\t- [Wiki full of links](https://wyrde.github.io/ComfyResources/nodes/)\n\t\t- [Tutorials from first principles](https://www.youtube.com/watch?v=reimr3jZ8lI)\n\t\t- [modular workflow](https://www.youtube.com/watch?v=ppE1W0-LJas)\n\t\t- [Detailed youtube tutorials](https://www.youtube.com/@sedetweiler)\n\t\t- [Prompt free diffusion](https://github.com/SHI-Labs/Prompt-Free-Diffusion)\n\t\t- Motion brush [chaojie/ComfyUI-DragNUWA (github.com)](https://github.com/chaojie/ComfyUI-DragNUWA)\n\t\t- [reference_only controlnet](https://gist.github.com/comfyanonymous/343e5675f9a2c8281fde0c440df2e2c6#file-workflow-json)\n\t\t- [Citivia autoprompt](https://civitai.com/models/123358/sdvn-comfyui-workflow-autoprompt-sdxl)\n\t\t- [Typescript client for comfyui](https://github.com/itsKaynine/comfy-ui-client)\n\t\t- [Animation workflow](https://www.reddit.com/r/comfyui/comments/15s6lpr/short_animation_img2img_in_comfyui_with/)\n\t\t- [Complex workflow tutorials](https://www.youtube.com/@ArchAi3D/videos)\n\t\t- [animation](https://www.youtube.com/watch?v=js4JeDF3v4g)\n\t\t- [Manual](https://blenderneko.github.io/ComfyUI-docs/)\n\t\t- [Turn comfyui to python](https://github.com/pydn/ComfyUI-to-Python-Extension)\n\t\t- [Share workflows](https://comfy.icu/)\n\t\t- [consistent character creation](https://www.reddit.com/r/comfyui/comments/16ceh10/i_succeeded_to_adapt_the_tutorial_character/)\n\t\t- [Edit in another tab](https://www.reddit.com/r/comfyui/comments/16d0wtx/workflow_using_15_scribble_controlnet_to_feed/)\n\t\t- [semi automated inpainting](https://www.youtube.com/watch?v=SMOM1bIY5yA)\n\t\t- [Canvas editor with layers](https://github.com/Lerc/canvas_tab)\n\t\t- [Build custom nodes howto](https://github.com/chrisgoringe/Comfy-Custom-Node-How-To/wiki)\n\t\t- [Images / workflows](https://comfyworkflows.com/)\n\t\t- [Interpolate everything (openpose)](https://github.com/shockz0rz/ComfyUI_InterpolateEverything)\n\t\t- [Autogen inside comfyui](https://github.com/xXAdonesXx/NodeGPT)\n\t\t- [autogen tutorial](https://www.youtube.com/watch?v=PUPO2tTyPOo)\n\t\t- [lcm consistency lora](https://github.com/0xbitches/ComfyUI-LCM)\n\t\t- [touch designer](https://github.com/olegchomp/TDComfyUI)\n\t\t- [NimaNzrii/comfyui-photoshop:](github.com)](https://github.com/NimaNzrii/comfyui-photoshop) [[Photoshop]] node inside of ComfyUi, send and get data from Photoshop\n\t\t\t- Reddit post on the matter  [(2) NEW AI NEWS! Photoshop to Comfyui V1 is Finally Released! : comfyui (reddit.com)](https://www.reddit.com/r/comfyui/comments/18jygtn/new_ai_news_photoshop_to_comfyui_v1_is_finally/)\n\t\t- [Aerial view of the building（建筑鸟瞰图）\n\t\t\t- v1.0 | Stable Diffusion LoRA | Civitai](https://civitai.com/models/121728/aerial-view-of-the-building)\n-\n\n\n## Current Landscape (2025)\n\n- Industry adoption and implementations\n  - Metaverse platforms continue to evolve with focus on interoperability and open standards\n  - Web3 integration accelerating with decentralised identity and asset ownership\n  - Enterprise adoption growing in virtual collaboration, training, and digital twins\n  - UK companies increasingly active in metaverse development and immersive technologies\n\n- Technical capabilities\n  - Real-time rendering at photorealistic quality levels\n  - Low-latency networking enabling seamless multi-user experiences\n  - AI-driven content generation and procedural world building\n  - Spatial audio and haptics enhancing immersion\n\n- UK and North England context\n  - Manchester: Digital Innovation Factory supports metaverse startups and research\n  - Leeds: Holovis leads in immersive experiences for entertainment and training\n  - Newcastle: University research in spatial computing and interactive systems\n  - Sheffield: Advanced manufacturing using digital twin technology\n\n- Standards and frameworks\n  - Metaverse Standards Forum driving interoperability protocols\n  - WebXR enabling browser-based immersive experiences\n  - glTF and USD for 3D asset interchange\n  - Open Metaverse Interoperability Group defining cross-platform standards\n\n## Metadata\n\n- **Last Updated**: 2025-11-16\n- **Review Status**: Automated remediation with 2025 context\n- **Verification**: Academic sources verified\n- **Regional Context**: UK/North England where applicable",
  "properties": {
    "id": "6633f4c0-358f-44cf-bf05-d43c75febe36",
    "collapsed": "true",
    "- ontology": "true",
    "- term-id": "BC-0072",
    "- preferred-term": "Node",
    "- source-domain": "blockchain",
    "- status": "complete",
    "- public-access": "true",
    "- version": "1.0.0",
    "- last-updated": "2025-10-28",
    "- definition": "Network participant computer within blockchain systems, providing essential functionality for distributed ledger technology operations and properties.",
    "- maturity": "mature",
    "- source": "[[ISO/IEC 23257:2021]], [[IEEE 2418.1]], [[NIST NISTIR]]",
    "- authority-score": "0.95",
    "- owl:class": "bc:Node",
    "- owl:physicality": "VirtualEntity",
    "- owl:role": "Object",
    "- owl:inferred-class": "bc:VirtualObject",
    "- belongsToDomain": "[[CryptographicDomain]]",
    "- implementedInLayer": "[[SecurityLayer]]",
    "- is-subclass-of": "[[Blockchain Entity]], [[NetworkComponent]]",
    "public": "true",
    "- The [video](https://www.youtube.com/watch?v=AU8NDSBIS1U) showcases a plugin that provides realtime AI assistance to - ### OntologyBlock id": "krita-ontology collapsed:: true - ontology:: true - term-id:: mv-523525042165 - [[krita]], a digital painting software. The channel, Nerdy Rodent, offers tutorials on [[Stable Diffusion]], Generative AI, [[Large language models]], and other AI tools, catering to AI enthusiasts and professionals. The plugin enhances various aspects of AI technology, such as voice cloning, text-to-speech, and style transfer, making it a valuable resource for AI enthusiasts and artists alike."
  },
  "backlinks": [],
  "wiki_links": [
    "Upscaling",
    "LoRA DoRA etc",
    "collaborative",
    "krita-plugin",
    "NetworkComponent",
    "Stable Diffusion",
    "Checkpoints",
    "Krita",
    "contents",
    "SecurityLayer",
    "stable-diffusion",
    "Blockchain Entity",
    "IEEE 2418.1",
    "Photoshop",
    "ComfyUI",
    "experiment",
    "krita",
    "Large language models",
    "Hardware and Edge",
    "Artificial Intelligence",
    "Langflow",
    "Courses and Training",
    "IPAdapter",
    "blocksizewars",
    "Google",
    "Apple",
    "Sampling",
    "Diffusion Models",
    "license",
    "ISO/IEC 23257:2021",
    "Infrastructure",
    "ComfyWorkFlows",
    "Generative AI",
    "json",
    "Node based visual interfaces",
    "CryptographicDomain",
    "Dall-e",
    "Inpainting",
    "Interfaces",
    "NIST NISTIR",
    "AnimateDiff",
    "Gemini",
    "Tips and Tricks",
    "Agents"
  ],
  "ontology": {
    "term_id": "BC-0072",
    "preferred_term": "Node",
    "definition": "Network participant computer within blockchain systems, providing essential functionality for distributed ledger technology operations and properties.",
    "source_domain": "blockchain",
    "maturity_level": null,
    "authority_score": 0.95
  }
}