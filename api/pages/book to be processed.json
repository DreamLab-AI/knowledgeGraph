{
  "title": "book to be processed",
  "content": "- ### OntologyBlock\n  id:: book-to-be-processed-ontology\n  collapsed:: true\n\t- ontology:: true\n\t- term-id:: mv-189722789702\n\t- preferred-term:: book to be processed\n\t- source-domain:: metaverse\n\t- status:: active\n\t- public-access:: true\n\t- definition:: A component of the metaverse ecosystem focusing on book to be processed.\n\t- maturity:: mature\n\t- authority-score:: 0.85\n\t- owl:class:: mv:BookToBeProcessed\n\t- owl:physicality:: ConceptualEntity\n\t- owl:role:: Concept\n\t- belongsToDomain:: [[MetaverseDomain]]\n\t- #### Relationships\n\t  id:: book-to-be-processed-relationships\n\t\t- enables:: [[ImmersiveExperience]], [[Presence]], [[SpatialComputing]]\n\t\t- requires:: [[DisplayTechnology]], [[TrackingSystem]], [[RenderingEngine]]\n\t\t- bridges-to:: [[HumanComputerInteraction]], [[ComputerVision]], [[Robotics]]\n\n\t- #### OWL Axioms\n\t  id:: book-to-be-processed-owl-axioms\n\t  collapsed:: true\n\t\t- ```clojure\n\t\t  Declaration(Class(mv:BookToBeProcessed))\n\n\t\t  # Classification\n\t\t  SubClassOf(mv:BookToBeProcessed mv:ConceptualEntity)\n\t\t  SubClassOf(mv:BookToBeProcessed mv:Concept)\n\n\t\t  # Domain classification\n\t\t  SubClassOf(mv:BookToBeProcessed\n\t\t    ObjectSomeValuesFrom(mv:belongsToDomain mv:MetaverseDomain)\n\t\t  )\n\n\t\t  # Annotations\n\t\t  AnnotationAssertion(rdfs:label mv:BookToBeProcessed \"book to be processed\"@en)\n\t\t  AnnotationAssertion(rdfs:comment mv:BookToBeProcessed \"A component of the metaverse ecosystem focusing on book to be processed.\"@en)\n\t\t  AnnotationAssertion(dcterms:identifier mv:BookToBeProcessed \"mv-189722789702\"^^xsd:string)\n\t\t  ```\n\nexclude-from-graph-view:: true\n\n-\n- This knowledge graph is a high level view of technologies and their potential within the developing digital society narrative, focusing around thetransmission of value within and across immersive global networks, witha further focus on the Bitcoin monetary network.\n\t- Cybersecurity is top of the list of concerns in the [EU digital societystrategy](https://digital-strategy.ec.europa.eu/en/policies), just aheadof digital inclusion, so we started out with security best practices inmind, and we tried to end the investigation with inclusion. We aimed tosupport small and especially developing companies in our sector, givingthem a foot in the door on a global stage, without their costsspiralling.\n\t- Fortunately, we discovered a wealth of carefully crafted open source tools which can support this. [Open sourcesoftware](https://opensource.org/osd) is software that is available with source code that can be modified and distributed by anyone. The model extended into other creative works through things like the [CreativeCommons](https://creativecommons.org/licenses/) licenses, which isitself an [‘openstandard’](https://open-stand.org/about-us/principles/). We will returnto these themes throughout the book, and both this text and thesupporting code are open source. This means that anyone can access, use,and modify the source code for their own purposes. Open source software is often developed and maintained by a community of volunteers. Thisapproach to software development allows for more collaboration andinnovation, as well as greater transparency and security. Many popularsoftware programs, such as the Linux operating system and the Apache webserver, are open source.\n\t- The next chapter is about Money. In expanding our research on Bitcoin,we found that it’s impossible to think about the tech without opening upa whole line of questions about money itself. This is fine because weset out to look at global value transfer for business. It’s not atrivial subject though, and this section tries to overview why value andBitcoin are so enmeshed, then what other options there might be in theend (because Bitcoin has kicked off a whole slew of global adoptionoutside of itself). Chapter five extends this look at global moneysystems and examines corruption, governance, and opportunities fordigital society to affect change and equality.\n\t- #### The view of big business and governments\n\t\t- As adoption of these technologies increases it will be necessary forpeople, and AI actors, to pass economic value between themselves. These‘goods and services’ interactions, within the digital and virtual socialspaces should be underpinned by a trust system, which scales globallyand presents low friction. Current secure international payment railsare poorly suited to such interactions; indeed it is likely with legacysystems, that parties would be forced to leave the metaverseapplication, and instead navigate their banking applications to exchangevalue with overseas entities in a secure fashion. This might conceivablytake several days.\n\t\t- Fortunately, the whole landscape of money and [value transfer ischanging](https://www.omfif.org/futureofpayments2021/). Huge globalfinancial players are entering the space. HSBC have [justbought](https://sandboxgame.medium.com/hsbc-to-become-the-first-global-financial-services-provider-to-enter-the-sandbox-c066e4f48163)metaverse ‘land’ in The Sandbox, JP Morgan have [opened a‘lounge’](https://www.forbes.com/sites/ronshevlin/2022/02/16/jpmorgan-opens-a-bank-branch-in-the-metaverse-but-its-not-for-what-you-think-its-for/?sh=2fbd1e90158d)in another. The worlds largest hedge fund Bridgewater is stepping into[acquisition of digitalassets](https://uk.finance.yahoo.com/news/bitcoin-latest-price-crypto-ray-dalio-bridgewater-investment-fund-ethereum-094946686.html),and the world’s largest pension fund manager Blackrock [partnered withcrypto behemothCoinbase](https://blog.coinbase.com/coinbase-selected-by-blackrock-provide-aladdin-clients-access-to-crypto-trading-and-custody-via-b9e7144f313d)and is adding these assets to their management engine (which managestens of trillions of dollars). America’s oldest bank [BNYMellon](https://www.bnymellon.com/emea/en/about-us/newsroom/press-release/bny-mellon-launches-new-digital-asset-custody-platform-130305.html),and even the Nasdaq stock exchange are [offering service toinstitutionalclients](https://www.nasdaq.com/articles/nasdaq-to-launch-institutional-bitcoin-crypto-custody-services%3A-report),and Fidelity asset management are about to add [Bitcoin to their pensionplans](https://www.wsj.com/articles/fidelity-weighs-bitcoin-trading-on-brokerage-platform-11663008698),and they have asserted their view that Bitcoin is [different, and moremeaningful](https://www.fidelitydigitalassets.com/sites/default/files/documents/bitcoin-first.pdf)than the rest of the industry. Fidelity are also offering a [dedicatedmetaverse tradable fund](), and considering more direct productofferings through their retail investment engine. [Citigroup have aminisite](https://www.citivelocity.com/citigps/metaverse-and-money/)dedicated to “Metaverse and Money”. The front page of Goldman Sachsrecently says it all (Figure<a href=\"#fig:goldmanFront\" data-reference-type=\"ref\" data-reference=\"fig:goldmanFront\">1.2</a>).\n\t\t  ![]./assets/02936ad42e93249d8c236007f887f7216643cd90.jpg\n\t\t  The landing page of global<br />financial giant Goldman Sachs shows the hype.\n\t\t- In Gartners [2022 hype cyclereport](https://www.itp.net/emergent-tech/gartner-says-nfts-metaverse-web3-will-expand-immersive-experiences)one of their three “trend themes” says: it“The future of digitalexperience is immersive. A collection of emerging technologies supportssuch experiences through dynamic virtual representations, environmentsand ecosystems of customers and people, as well as new modes of userengagement. With these technologies, individuals can control their ownidentities and data and experience virtual ecosystems that can beintegrated with digital currencies. These technologies help reachcustomers in new ways to strengthen or open new revenue streams. Thetechnologies to watch that deliver evolving and expanding immersiveexperiences are metaverse, non-fungible tokens (NFTs), super apps andWeb3, decentralized identity, digital humans, digital twin of thecustomer and internal talent marketplaces.”\n\t\t- Of their recent investments KPMG global said: it“We’ve invested in astrong cryptoassets practice and we will continue to enhance and buildon our capabilities across Decentralized Finance (DeFi), Non-FungibleTokens (NFTs) and the Metaverse, to name a few”. This is not to say thatall fund managers are so positive. PGIM who manage over a trillionpounds globally have come out very strongly against the technology, witha [slew ofreports](https://www.pgim.com/megatrends/cryptocurrency-investing/bitcoin?)to warn off investors (Figure<a href=\"#fig:pgim\" data-reference-type=\"ref\" data-reference=\"fig:pgim\">1.3</a>).\n\t\t  ![]./assets/c55a94f2cd647ff4dd56b1b6e4b30f8488387bc4.jpg\n\t\t  PGIM cite ‘digiconomist’, a prominent critic.\n\t\t- It’s possible that for such huge organisations it makes better businesssense to take a punt on hype bubbles like this, than to do a proper duediligence with a team of internal staff who understand their business.These endorsements should be taken with a large pinch of salt. As [AlexJohnsonsays](https://newsletter.fintechtakes.com/p/metaverse-branches?s=r):it“At some point in the future, it’s possible that the digital worldsbeing built today will have aggregated sufficient user attention andengagement that financial services companies will need to invest in themetaverse as an acquisition and customer service channel. But we’re notthere yet. Until the metaverse is a little less empty, resist thetemptation to colonize it with branches and billboards.”\n\t\t- Meanwhile, Meta (ex Facebook) are launching their own [META Web3 andmetaverse](https://archive.ph/coyp2) token after abandoning Libre, theirglobal cryptocurrency. Libre became Diem, then was quietly acquired bySilvergate bank, who likely integrated it into their SEN settlementnetwork. Following the collase of Silvergate the bank was sold onwithout the SEN network, marking an ignominious end to the technologywhich possibly started the the rush to central bank digital currencies.[Google have formed a strategicpartnership](https://www.coinbase.com/blog/announcing-coinbase-google-cloud)with Coinbase, and [recentlyblogged](https://blog.youtube/inside-youtube/innovations-for-2022-at-youtube/):it“Web3 also opens up new opportunities for creators. We believe newtechnologies like blockchain and NFTs can allow creators to build deeperrelationships with their fans. Together, they’ll be able to collaborateon new projects and make money in ways not previously possible. Forexample, giving a verifiable way for fans to own unique videos, photos,art, and even experiences from their favourite creators could be acompelling prospect for creators and their audiences. There’s a lot toconsider in making sure we approach these new technologies responsibly,but we think there’s incredible potential as well. Finally, we couldn’thave a piece about innovation without touching on the metaverse! We’rethinking big about how to make viewing more immersive. ”\n\t\t- It’s already the case that the recent bubble of [hype isdwindling](https://www.forbes.com/sites/paultassi/2022/03/10/interest-in-nfts-and-the-metaverse-is-falling-fast/?),but the enormous investment into teams and startups will potentiallybear fruit in the next couple of years, and this perhaps hasimplications for small and medium-sized enterprises.\n\t\t- It’s fortunate timing for this book that the UK government has signalledenthusiasm for so called ‘stablecoins’ at the same time that the Bitcoinnetwork is being upgraded to transmit these GBP equivalent tokensaround. This gives us a very good idea what it is we can build into ourapplication stack. In the UK the government has stated it’s ambition tobe a [global cryptoasset technologyhub](https://www.gov.uk/government/news/government-sets-out-plan-to-make-uk-a-global-cryptoasset-technology-hub),and announced, then scrapped plans for the Royal Mint to issue a(novelty) NFT. Fuller, Economic Secretary to the Treasury [said in aspeech](https://drive.google.com/file/d/19ZYKLeT-ds3TueTpqSM22MUqB4gmN_Pl/view):it“We want to become the country of choice for those looking to create,innovate and build in the crypto space \\[...\\] By making this country ahospitable place for crypto technologies, we can attract investment,generate new jobs, benefit from tax revenues, create a wave of groundbreaking new products and services, and bridge the current position ofUK financial services into a new era.”\n\t\t- Their outline plans for [‘robustregulation’](https://www.gov.uk/government/news/uk-sets-out-plans-to-regulate-crypto-and-protect-consumers)were published after these seemingly supportive moves, and with thepublic consultation drawing to a close they have signalled theirwillingness to differentiate from Europe [within ayear](https://www.cnbc.com/2023/04/18/britain-could-see-crypto-regulation-in-12-months-lawmaker-says.html).Like the assertion by major global businesses it is too early to tellhow ‘sticky’ these claims are. Indeed the findings of a recent treasurycommittee looking at the sector suggest that there is much work to do,with [85% ofcompanies](https://committees.parliament.uk/committee/158/treasury-committee/news/175634/treasury-committee-85-of-crypto-firms-failed-to-meet-minimum-standards-according-to-fca/)failing to comply with itexisting law. The UK legal system is clear init’s view that all crypto assets [are‘property’](https://blockchain.bakermckenzie.com/2020/02/03/uk-court-confirms-bitcoins-status-as-property/).\n\t\t- A Law Commission consultation on “digital assets” [has proposed a new**third category** ofproperty](https://s3-eu-west-2.amazonaws.com/lawcom-prod-storage-11jsxou24uy7q/uploads/2022/07/Digital-Assets-Summary-Paper-Law-Commission-1.pdf):it\n\t\t-\n\t\t- it is composed of data represented in an electronic medium, including in the form of computer code, electronic, digital or analogue signals;\n\t\t-\n\t\t- it exists independently of persons and exists independently of the legal system;\n\t\t-\n\t\t- it is rivalrous such that use by one prejudices the ability of others;\n\t\t- Consensus seems to be that this is a thorough paper, and demonstratesstrong knowledge of digital assets by the authors.\n\t\t- Gartner’s [hype cycle](https://en.wikipedia.org/wiki/Gartner_hype_cycle)2022 features [Web3, distributed identity, NFTs, andMetaverse](https://www.gartner.com/en/articles/what-s-new-in-the-2022-gartner-hype-cycle-for-emerging-technologies)and can be seen in Figure<a href=\"#fig:gartners\" data-reference-type=\"ref\" data-reference=\"fig:gartners\">1.4</a>.\n\t\t  ![]./assets/c15696ca0d69da30a9e93ab5b15a3bf0fbce06da.png\n\t\t  The Gartners Hype Cycle for 2022.\n- .\n- ### Are DAOs useful for us?\n- A distributed autonomous organisation, or DAO is a governance structurewhich is built in distributed code on a blockchain smart contractsystem. Token holders have voting rights proportional to their holding.The first decentalised autonomous organisation was simply called “TheDAO” and was launched on the Ethereum network in 2016 after raisingaround $100M. [It quickly succumbed to a hack and the money wasdrained](https://www.gemini.com/cryptopedia/the-dao-hack-makerdao#section-what-is-a-dao).This event was an important moment in the development of Ethereum andresulted in a code fork which preserves two separate versions of thenetwork to this day, though one is falling into obsolescence. Again,this is covered in Shin’s book on the period in extreme detail, but itseems this stuff is falling into dusty history now, leaving only asomewhat tarnished and technically shaky legacy.[[cryptopians]]  In practice DAOs have very few committed ‘stakeholders’ and the samenames seem to crop up across multiple projects. Some crucial communitydecisions within large projects only poll a couple of dozen eligibleparticipants. Its might be that the experiment of distributed governanceis failing at this stage.\n- Perhaps more interesting is the use of the DAO concept to crowd fundglobal projects, currently especially for the acquisition of importantart or cultural items. DAOs are also emerging as a way to fund promisingtechnology projects, though this is reminiscent of the 2017 ICO crazewhich ended badly and is likely to [fall foul ofregulations](https://www.cftc.gov/PressRoom/PressReleases/8590-22).\n- Within the NFT and digital art space PleaserDAO has quickly establisheda strong following. “PleasrDAO is a collective of DeFi leaders, earlyNFT collectors and digital artists who have built a formidable yetbenevolent reputation for acquiring culturally significant pieces with acharitable twist.\n- Opensea wrangle between IPO and governance token.\n- ConstitutionDAO, Once upon a time in Shaolin etc\n- ##### Problems experienced to date\n- DAOs exist in a gray area with unclear legal recognition, leading tochallenges in fitting into existing legal frameworks and regulatorysystems. This lack of clarity raises concerns about how DAOs can complywith laws and regulations, potentially leading to legal disputes orconflicts with regulatory authorities. Security Risks and TechnologicalVulnerabilities:\n- Given their reliance on blockchain technology and smart contracts, DAOsare vulnerable to cyber threats, such as hacking and exploitation ofcode weaknesses. The decentralized nature of DAOs can further complicatesecurity management and responses to breaches, raising concerns aboutthe safety of assets and data managed by DAOs. Governance Inefficienciesand Democratic Deficiencies:\n- The non-hierarchical structure of DAOs, while innovative, may lead togovernance challenges, including potential inefficiencies indecision-making processes. There’s also a risk of DAOs deviating fromdemocratic principles, possibly leading to control by a limited group oftechnologically adept individuals (technocracy) or oligarchictendencies, which could marginalize participants who are lesstech-savvy. Fragmentation and Complexity in Governance Models:\n- The existence of multiple, simultaneous governance models within DAOscan lead to fragmentation, resulting in a lack of coherent and unifiedgovernance. This complexity can create confusion among participants andhinder effective decision-making, posing challenges to the democraticfunctioning and overall effectiveness of DAOs.\n- #### DAOs on Bitcoin\n- ##### Bisq DAO\n- One of the better designed DAOs is [BisqDAO](https://bisq.network/dao/). It’s slightly different design trys toaddress the issue of overly rigid software intersecting with moreintangible and fluid human governance needs. From their website:\n- it“Revenue distribution and decision-making cannot be decentralized withtraditional organization structures they require legal entities,jurisdictions, bank accounts, and more—all of which are central pointsof failure. The Bisq DAO replaces such legacy infrastructure withcryptographic infrastructure to handle project decision-making andrevenue distribution without such central points of failure.”\n- ##### Stackerstan\n- Stackerstan is a layer two protocol that operates on top of the Bitcoinand Nostr protocols. It aims to provide a decentralized and efficientplatform for people to collaborate and build valuable products andservices, without the need for agreement on what to build or how tobuild it, in a fully decentralised way.\n- Github contributor GazHayes [has awriteup](https://github.com/Stackerstan/interfarce/issues/20#issuecomment-1369329734)which is paraphrased below, explaining this very new and emergenttechnology stack.\n- The Stackerstan protocol is designed to be infinitely scalable, due tothe absence of “organizational mutexes”.\n- Stackerstan was anonymously posted in the Nostr telegram group at theend of 2022 and is a new project that aims to offer a more efficient anddecentralized way of solving problems compared to existing companies,institutions, and decentralized organizations. It utilizes a combinationof existing technologies, protocols, and concepts to create a systemthat allows people to spontaneously organize into highly efficient andintelligent groups. The platform is designed to be fair to everyoneinvolved and is completely non-custodial, meaning that it doesn’trequire a shared pot of money or any funding.\n-\n\t- Anyone can become a participant in Stackerstan by being added by an existing participant, creating a tree of participants that can be severed if a bad actor is present.\n-\n\t- Work is done within Stackerstan by continuously identifying problems and applying the simplest possible solution to these problems, expanding the scope of what Stackerstan can do.\n-\n\t- Any participant can log a problem and claim it to solve it, and the scope of what can become a problem to solve is not limited.\n-\n\t- Shares are created by a participant filing an expense to indicate the relative value of their work, which is a request to be repaid when Stackerstan generates revenue.\n-\n\t- Shares are approved expenses, and the only way for new shares to be created is by approving expenses for work done to solve problems.\n-\n\t- Participants with shares can vote to approve or reject new expenses, and there are rules to follow when voting on expenses.\n-\n\t- Stackerstan was created at block 761151 and has a single share to bootstrap the process, with a small number of shares created by approved expenses so far.\n-\n\t- Shareholders own all revenue generated by Stackerstan’s products and services, and revenue is distributed through two algorithms: first, paying back expenses in the order they were filed, and second, streaming dividends to whoever has received the least dividends per share owned.\n-\n\t- Stackerstan is non-custodial and does not require a shared pot of money, making it more effective and avoiding toxic situations.\n-\n\t- Voting on things like approving expenses is done with votepower, which quantifies a participant’s skin in the game.\n-\n\t- Lead time is a measure of a participant’s votepower and can be increased or decreased by one unit every 2016 blocks.\n-\n\t- A participant’s shares can only be transferred if their lead time is 0, and a participant can reduce their lead time to sell their shares.\n- ##### Mindmachine\n- The Mindmachine is a stateful Nostr client written in Go. This text isdirectly quoted from the [GazHayesgithub](https://github.com/gazhayes/mindmachine).\n-\n\t- Participants interact with the Mindmachine using Nostr Events. The Mindmachine subscribes to all Nostr event Kinds that it can handle, and attempts to update its state by processing them based on the rules in the Stackerstan Superprotocol.\n-\n\t- If an Event successfully triggers the Mindmachine to change state, the Event ID is appended to a Kind 640001 Nostr Event which the Mindmachine publishes once per Bitcoin block.\n-\n\t- The Mindmachine can rebuild anyone’s state by subscribing to their 640001 events and replaying the list of Nostr Events contained within.\n-\n\t- Consensus is based on Votepower. When a Participant with Votepower greater than 0 witnesses a new Mindmachine state, the Mindmachine hashes the state and publishes it in a Kind 640000 Nostr Event. This is effectively a vote for the witnessed state at a particular Bitcoin height.\n-\n\t- A Mindmachine state is considered stable when in excess of 50% of total Votepower has signed the same state and there is a chain of signatures back to the Ignition state. There are mechanisms to deal with voters disappearing.\n-\n\t- Participants who have a lot of Votepower will want to be able to prove they had a certain Mind-state at a particular height. To do so, they broadcast a Bitcoin transaction containing an OPRETURN of the state.\n- Because of the tight integration with Nostr it seems that is we were toallocate work to open communities then this would be the way to do it.\n- #### Risks\n- The most interesting thing about DAOs is that they belong more in thismoney chapter than they do in blockchain. As we have seen they’refinding most success as loosely regulated crowd funding platforms. If asmall company did find itself wishing to explore this fringe mechanismfor raising capital, then we would certainly recommend keeping a globaleye on evolving regulation and the onward legal exposure of the company.\n- ### Risks & Challenges?\n- Classic DID/SSI risks fragmentations. In all DID applications, scalingto a world where the user is managing potentially thousands of thesecritical cryptographic data files is daunting. Abstracting the guts ofthis away to make the use simple, and only mindful of thet right levelof information, turns out to be huge problem that nobody has solved It’snot clear that users want this. In the case of web of trust likepubky it’s a big piece of work for the users to rate all of theirdigital interactions with a trust metric.\n- Collaborative mixed reality---------------------------\n- ### Toward an open metaverse\n- The Openstand principles are a great starting place for what an openmetaverse might mean. [Theyare](https://open-stand.org/about-us/principles/):\n-\n\t- Cooperation: Respectful cooperation between standards organizations, whereby each respects the autonomy, integrity, processes, and intellectual property rules of the others.\n-\n\t- Adherence to Principles: Adherence to the five fundamental principles of standards development:\n-\n\t- Due process. Decisions are made with equity and fairness among        participants. No one party dominates or guides standards        development. Standards processes are transparent and        opportunities exist to appeal decisions. Processes for periodic        standards review and updating are well defined.\n-\n\t- Broad consensus. Processes allow for all views to be considered        and addressed, such that agreement can be found across a range        of interests.\n-\n\t- Transparency. Standards organizations provide advance public        notice of proposed standards development activities, the scope        of work to be undertaken, and conditions for participation.        Easily accessible records of decisions and the materials used in        reaching those decisions are provided. Public comment periods        are provided before final standards approval and adoption.\n-\n\t- Balance. Standards activities are not exclusively dominated by        any particular person, company or interest group.\n-\n\t- Openness. Standards processes are open to all interested and        informed parties.\n-\n\t- Collective Empowerment: Commitment by affirming standards organizations and their participants to collective empowerment by striving for standards that:\n-\n\t- are chosen and defined based on technical merit, as judged by        the contributed expertise of each participant;\n-\n\t- provide global interoperability, scalability, stability, and        resiliency;\n-\n\t- enable global competition;\n-\n\t- serve as building blocks for further innovation;\n-\n\t- contribute to the creation of global communities, benefiting        humanity.\n-\n\t- Availability: Standards specifications are made accessible to all for implementation and deployment. Affirming standards organizations have defined procedures to develop specifications that can be implemented under fair terms. Given market diversity, fair terms may vary from royalty-free to fair, reasonable, and non-discriminatory terms (FRAND).\n-\n\t- Voluntary Adoption: Standards are voluntarily adopted and success is determined by the market.\n- The push toward open standards is being joined (somewhat late) bycredible and established bodies [like theIEEE](https://spectrum.ieee.org/laying-foundation-for-extended-reality).It’s such a fast moving and under explored set of problems that thismovement toward standards will take a long time to even find it’s feet.Hopefully it’s clear to the reader that this kind of development guidesthe work here. In the wider “real-time social VR” various companies haveattempted to build closed ecosystems, for years. These now look morelike attempts at digital society, but are closer to isolated metaverses,or more usefully isolated digital ecosystems. This is still happening.There’s every chance that when Apple make their augmented reality playthis year or next they will keep their system closed off as this tendsto be their business model. Theo Priestly, CEO at Metanomics [pointsout](https://www.linkedin.com/feed/update/urn:li:activity:6977366421034967040/)that Chinese Giant Tencent are doing similar, and he cited Figure<a href=\"#fig:tencent\" data-reference-type=\"ref\" data-reference=\"fig:tencent\">7.1</a>;building a closed but tightly linked suite of businesses into somethingthat looks like a metaverse. The levels of investment which are beinghung under the metaverse moniker [are mindblowing](https://www.scmp.com/tech/policy/article/3194092/chinas-iphone-production-hub-henan-bets-its-future-metaverse),but that is not what we want to discuss as an end point for this book.\n  ![]./assets/e63f7f380108db361a71cb5cb9351e68f7c23a21.png\n  <a href=\"https://www.notboring.co/p/tencents-dreams\">McCormick attempts to guess the Tencent metaverse</a>\n- For our purposes in this product design the interface between theprevious chapter (NFTs) and this metaverse chapter is crucial. Punk6529is a pseudonymous twitter account and thought leader in the “crypto”space. The text below encapsulates much of the reasoning that led tothis book and product exploration, and is paraphrased [from thisthread](https://twitter.com/punk6529/status/1536046831045685248) for ourpurposes.\n- itBit by bit, the visualization layer of the internet will get betteruntil it is unrecognisably better (+/- 10 years). As the visualizationlayer of the internet gets better, digital objects will become moreuseful and more important. Avatars (2D and 3D), art, schoolwork, workwork, 3D virtual spaces and hundreds of other things. Not only will theobjects themselves become more important, they will lead to differentemergent behaviours. We see this already with avatars and mixedeponymous/pseudonymous/anonymous communities. Yes, it is the internetplumbing underneath, but just like social media changed human behaviouron the internet, metaverse type experiences will further change it. NFTTwitter + Discord + various virtual worlds is a form of early metaverse.I feel like I am entering a different world here, not just somewebsites. The most important question for the health of theinternet/metaverse/human society in the 2030s will be decided now. Andthat question is: \"who stores the definitive ownership records of thosedigital objects\". There are two answers: a company’s database OR ablockchain. If we end up with \"a company’s database\" we will end up withall the web dysfunctions, but worse. SMTP is an open protocol thatanyone can use so we don’t have societal level fights on \"who is allowedto use email\". Short messaging online ended up becoming Twitter. So weend up having the most absurd, surreal discussions on the topic of \"whois allowed to use short-messaging\" being dependant on \"who is the CEO ofTwitter\". There is no way this is the correct architecture for ourprogressively more digital economy.... If this is your first time aroundhere, we are fighting for an open metaverse.”\n- It seems that industry shares much of this opinion regarding an openmetaverse. The proposal of a persistent interactive digital universeonline is **so** vast that major players recognise that they will not beable to monopolise this space, though Facebook/Meta are clearlyattempting to. The [Metaverse StandardsForum](https://metaverse-standards.org/news/press-releases/leading-standards-organizations-and-companies-unite-to-drive-open-metaverse-interoperability/)is clearly an attempt by the other industry players to catch up and thenget out ahead of Meta in this regard. It’s also possible to view this asjust another land grab, but through the vehicle of a standards body.Time will tell. They say:\n- it“Announced today, The Metaverse Standards Forum brings togetherleading standards organizations and companies for industry-widecooperation on interoperability standards needed to build the openmetaverse. The Forum will explore where the lack of interoperability isholding back metaverse deployment and how the work of StandardsDeveloping Organizations (SDOs) defining and evolving needed standardsmay be coordinated and accelerated. Open to any organization at no cost,the Forum will focus on pragmatic, action-based projects such asimplementation prototyping, hackathons, plugfests, and open-sourcetooling to accelerate the testing and adoption of metaverse standards,while also developing consistent terminology and deployment guidelines.”\n- This looks like it will be a useful project and community for thepurposes outlined in this book, but the technology is young enough (inthat it doesn’t really exist) for multiple approaches to be trailed.\n- Europe is making metaverse a priority with [The Virtual and AugmentedReality IndustrialCoalition](https://digital-strategy.ec.europa.eu/en/policies/virtual-and-augmented-reality-coalition).President von der Leyen’s State of the Union [letter of intentsays](https://state-of-the-union.ec.europa.eu/system/files/2022-09/SOTEU_2022_Letter_of_Intent_EN_0.pdf):“We will continue looking at new digital opportunities and trends, suchas the metaverse.”\n- #### Primitives\n- OpenAI identified the following 5 points about metaverse, in response tothe query “What are 5 key points I should know when studying metaverse?”\n-\n\t- Metaverse is a virtual reality platform that allows users to interact with each other and with digital objects in a virtual space.\n-\n\t- Metaverse is a decentralized platform, meaning that there is no central authority or server that controls the platform.\n-\n\t- Metaverse is an open platform, meaning that anyone can develop applications for the platform.\n-\n\t- Metaverse is a secure platform, meaning that all data and transactions are encrypted and secure.\n-\n\t- Metaverse is a scalable platform, meaning that it can support a large number of users and a large number of transactions.\n- This is an unexpectedly great answer, probably the cleanest we havefound. The [Metaverse Standard Forum](https://metaverse-standards.org/)highlights the following, which reads like the output from a brainstormbetween academia and industry stakeholders.\n-\n\t- collaborative spatial computing\n-\n\t- interactive 3D graphics\n-\n\t- augmented and virtual reality\n-\n\t- photorealistic content authoring\n-\n\t- geospatial systems\n-\n\t- end-user content tooling\n-\n\t- digital twins\n-\n\t- real-time collaboration\n-\n\t- physical simulation\n-\n\t- online economies\n-\n\t- multi-user gaming\n-\n\t- new levels of scale and immersiveness.\n- It’s not a useless list by any means, but it lacks the kind of productfocus we need for detailed exploration of value and trust transfer.\n- Mystakidis identifies the following:[[mystakidis2022metaverse]]\n-\n\t- Principles\n-\n\t- Interoperable\n-\n\t- Open\n-\n\t- Hardware agnostic\n-\n\t- Network\n-\n\t- Technologies\n-\n\t- Virtual reality\n-\n\t- Augmented reality\n-\n\t- Mixed reality\n-\n\t- Affordances\n-\n\t- Immersive\n-\n\t- Embodiment\n-\n\t- Presence\n-\n\t- Identity construction\n-\n\t- Challenges\n-\n\t- Physical well-being\n-\n\t- Psychology\n-\n\t- Ethics\n-\n\t- Privacy\n- This is quite an academic list. A lot of these words will be explored inthe next section which is more of an academic literature review.\n- Nevelsteen attempted to identify key elements for a ‘virtual work’ in2018 and these are relevant now, and described rigorously in theappendix of his paper:[[nevelsteen2018virtual]]\n-\n\t- Shared Temporality, meaning that the distributed users of the virtual world share the same frame of time.\n-\n\t- Real time which he defines as “not turn based”.\n-\n\t- Shared Spatiality, which he says can include an ‘allegory’ of a space, as in text adventures. It seems this might extend to a spoken interface to a mixed reality metaverse.\n-\n\t- ONE Shard is a description of the WLAN network architecture, and conforms to servers in a connected open metaverse.\n-\n\t- Many human agents simply means that more than one person can be represented in the virtual world and corresponds to ‘social’ in our description.\n-\n\t- Many Software Agents corresponds to AI actors in our descriptions. Non playing characters would be the gaming equivalent.\n-\n\t- Virtual Interaction pertains to any ability of a user to interact actively with the persistent virtual scene, and is pretty much a given these days.\n-\n\t- Nonpausable isn’t even a word, but is pretty self explanatory.\n-\n\t- Persistence means that if human participants leave then the data of the virtual world continues. This applies to the scenes, the data representing actions, and objects and actors in the worlds.\n-\n\t- Avatar is interesting as it might seem that having avatar representations of connected human participants is a given. In fact the shared spaces employed by Nvidia for digital engineering do not.\n- Turning to industry; John Riccitiello, CEO of Unity Technologies saysthat metaverse is it\n- “The next generation of the internet that is:\n-\n\t- always real-time\n-\n\t- mostly 3D\n-\n\t- mostly interactive\n-\n\t- mostly social\n-\n\t- mostly persistent”\n- Expanding this slightly we will us the following primitives of what wethink are important for a metaverse:\n-\n\t- Fusing of digital and real life\n-\n\t- Social first\n-\n\t- Real time interactive 3d graphics first\n-\n\t- Persistent\n-\n\t- Supports ownership\n-\n\t- Supports user generated content[[ondrejka2004escaping]]\n-\n\t- Open and extensible\n-\n\t- Low friction economic actors and actions\n-\n\t- Trusted / secure\n-\n\t- Convergence of film and games\n-\n\t- Blurring of IP boundaries\n-\n\t- Blurring of narrative flow\n-\n\t- Multimodal and hardware agnostic\n-\n\t- Mobile first experiences\n-\n\t- Safeguarding, and governance\n- There is a **lot** of work for the creative and technical industries todo to integrate human narrative creativity this nascent metaverse, andit’s not even completely clear that this is possible, or even whatpeople want.\n- ### History\n- The word metaverse was coined by the author Neal Stephenson in his 1992novel Snowcrash. It started popping up soon after in [newsarticles](https://www.newscientist.com/article/mg14819994-000-how-to-build-a-metaverse/)and research papers,[[mclellan1993avatars]] but in the last five years ithas been finding a new life within a silicon valley narrative. Perhapsin response to this Stephenson is now working with a company called[Lamina1](https://www.lamina1.com/) which actually looks a lot like therest of this book, so perhaps we have been on the right track.\n- There were clear precursors to modern social VR, such as [VRML in the1990’s](https://www.howtogeek.com/778554/remembering-vrml-the-metaverse-of-1995/)which laid much of the groundwork for 3D content over networkedcomputers.\n- It might seem that there would be a clear path from there to now, interms of a metaverse increasingly meaning connected social virtualspaces, but this has not happened. Instead interest in metaverse as aconcept waned, MMORG (described later) filled in the utility, and thenrecently an entirely new definition emerged. Park and Kim surveyeddozens of different historical interpretations of the word, and thegenerational reboot they describe makes it even lessclear.[[park2022metaverse]] The concept of the Metaverse is extremelyplastic at this time (Figure<a href=\"#fig:muskWeb3\" data-reference-type=\"ref\" data-reference=\"fig:muskWeb3\">7.2</a>).\n- It’s arguable that what will be expanding in this chapter is moreappropriately ‘Cyberspace’ as described by William Gibson inNeuromancer[[gibson2019neuromancer]] it“A global domain within theinformation environment consisting of the interdependent network ofinformation systems infrastructures including the Internet,telecommunications networks, computer systems, and embedded processorsand controllers.”\n- Park and Kim identify the generational inflection point which has led tothe resurgence of the concept of Metaverse:[[park2022metaverse]]it“Unlike previous studies on the Metaverse based on Second Life, thecurrent Metaverse is based on the social value of Generation Z thatonline and offine selves are not different.”\n- Brett Leonard, writer director of Lawnmower Man talks about the pressingneed to get out in front of moral questions in the development ofmetaverse applications. He stressed that wellbeing will be a crucialunderpinning of the technology because of the inherent intimacy ofimmersion in virtual spaces. He suggests that emotional engagement withstoried characters is needed to satisfy the human need for narrative,and that this should be utopian by design to stave off the worst ofdystopian emergent characteristics of the technology.\n- The book will aim to build toward an understanding of metaverse as auseful social mixed reality, that allows low friction communication andeconomic activity, within groups, at a global scale. Cryptography anddistributed software can assist us with globally ‘true’ persistence ofdigital data, so we will look to integrate this with our social XR. Thisfocus on persistence, value, and trust means it’s most appropriate tofocus on business uses as there is more opportunity for value creationwhich will be important to bootstrap this technology.\n- Elsewhere in the book we state that metaverse is the worst of thetele-collaboration tool-kits, and in general we ‘believe’ this to betrue at this time. With that said Hennig-Thurau says the following in a[LinkedInpost](https://www.linkedin.com/feed/update/urn:li:activity:7020679507141361664/):itOur research finds that the performance of social interactions in theVR metaverse varies for different outcomes and settings, withproductivity and creativity being on par with Zoom (not higher, but alsonot lower) for the two experimental settings in which we studied theseconstructs. Thus, as of today, meeting in VR does not overcome all thelimitations that we are facing when using Zoom or Teams. But mostimportantly (to us), we find clear evidence that when people gettogether in the metaverse via VR, it creates SUBSTANTIALLY higher levelsof social presence among group members across ALL FIVE STUDY CONTEXTS,from idea generation to joint movie going. This is the main insight fromour study and the stuff we believe future uses of social virtual realitycan (and should) build on. We also explain that the effectiveness of VRmeetings can be further increased, and also how this can be done (byselecting the most appropriate settings, people, avatars, hardware,environments etc.).[[hennig2022social]]\n- We agree that with sufficiently informed guiding constraints in place,and smaller group sizes (ie, not a large scale social metaverse), thatthere is a path forward.\n- This chapter will first attempt to frame the context for telepresence(the academic term for communicating through technology), and thenexplain the increasingly polarised options for metaverse. It’s useful toprecisely identify the primitives of the product we would like to seehere, so this chapter is far more a review of academic literature in thefield, culminating in a proposed framework.\n  ![]./assets/05c60abfdb5138796c3e168be7f5b9653d60edbb.png\n  Elon Musk agrees with this on Twitter. It’s notable that Musk is now Twitters’ <a href=\"https://twitter.com/paraga/status/1511320953598357505\">biggest shareholder</a>, and has been vocal about web censorship on the platform.\n- ### Video conferencing, the status quo\n- This section has been adapted and updated for open source release, fromthe authors PhD thesis, with the permission of the University ofSalford.\n- Video-conferencing has become more popular as technology improves, as itgets better integrated with ubiquitous cloud business support suites,and as a function of the global pandemic and changing work patterns.There is obviously increasing demands for real-time communication acrossgreater distances.\n- The full effects of video-conferencing on human communication are stillbeing explored, as seen in the experimental [“TogetherMode”](https://news.microsoft.com/innovation-stories/microsoft-teams-together-mode/)within Microsoft Teams. Video-conferencing is presumed to be a somewhatricher form of communication than email and telephone, but not quite asinformative as face-to-face communication.\n- In this section we look at the influence of eye contact on communicationand how video-conferencing mediates both verbal and non-verbalinteractions. Facilitation of eye contact is a challenge that must beaddressed so that video-conferencing can approach the rich interactionsof face-to-face communication. This is an even bigger problem in theemerging metaverse systems, so it’s important that we examine thehistory and trajectory.\n- There is a tension emerging for companies who do not necessarily need toemploy remote meeting technology, but also cannot afford to ignore thecompetitive advantages that such systems bring. In an experimentpreformed well before the 2020 global pandemic at CTrip, Bloom et aldescribe how home working led to a 13% performance increase, of whichabout 9% was from working more minutes per shift (fewer breaks andsick-days) and 4% from more calls per minute (attributed to a quieterworking environment).[[Bloom2015]] Home workers also reported improvedwork satisfaction and experienced less turnover, but their promotionrate conditional on performance fell. This speaks to a lack ofmanagement capability with such systemic change. It’s clearly a complexand still barely understood change within business and management.\n- Due to the success of the experiment, CTrip rolled-out the option towork from home to the whole company, and allowed the experimentalemployees to re-select between the home or office. Interestingly, overhalf of them switched, which led to the gains almost doubling to 22%.This highlights the benefits of learning and selection effects whenadopting modern management practices like working from home.Increasingly this is becoming a choice issue for prospective employees,and an advantage for hiring managers to be able to offer it.\n- More recent research by Barrero, Bloom and Davies found that workingfrom home is likely to be “sticky.”[[barrero2021working]] They found:\n-\n\t- better-than-expected WFH experiences,\n-\n\t- new investments in physical and human capital that enable WFH,\n-\n\t- greatly diminished stigma associated with WFH,\n-\n\t- lingering concerns about crowds and contagion risks,\n-\n\t- a pandemic-driven surge in technological innovations that support WFH.\n- More recently Enterprise Collaboration Systems (ECS) provide richdocument management, sharing, and collaboration functionality across anorganisation. The enterprise ECS system may integrate collaborativevideo.[[prakash2020characteristic]] This is for instance the case withMicrosoft Teams / Sharepoint. This integration of ECS should beconsidered when thinking about social VR systems which wish to supportbusiness, value, and trust. It is very much the case that largetechnology providers are attempting to integrate their ‘business backend’ systems into their emerging metaverse systems. Open sourceequivalents are currently lacking.\n- #### Pandemic drives adoption\n- The ongoing global COVID-19 pandemic is [changing how peoplework](https://blog.yelp.com/news/the-future-of-work-is-remote/), towarda new global ‘normal’. Some ways of working are overdue transformation,and will be naturally disrupted. In the UK at least it seems that theremay be real appetite to shift away from old practises. This upheavalwill inevitably present both challenges and opportunities.\n- Highly technical workforces, especially, can [operate fromanywhere](https://globalworkplaceanalytics.com/telecommuting-statistics).The post pandemic world seems to have stronger national border controls,with a resultant shortage of highly technical staff. This has forced thehand of global business toward [internationally distributedteams](https://www.lifeatspotify.com/being-here/work-from-anywhere).\n- If only a small percentage of companies allow the option of remoteworking, then they gain a structural advantage, enjoying benefits ofreduced travel, lower workplace infection risk across all disease, andglobal agility for the personnel. Building and estate costs willcertainly be reduced. More diversity may be possible. Issues such assexual harassment and bullying may be reduced. With reduced overheadsproduct quality may increase. If customers are happier with theirservices, then over time this ‘push’ may mean an enormous shift awayfrom centralised working practises toward distributed working.\n- Technologies which support this working style were still in theirinfancy at the beginning of the pandemic. The rush to ‘Zoom’, apreviously relatively unknown and insecure[[aiken2020zooming]] webmeeting product, shows how naive businesses were in this space.\n- Connection of multiple users is now far better supported, with Zoom and[MircosoftTeams](https://www.microsoft.com/en-us/Investor/earnings/FY-2021-Q1/press-release-webcast)alone supporting hundreds of millions of chats a day. This is a 20xincrease on market leader Skype’s 2013 figure of [280million](https://www.microsoft.com/en-us/Investor/earnings/FY-2013-Q1/press-release-webcast)connections per month. Such technologies extend traditional telephony toprovide important multi sensory cues. However, these technologiesdemonstrate shortfalls compared to a live face-to-face meeting, which isgenerally agreed to be optimal for human-human interaction.[[Wolff2008]]\n- ## KPMG CEO Survey Highlights\n- [KPMG](https://fortune.com/company/kpmg/) surveyed U.S. CEOs from companies with over $500 million in revenue, finding that only [one-third anticipate a full office return](https://kpmg.com/kpmg-us/content/dam/kpmg/pdf/2024/KPMG-2024-US-CEO-Outlook-Pulse-Survey.pdf) in the next three years.\n- A drastic shift from last year's outlook where 62% believed remote work would end by 2026, as detailed in this [Fortune article](https://fortune.com/2023/10/05/ceo-rto-kpmg-research-return-to-work-full-time-5-days/).\n- The trend towards hybrid work models is growing, with nearly half of the CEOs supporting this flexible arrangement, up from 34% last year. ## Corporate Resistance and Work Model Evolution\n- Deutsche Bank and [Amazon](https://fortune.com/company/amazon-com/) faced significant resistance to their return-to-office mandates, with Deutsche Bank staff voicing displeasure internally, and 30,000 Amazon employees [signing a petition](https://www.businessinsider.com/amazons-hr-boss-rejects-employee-remote-work-petition-2023-3) against the in-office requirements.\n- Research indicates that nearly half of companies enforcing office returns observed increased employee attrition, supporting a shift towards hybrid models as a middle ground solution.\n- ## Future Workforce Dynamics\n- Insights from Amrit Sandhar of &Evolve and Lewis Maleh of Bentley Lewis highlight the ongoing transition to more flexible work arrangements. Maleh notes a significant rise in job postings for remote or hybrid roles, underscoring a broader acceptance of flexibility in work environments as crucial for attracting and retaining talent.\n- While the research community and business are learning how to adaptworking practises to web based telepresence,[[oeppen2020human]] thereremains little technology support for ad-hoc serendipitous meetingsbetween small groups. It’s possible that Metaverse applications can helpto fill this gap, by gamification of social spaces, but the underdiscussed problems with video conferencing are likely to be even worsein such systems.\n- Chris Herd of “FirstBase” (who admittedly have a bias) provides somefascinating speculations:\n- it\n- “I’ve spoken to 2,000+ companies with 40M+ employees about remote workin the last 12 months A few predictions of what will happen before 2030:\n-\n\t- Rural Living: World-class people will move to smaller cities, have a lower cost of living & higher quality of life.\n-\n\t- These regions must innovate quickly to attract that wealth. Better schools, faster internet connections are a must.\n-\n\t- Async Work: Offices are instantaneous gratification distraction factories where synchronous work makes it impossible to get stuff done.\n-\n\t- Tools that enable asynchronous work are the most important thing globally remote teams need. A lot of startups will try to tackle this.\n-\n\t- Hobbie Renaissance: Remote working will lead to a rise in people participating in hobbies and activities which link them to people in their local community.\n-\n\t- This will lead to deeper, more meaningful relationships which overcome societal issues of loneliness and isolation.\n-\n\t- Diversity & Inclusion: The most diverse and inclusive teams in history will emerge rapidly Companies who embrace it have a first-mover advantage to attract great talent globally. Companies who don’t will lose their best people to their biggest competitors.\n-\n\t- Output Focus: Time will be replaced as the main KPI for judging performance by productivity and output.\n-\n\t- Great workers will be the ones who deliver what they promise consistently\n-\n\t- Advancement decisions will be decided by capability rather than who you drink beer with after work.\n-\n\t- Private Equity: The hottest trend of the next decade for private equity will see them purchase companies, make them remote-first The cost saving in real-estate at scale will be eye-watering. The productivity gains will be the final nail in the coffin for the office Working Too Much: Companies worry that the workers won’t work enough when operating remotely.\n-\n\t- The opposite will be true and become a big problem.\n-\n\t- Remote workers burning out because they work too much will have to be addressed.\n-\n\t- Remote Retreats: Purpose-built destinations that allow for entire companies to fly into a campus for a synchronous week.\n-\n\t- Likely staffed with facilitators and educators who train staff on how to maximize effectiveness.\n-\n\t- Life-Work Balance: The rise of remote will lead to people re-prioritizing what is important to them.\n-\n\t- Organizing your work around your life will be the first noticeable switch. People realizing they are more than their job will lead to deeper purpose in other areas.\n-\n\t- Bullshit Tasks: The need to pad out your 8 hour day will evaporate, replaced by clear tasks and responsibilities.\n-\n\t- Workers will do what needs to be done rather than wasting their trying to look busy with the rest of the office\n- ”\n- #### Point to Point Video Conferencing\n- O’Malley et al. showed that face-to-face and video mediated employedvisual cues for mutual understanding, and that addition of video to theaudio channel aided confidence and mutual understanding. However, videomediated did not provide the clear cues of beingco-located.[[OMalley1996]]\n- Dourish et al. make a case for not using face-to-face as a baseline forcomparison, but rather that analysis of the efficacy of remotetele-collaboration tools should be made in a wider context of connectedmultimedia tools and ‘emergent communicative practises.’[[Dourish1996]]While this is an interesting viewpoint it does not necessarily map wellto a recreation of the ad-hoc meeting.\n- There is established literature on human sensitivity to eye contact inboth 2D and 3D VC,[[Criminisi2003; @Van_Eijk2010]] with an acceptedminimum of 5-10 degrees before observers can reliably sense they are notbeing looked at.[[Chen2002]] Roberts et al. suggested that at the limitof social gaze distance ( 4m) the maximum angular separation betweenpeople standing shoulder to shoulder in the real world would be around 4degrees[[Roberts2013]].\n- Sellen found limited impact on turn passing when adding a visual channelto audio between two people when using Hydra, an early system whichprovided multiple video conference displays in an intuitive spatialdistribution[[Sellen1992]]. She did however, find that the design of thevideo system affected the ability to hold multi-partyconversations.[[Sellen1995]]\n- Monk and Gale describe in detail experiments which they used forexamining gaze awareness in communication which is mediated andunmediated by technology. They found that gaze awareness increasedmessage understanding.[[Monk2002]]\n- Both Kuster et al. and Gemmel et al. have successfuly demonstratedsoftware systems which can adjust eye gaze to correct for off axiscapture in real time video systems[[Gemmell2000; @Kuster2012]].\n- Shahid et al. conducted a study on pairs of children playing games withand without video mediation and concluded that the availability ofmutual gaze affordance enriched social presence and fun, while itsabsence dramatically affects the quality of the interaction. They usedthe ‘Networked Minds’, a social presence questionnaire.\n- #### Triadic and Small Group\n- Early enthusiasm in the 1970’s for video conferencing, as a medium forsmall group interaction quickly turned to disillusionment. It was agreedafter a flurry of initial research that the systems at the time offeredno particular advantage over audio only communication, and atconsiderable cost.[[Williams1977]]\n- Something in the breakdown of normal visual cues seems to impact theability of the technology to support flowing group interaction.Nonetheless, some non-verbal communication is supported in VC withlimited success.\n- Additional screens and cameras can partially overcome the limitation ofno multi-party support (that of addressing a room full of people on asingle screen) by making available more bidirectional channels. Forinstance, every remote user can be a head on a screen with acorresponding camera. The positioning of the screens must thennecessarily match the physical organization of the remote room.\n- Egido provides an early review of the failure of VC for group activity,with the “misrepresentation of the technology as a substitute forface-to-face\" still being valid today.[[Edigo1988]]\n- Commercial systems such as Cisco Telepresence Rooms cluster theircameras above the centre screen of three for meetings using theirtelecollaboration product, while admitting that this only works well forthe central seat of the three screens. They also group multiple peopleon a single screen in what Workhoven et al. dub a “non-isotropic\"configuration.[[Pejsa2016]] They maintain that this is a suitable tradeoff as the focus of the meeting is more generally toward the importantcontributor in the central seat. This does not necessarily follow forless formal meeting paradigms.\n- In small groups, it is more difficult to align non-verbal cues betweenall parties, and at the same time, it is more important because thehand-offs between parties are more numerous and important in groups. Abreakdown in conversational flow in such circumstances is harder tosolve. A perception of the next person to talk must be resolved for allparties and agreed upon to some extent.\n- However, most of the conventional single camera, and expensive multicamera VC systems, suffer a fundamental limitation in that the offsetbetween the camera sight lines and the lines of actual sight introduceincongruities that the brain must compensate for.[[Wolff2008]]\n- #### Other Systems to Support Business\n- There have been many attempts to support group working and rich datasharing between dispersed groups in a business setting. So called ’smartspaces’ allow interaction with different displays for differentactivities and add in some ability to communicate with remote or evenmobile collaborators on shared documents,[[Bardram2012]] with additionalchallenges for multi-disciplinary groups who are perhaps less familiarwith one or more of the technology barriers involved.[[Adamczyk2007]]\n- Early systems like clearboard[[Ishii1993]] demonstrated the potential forsmart whiteboards with a webcam component for peer-to-peer collaborativeworking. Indeed it is possible to support this modality with Skype and asmartboard system (and up to deployments such as Accessgrid). Theyremain relatively unpopular however.\n- #### Mona Lisa Type Effects\n- Almost all traditional group video meeting tools suffer from theso-called Mona Lisa effect which describes the phenomenon where theapparent gaze of a portrait or 2 dimensional image always appears tolook at the observer regardless of the observer’sposition.[[Vishwanath2005; @Anstis1969; @Wollaston1824]] This situationmanifests when the painted or imaged subject is looking into the cameraor at the eyes of the painter.[[Loomis2008; @Fullwood2006]]\n- Single user-to-user systems based around bidirectional video implicitlyalign the user’s gaze by constraining the camera to roughly the samelocation as the display. When viewed away from this ideal axis, itcreates the feeling of being looked at regardless of where this observeris,[[Moubayed2012; @Vishwanath2005; @Anstis1969; @Wollaston1824]] or the“collapsed view effect”[[Nguyen2005]] where perception of gazetransmitted from a 2 dimensional image or video is dependent on theincidence of originating gaze to the transmission medium.\n- Multiple individuals using one such channel can feel as if they arebeing looked at simultaneously, leading to a breakdown in the normalnon-verbal communication which mediates turn passing.[[Vertegaal2002]]There is research investigating this sensitivity when the gaze ismediated by a technology, finding that “disparity between the opticalaxis of the camera and the looking direction of a looker should be atmost 1.2 degrees in the horizontal direction, and 1.7 degrees invertical direction to support eye contact\".[[Van_Eijk2010; @Bock2008]] Itseems that humans assume that they are being looked at unless they aresure that they are not.[[Chen2002]]\n- To be clear, there are technological solutions to this problem, but it’suseful in the context of discussing metaverse to know that this problemexists. It’s known that there are cognitive dissonances around panes ofvideo conference images, but it seems that the effect is truely limitedto 2D surfaces. A 3D projection surface (a physical model of a human)designed to address this problem completely removed the Mona Lisaeffect.[[Moubayed2012]]\n- Metaverse then perhaps offers the promise of solving this, making morenatural interaction possible, but it’s clearly a long way fromdelivering on those promises right now. We need to understand what’simportant and try to map these into a metaverse product.\n- ### What’s important for human communication\n- #### Vocal\n- The ubiquitous technology to mediate conversation is, of course, thetelephone. The [2021 Ericsson mobilityreport](https://www.ericsson.com/en/reports-and-papers/mobility-report/reports/november-2021)states that there are around 8 billion mobile subscriptions globally.More people have access to mobile phones than to working toilets[according toUNICEF](https://www.unicef.org/innovation/stories/more-cellphones-toilets).\n- Joupii and Pan designed a system which focused attention on spatiallycorrect high definition audio. They found “significant improvement overtraditional audio conferencing technology, primarily due to theincreased dynamic range and directionality..[[Jouppi2002]] Aoki et al.also describe an audio only system with support for spatialcues.[[Aoki2003]]\n- In the following sections we will attempt to rigorously identify justwhat is important for our proposed application of business centriccommunication, supportive of trust, and thereby value transfer.\n- In his book ‘Bodily Communication’[[Argyle1988]] Michael Argyle dividesvocal signals into the following categories:\n- 1.  Verbal\n- 2.  Non-Verbal Vocalisations\n- 1.  Linked to Speech\n- 1.  Prosodic\n- 2.  Synchronising\n- 3.  Speech Disturbances\n- 2.  Independent of Speech\n- 1.  Emotional Noises\n- 2.  Paralinguistic (emotion and interpersonal attitudes)\n- 3.  Personal voice and quality of accent\n- Additional to the semantic content of verbal communication there is arich layer of meaning in pauses, gaps, and overlaps[[Heldner2010]] whichhelp to mediate who is speaking and who is listening in multi-partyconversation. This mediation of turn passing, to facilitate flow, is byno means a given and is highly dependent on context and otherfactors.[[Kleinke1986]] Interruptions are also a major factor in turnpassing.\n- This extra-verbal content[[Ting-Toomey2012]] extends into physical cues,so-called ‘nonverbal’ cues, and there are utterances which link theverbal and non-verbal.[[Otsuka2005]] This will be discussed later, but toan extent, it is impossible to discuss verbal communication withoutregard to the implicit support which exists around the words themselves.\n- In the context of all technology-mediated conversation the extra-verbalis easily compromised if technology used to support communication over adistance does not convey the information, or conveys it badly. This canintroduce additional complexity.[[Otsuka2005]]\n- These support structures are pretty much lacking in metaverse XRsystems. The goal then here perhaps is to examine the state-of-the-art,and remove as many of the known barriers as possible. Such a processmight better support trust, which might better support the kind ofeconomic and activity we seek to engineer.\n- When examining just verbal / audio communication technology it can beassumed that the physical non-verbal cues are lost, though notnecessarily unused. In the absence of non-verbal cues it falls to timelyvocal signals to take up the slack when framing and organising the turnpassing. For the synchronising of vocal signals between the parties tobe effective the systemic delays must remain small. System latency, theinherent delays added by the communication technology, can allow slipsor a complete breakdown of ’flow’.[[katagiri2007aiduti]] This problem canbe felt in current social VR platforms, though people don’t necessarilyidentify the cause of the breakdown correctly. In the main they feel tothe users like a bad “audio-only” teleconference.\n- With that said, the transmission of verbal / audio remains the mostcritical element for interpersonal communication as the most essentialmeaning is encoded semantically. There is a debate about ratios of howmuch information is conveyed through the various humanchannels,[[Loomis2012]] but it is reasonable to infer from its ubiquitythat support for audio is essential for meaningful communication over adistance. We have seen that it must be timely, to prevent a breakdown offraming, and preferably have sufficient fidelity to convey sub-vocalutterances.\n- For social immersive VR for business users, a real-time network such aswebsockets, RTP, or UDP seems essential, much better microphones areimportant, and the system should support both angular spatialisation,and respond to distance between interlocutors.\n- #### Nonverbal\n- We have already seen that verbal exchanges take place in a wider contextof sub vocal and physical cues. In addition, the spatial relationshipbetween the parties, their focus of attention, their gestures andactions, and the wider context of their environment all play a part incommunication.[[Goodwin2000]] These are identified as follows by Gilliesand Slater[[Gillies2005]] in their paper on virtual agents.\n-\n\t- Posture and gesture\n-\n\t- Facial expression\n-\n\t- Gaze\n-\n\t- Proxemics\n-\n\t- Head position and orientation\n-\n\t- Interactional synchrony\n- This is clearly important for our proposed collaborative mixed realityapplication. Below we will examine these six areas by looking across thewider available research.\n- ##### Gaze\n- Of particular importance is judgement of eye gaze which is normallyfast, accurate and automatic, operating at multiple levels of cognitionthrough multiplecues.[[Argyle1988; @Argyle1976; @Argyle1965; @Argyle1976; @Argyle1969; @Kendon1967; @Monk2002]]\n- Gaze in particular aids smooth turn passing[[Hedge1978]][[Novick1996]] andlack of support for eye gaze has been found to decrease the efficiencyof turn passing by 25%.[[Vertegaal2000]]\n- There are clear patterns to eye gaze in groups, with the person talking,or being talked to, probably also being lookedat[[Vertegaal2001]][[Langton2000]]. To facilitate this groups will tend toposition themselves to maximally enable observation of the gaze of theother parties.[[Kendon1967]] This intersects with proxemics which will bediscussed shortly. In general people look most when they are listening,with short glances of 3-10 seconds.[[Argyle1965]] Colburn et al. suggestthat gaze direction and the perception of the gaze of others directlyimpacts social cognition[[Colburn2000]] and this has been supported in afollow up study.[[Macrae2002]]\n- The importance of gaze is clearly so significant in evolutionary termsthat human acuity for eye direction is considered high at  30 secarc[[Symons2004]] with straight binocular gaze judged more accuratelythan straight monocular gaze,[[Kluttz2009]] when using stereo vision.\n- Regarding the judgement of the gaze of others, Symons et al. suggestedthat “people are remarkably sensitive to shifts in a person’s eye gaze”in triadic conversation.[[Symons2004]] This perception of the gaze ofothers operates at a low level and is automatic. Langton et al. citeresearch stating that the gaze of others is “able to trigger reflexiveshifts of an observer’s visual attention” and further discuss the deepbiological underpinnings of gaze processing.[[Langton2000]]\n- When discussing technology-mediated systems, Vertegaal & Ding suggestedthat understanding the effects of gaze on triadic conversation is“crucial for the design of teleconferencing systems and collaborativevirtual environments,”[[Vertegaal2002]] and further found correlationbetween the amount of gaze, and amount of speech. Vertegaal & Slagtersuggest that “gaze function(s) as an indicator of conversationalattention in multiparty conversations.”[[Vertegaal2001]] It seems like iswe are to have useful markets within social immersive environments thensupport for natural gaze effects should be a priority.\n- Wilson et al. found that subjects can “discriminate gaze focused onadjacent faces up to \\[3.5m\\].”[[Wilson2000]] This perhaps gives us atestable benchmark within a metaverse application which is eye gazeenabled. In this regard Schrammel et al. investigated to what extentembodied agents can elicit the same responses in eye gazedetection.[[Schrammel2007]]\n- Vertegaal et al. found that task performace was 46% better when gaze wassynchronised in their telepresence scenario. As they point out, gazesynchonisation (temporal and spatial) is ‘commendable’ in all such groupsituations, but the precise utility will depend upon thetask.[[Vertegaal2002]]\n- There has been some success in the automatic detection of the focus ofattention of participants in multi partymeetings.[[Stiefelhagen2001; @Stiefelhagen2002]] More recently, eyetracking technologies allow the recording and replaying of accurate eyegaze information[[Steptoe2009]] alongside information about pupildilation toward determination of honesty and socialpresence.[[Steptoe2010]] It seems there are trust and honesty issuesconflated with how collaborants in a virtual space are represented.\n- In summary, gaze awareness does not just mediate verbal communicationbut rather is a complex channel of communication in its own right.Importantly, gaze has a controlling impact on those who are involved inthe communication at any one time, including and excluding even beyondthe current participants. Perhaps the systems we propose in this bookneed to demand eye gaze support, but it is clear that it should berecommended, and that the software selected should support thetechnology integration in principle.\n- ##### Mutual Gaze\n- Aygyle and Cook established early work around gaze and mutual gaze, withtheir seminal book of the same title,[[Argyle1976]] additionallydetailing confounding factors around limitations and inaccuracies inobservance of gaze and how this varies withdistance.[[Argyle1969; @Argyle1988; @Cook1977]]\n- Mutual gaze is considered to be the most sophisticated form of gazeawareness with significant impact on dyadic conversationespecially.[[Cook1977; @Kleinke1986; @Fagel2010]] The effects seem moreprofound than just helping to mediate flow and attention, with mutualeye gaze aiding in memory recall and the formation ofimpressions.[[Bohannon2013]]\n- While reconnection of mutual eye gaze through a technology boundary doesnot seem completely necessary it is potentially important, with impacton subtle elements of one-to-one communication, and thereforediscrimination of eye gaze direction should be bi-directional ifpossible, and if possible have sufficient accuracy to judge direct eyecontact. In their review Bohannon et al. said that the issue ofrejoining eye contact must be addressed in order to fully realise therichness of simulating face-to-face encounters.[[Bohannon2013]]\n- Mutual gaze is a challenging affordance as bi-directional connection ofgaze is not a trivial problem. It’s perhaps best to view this as at the‘edge’ of our requirements for a metaverse.\n- ##### Mutual Gaze in Telepresence\n- We have seen that transmission of attention can broadly impactcommunication in subtle ways, impacting empathy, trust, cognition, andco-working patterns. Mutual gaze (looking into one another’s eyes), iscurrently the high water mark for technology-mediated conversation.\n- Many attempts have been made to re-unite mutual eye gaze when usingtele-conferencing systems. In their 2015 review of approachesRegenbrecht and Langlotz found that none of the methods they examinedwere completely ideal.[[Regenbrecht2015]] They found most promise in 2Dand 3D interpolation techniques, which will be discussed in detaillater, but they opined that such systems were very much ongoing researchand lacked sufficient optimisation.\n- A popular approach uses the so called ’Peppers Ghost’phenomenon,[[Steinmeyer2013]] where a semi silvered mirror presents animage to the eye of the observer, but allows a camera to view throughfrom behind the angled mirror surface. The earliest example of this isRosental’s two way television system in 1947,[[Rosenthal1947]] thoughBuxton et al. ‘Reciprocal Video Tunnel’ from 1992 is more oftencited.[[Buxton1992]] This optical characteristic isn’t supported byretroreflective projection technology, and besides requires carefulcontrol of light levels either side of the semi-silvered surface.\n- The early GAZE-2 system (which makes use of Pepper’s ghost) is novel inthat it uses an eye tracker to select the correct camera from severaltrained on the remote user. This ensures that the correct returned gaze(within the ability of the system) is returned to the correct user onthe other end of the network.[[Vertegaal2003]] Mutual gaze capability islater highlighted as an affordance supported or unsupported by keyresearch and commercial systems.\n- ##### Head Orientation\n- Orientation of the head (judged by the breaking of bilateral symmetryand alignment of nose) is a key factor when judging attention.Perception of head orientation can be judged to within a couple ofdegrees.[[Wilson2000]]\n- It has been established that head gaze can be detected all the way outto the extremis of peripheral vision, with accurate eye gaze assessmentonly achievable in central vision.[[Loomis2008]] This is less of use forour metaverses at this time, because user field of view is almost alwaysrestricted in such systems. More usefully, features of illumination canalter the apparent orientation of the head.[[Troje1998]]\n- Head motion over head orientation is a more nuanced propostion and canbe considered a micro gesture.[[Boker2011]] Head tracking systems withinhead mounted displays can certainly detect these tiny movements, butit’s clear that not all of this resolution is passed into shared virtualsettings through avatars. It would be beneficial to be able to fine tunethis feature within any software selected.\n- It is possible that 3D displays are better suited to perception of headgaze since it is suggested that they are more suitable for “shapeunderstanding tasks”[[St_John2001]]\n- Bailenson, Baell, and Blascovich found that giving avatars rendered headmovements in a shared virtual environment decreased the amount oftalking, possibly as the extra channel of head gaze was opened up. Theyalso reported that subjectively, communication wasenhanced.[[Bailenson2002]]\n- Clearly head orientation is an important indicator of the direction ofattention of members of a group and can be discerned even in peripheralvision. This allows the focus of several parties to be followedsimultaneously and is an important affordance to replicate on anymulti-party communication system.\n- ##### Combined Head and Eye Gaze\n- Rienks et al. found that head orientation alone does not provide areliable cue for identification of the speaker in a multipartysetting.[[Rienks2010]] Stiefelhagen & Zhu found “that head orientationcontributes 68.9% to the overall gaze direction onaverage,”[[Stiefelhagen2002]] though head and eye gaze seem to be judgedinterdependently.[[Kluttz2009]] Langton noted that head and eye gaze are“mutually influential in the analysis of socialattention,”[[Langton2000]] and it is clear that transmission of ‘headgaze’ by any mediating system, enhances rather than replaces timelydetection of subtle cues. Combined head and eye gaze give the best ofboth worlds and extend the lateral field of view in which attention canbe reliably conveyed to others.[[Loomis2008]]\n- ##### Other Upper Body: Overview\n- While it is well evidenced that there are advantages to accurateconnection of the gaze between conversationalpartners,[[Argyle1969; @Kleinke1986]] there is also a body of evidencethat physical communication channels extend beyond theface[[Kleinke1986; @Nguyen2009]] and include both micro (shrugs, handsand arms), and macro movement of the upper body.[[Ekman1993]]Goldin-Meadow suggests that gesturing aids conversational flow byresolving mismatches and aiding cognition.[[Goldin-Meadow1999]]\n- In their technology-mediated experiment which compared face to upperbody and face on a flat screen, Nguyen and Canny found that “upper-bodyframing improves empathy measures and gives results not significantlydifferent from face-to-face under several empathymeasures.”[[Nguyen2009]]\n- The upper body can be broken up as follows:\n- bfFacial  Much emotional context can be described by facial expression (display)alone,[[Ekman1993; @Chovil1991]] with smooth transition betweenexpressions seemingly important.[[schiano2004]] This suggests thatmediating technologies should support high temporal resolution, or atleast that there is a minimum resolution between which transitionsbetween expressions become too ’categorical’. Some aspects ofconversational flow appear to be mediated in part by facialexpression.[[ohba1998]] There are gender differences in the perception offacial affect.[[Hofmann2006]]\n- bfGesturing  (such as pointing at objects) paves the way for more complex channels ofhuman communication and is a basic and ubiquitous channel.[[Iverson2005]]Conversational hand gestures provide a powerful additional augmentationto verbal content.[[Krauss1996]]\n- bfPosture  Some emotions can be conveyed through upper body configurations alone.Argyle details some of these[[Argyle1988]] and makes reference to theposture of the body and the arrangement of the arms (i.e. folded acrossthe chest). These are clearly important cues. Kleinsmith andBianchi-Berthouze assert that \"some affective expressions may be bettercommunicated by the body than the face\".[[Kleinsmith2013]]\n- bfBody Torque  In multi-party conversation, body torque, that is the rotation of thetrunk from front facing, can convey aspects of attention andfocus.[[Schegloff1998]]\n- In summary, visual cues which manifest on the upper body and face canconvey meaning, mediate conversation, direct attention, and augmentverbal utterances.\n- ##### Effect of Shared Objects on Gaze\n- Ou et al. detail shared task eye gaze behaviour “in which helpers seekvisual evidence for workers’ understanding when they lack confidence ofthat understanding, either from a shared, or commonvocabulary.”[[Ou2005]]\n- Murray et al. found that in virtual environments, eye gaze is crucialfor discerning what a subject is looking at.[[Murray2009]] This work isshown in Figure<a href=\"#fig:murrayeyegaze\" data-reference-type=\"ref\" data-reference=\"fig:murrayeyegaze\">7.3</a>.\n- It is established that conversation around a shared object or task,especially a complex one, mitigates gaze between parties[[Argyle1976]]and this suggests that in some situations around shared tasks inmetaverses it may be appropriate to reduce fidelity of representation ofthe avatars.\n- <span class=\"image\">Eye tracked eye gaze awareness in VR. Murray et al.used immersive and semi immersive systems alongside eye trackers toexamine the ability of two avatars to detect the gaze awareness of asimilarly immersed collaborator.</span>\n- ##### Tabletop and Shared Task\n- In early telepresence research Buxton and William argued throughexamples that “effective telepresence depends on quality sharing of bothperson and task space.[[Buxton1992]]\n- In their triadic shared virtual workspace Tang et al. found difficultyin reading shared text using a ‘round the table’ configuration, a markedpreference for working collaboratively on the same side of the table.They also found additional confusion as to the identity of remoteparticipants.[[Tang2010]] Tse et al. found that pairs can work well overa shared digital tabletop, successfully overcoming a single userinterface to interleave tasks.[[Tse2007]]\n- Tang et al. demonstrate that collaborators engage and disengage around agroup activity through several distinct, recognizable mechanisms withunique characteristics.[[Tang2006]] They state that tabletop interfacesshould offer a variety of tools to facilitate this fluidity.\n- Camblend is a shared workspace with panoramic high resolution video. Itmaintains some spatial cues between locations by keeping a shared objectin the video feeds.[[Norris2013; @Norris2012]] Participants successfullyresolved co-orientation within the system.\n- The t-room system implemented by Luff et al. surrounds co-locatedparticipants standing at a shared digital table with life sized body andhead video representations of remote collaborators[[Luff2011]] but foundthat there were incongruities in the spatial and temporal matchingbetween the collaborators which broke the flow of conversation.Tuddenham et al. found that co-located collaborators naturally devolved’territory’ of working when sharing a task space, and that this did nothappen the same way with a tele-present collaborator.[[Tuddenham2009]]Instead remote collaboration adapted to use a patchwork of ownership ofa shared task. It seems obvious to say that task ownership is a functionof working space, but it is interesting that the research found nomeasurable difference in performance when the patchwork coping strategywas employed.\n- The nature of a shared collaborative task and/or interface directlyimpacts the style of interaction between collaborators. This will have abearing on the choice of task forexperimentation.[[Jamil2011; @Jetter2011]]\n- ### Psychology of Technology-Mediated Interaction\n- #### Proxemics\n- Proxemics is the formal study of the regions of interpersonal spacebegun in the late 50’s by Hall and Sommers and building toward TheHidden Dimension,[[Hall1969]] which details bands of space (Figure<a href=\"#fig:proxemics\" data-reference-type=\"ref\" data-reference=\"fig:proxemics\">7.4</a>)that are implicitly and instinctively created by humans and which have adirect bearing on communication.\n  ![]./assets/93e5e31635612a45cbb73bc9a52a19c538eeea0c.png\n  Bands of social space around a person Image CC0 <a href=\"https://en.wikipedia.org/wiki/Proxemics\">from wikipedia</a>.\n- Distance between conversational partners, and affiliation, also have abearing on the level of eye contact[[Argyle1965]] with a natural distanceequilibrium being established and developed throughout, through both eyecontact and a variety of subtle factors. Argyle & Ingham provide levelsof expected gaze and mutual gaze against distance.[[Argyle1969]] Theseboundaries are altered by ethnicity[[Watson1966; @Argyle1988]] andsomewhat by gender,[[Bruno2013]] and age.[[Slessor2008; @Hofmann2006]]\n- Even with significant abstraction by communication systems (such asSecondLife) social norms around personal spacepersist.[[Yee2007; @Bailenson2001; @Bailenson2003]] Bailenson &Blascovich found that even in Immersive Collaborative VirtualEnvironments (ICVE’s) “participants respected personal space of thehumanoid representation”[[Bailenson2001]] implying that this is a deeplyheld ’low-level’ psychophysical reaction.[[Blascovich2002]] The degree towhich this applies to non-humanoid avatars seems under explored.\n- Maeda et al.[[Maeda2004]] found that seating position impacts the levelof engagement in teleconferencing. Taken together with the potential forreconfiguration within the group as well as perhaps signalling for theattention of participants outside of the confines of the group in anopen business metaverse setting.\n- When considering the attention of engaging with people outside theconfines of a meeting Hager et al. found that gross expressions can beresolved by humans over long distances.[[Hager1979; @Argyle1988]] Itseems that social interaction begins around 7.5m in the so-called‘public space.’[[Hall1969]] Recreating this affordance in a metaversewould be a function of the display resolution, and seems another‘stretch goal’ rather than a core requirement.\n- #### Attention\n- The study of attention is a discrete branch of psychology. It is thestudy of cognitive selection toward a subjective or objective sub focus,to the relative exclusion of other stimulae. It has been defined as “arange of neural operations that selectively enhance processing ofinformation.”[[Carlston2013]] In the context of interpersonalcommunication it can be refined to apply to selectively favouring aconversational agent or object or task above other stimuli in thecontextual frame.\n- Humans can readily determine the focus of attention of others in theirspace[[Stiefelhagen2001]] and preservation of the spatial cues whichsupport this are important for technology-mediatedcommunication[[Sellen1992]][[Stiefelhagen2002]].\n- The interplay between conversational partners, especially the reciprocalperception of attention, is dubbed the perceptualcrossing.[[Deckers2013; @Gibson1963]]\n- This is a complex field of study with gender, age, and ethnicity allimpacting the behaviour of interpersonalattention.[[Bente1998; @Slessor2008; @Argyle1988; @Hofmann2006; @Pan2008]]Vertegaal has done a great deal of work on awareness and attention intechnology-mediated situations and the work of his group is citedthroughout this chapter.[[Vertegaal1997]] As an example it is still sucha challenge to “get” attention through mediated channels ofcommunication, that some research[[Fels2000; @Sellen1992]] and manycommercial systems such as ‘blackboard collaborate’, Zoom, and Teams usetell tale signals (such as a microphone icon) to indicate when aparticipant is actively contributing. Some are automatic, but many arestill manual, requiring that a user effectively hold up a virtual handto signal their wish to communicate.\n- Langton et al. cite research stating that the gaze of others is “able totrigger reflexive shifts of an observer’s visual attention”.\n- Regarding the attention of others, Fagal et el demonstrated that eyevisibility impacts collaborative task performance when considering ashared task.[[Fagel2010]] Novick et al. performed analysis on taskhand-off gaze patterns which is useful for extension into shared taskproduct design.[[Novick1996]]\n- #### Behaviour\n- Hedge et al. suggested that gaze interactions between strangers andfriends may be different which could have an impact on the kinds ofinteractions a metaverse might best support.[[Hedge1978]] Voida et al.elaborate that prior relationships can cause “internal fault lines” ingroup working.[[Voida2012]] When new relationships are formed the“primary concern is one of uncertainty reduction or increasingpredictability about the behaviour of both themselves and others in theinteraction.”[[Berger1975]] This concept of smoothness in theconversation is a recurring theme, with better engineered systemsintroducing less extraneous artefacts into the communication, and sodisturbing the flow less. Immersive metaverse are rife with artefacts.\n- In a similar vein the actor-observer effect describes the mismatchbetween expectations which can creep into conversation. Conversationsmediated by technology can be especially prone to diverging perceptionsof the causes of behaviour.[[Jones1971]] Basically this meansmisunderstandings happen, and are harder to resolve with more mediatingtechnology.\n- Interacting subjects progress conversation through so-called‘perception-action’ loops which are open to predictive modelling throughdiscrete hidden Markov models.[[Mihoub2015]] This might allow product OKRtesting of the effectiveness of engineered systems.[[doerr2018measure]]\n- It may be that the perception-behaviour link where unconscious mirroringof posture bolsters empathy between conversational partners, especiallywhen working collaboratively,[[Chartrand1999]] and the extent to whichposture is represented through a communication medium may be important.\n- Landsberger posited the Hawthorne effect.[[Parsons1974]] Put simply thisis a short term increase in productivity that may occur as a result ofbeing watched or appreciated. The impression of being watched changesgaze patterns during experimentation, with even implied observationthrough an eye tracker modifying behaviour.[[Risko2011]]\n- There are also some fascinating findings around the neural correlates ofgratitude, which turn out not to be linked to gratitude felt by aparticipant, but rather the observation of gratitude received within asocial context.[[fox2015neural]] These findings have potentially usefulimplications for the behaviours of AI actors and avatars within animmersive social scene.\n- There is much historic work describing “the anatomy ofcooperation\",[[Kollock1998]] and this might better inform how educationalor instructional tasks are built in metaverse applications.\n- Cuddihy and Walters defined an early model for assessing desktopinteraction mechanisms for social virtual environments.[[Cuddihy2000]]\n- ##### Perception Of Honesty\n- Hancock et al. state that we are most likely to lie, and to be lied to,on the telephone.[[Hancock2004]] Technology used for communicationimpacts interpersonal honesty. It seems that at some level humans knowthis; lack of eye contact leads to feelings of deception, impactingtrust.[[Holm2010]] This has a major impact on immersive social XR, whichoften does not support mutual gaze. Trust is crucial for businessinteractions.\n- Further there are universal expressions, micro-expressions, and blinkrate which can betray hidden emotions,[[Porter2008]] though the effectsare subtle and there is a general lack of awareness by humans of theirabilities in this regard.[[Holm2010]] Absence of support for suchinstinctive cues inhibits trust.[[Roberts2015]] Support for these rapidand transient facial features demands high resolution reproduction inboth resolution and time domains. There is detectable difference in aparticipant’s ability to detect deception when between video conferencemediated communication and that mediated by avatars.[[Steptoe2010]]Systems should aim for maximally faithful reproduction.\n- #### Presence, Co-presence, and Social Presence\n- Presence is a heavily cited historic indicator of engagement in virtualreality, though the precise meaning has been interpreted differently bydifferent specialisms.[[Beck2011; @Schuemie2001]] It is generally agreedto be the ’sense of being’ in a virtual environment.[[Slater1999]] Slaterextends this to include the “extent to which the VE becomes dominant\".\n- Beck et al. reviewed 108 articles and synthesised an ontology ofpresence[[Beck2011]] which at its simplest is as follows:\n- 1.  Sentient presence\n- 1.  Physical interaction\n- 2.  Mental interaction\n- 2.  Non-sentient\n- 1.  Physical immersion\n- 2.  Mental immersion = psychological state\n- When presence is applied to interaction it may be split intoTelepresence, and Co/Social presence.[[Heeter1992; @Biocca1997]]Co-presence and/or social presence is the sense of “being there withanother\", and describes the automatic responses to complex socialcues.[[Fulk1987; @Haythornthwaite1995]] Social presence (and co-presence)refers in this research context to social presence which is mediated bytechnology (even extending to text based chat[[Gunawardena1997]]), andhas its foundations in psychological mechanisms which engender mutualismin the ‘real’. This is analysed in depth by Nowak.[[Nowak2001]] Anexamination of telepresence, co-presence and social presence necessarilyrevisits some of the knowledge already elaborated.\n- The boundaries between the three are blurred in research withconflicting results presented.[[Bulu2012]] Biocca et al. attempted toenumerate the different levels and interpretations surrounding thesevague words,[[Biocca2003]] and to distill them into a more robust theorywhich better lends itself to measurement. They suggest a solidunderstanding of the surrounding psychological requirements which needsupport in a mediated setting, and then a scope that is detailed andlimited to the mediated situation.\n- Since ‘social presence’ has been subject to varieddefinitions[[Biocca2003]] it is useful here to consider a singledefinition from the literature which defines it as “the ability ofparticipants in the community of inquiry to project their personalcharacteristics into the community, thereby presenting themselves to theother participants as real people..”[[Garrison1999; @Beck2011]] Similarlyto specifically define co-presence for this research it is taken to bethe degree to which participants in a virtual environment are“accesible, available, and subject to one another\".[[Biocca2003]]\n- Social presence has received much attention and there are establishedquestionnaires used in the field for measurement of the levels ofperceived social presence yet the definitions here also remain broad,with some confusion about what is being measured.[[Biocca2003]]\n- Telepresence meanwhile is interaction with a different (usually remote)environment which may or may not be virtual, and may or may not containa separate social/co-presence component.\n- Even in simple videoconferencing Bondareva and Bouwhuis stated (as partof an experimental design) that the following determinants are importantto create social presence.[[Bondareva2004; @Jouppi2002]]\n- 1.  Direct eye contact is preserved\n- 2.  Wide visual field\n- 3.  Both remote participants appear life size\n- 4.  Possibility for participants to see the upper body of the interlocutor\n- 5.  High quality image and correct colour reproduction\n- 6.  Audio with high S/N ratio\n- 7.  Directional sound field\n- 8.  Minimization of the video and audio signal asynchrony\n- 9.  Availability of a shared working space.\n- Bondareva et al. went on to describe a person-to-person telepresencesystem with a semi-silvered mirror to reconnect eye gaze, which theyclaimed increased social presence indicators. Interestingly they chose achecklist of interpersonal interactions which they used againstrecordings of conversations through the system.[[Bondareva2004]]\n- The idea of social presence as an indicator of the efficacy of thesystem, suggests the use of social presence questionnaires in theevaluation of the system.[[Biocca2003]] Subjective questionnaires arehowever troublesome in measuring effectiveness of virtual agents andembodiments, with even nonsensical questions producing seemingly validresults.[[Slater2004]] Usoh et al. found that ’the real’ produced onlymarginally higher presence results than the virtual.[[Usoh2000]] It wouldbe difficult to test products this way.\n- Nowak states that “A satisfactory level of co-presence with another mindcan be achieved with conscious awareness that the interaction ismediated\" and asserts that while the mediation may influence the degreeof co-presence it is not a prohibiting factor.[[Nowak2001]]\n- Baren and IJsselsteijn[[Van_Baren2004; @Harms2004]] list 20 usefulpresence questionnaires in 2004 of which “Networked Minds\" seemed mostappropriate for the research. Hauber et al. employed the “NetworkedMinds\" Social Presence questionnaire experimentally and found that whilethe measure could successfully discriminate between triadic conversationthat is mediated or unmediated by technology, it could not find adifference between 2D and 3D mediatedinterfaces.[[Hauber2005; @Gunawardena1997]]\n- In summary, social presence and co-presence are important historicmeasures of the efficacy of a communication system. Use of the term inliterature peaked between 1999 and 2006 according to Google’s ngramviewer and has been slowly falling off since. The questionnairemethodology has been challenged in recent research and while moreobjective measurement may be appropriate, the networked minds questionsseem to be able to differentiate real from virtualinteractions.[[Harms2004]]\n- ### Other Systems to Support Business\n- There have been many attempts to support group working and rich datasharing between dispersed groups in a business setting. So called ’smartspaces’ allow interaction with different displays for differentactivities and add in some ability to communicate with remote or evenmobile collaborators on shared documents,[[Bardram2012]] with additionalchallenges for multi-disciplinary groups who are perhaps less familiarwith one or more of the technology barriers involved.[[Adamczyk2007]]\n- Early systems like clearboard[[Ishii1993]] demonstrated the potential forsmart whiteboards with a webcam component for peer to peer collaborativeworking. Indeed it is possible to support this modality with Skype and asmartboard system (and up to deployments such as Accessgrid). Theyremain relatively unpopular however.\n- Displays need not be limited to 2 dimensional screens and can beenhanced in various ways.\n- Stereoscopy allows an illusion of depth to be added to a 2D image byexploiting the stereo depth processing characteristics of the humanvision system. This technical approach is not perfect as it does notfully recreate the convergence and focus expected by the eyes and brain.\n- There are multiple approaches to separating the left and right eyeimages, these primarily being active (where a signal selectively blanksthe input to left then right eyes in synchronicity with the display),passive, where either selective spectrum or selective polarisation oflight allow different portions of a display access to different eyes, orphysical arrangements which present different displays (or slices oflight as in lenticular systems) to different eyes.\n- These barrier stereoscopy / lenticular displays use vertical lightbarriers built into the display to create multiple discrete channels ofdisplay which are accessed by moving horizontally with respect to thedisplay. In this way it is possible to generate either a left/right eyeimage pair for ’autostereoscopic’ viewing, or with the addition of headtracking and small motors. With these techniques multiple viewpoint oran adaptive realtime viewpoint update can be presented without theglasses required for active or passive stereoscopic systems.\n- #### Spatially Faithful Group\n- Hauber et al. combined videoconferencing, tabletop, and social presenceanalysis and tested the addition of 3D. They found a nuanced responsewhen comparing 2D and 3D approaches to spatiality: 3D showed improvedpresence over 2D (chiefly through gaze support), while 2D demonstratedimproved task performance because of task focus.[[Hauber2006]]\n- I3DVC reconstructs participants from multiple cameras and places themisotropically (spatially faithful).[[Kauff2002; @Kauff2002a]] The systemuses a large projection screen, a custom table, and carefully definedseating positions. They discussed an “extended perception space\" whichused identical equipment in the remote spaces in a tightly coupledcollaborative ‘booth’. It employed head tracking and multi camerareconstruction alongside large screens built into the booth. This systemexemplified the physical restrictions which are required to limit theproblems of looking into another space through the screen. Fuchs et al.demonstrated a similar system over a wide area network but achieved onlylimited resolution and frame rate with the technology of theday.[[Fuchs2002]]\n- University of Southern California used a technically demanding real-timeset-up with 3D face scanning and an autostereoscopic 3D display togenerate multiple ‘face tracked’ viewpoints.[[Jones2009]] This had thedisadvantage of displaying a disembodied head.\n- MAJIC is an early comparable system to support small groups with lifesize spatially correct video, but without multiple viewpoints onto theremote collaborators it was a one to ’some’ system rather than ’some’ toone. Additionally users were rooted to definedlocations.[[Ichikawa1995; @Okada1994]]\n- There seems to be less interest recently in large display screens forspatially correct viewpoints between groups. The hardware is technicallydemanding and there may have been sufficient research done to limitinvestment in research questions. This doesn’t mean that there is nofuture for metaverse applications. Imagine one of the new XR studiowalls such as that used to film the Mandalorian. With application oftelepresence research it would be possible to bring external metaverseparticipants into the ‘backstage’ virtual scene. These avatars would beable to explore the scene invisible to the actors, but could be givenaccess to visual feeds from the stage side. This is a hybridvirtual/real metaverse with a well researched and understood boundaryinterface. It would be possible to give different access privileges todifferent levels of paying ‘film studio tourist’ or investor, with VIPsperhaps commanding a view onto the live filming. At the nadir of this itmay be possible to bring producers and directors directly into thevirtual studio as avatars on the screen boundary, with a spatiallyfaithful view onto the set. For the purposes of this book it’s alsoworth noting that NFTs of the experience and corresponding virtualobjects from the scene could be monetised and sold within the metaverse.\n- ##### Multiview\n- In order to reconnect directional cues of all kinds it is necessary foreach party in the group to have a spatially correct view of the remoteuser which is particular for them. This requires a multi-view display,which has applications beyond telepresence but are used extensively inresearch which attempts to address these issues.\n- Nguyen and Canny demonstrated the ‘Multiview’ system.[[Nguyen2005]]Multiview is a spatially segmented system, that is, it presentsdifferent views to people standing in different locationssimultaneously. They found similar task performance in trust tasks toface-to-face meetings, while a similar approach without spatialsegmentation was seen to negatively impact performance.\n- In addition to spatial segmentation of viewpoints[[Gotsch2018]] it ispossible to isolate viewpoints in the time domain. Different trackedusers can be presented with their individual view of a virtual scene fora few milliseconds per eye, before another viewpoint is shown to anotheruser. Up to six such viewpoints are supported in the c1x6system[[Kulik2011]] Similarly MM+Space offered 4 Degree-Of-FreedomKinetic Display to recreate Multiparty Conversation Spaces[[Otsuka2013]]\n- #### Holography and Volumetric\n- Blanche et al. have done a great deal of research into holographic andvolumetric displays using lasers, rotating surfaces, and light fieldtechnology.[[Blanche2010; @Tay2008]] They are actively seeking to usetheir technologies for telepresence and their work is very interesting.\n- Similarly Jones et al. “HeadSPIN\" is a one-to-many 3D videoteleconferencing system[[Jones2009]] which uses a rotating display torender the holographic head of a remote party. They achievetransmissible and usable framerate using structured light scanning of aremote collaborator as they view a 2D screen which they say shows aspatially correct view of the onlooking parties.\n- Eldes et al. used a rotating display to present multi-viewautostereoscopic projected images to users.[[Eldes2013]]\n- Seelinder is an interesting system which uses parallax barriers torender a head which an onlooking viewer can walk around. The system uses360 high resolution still images which means a new spatially segmentedview of the head every 1 degreesof arc. They claim the system is capableof playback of video and this head in a jar multi-view system clearlyhas merit but is comparatively small, and as yet untested fortelepresence.[[Yendo2010]]\n- These systems do not satisfy the requirement to render upper body forthe viewers and are not situated (as described soon).\n- There’s a future possible where real-time scanned avatar representationin persistent shared metaverse environments will be able to supportbusiness, but the camera rigs which currently generate such models aretoo bulky and involved for a good costs benefit analysis. It is morelikely that recent advances in LIDAR phone scanning show the way. Theallow realistic avatars to be quickly created for animation withinmetaverse scenes.[[authenticVolume2022]]\n- ##### Project Skyline\n- Project Starline, is a next-generation video conferencing technologythat aims to create a sense of presence, making you feel like you’resitting across the table from someone. It uses advanced hardware andsoftware to achieve this.\n-\n\t- The newer Starline booth is a refined version of earlier models and looks like a large 65-inch display on a stand. It contains color cameras, depth sensors, microphones, and speakers. Additionally, there are lights on the back of the display that serve as a key light for the person on the call. These lights are mounted around the person and used to create a depth map of the subject and the room they’re in.\n-\n\t- The display creates an immersive 3D depth effect. It uses a barrier lenticular light field display that shows a different image to your left eye and to your right eye. This effect lets you compute depth on the fly while doing all the head tracking in real time. The display technology in Project Starline is significantly smoother and more realistic than what you would experience with traditional 3D movies.\n-\n\t- The computing side of Project Starline is responsible for rendering the people using the system into realistic 3D models in real-time. It uses AI and depth information gathered by the cameras to map the exact shape, depth, texture, and lighting of the person. The result is an ultra-realistic 3D representation of the person on the other end of the call.\n-\n\t- The system features spatial audio such that the perceived audio changes based on where you are leaning or moving, creating an even more immersive and realistic experience.\n- At this point, Google has been working with several companies who areusing these booths for meetings, and it’s hoped that as the technologybecomes cheaper and more refined, it they assert that it couldrevolutionize the way we communicate, though the cost of the system and‘single user to single user’ restriction is likely to be a blocker tocrucial business adoption.\n- #### Simulated Humans\n- ##### Uncanniness\n- When employing simulation representations of humans it may be the casethat there is an element of weirdness to some of these systems,especially those that currently represent a head without a body. Morihas demonstrated The Uncanny Valley[[Mori1970]] effect in which imperfectrepresentations of humans elicit revulsion in certain observers. Thisprovides a toolkit for inspecting potentially ‘weird’ representations,especially if they are ‘eerie’ and is testable through Mori’s GODSPEEDquestionnaire.\n- With an improved analysis of the shape of the likeability curveestimated later showing a more nuanced response from respondents whereanthropomorphism of characters demonstrated increased likeability evenagainst a human baseline.[[Bartneck2007; @Bartneck2009]]\n- A mismatch in the human realism of face and voice also produces anUncanny Valley response.[[Mitchell2011]]\n- However, there is a possibility that Mori’s hypothesis may be toosimplistic for practical everyday use in CG and robotics research sinceanthropomorphism can be ascribed to many and interdependent featuressuch as movement and content of interaction.[[Bartneck2009]]\n- Bartneck et al. also performed tests which suggest that the originalUncanny Valley assertions may be incorrect, and that it may beinappropriate to map human responses to human simulacrum to such asimplistic scale. They suggest that the measure has been a convenient‘escape route’ for researchers.[[Bartneck2009]] Their suggestion that themeasure should not hold back the development of more realistic robotsholds less bearing for the main thrust of this telepresence researchwhich seeks to capture issues with imperfect video representation ratherthan test the validity of an approximation.\n- Interestingly Ho et al. performed tests on a variety of facialrepresentations using images. They found that facial performance is a‘double edged sword’ with realism being important to roboticrepresentations, but there also being a significant Uncanny Valleyeffect around ‘eerie, creepy, and strange’ which can be avoided by gooddesign.[[Ho2008]]\n- More humanlike representations exhibiting higher realism produce morepositive social interactions when subjective measures are used[[Yee2007]]but not when objective measures are used. This suggests thatquestionnaires may be more important when assessing potentialuncanniness.\n- A far more objective method would be to measure user responses tohumans, robots, and representations with functional near-infraredspectroscopy and while this has been attempted it is early exploratoryresearch,[[Strait2014]] an emotional response to ‘eerie’ was discovered.\n- ##### Embodiment through robots\n- Virtuality human representation extends beyond simple displays intorobotic embodiments (which need not be humanoid[[Marti2005]]), shapemapped projection dubbed “shader lamps\", and hybridisations of the two.\n- Robots which carry a videoconference style screen showing a head can addmobility and this extends the availablecues.[[Adalgeirsson2010; @Lee2011; @Tsui2011; @Paulos1998; @Kristoffersson2013]]Interestingly Desai and Uhlik maintain that the overriding modalityshould be high quality audio.[[Desai2011]]\n- Tsui et al. asked 96 participants to rate how personal and interactivethey found interfaces to be. Interestingly they rated videoconferencingas both more personal and more interactive than telepresence robots,suggesting that there is a problem with the overall representation orembodiment.[[Tsui2012]]\n- Kristoffersson et al. applied the Networked Minds questionnaire to judgepresence of a telepresence robot for participants with little or noexperience of videoconferencing. Their results were encouraging, thoughthey identified that the acuity of the audio channel needingimprovement.[[Kristoffersson2011]]\n- There are a very few lifelike robots which can be used for telepresence,and even these are judged to be uncanny.[[Sakamoto2007]] This is only anissue for a human likeness since anthropomorphic proxies such as robotsand toys perform well.[[Mori1970]]\n- ##### Physical & Hybrid embodiment\n- Embodiment through hybridisation of real-time video and physicalanimatronic mannequins has been investigated as a way to bring theremote person into the space in a more convincingway.[[Lincoln2009; @Lincoln2010; @Raskar2001]]  These includetelepresence robots,[[Lee2011; @Sakamoto2007; @Tsui2011]] head in a jarimplementations such as SphereAvatar[[Oyekoya2012; @Pan2014; @Pan2012]]and BiReality,[[Jouppi2004]]  UCL’s Gaze Preserving Situated Multi-ViewTelepresence System,[[Pan2014]] or screen on a stick stylerepresentations.[[Kristoffersson2013]]\n- Nagendran et al. present a 3D continuum of these systems into which theysuggest all such systems can be rated from artificial to real on thethree axes, shape, intelligence, and appearance.[[Nagendran2012]]\n- Itoh et al. describe a ’face robot’ to convey captured human emotionover a distance. It uses an ‘average face’ and actuators to manipulatefeature points.[[Itoh2005]] It seems that this is an outlier method forcommunication of facial affect but demonstrates that there are manydevelopment paths to a more tangible human display.\n- It seems increasingly likely that machine learning models whichmanipulate images in real time can simulate humans into metaverseapplications with very little input data. One such example is Samsung’sMegaportraits which can product a realistic human face from a singleinput stream such as a webcam.[[Drobyshev22MP]]\n- ##### Shader lamps\n- Projection mapping is a computational augmented projection techniquewhere consideration of the relative positions and angles of complexsurfaces allows the projection from single or multiple sources toaugment the physical shapes onto which they appear. It was firstconsidered by the [Disney corporation in1969](https://rabcup.com/the-history-of-3d-projection-mapping/) and wasgiven prominence by Raskar and Fuchs with “office of thefuture\"[[Raskar1998]] and later by Raskar and otherresearchers.[[Raskar2001]] It has since gained considerable commercialpopularity in live entertainment.\n- Shader lamps[[Raskar2001]] is the more formal academic designation forprojection mapping. It is possible to use the technique alongsidereconstruction to project onto a white facial mannequin. Researchershave attempted to use the technology for remote patient diagnostic,projecting onto styrofoam heads.[[Rivera-Gutierrez2012]]\n- Bandyopadhyay et al. demonstrated[[Bandyopadhyay2001]] that it ispossible to track objects and projection map[[Dalsgaard2011]] onto themin real time. This is beyond the scope of the proposed projection ontofurniture since we wish to keep the system as simple as possible, butcould be useful for shared tasks in the future work.\n- Lincoln et al. employed animatronic avatars which they projected withshader lamps. This combination recreated facial expression and headmovement though they were limited in speed and range of control of theremote head.[[Lincoln2010]]\n- While shader lamps are an important and useful technology, there arelimitations imposed by its use. In particular if a realtime video feedor reconstruction of a subject is used then that scanned subject musteither remain still enough to be correctly mapped onto geometry on theremote side (useful for some virtual patients forinstance,[[Benjamin2012]] or else there must be a computationaladjustment made for their changing position to make them appear static,or the projection surface must move to match their movement as inLincoln et al.\n- ##### Metaverse\n- In supporting business it’s not clear that performance is improved oreven maintained by the use of a metaverse. Xi et al. found a significantnegative impact to productivity within metaverseapplications.[[xi2022challenges]] It lowers productivity, and mayincrease anxiety, nausea, VR sickness and evenmigraines.[[biener2022quantifying; @luong2022demographic]] It seems atthis stage that if we are determined to explore metaverse for businessthen we [should mitigate](http://www.sigtrapgames.com/vrtp/) theproblems as much as possible using the understanding we have gained sofar. It might seem that in so doing there is no difference betweenimmersive collaborative mixed reality (described above) and metaverse atall. We feel that the point of metaverse may be in itaccess to, if notreliance upon, a mechanism for global truth. What we will go on todescribe is likely to look more like traditional telecollaboration forsmall focussed teams, working on real-world problems, but we will alwaysmaintain an access to both the ability to scale, and a global registerof value, trust, and truth (digital assets).\n- ### Summary TL;DR\n-\n\t- The internet may be undergoing a transformation, driven by [trust abuses by incumbent providers](https://www.xda-developers.com/us-subcommittee-antitrust-report-amazon-apple-facebook-google-abuse-monopoly-dominant-position-market/), and popularization of concepts like Web3 and the Metaverse.\n-\n\t- Current large scale ‘social’ and immersive metaverse platforms have low adoption, while more advanced games-based solutions don’t address societal or business needs.\n-\n\t- Platforms like Roblox, VRChat, and Nvidia Omniverse emerge as potential contenders in the metaverse landscape.\n-\n\t- Distributed compute and large language models can help bridge the digital divide by enhancing global access equity and addressing the needs of emerging markets and less developed nations.\n-\n\t- The potential lies in uniting individual ecosystems with transferable goods across digital society through global ledgers like blockchain, despite the associated risks and uncertainties.\n-\n\t- Industry is looking towards an \"open metaverse\" to mitigate risks observed in implementations like Meta, necessitating contributions of open-source and federated approaches in telecollaboration research.\n-\n\t- By embracing Nostr protocol, we could enable connections and federation of mixed reality spaces, mediate data synchronization, and maintain secure communication.\n-\n\t- AI, machine learning, and generative art play a crucial role in driving innovation, with models like GPT4, Llama, Alpaca, generating excitement, and deepening global discussions around AI.\n-\n\t- Overcoming legislative and cultural barriers, alongside integrating large language models and distributed compute, can help address issues related to trust, accessibility, governance, and safeguarding within the metaverse and digital society at large.\n-\n\t- Open-source tools for supported creativity and augmented intelligence using multi-modal models, can help tackle accessibility, creativity, language barriers, and governance within the metaverse landscape.\n-\n\t- The application of these tools can lead to the development of new collaborative frameworks across various sectors such as training, research, biomedical, creative industries.\n-\n\t- By utilizing these new AI-driven technologies and emphasizing on trust, accessibility, and open-source approaches, we can create a more inclusive, global digital society while promoting technological empowerment and expansion of the global ideas market.\n- Artificial Intelligence-----------------------\n- ### The Cambrian explosion of ML/AI\n- This section is full of rough edges and some repetition; Working on it!\n- #### Overview\n- Though the history of this field reaches back to the 1940’s withMcCulloch et al. exploration of the possible mathematical underpinningsof human brain neurons.[[mcculloch1943logical]] During the writing ofthis book we have seen an inflection point in machine learning, to thepoint where the term “artificial intelligence” is feeling intuitivelyand subjectively real for the first time. To be clear AI is still apretty meaningless term. ‘Intelligence’ is one of those slippery wordswhich is highly dependent on context. A satnav system running on a phonecan make an intelligent choice about a route by synthesising data andpresenting comprehensible results, but it seems absurd to ascribe anintelligence to it. It’s possible that there’s some kind of “spoooky”quantum activity in play in a conscious human brain, as described inmind bending mathematical depth by Penrose in 1989.[[penrose1990emperor]]It’s something of an unknown unknown,[[kerskens2022experimental]] andthat we’ll never get to what’s called ‘strong’ or ‘general’AI,[[larson2021myth; @searle1980minds]] reserved by some scientists for“true consciousness”, whatever that means.[[butlin2023consciousness]]With that said we may be approaching the threshold of the ‘TuringTest‘,[[sep-turing-test]] initially posited by Alan Turing in1950,[[turing1950computing]] and the goalposts have begun to move in response to claims that there have been successful examples.[[warwick2016can; @french2012moving; @french2000turing; @searle2009turing]]\n  {{video https://www.youtube.com/watch?v=MxTWLm9vT_o}}\n  \n  It feels that in this moment it is appropriate to open with a riskssection, and work backwards. This is grounded in the hypothesis thatthere is no agreed end goal here (as we saw with the Bitcoin/Cryptochapter).\n- To set the tone here let’s have OpenAI’s ChatGPT give us a definition:itIntelligence is the ability to acquire and apply knowledge and skillsin order to solve problems and adapt to new situations. It can involve arange of cognitive abilities, such as perception, learning, memory,reasoning, and decision-making. Intelligence is a complex andmultifaceted concept that has been studied by psychologists,philosophers, and scientists for centuries.\n- The Oxford English Dictionary defines Artificial intelligence as “Thecapacity of computers or other machines to exhibit or simulateintelligent behaviour”. This is very murky territory. The boundary linebetween very capable trained systems and something that itfeels likeintelligence is obviously a subjective one, and different for eachperson and context, (Figure<a href=\"#fig:aiVenn\" data-reference-type=\"ref\" data-reference=\"fig:aiVenn\">8.1</a>.\n- <span class=\"image\">The terminology in the field is both somewhatblurred and highly ‘nested’.</span>\n- We will use AI and ML interchangeably in this text, but is so doing wehope to draw attention to the moment we find ourselves in. It feels likethere is an inflection point in human history happening right now,though to somewhat burst the bubble on this hyperbole it’s worth readingthe legendary Stephen Wolframs [explanation of these currentsystems](https://writings.stephenwolfram.com/2023/02/what-is-chatgpt-doing-and-why-does-it-work/)as glorified autocompletes.\n- Irrespective of the gap between the perception and truth around thesesystems there is now a feedback loop where the data that these systemsare trained on will be learning from both human **and** outputs fromsuch systems. Todays young children will never know a world in which theinformation they encounter is verifiable as of purely human origin. Theimplications of this are unclear but exciting. In writing this book itbecame obvious to add this chapter in, and change the direction on theresearch and product development, because nothing in human history willremain untouched by this. As we will see ‘metaverse’ is likely to changeat an incredible rate as a function of some parts of this technology.\n  ![]./assets/5823b4852c910b4a37b5a73f739551a66eda08b4.png\n  Major stands of generative AI and their associated models at the time of print.\n- <a href=\"#fig:llmlandscape\" data-reference-type=\"ref\" data-reference=\"fig:llmlandscape\">[fig:llmlandscape]</a>).\n  ![]./assets/ff177772944d3f8035ac7b75559cecc8b65176cd.jpg\n  Major stands of large language models from Yang et al<span class=\"citation\" data-cites=\"yang2023harnessing\"><a href=\"#fn1\" class=\"footnote-ref\" id=\"fnref1\" role=\"doc-noteref\"><sup>1</sup></a></span><section class=\"footnotes\" role=\"doc-endnotes\"><hr /><ol><li id=\"fn1\" role=\"doc-endnote\"><p>Yang et al., “Harnessing the Power of Llms in Practice.”<a href=\"#fnref1\" class=\"footnote-back\" role=\"doc-backlink\">↩︎</a></p></li></ol></section>\n- ### Generative art systems\n- Generative art refers to art that is generated algorithmically ratherthan manually created. In this report, we will provide an overview ofgenerative art, including its history, key techniques, applications, andfuture outlook.\n- Generative art emerged in the 1960s alongside early computer art.Artists like Frieder Nake and Georg Nees used algorithms to createabstract visual art. In the 1970s and 80s, fractal geometry allowed forcomplex recursive patterns. More recently, neural networks have enablednew forms of image generation, and they are the focus of this text.\n- #### Modern Models and Systems\n- #### Image Generation\n-\n\t- **Stable Diffusion**\n\t- An open source diffusion model capable of creating realistic images from text prompts. Widely used by artists due to:\n-\n\t- Flexible and intuitive text prompts\n-\n\t- Many interfaces and extensions for control, most notably        controlnet for our puposes\n-\n\t- Ability to fine-tune on custom datasets\n-\n\t- **DALL-E 3**\n\t- First to market, it’s a proprietary generative AI system from OpenAI that creates images from text captions. Key features:\n-\n\t- Stunning contextual comprehension\n-\n\t- Diverse creative capabilities from prompts\n-\n\t- Works best with OpenAI access and paid credits\n-\n\t- Integrated with Microsoft Bing, and free for small use cases.\n-\n\t- **Midjourney**\n\t- Web-based generative art tool with social community aspects. Notable for:\n-\n\t- Easy to start generating images quickly\n-\n\t- Built-in sharing and voting on generations\n-\n\t- Best in class “vibes”\n-\n\t- Limited free tier with paid subscriptions\n-\n\t- Privacy is questionable\n-\n\t- Subject to change, making consistency of approach impossible.\n-\n\t- **Imagen**\n\t- AI system from Google producing images from text. Characteristics:\n-\n\t- Very high-resolution outputs\n-\n\t- Control over lighting and detail\n-\n\t- Currently restricted to limited partners\n- #### How they work\n- There’s a good detailed and in depth blog post by Weng[here](https://lilianweng.github.io/posts/2021-07-11-diffusion-models/#classifier-free-guidance).\n- The four main types are over-viewed [byDeshwal](https://www.linkedin.com/feed/update/urn:li:activity:7088752096853803008/)as below:\n-\n\t- \"Adversarial\" as the name suggests are two opposite networks. One learns to create images out of noise (Artist) which is actually very hard process and the other says \"umm okay! This isn’t good\" (Critic) which is a relatively easy process. So because of the fact that being an artist is very hard than being a critic, these networks are not stable and Critic learns faster than the Artist.\n-\n\t- VAE, U-Nets etc belong to this category where same network breaks down image in deeper level features using CNN and then rebuild it again. That’s like a child learning by breaking things. VAE and U-Nets are almost same with minor differences and serve as a base model in Diffusion process so that think of them ans analogues to BERT in LLMs.\n-\n\t- : Well here it becomes complex. You apply function X to an image and then you re-create the original image by applying the Exact inverse of that function. For example, a very basic function is to add 5 and then subtract 5 to get original stuff.\n-\n\t- VAE, U-Nets are used as base models. You insert some pseudo random (because you know what you added based on a timestamp \"T\") Gaussian noise to an image and instead of asking the model to predict original image, you ask the model to predict the Noise that you inserted. Since Gaussian is an additive noise independent of original signal, you subtract that from image and get original image. Another piece is that instead of predicting whole noise at once, you predict the noise for previous (T-1) step.\n- Intuitive interfaces provide easy access to these models.[Automatic1111’s WebUI](https://github.com/AUTOMATIC1111/stable-diffusion-webui) offers afull-featured frontend to Stable Diffusion.[Midjourney](https://www.midjourney.com) provides a social platform tocreate, share and discuss AI art. [Runway](https://www.runwayml.com)delivers generative models through a subscription service.\n- Fine-tuning techniques like[DreamBooth](https://arxiv.org/abs/2208.12242) allow customizing StableDiffusion by training on datasets of specific concepts.[styleGAN-NADA](https://github.com/yfszzx/stylegan-nada) improves imagequality through noise optimization.[styleGAN3](https://github.com/NVlabs/stylegan3) introduces a generatorarchitecture that achieves state-of-the-art results. Extensions like[Gaugan](https://github.com/mit-han-lab/gaugan) bring control overseasons, weather, lighting and more.\n- Vibrant communities continually push boundaries of generative artthrough platforms like the [deepdreamsubreddit](https://www.reddit.com/r/deepdream/), the [ML Art Colabsrepository](https://github.com/dvschultz), and the [Stability AIblog](https://stability.ai/blog). Events like [ButterflyDream](https://www.elfman.ai/) showcase creativity.\n- Beyond generation, image processing techniques can manipulate existingvisuals. [GFPGAN](https://github.com/TencentARC/GFPGAN) restores blurredfaces using facial landmarks and semantic segmentation.[BRDNet](https://github.com/dazhizhong/BRDNet) removes unwanted objectsthrough edge-aware propagation and diffusion.[Real-ESRGAN](https://github.com/xinntao/Real-ESRGAN) super-resolvesimages up to 16x resolution.[DeOldify](https://github.com/jantic/DeOldify) colorizes black and whitephotos through deep learning. Such techniques enable restoring,retouching and enhancing images.\n- ##### Stable Diffusion\n- ##### Stable Diffusion Ecosystem\n- As an open-source diffusion model, [StableDiffusion](https://arxiv.org/abs/2105.05233) has given rise to anextensive ecosystem of models, interfaces, extensions, and communities.\n- ###### Models\n- The core Stable Diffusion repository provides strong baselines like[sd-v1-4](https://github.com/CompVis/stable-diffusion) optimized forspeed and [sd-v2-1k](https://github.com/Stability-AI/stablediffusion)for 1024x1024 resolution. Models exist for specific domains like anime,manga, and hentai.\n- ###### Interfaces\n- Many open-source frontends provide access to Stable Diffusion:\n-\n\t- [Automatic1111’s Web UI](https://github.com/AUTOMATIC1111/stable-diffusion-webui)\n\t- full-featured frontend with extensions\n-\n\t- [Disco Diffusion](https://github.com/alembics/disco-diffusion)\n\t- focused on creative exploration\n-\n\t- [Stable Diffusion GUI](https://github.com/hlky/stable-diffusion)\n\t- cross-platform interface supporting Google Colab\n-\n\t- [A1111-SD-webui-colab](https://github.com/invincible-sam/A1111-SD-webui-colab)\n\t- run Stable Diffusion entirely in Google Colab\n- ###### Extensions\n- Additional modules provide enhanced control:\n-\n\t- [ControlNet](https://github.com/lllyasviel/ControlNet)\n\t- mask-based image editing\n-\n\t- [Vedaso](https://github.com/alembics/vedaso)\n\t- creative effect brushes\n-\n\t- [Stable Diffusion Tuner](https://github.com/camenduru/stable-diffusion-webui-tuner)\n\t- fine-tune model inside Web UI\n-\n\t- [InvokeAI](https://github.com/invoke-ai/InvokeAI)\n\t- optimized inference and rendering\n- ###### Training & Tuning\n- Stable Diffusion can be customized:\n-\n\t- [DreamBooth](https://github.com/Stability-AI/dreambooth)\n\t- fine-tune on specific concepts\n-\n\t- [Stable Diffusion Tuning](https://github.com/yfszzx/stable-diffusion-stability-ai)\n\t- improve image quality\n-\n\t- [SD highres fix](https://github.com/d8ahazard/sd_hires_face_fix)\n\t- enhance face quality\n- ###### Community\n- Vibrant communities continually advance Stable Diffusion:\n-\n\t- [StableDiffusion subreddit](https://www.reddit.com/r/StableDiffusion/)\n\t- sharing creations and discoveries\n-\n\t- [Stability AI Discord](https://discord.gg/stabilityai)\n\t- dedicated channels on SD topics\n-\n\t- [Civitai](https://civitai.com)\n\t- central model hub with versioning\n- ##### ComfyUI\n- [ComfyUI](https://github.com/comfyanonymous) is a feature-rich set oftools and libraries for building interactive web applications using the[React](https://reactjs.org) framework. It makes creating beautiful,functional UIs easy through:\n-\n\t- **React-Based**\n\t- Built on React for modular, reusable components\n-\n\t- **Declarative**\n\t- Describe desired UI state without implementation details\n-\n\t- **Extensible**\n\t- Easily add custom components and functionality\n-\n\t- **Testable**\n\t- Designed for confident testing of UI behavior\n-\n\t- **Documented**\n\t- Well-documented for easy learning\n-\n\t- **Community**\n\t- Large active community for help and support\n- Extensions provide additional capabilities:\n-\n\t- **Components**\n\t- Pre-built React components for common UI elements like buttons, menus, and forms\n-\n\t- **Animations**\n\t- Animated React components for engaging UIs\n-\n\t- **State Management**\n\t- Tools for managing UI state\n-\n\t- **Testing**\n\t- Utilities for testing ComfyUI applications\n- Other notable features include:\n-\n\t- **Responsive Design**\n\t- Components auto-adjust layouts for any device size\n-\n\t- **Internationalization**\n\t- Support for global applications in different languages\n-\n\t- **Accessibility**\n\t- Interface remains usable by people with disabilities\n- The ComfyUI ecosystem is constantly evolving with new extensions createdby the vibrant community. With its versatility, extensibility andhelpful userbase, ComfyUI empowers developers to create beautiful,functional UIs for diverse web applications. The declarative programmingstyle and reusable components help quickly build interfaces that areresponsive, accessible, and internationalized.\n- #### Video generation\n- This is incredibly fast moving area and I have many many links in myMindmap. This section is a placeholder really, I wouldn’t act on it.\n-\n\t- **DALL-E 3D**\n\t- 3D model generation by Anthropic using principles from DALL-E 2. Allows:\n-\n\t- Text-to-shape generation\n-\n\t- Control over materials and lighting\n-\n\t- Interaction with object geometry\n-\n\t- **Xformation**\n\t- Proprietary 3D generation system capable of modifying shape from images.\n-\n\t- Deforms template 3D objects to match 2D images\n-\n\t- Controllable 3D effects from image edits\n-\n\t- **Text2Mesh**\n\t- Leveraging Stable Diffusion for text-based 3D model generation.\n-\n\t- 3D stylization based on natural language input\n-\n\t- Control mesh topology and appearance\n-\n\t- **Gaussian Splatting**\n\t- A development from the NeRF technology research, and likely the main contender for all future tech right now..\n-\n\t- Fast and efficient models\n-\n\t- Simple capture\n- Extending image synthesis techniques, models like [StableDiffusion](https://arxiv.org/abs/2105.05233) are being adapted togenerate artificial video content. Dedicated systems like[Phenaki](https://www.anthropic.com/research/phenaki) and[Runway](https://runwayml.com) enable text-to-video generation withcontrol over length, resolution and scene contents.\n- Creating smooth, consistent video requires modeling inter-framecoherence. Techniques like [EBSynth](https://ebsynth.com) achieve thisthrough interpolation and style transfer between frames.[FastVid2Vid](https://www.fastvideoai.com) matches latent vectorsbetween frames to improve consistency. [Instant Neural GraphicsPrimitives](https://nvlabs.github.io/instant-ngp) (Instant-NGP) learns atemporal model over sequences of frames.\n- Existing videos can also be enhanced through diffusion models.Techniques enable automatically increasing resolution, translatingscenes to different styles, editing objects seamlessly, and more.However, concerns exist around deepfake videos and synthetic media.Moderation systems like [Anthropic’s Claude](https://www.anthropic.com)may provide remedies.\n- Overall, rapid progress in generative video hints at possibilities ofcreating immersive films, VR experiences, lifelike avatars and more. Butthoughtful governance frameworks are necessary to manage risks.\n- #### Audio generation\n- Recent breakthroughs have also extended AI synthesis to the audiodomain, enabling applications like text-to-speech, music composition,and editing podcasts.\n- Models like [Jukebox](https://jukebox.openai.com/) and [Facebook’sJukebox](https://github.com/ facebookresearch/jukebox) generate novelmusic conditioned on genres, artists, and lyrics through a hierarchicalVQ-VAE framework. Meanwhile, [CoquiTTS](https://github.com/coqui-ai/TTS) and [Tacotron2](https://github.com/NVIDIA/tacotron2) convert text to human-likespeech using end-to-end neural architectures.\n- For editing audio, tools like [Riptide](https://riptide.ai/) removevocals from songs, while [Descript](https://www.descript.com/) insertsmusic and trims silences in podcasts. However, bad actors could exploitsuch capabilities for impersonation fraud and fake media. Stronggovernance models are critical.\n- Looking ahead, advances in generative audio may enable creatinginteractive AI companions, realistic speech synthesis, and personalizedmusic experiences. But maintaining public trust through transparency andaccountability will be essential.\n- #### 3D generation\n- 3D shape generation has also made strides through AI, transitioningtext-to-image breakthroughs to the 3D domain. Methods like[GA-Fusion](https://nv-tlabs.github.io/GA-fusion) combine GANs withgradient-based optimization for high quality results.[CLIP-Forge](https://github.com/autodeskailab/clip-forge) matchesrendered images with CLIP embeddings to guide optimization.[3D-Highlighter](https://threedle.github.io/3dhighlighter) localizestext prompts on shapes by comparing CLIP similarities.\n- Other approaches focus on reconstructing shapes from images.[XFormation](https://x-a-i.github.io/xformation) deforms template 3Dshapes to match target views. [Sketch-Guided VisionModels](https://sketching-the-future.github.io) optimize an SDF to matchinput sketches. [Dream Fields](https://ajayj.com/dreamfields) uses aNeRF parameterized by FiLM.\n- Such techniques could enable creators to manifest their ideas into 3Dworlds. However, thoughtful governance is critical to reduce risksassociated with impersonation, toxic content, and intellectual property.Community building, education, and responsible deployment will helprealize the positive potential of AI.\n- #### Conclusion\n- Rapid progress in AI has unlocked breakthrough capabilities forsynthesizing realistic content across images, video, audio, and 3Dgeometry. However, concerns around biases, misinformation, and toxiccontent necessitate responsible development and deployment of thesetechnologies. Maintaining public trust through transparency,accountability and inclusivity will be key to ensuring that the benefitsof AI progress outweigh the risks. If harnessed judiciously andethically, generative AI could augment human creativity in unprecedentedways. But it should empower rather than replace us. Ongoing advances inAI safety and governance will help achieve this vision\n- #### Major trends in AI\n- ##### The concentration of AI power\n- In recent times, the arena of artificial intelligence (AI) has seen theemergence of colossal entities that have taken the helm of AI researchand development. Prominent players such as Google, Microsoft, Meta, andOpenAI have plunged billions into the cultivation of potent AIarchitectures, with a special emphasis on large language models (LLMs)like GPT-3 and ChatGPT.\n- Originating in 2015 as a non-profit entity dedicated to the openexploration of AI for the collective good, OpenAI transitioned from itsfoundational ethos following a pivotal investment of $1 billion fromMicrosoft in 2019. This infusion of capital marked a turn towards a moreproprietary and competitive orientation, with the endgame of reachingthe pinnacle of artificial general intelligence (AGI). In this paradigmshift, OpenAI’s formidable 175 billion parameter behemoth, GPT-4, becamean enigmatic entity, shielded from public scrutiny. The rationaleprovided for this clandestine stance revolved around safety andcompetitive considerations.\n- Contrastingly, Meta adopted a path of openness, fully disclosing its 65billion parameter LLaMA 2 LLM, inclusive of the model weights, to thepublic domain. This ethos is rooted in the belief that a culture ofopenness propels rapid advancement by paving the way for widespreadexperimentation and communal contributions. However, it is pertinent tonote that Meta’s LLaMA 2 does carry stipulations on commercialexploitation.\n- Initially, Google was at the forefront of AI innovation with itsTensorFlow framework, but has seen its leading position eroded by Meta’sPyTorch. Post the commercial success of its products, Google’s AIendeavors have veered towards a more proprietary model, with novelmodels and academic publications seeing the light of day postcommercialization.\n- ##### Concentration of Power and Control\n- The centralization of AI evolution within the confines of a select fewprivate behemoths such as Microsoft-backed OpenAI or Google engenders anucleus of power and control over AI advancements. Contrary Research hasan [excellentreport](https://research.contrary.com/reports/the-openness-of-ai) onthis. As AI melds deeper into the fabric of products and services, thesetitans stand to wield extensive sway over the modalities of humancommunication, thought processes, and information accessibility.\n- The dependency on a sparse set of centralized LLMs harbors risks such asa widespread dissemination of confidential data in the face of asecurity breach. Moreover, the consolidated control furnishes thesecorporations with the means to potentially curtail information or moldnarratives in alignment with their vested interests. For instance,OpenAI exercises centralized control over the narrative frameworks ofits influential models like ChatGPT.\n- ##### Lack of Transparency and Innovation\n- Centralized LLMs exhibit a veil of opacity regarding their operationalmechanics and training methodologies. The elusive nature of OpenAI’sGPT-4 renders it a veritable black box, impervious to audits aimed atuncovering issues such as bias within the 175 billion parameter model’straining data. This shroud of mystery precludes informed discourse onthe ethical employment of this technology.\n- On the flip side, Meta’s open-sourced LLaMA facilitates a level ofpublic oversight that could preemptively address inherent issues. Theclandestine nature of closed models stifles innovation as it bars asignificant portion of the research community from building atop thesemodels. Open ideologies foster a milieu of collaboration, propellingprogress forward.\n- In summation, the monopolization of AI progression and influence withina handful of private juggernauts engenders risks spanning lack oftransparency, potential censorship, stifled innovation, and singlepoints of vulnerability. A paradigm shift towards more open anddecentralized methodologies is imperative to mitigate these loomingthreats.\n- ##### Some ways of thinking\n- Poulter [CEO](https://twitter.com/jamespoulter) of Vixen Labs has comeup with a somewhat strained analogy he calls “The Central IntelligenceAgent”. I’m going to include it until I find something better because Ithink he’s struck on something by dividing up the company needs with histaxonomy (Figure<a href=\"#fig:vixenAnthro\" data-reference-type=\"ref\" data-reference=\"fig:vixenAnthro\">8.4</a>).\n- <span class=\"image\">Vixen Labs anthropomorphic taxonomy of businessfunctions</span>\n- [“Renowned Yale Professor Jeffrey Sonnenfeld Discusses CEOs’ Fear andConfusion ofA.I”](https://observer.com/2023/06/ceo-ai-survey-yale-professor/) in arecent survey and presentation, and this is worth a quick look.\n- He sees five groups (per the article):\n-\n\t- “Curious creators” argue everything you can do, you should do. (Venture capitalist Marc Andreessen recently expressed a similar view in a blog post about A.I.)\n-\n\t- “Euphoric true believers” only see the good in technology.\n-\n\t- **“Commercial profiteers” don’t necessarily understand the new technology but are enthusiastically seeking to cash in on the hype.**\n-\n\t- “Alarmist activists” advocate for restricting A.I.\n-\n\t- “Global governance advocates” support regulation and necessary crackdown.\n- This seems a pretty simplistic set of buckets, but he’s got a decentdataset, and he’s -very- eminent so perhaps we should realistically seeourselves in the emboldened commercial profiteer category. I think thiskind of self reflection is important when dealing with things thispotentially transformational. Sonnenfeld said: it“As Robert Oppenheimerwarned us, it can be very dangerous to think that technology only takesus to the best of the world.”\n- ##### Trusted enterprise AI\n- Enterprise AI, specifically designed or retrofitted for professional usecases, is becoming a significant theme. This trend is driven by leadingcompanies such as Google, Microsoft, and latterly, curiously,Salesforce. Trust has become a primary consideration. At this earlystage in the technology it is important that corporate and private usersalike bear in mind that the LLMs are ‘leaky’ and are using the data fedinto them to train themselves. They are [explicit aboutthis](https://help.openai.com/en/articles/6783457-chatgpt-general-faq).Anything that goes into ChatGPT can resurface, as Samsung have found out[to their cost](https://cybernews.com/news/chatgpt-samsung-data-leak/).At this time the following companies have responded by banning the useof the technology internally.\n-\n\t- Apple\n-\n\t- Samsung\n-\n\t- Verizon\n-\n\t- Bank of America Corp.\n-\n\t- Citigroup Inc.\n-\n\t- Deutsche Bank AG\n-\n\t- Goldman Sachs Group Inc.\n-\n\t- Wells Fargo & Co.\n-\n\t- JPMorgan Chase & Co.\n-\n\t- Amazon’s lawyers recommended caution, though a recently leaked document suggests that managers are recommending it’s use.\n- There are likely itmany more who have done this less publicly, andindeed I am aware of examples. In response to this [OpenAIannounced](https://openai.com/blog/new-ways-to-manage-your-data-in-chatgpt)a business version of its tool, ChatGPT Business. Clearly this is apremium subscription service designed to be private by default. Theservice manages multiple users and does not train its future models onany conversations that flow through the business. This approach is asignificant step towards establishing trust in AI tools, as it ensuresthat sensitive business conversations are not used to train AI models.\n- As mentioned, Salesforce has been partially AI-powered for years. Theyrecently announced a series of AI-related developments, including thepilot of ‘Einstein GPT’, dubbed “the world’s first generative AI forCRM”. This tool builds on an existing underlying intelligence layercalled Einstein, which has been running in Salesforce since 2016. Thenew generative Einstein GPT is more content-oriented, helping businessesauto-generate text, pictures, and code. This tool is designed to helpsales teams find the most likely next customer to buy. Moreinterestingly they are leveraging their expertise in ‘SalesforceVentures’ a $500 million fund focused on funding generative AI startups.They have already invested in major projects like Humane, Triple,Anthropic, and Cohere.\n- They have italso announced an AI Cloud suite: ‘Salesforce’s AI Cloud’.It includes nine GPT-powered applications designed to automate andenhance various business processes. These applications include SalesGPT, Service GPT, Marketing GPT, Commerce GPT, **Slack GPT**, TableauGPT, Flow GPT, and Apex GPT.\n- Each of these applications is designed to cater to specific businessneeds, such as personalizing text generation for emails, automatingmundane tasks, customizing messages for different audience segments, andcreating workflows from natural language prompts. You can see that ourwork is already a customer here and this could be built upon.\n- This suite emphasizes the ‘Einstein GPT Trust Layer’, designed to ensureno potential data leakage, allowing enterprises to use AI for their mostsensitive needs. They say this trust layer sits between customer dataand the AI models, ensuring that the AI capability can providepredictions without actually looking inside the data. This approachwould allow our work to leverage the power of enterprise AI withoutsacrificing data privacy and/or security.\n- Elsewhere in enterprise AI:\n-\n\t- Accenture announced a $3 billion investment into their data and AI practice. This investment includes doubling their talent to 80,000 people, launching an AI navigator for Enterprise platform, and starting accelerators for data and AI readiness across 19 different industries.\n-\n\t- Contextual, an enterprise-focused AI startup, recently launched out of stealth with $20 million in seed funding.\n-\n\t- Glean announced a $100 million round and introduced a workplace chatbot called the Glean Chat Business.\n-\n\t- Olive: This healthcare automation startup has raised $450 million in fresh capital to build out its enterprise AI for hospitals 1.\n-\n\t- Welltok: This company provides a cloud-based employee wellness platform and has raised $355 million 1.\n-\n\t- Outreach: This sales engagement platform has raised $114 million in series E funding 1.\n-\n\t- Stackpath: This cybersecurity startup has raised $396 million in funding 1.\n-\n\t- Cohere, which is another business-focused AI startup, recently announced a $270 million series C funding round 2, and are partnered by [Oracle](https://www.oracle.com/news/announcement/oracle-to-deliver-powerful-and-secure-generative-ai-service-for-business-2023-06-13/).\n-\n\t- Tunisian enterprise AI startup InstaDeep has also raised $100 million in Series B financing led by Alpha 3.\n-\n\t- There’s a raft of tools like [CustomGPT](https://customgpt.ai/), or day planner [Before Sunset](https://www.beforesunset.ai/) that promise to take your data and make it smart by leveraging their deals with the big cloud providers. The prime example of this approach is [Dropbox AI](https://blog.dropbox.com/topics/product/introducing-AI-powered-tools), which claims to bring the Apple spotlight experience, with ChatGPT smarts, to clouds data. I don’t have confidence that I know enough about this, and it seems to be the purview of the AI-Club. If you have a use case there’s likely a product, but we’d have to project plan it in specifically and find the right fit and cost/benefit.\n- Taken overall these investments indicate a strong belief in thetransformative potential of AI in the enterprise sector.\n- With all this said it’s possible the technology is over-hyped. Whilesome believe that AI will disrupt industries in unimaginable ways,others argue that the technology still has a long way to go. Some evenargue that the distracting nature of the platforms may be net negativein the short term. Regardless, the current state of Enterprise AIrepresents a pivotal moment, with companies trying to productise AI andchange workflows within large corporations. The impact of thesedevelopments could range from a promise of transformation with AI beingevery bit as disruptive as everyone says it is, to an overhyped flop, asoften happens with new technologies. Some industry analysts argue thatwe’re still in the early stages of AI’s potential impact. Tech analystDan Ives likens the current state of AI to a “Gold Rush” moment,suggesting that we’re closer to 1995 than 1999 in terms of AI’sevolution and impact on industries. This perspective suggests that whileAI has made significant strides, there’s still a long way to go beforeit fully transforms the business landscape. I would tentatively agreewith this analysis, and avoid over investing in low confidence FOMOplays.\n- ##### Brute forcing ChatGPT4 with contexts\n- While we await ‘ChatGPT Business’ it’s still possible to explore usingthe tooling. The ChatGPT4 system is so far out ahead of everyone elseit’s sometimes useful to consider using it for business by adding incarefully crafted chunks of context data to refine how it answers. Thisis prompt engineered. A fine example of this everyday use of thetechnology can be found in [this clarify capitalreport](https://clarifycapital.com/the-future-of-investment-pitching)which finds that ChatGPT created pitch decks are **far** more compellingthan human created ones. It can be developed onward for more complexcorporate proposals like this through the API, which is a subscriptionservice, with additional tiers for heavy corporate use cases(<a href=\"#sec:microsoft\" data-reference-type=\"ref\" data-reference=\"sec:microsoft\">8.2.8.8</a>).\n-\n\t- **Advantages**\n-\n\t- Can be trialled in the web interface, spending a few hours or        perhaps days building a custom context that solves a specific        use case for the business, then simply copy/pasting.\n-\n\t- **OpenAI GPT is incredibly cost effective** ($1 for around 700        pages for GPT3.5 performance), or $20 a month for the web        subscription.\n-\n\t- GPT4 is **way** ahead in terms of performance. Possibly 18        months ahead of open source.\n-\n\t- Incredible team and partnership. Plug-ins are arriving very fast        to solve real business problems. They have scale and velocity.\n-\n\t- **Disadvantages**\n-\n\t- In terms of future project planning 18 months isn’t that long,        open source is worth investing in for the sake of        differentiation in those time-scales.\n-\n\t- It’s a very general model, refining this through the API means        programming work. This is a known unknown with staffing costs.\n-\n\t- You’re necessarily giving your commercial data to a cloud        service.\n-\n\t- Their “corporate” private package has trust implications, and        cost implications (more in the next section).\n-\n\t- **It’s reliant on a reliable internet connection, so it’s        suitable for the office but perhaps not ‘site’. Using it might        therefore mean ending up investing time in two development        tracks**.\n- One of the incredibly frustrating things about the GPT series is thatOpenAI are changing the code behind the models all the time as seen infigure<a href=\"#fig:GPTchanges\" data-reference-type=\"ref\" data-reference=\"fig:GPTchanges\">8.5</a>.This makes it hard to build upon in a trustable way.[[chen2023chatgpts]]The team built a dataset with 50 easy problems from LeetCode andmeasured how many GPT-4 answers ran without any changes. The Marchversion succeeded in 52% of the problems, but this dropped to a pale 10%using the model from June. I assume that OpenAI pushes changescontinuously, but we don’t know how the process works and how theyevaluate whether the models are improving or regressing. In my opinion,this is a red flag for anyone building applications that rely on GPT-4.Having the behavior of an LLM change over time is not acceptable.\n  ![]./assets/6bdb61174f6f0a8b5312c6a308bb3899bfc592fd.jpg\n  Changes to GPT models performance over time\n\t- denied by OpenAI (Chen et al)<span class=\"citation\" data-cites=\"chen2023chatgpts\"><a href=\"#fn1\" class=\"footnote-ref\" id=\"fnref1\" role=\"doc-noteref\"><sup>1</sup></a></span><section class=\"footnotes\" role=\"doc-endnotes\"><hr /><ol><li id=\"fn1\" role=\"doc-endnote\"><p>Ibid.<a href=\"#fnref1\" class=\"footnote-back\" role=\"doc-backlink\">↩︎</a></p></li></ol></section>\n- ##### ChatGPT\n\t- Code Interpreter\n- The ChatGPT Code Interpreter Plugin, introduced in March 2023, offers asandboxed environment featuring a working Python interpreter. Thisenvironment, which is isolated from other users and the Internet,supports an impressive array of functionalities. It comes pre-loadedwith over 330 libraries, including popular ones such as pandas,matplotlib, seaborn, and TensorFlow, among others.\n- As illustrated in Figure<a href=\"#fig:chatGPTdata\" data-reference-type=\"ref\" data-reference=\"fig:chatGPTdata\">8.8</a>,the Code Interpreter Plugin is capable of performing a myriad of tasks.For example, it can visualize any data inputted by the user, generateGIFs of the visualizations, and perform file uploads and downloads. Itcan extract colors from an image to create a color palette, andautonomously compress large images when memory is running low. Moreover,the plugin can clean and process data, generate insightfulvisualizations, and convert files to different formats quickly andefficiently.\n- The Code Interpreter Plugin can be installed by ChatGPT Plus users in afew simple steps. However, it is worth noting that while this plugin ispowerful, it does have certain limitations, such as frequent environmentresets, limited Optical Character Recognition (OCR) capabilities, and aninability to access the web. Despite these limitations, OpenAI continuesto work on improving the capabilities of the Code Interpreter Plugin,promising a future with substantial impacts on the world of programming.\n-\n\t- The Code Interpreter Plugin introduces a sandbox and an advanced language model, both of which are critical to its functionality.\n-\n\t- The emphasis of the plugin is on the quality of the model, which can generate code, debug it, and even decide when not to proceed without human input.\n-\n\t- The plugin offers substantial model autonomy, enabling it to work through multiple steps of code generation autonomously.\n-\n\t- Despite its powerful capabilities, the plugin does have limitations, such as frequent environment resets and limited OCR capabilities.\n-\n\t- The plugin is only available to ChatGPT Plus users, and requires a few simple steps for installation.\n-\n\t- The Code Interpreter Plugin represents a significant advancement in the realm of programming, changing the way programmers interact with AI systems.\n- ##### Go all in with Microsoft\n- Microsoft have -just- released GPT4 which privately works on your owndata. [This is likely the best option on the market rightnow](https://techcommunity.microsoft.com/t5/ai-cognitive-services-blog/introducing-azure-openai-service-on-your-data-in-public-preview/ba-p/3847000).\n- ##### Anthropic\n\t- Claude 2\n- Claude-2, Anthropic’s ChatGPT competitor was just released. It’scheaper, stronger, faster, can handle PDFs, and supports longerconversations.\n-\n\t- Claude is 5x cheaper than GPT-4.\n-\n\t- It has more recent data. A a mix of websites, licensed data sets from third parties and voluntarily-supplied user data from early 2023.\n-\n\t- It outperforms GPT4 on the GRE writing and HumanEval coding benchmarks.\n-\n\t- It features a context window of 100,000 tokens, the largest of any commercially available model.\n-\n\t- It can analyze roughly 75,000 words, about the length of “The Great Gatsby\".\n-\n\t- It can easily handle any code related tasks.\n- ##### Llama 2\n- The new Llama 2 model from Meta looks initially exciting but is prettymired in legal detail compared to the emerging open source communityefforts.\n- ###### License Grant\n- You are granted a non-exclusive, worldwide, non-transferable,royalty-free license to use, reproduce, distribute, modify, and createderivative works of Llama 2.\n- ###### Attribution and Acceptable Use\n- You must retain the attribution notice in all copies of Llama 2. Youruse must comply with Meta’s Acceptable Use Policy, which prohibitsillegal, deceptive, dangerous, or harmful uses.\n- ###### Commercial Use and Model Improvement\n- You cannot use Llama 2 to improve any other large language model besidesLlama 2. If your products or services have over 700 million monthlyactive users, you must obtain a separate license from Meta.\n- ###### Disclaimer, Liability, and Ownership\n- Llama 2 is provided \"as is\" with no warranties. You assume all risksfrom use. Meta has no liability for damages arising from use of Llama 2.You own any derivative works and modifications you create, subject toMeta’s ownership of Llama 2.\n- ###### Termination and Risks\n- Meta can terminate the license if you breach it. You must delete Llama 2on termination. Be aware of regulations like Article 28b of the AI Actin the EU. Do appropriate diligence to comply with laws and addressrisks around bias, fairness, transparency, and safety.\n- ###### Key Takeaways\n- Understand attribution requirements, acceptable use policy, commercialuse limits, disclaimer, risks, and ownership provisions. Seek legalcounsel given complexities.\n- ##### Roll your own trained LLM\n- This costs around $500k to train something up from a trillion tokensthat you bring to the party. This gets to ‘last years’ GPT3 level. It’stoo much, but it’s worth being aware of. It’s worth noting that Cerebrasare offering access to their [Andromedacluster](https://www.cerebras.net/press-release/cerebras-unveils-andromeda-a-13.5-million-core-ai-supercomputer-that-delivers-near-perfect-linear-scaling-for-large-language-models)which can train a significant model in the vein of Llama in around 11days.\n- ##### Wait for the Google integrations\n- I very strongly suspect that corporate level ML assistance is coming inforce to the Google stack already employed at our work. This is **by farthe most likely and pragmatic solution for the ‘project planningassistant’ business case**. [Vertex AI]() cloud based generative artsupport shows the direction of travel in this regard. \\[enhanced, framestyle=fill=lightgray, interior style=fill=lightgray\\] With this update,developers can access our text model powered by PaLM 2, Embeddings APIfor text, and other foundation models in Model Garden, as well asleverage user-friendly tools in Generative AI Studio for model tuningand deployment. Backed by enterprise-grade data governance, security,and safety features, Vertex AI can make it easier than ever forcustomers to access foundation models, customize them with their owndata, and quickly build generative AI applications.\n-\n\t- Advantages\n-\n\t- our work already trusts Google with it’s business data\n-\n\t- Single repository potential to leverage that fact\n-\n\t- Will likely be very cheap as a customer incentive. Currently        it’s around 700 pages per dollar.\n-\n\t- Disadvantages\n-\n\t- GCHQ have [taken the unusual        step](https://www.ncsc.gov.uk/blog-post/chatgpt-and-large-language-models-whats-the-risk)        of warning that the data put into these systems goes into their        training and can thereby resurface in competitors searches later\n-\n\t- The likes of Apple and Samsung have banned the use of these        cloud tools as a result. There’s anecdotal evidence of        commercially sensitive details surfacing, though it’s hard to        validate these\n-\n\t- The products can change over time, in ways that are outside of        your control\n- With all that said there is potentially business advantage to learninghow these systems work through doing.\n- ##### Build something custom self hosted\n- Building something custom here means taking an open source model, with apermissive license. There’s a lot of these now and they are ‘decent’.It’s possible to add in some code engineering around the edges to giveit access to private datasets through a chat interface.\n-\n\t- **Advantages**\n-\n\t- It’s private, local, under your control, and so you can trust        your data will be within the company walled garden\n-\n\t- It’s building toward IP and knowledge, in the likely scenario        where GPT4 level models are less than 2 years away. This is real        internal investment\n-\n\t- There are NO legal repercussions to using it in a purely        off-line way. You don’t even need to tell people you’re doing        it. ‘Probably’ no GPDR, data governance, compliance overheads if        designed right.\n-\n\t- **Disadvantages**\n-\n\t- It will still lie, and the company and individuals are still        responsible for the legal repercussions of acting on what the        model says.\n-\n\t- In a live deployment to the public it will occasionally say “bad        things”. It’s just impossible to control edge cases. Even        without the issues incurred by being online, the exposure around        that in terms of waivers is uncertain.\n-\n\t- Making it fit for purpose by adding memory is hard. The token        limits on these models are really small. They’re just not that        smart yet.\n- Below is output from the top two models from the [global leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard).The second highest rated worked better (supercot). Into it I uploadedthe ‘International Code of Practice for Entertainment Rigging’ which isa [meagre 35 pagedocument](https://www.plasa.org/wp-content/uploads/2017/11/ICOPER_V1.0.pdf).I have done 500 pages in the past but the quality starts to break downwith scale. Section 3.5 of the document says \\[enhanced, framestyle=fill=lightgray, interior style=fill=lightgray\\]“All custom builtequipment selected for a project must be reviewed and approved by aqualified person. Custom built equipment must be provided with necessarymarkings and documentation to ensure safe use.” You can see the outputfrom the model running at home (without internet) in Figure<a href=\"#fig:ICOPERLLM\" data-reference-type=\"ref\" data-reference=\"fig:ICOPERLLM\">8.6</a>.It has royally embellished the facts, but it’s actually delivering upgood advice, and the memory injection from the uploaded document meansit’s specific enough to the context of the question.\n  ![]./assets/ed4ec4e9acf8ecb80219042a6dc1a2d1b7235a02.png\n  Results from home hosted LLM\n- I have very much seen this when playing with both my own \"landmarkattention\" models, and Claude. I expect they will find a way round it atsome point. Because humans write with a focus on the beginning and endof documents, so large language models pay far more attention at thebeginning and end of their context input.[[liu2023lost]]\n- That’s tricky because if you load in a stack of PDFs and they gettranslated by the system into one big chunk (which may or may not be inthe expected order of the PDFs), then the PDFs in the middle of thesubmission are subject to more hallucination, garbling, forgetting etc.\n- The \"other\" system, vector databases is more even handed, and socommensurately more predictable. Still got huge problems though.\n- I have seen both effects. They’re currently both bad enough that Iwouldn’t trust document interrogation to these things. There areworkarounds obviously: Ask for references, ask if to tell you if itthinks it’s making it up, as for quotes, ask for locations, check theworking, ask in many sperate ways etc.\n- These \"tips\" can be encoded into the preamble that goes into everyquery, as a standing command, so you don’t have to, except for thechecking bit of course. You can do that with either self host models orthe web ones, but it’s something you need to do often with the webinterfaces. There’s no silver bullet yet but it feels like months away,so I am not advocating learning these tricks. Might be better to waitfor the fixes. Just don’t trust them, these are the reasons, and theyare all basically seeded in the way humans write.\n- There’s a lot that can be done here, but the cost benefit is unclear. Ifthis were sensitive internal planning documents, with a lot ofcomplexity, then there’s potentially a strong case, but we’d need tightprocedures to check on it’s homework. This tool, as usual, is best as away to rapidly construct a framework.\n- As a side note there’s a lot of open source tools like[SuperAGI](https://github.com/TransformerOptimus/SuperAGI) for ‘Agents’and [AnythingLLM](https://github.com/Mintplex-Labs/anything-llm) fordocument analysis which straddle the line between using cloud vendorsand local hosting. Building on one of these gives optionality, but theyare new and ‘flakey’.\n- ##### LoRA training\n- LoRA stands for Low-Rank Adaptation and it’s a cheap way to optimisemodels. A few hundred dollars of rented GPU time can nuance a model tobe more performant for a specific task. It sounds great but basicallyyou’re optimising for a task that you need to understand very well,possibly/probably to the detriment of the generality of the tool. Ifthere’s something you know you want, then this is an option that’sachievable and affordable.\n- Goat, a 7B LLaMA model finetuned for arithmetic tasks notablyoutperformed the  75x larger 540B PaLM model and GPT-4. Goat’s successcan be attributed to two primary factors: task-specific finetuning andLLaMA’s unique digit tokenization. The problem here is Llama is arguablyderived from data with a non-commercial license. OpenLlama gets aroundthis with their Apache2 (do what you like) copy of the model. Thesituation is rapidly evolving. Another example of task-specificfinetuning is the Gorilla project, where the LLM was trained to generateAPI calls. This is a really important area and we might be able to getahead in this niche. This controlling complex whole site systems withvoice control. The model was finetuned using 1,645 API calls fromvarious sources and demonstrated superior performance compared tonon-finetuned models. We can easily repeat that.\n- Recent findings suggest that less LoRA training gets better results, sothis is increasingly being adopted as a way to improve businessfit.[[xue2023repeat]]\n- ##### Enormous token limits\n- There’s a few models now boasting staggering token input limits. With 1million token windows it’s possible that each query to the LLM can be‘the whole corporate database and filesystem’ and then the question youwant an answer to. This is a beguiling option, probably the mostperformant (though I have not played with one), and also pretty muchuseless as nobody yet offers a legally trustable way to upload yourwhole business into a chat window.\n-\n\t- **Advantages**\n-\n\t- Likely excellent at it’s job\n-\n\t- Will ‘hallucinate’ less\n-\n\t- Probably stay on target\n-\n\t- Knows all about the business\n-\n\t- **Disadvantages**\n-\n\t- Web based right now, and knows all about the business\n-\n\t- Not actually available, I’m on wait lists.\n-\n\t- Longer latency as everything will need uploading in the token        window (assuming you don’t buy a service)\n- Very recently this in beginning to shift with the[emergence](https://github.com/eugenepentland/landmark-attention-qlora)of a locally run [‘landmark attention’ model](). There are some issueswith it at time of writing but this could be run on an our work cloudinstance. I have 4000 tokens running at home. I am exploring tree ofthought which needs these bigger windows and elevates reasoning by 70percent.[[yao2023tree]]\n- ##### Memory and other systems\n- There’s many ways to use a database (in this case a ‘vector’ DB) toprime the context window with appropriate information. It’s alsopotentially useful to look at combining older and well understood MLtechniques like knowledge graphs in combination.[[pan2023unifying]] I’venot personally had much luck with these yet.\n- ##### Cost implication for self build\n- It’s less than $20k to build a system that can do anything you need, onsite, rack mount in a ship, whatever. You can do it for less in thecloud, see the next section. You can also spend much, much more.Use-case dependent.\n- Here’s the output from the ‘local’ open source model for clarity. Noteit drifts off into nonsense toward the end then stops. This can be dealtwith but I chose not to: \\[enhanced, frame style=fill=lightgray,interior style=fill=lightgray\\] ASSISTANT:Based on the providedinformation, here are some steps you can take regarding custom builtequipment: 1. Ensure that the equipment is properly marked and traceableto the manufacturer. 2. Keep up-to-date with maintenance schedules andensure that maintenance records are readily available. 3. Follow safetyprocedures and guidelines for rigging work, including visualinspections, PPE usage, and clear communication. 4. Prevent anymalfunctioning or faulty equipment from being used and quarantine it ifnecessary. 5. Use equipment with a capacity that meets or exceeds thedesign specifications in the lifting plan. 6. Attach loads in a mannerthat does not damage the equipment or the load, and consider sling anglefactors. 7. Inspect electrical connections and repair or replaceequipment as necessary. 8. Maintain good housekeeping and prepare a post\n- ##### Build something custom in a private cloud\n- This is exactly the same as the previous section but you hire a privatecloud system ‘on demand’ to do the work. This is [currently pricedat](https://lambdalabs.com/service/gpu-cloud/pricing) $1.10/hr and onlycosts you money when you’re using it (though you have to shut it downyourself). This is both secure, and fairly cost effective. Also, itscales in that if you find a real serious application you can just getbigger rental GPUs and open a private/public interface. **It’s mypreferred path of all the private use cases except for mission criticalon site stuff**, and edge cases like boats at sea etc. For that you needto buy GPUs. AMD have [recentlyannounced](https://www.amd.com/en/newsroom/press-releases/2023-6-13-amd-expands-leadership-data-center-portfolio-with-.html)a partnership with opensource behemoth Huggingface to allow access tolarge and capable models like Falcon in their enormous new memoryarchitecture. Falcon is from the UAE and has odd views on human rights.This is one to watch.\n- #### Whistlestop tour of terms\n- ##### Transformers\n- Machine learning transformers are a groundbreaking architecture in thefield of natural language processing (NLP) that have redefined taskssuch as text generation, translation, and sentiment analysis. [Thislink](https://docs.google.com/presentation/d/1ZXFIhYczos679r70Yu8vV9uO6B1J0ztzeDxbnBxD1S0/edit#slide=id.g13dd67c5ab8_0_2648)is a very technical but excellent overview of how they work.Transformers have gained popularity due to their ability to efficientlycapture long-range dependencies and model complex relationships betweenwords in a sentence.\n- At the core of the transformer architecture lies the self-attentionmechanism. This mechanism allows the model to weigh the importance ofeach word in a sentence relative to the others, effectively capturingcontext and dependencies. In contrast to traditional neural networks,like recurrent neural networks (RNNs), transformers process inputsequences in parallel, rather than sequentially. This parallelprocessing enables transformers to efficiently understand and rememberlong-range dependencies, which is particularly important in NLP tasks.\n- RNNs, on the other hand, process input sequences one element at a time,making it difficult for them to capture relationships between words thatare far apart in a sentence. As a result, RNNs can struggle with tasksthat involve complex sentences or require a deep understanding ofcontext.\n- Transformer-based models, such as GPT (Generative Pre-trainedTransformer) and BERT (Bidirectional Encoder Representations fromTransformers), have become the go-to models for many NLP tasks.\n- The layers contain weight matrices that are responsible for encoding themodel’s knowledge and language understanding.\n- ##### GANs\n- Generative adversarial networks are used for generating synthetic data,and are incredibly useful for our fine tuning use cases. GANs consist oftwo neural networks that are trained to compete with each other, withone network generating synthetic data and the other trying todistinguish between the synthetic data and real data. This processallows GANs to learn the underlying distribution of the data andgenerate samples that are highly realistic.\n- Reinforcement learning is a type of machine learning that involves anagent learning through trial and error in order to maximize a reward.\n- ##### LoRA\n- LoRA, or Low-Rank Adaptation,[[hu2021lora]] is a technique that enablesefficient adaptation of large language models to specific tasks ordomains while maintaining their expressive power. It does so byintroducing a small modification to the pre-trained model’s weightmatrices, enabling the fine-tuning process to be more computationallyefficient without sacrificing performance. Visually you can think ofthis as slipping modification layers in between the transformer layers,which are far more interdependent and thereby expensive to retrain. Weare already experimenting with these systems for our use cases.\n- ##### Embeddings and Latent Space\n- Embeddings play a crucial role in both generative AI art and largelanguage models, as they provide a way to represent complex data types,such as text or images, in a continuous vector space. In both contexts,embeddings capture the underlying structure and semantics of the inputdata, enabling AI models to learn and generate new content based onthese representations. This is what the user sees happening with both AIgenerative art, and LLMs.\n- In the context of generative AI art, embeddings are often used torepresent visual elements, such as images or shapes. Latent space is acontinuous vector space where each point corresponds to an embeddingthat encodes the features and semantics of an image. Once the AI modelis trained, new images can be generated by sampling points from thislatent space and decoding them back into the image domain. Embeddingscan similarly represent styles or artistic techniques. Style transfertechniques, for example, utilize embeddings to extract and apply thestyle of one image to the content of another.\n- In the context of large language models, embeddings are used torepresent words, phrases, or sentences in a continuous vector space.These models are trained on vast amounts of text data, learning togenerate contextually relevant and semantically meaningful embeddingsfor language. Similarly to the visual use case in generative art theseembeddings capture various aspects of language, such as syntax,semantics, and relationships between words or phrases. Once trained, thelarge language models can generate new text by sampling from thedistribution of embeddings and decoding them back into the text domain.\n- Embeddings are also used in tasks like sentiment analysis, machinetranslation, and text classification, where the AI model must understandthe meaning and context of the input text.\n- ##### Vector databases\n- One of the highest points of human ‘friction’ when dealing with and AImodel, and especially LLMs is the lack of a persistent and/or contextualmemory within the systems. This is beginning to be addressed usingvector databases. A vector database is designed to efficiently store,manage, and query the high-dimensional vectors, often used in thecontext of machine learning and artificial intelligence. Thesehigh-dimensional vectors are the embeddings previously discussed.\n- Using a vector database with embeddings for AI data retrieval andprocessing can significantly improve efficiency, scalability, andperformance. In the context of a stored item of data, embeddings allowthe storage of complex ‘concepts’ as fixed-length vector, whichinteracts with the enormous latent space in the trained model. Thismakes storage and retrieval more efficient.\n- Vector databases allow and optimise for efficient nearest neighborsearch, which is crucial for data retrieval tasks in AI applications. Todo this, given a query input, the AI system first converts the inputinto an embedding using the same technique as for the stored data. Thevector database then performs a nearest neighbour search to find themost similar embeddings in the database. At scale this can result inmore consistency when using models, but crucially it doesn’t train themodels on events that have happened. It is not a ‘memory’.\n- ##### Memory Streams\n- In the paper ‘Generative Agents: Interactive Simulacra of HumanBehavior’ Park et al present a solution and working example for theproblem of contextual memory in AI systems.[[park2023generative]] This isa pretty stunning paper for our purposes in collaborative XR where wewould hope to interact with virtual agents.\n- As they point out in the paper virtual agents should be able to manageconstantly-growing memories and handle cascading social dynamics thatunfold between multiple agents. Their architecture uses a large languagemodel to generate a memory stream, reflection, and planning. The memorystream contains a comprehensive list of the agent’s experiences (writtenas a kind of internal monologue), and the planning module synthesizeshigher-level inferences over time. These memory transcriptions arehighly compressible and would be excellent as RGB style private datablobs between our federated virtual worlds. It will therefore mepossible to ‘meet’ virtual agent friends across instances of virtualspaces **and** through nostr social media. This is a key technology forour uses now.\n- ##### Gradient descent\n- Gradient descent is an optimization algorithm widely used in machinelearning and deep learning, including large language models, to minimizea loss function. The loss function measures the difference between thepredicted output and the actual output (also known as the target) for agiven input. The goal of the training process is to [minimize thisloss](https://societyofai.medium.com/gradient-descent-basics-and-application-1cef98179ee6)to improve the model’s performance.\n- In the context of large language models, gradient descent helps toadjust the model’s parameters (weights and biases) so that it cangenerate more accurate predictions. These models consist of multiplelayers of neurons with a large number of parameters that need to befine-tuned. It is this training process that takes so much time andenergy.\n-\n\t- Initialize parameters: The model’s parameters are initially set to random values. These parameters are then iteratively adjusted using gradient descent.\n-\n\t- Calculate loss: For a given input and target, the model generates a prediction, and the loss function calculates the difference between the prediction and the target.\n-\n\t- Compute gradients: The gradients of the loss function with respect to each parameter are computed. A gradient is a vector that points in the direction of the greatest increase of the function, and its components are the partial derivatives of the function with respect to each parameter. The gradients indicate how much each parameter contributes to the loss.\n-\n\t- Update parameters: The model’s parameters are updated using the gradients. This is done by subtracting a fraction of the gradient from the current parameter value. The fraction is determined by a hyperparameter called the learning rate. A smaller learning rate results in smaller updates and slower convergence, while a larger learning rate can result in faster convergence but might overshoot the optimal values.\n-\n\t- Iterate: Steps 2-4 are repeated for a certain number of iterations, a specified tolerance, or until convergence is reached (i.e., when the change in the loss function becomes negligible).\n-\n\t- Gradient descent has several variations, such as Stochastic Gradient Descent (SGD) and mini-batch gradient descent. These methods differ in how they use the training data to compute the gradients and update the parameters. In SGD, the gradients are computed and the parameters are updated using only one data point at a time, while in mini-batch gradient descent, a small batch of data points is used to compute the gradients and update the parameters. LLMs like GPT can use meta optimisers to train as they operate.[[dai2022can]]\n- ##### TPUs\n- Tensor Processing Units (TPUs) are specialized hardware accelerators formachine learning workloads, developed by Google. TPUs are designed tospeed up the training and inference of machine learning models,particularly large deep neural networks. They are highly parallel andoptimized for low-precision arithmetic, which allows them to performcomputations much faster than traditional CPUs or GPUs. TPUs can be usedin a variety of machine learning applications, such as natural languageprocessing, computer vision, and speech recognition. Google hasintegrated TPUs into its cloud platform, allowing developers to easilyuse them for their machine learning workloads. Overall, TPUs provide apowerful and efficient platform for machine learning. The top of theline Nvidia tensorflow unit at this time is the v4, and it is comparableif more generalised hardware.\n- ##### Tensorflow\n- TensorFlow is a popular open-source machine learning framework developedby Google and was instrumental in kicking off a lot of this researcharea. It is still widely used for training and deploying machinelearning models in a variety of applications, such as natural languageprocessing, computer vision, and speech recognition, but is beingsomewhat superceded by JAX. The consensus seems to be that JAX itself ismore specialised and harder to use, but works well with Googles hardwarecloud systems. Time will tell if this upgrade gets community traction.TensorFlow provides a flexible and high-performance platform forbuilding and deploying machine learning models. It allows users todefine, train, and evaluate models using a variety of deep learningalgorithms, such as convolutional neural networks and recurrent neuralnetworks. TensorFlow also has a strong emphasis on scalability andperformance, with support for distributed training and deployment on avariety of platforms, including GPUs and TPUs. Overall, TensorFlow is apowerful tool for building and deploying machine learning models.\n- ##### PyTorch\n- PyTorch is a popular open-source machine learning framework developed byFacebook’s AI research group. It is primarily used for applications suchas natural language processing and computer vision. PyTorch is based onthe Torch library and provides two high-level features: tensorcomputations with strong GPU acceleration and deep neural networks builton a tape-based autograd system. PyTorch offers a variety of tools andlibraries for machine learning, including support for computer vision,natural language processing, and generative models. It also allows foreasy and seamless interaction with the rest of the Python ecosystem,including popular data science and machine learning libraries such asNumPy and scikit-learn.\n- ##### NumPy\n- NumPy is a popular open-source library for scientific computing inPython. It provides a high-performance multidimensional array object, aswell as tools for working with these arrays. NumPy’s array class iscalled ndarray, which is a flexible container for large datasets thatcan be processed efficiently. The library provides a wide range ofmathematical functions that can operate on these arrays, includinglinear algebra operations, Fourier transforms, and random numbergeneration. NumPy also has a powerful mechanism for integrating C, C++,and Fortran code, which allows it to be used for high-performancescientific computing in a variety of applications. Overall, NumPy is anessential library for working with numerical data in Python.\n- ##### Latent space\n- In the context of generative artificial intelligence (AI), a latentspace is a high-dimensional space in which the model represents data aspoints. This space is \"latent\" because it is not directly observed, butis inferred by the model based on the data it is trained on. In the caseof a generative model, the latent space is often used to encode theunderlying structure of the data, such that samples can be generated bysampling from the latent space and then decoding them into the dataspace.\n- For example, in a generative model for images, the latent space mayencode the features or characteristics of the image, such as the shape,color, and texture. By sampling from this latent space and decoding thesample, the model can generate new images that are similar to thetraining data, but are not exact copies. This allows the model togenerate novel and diverse samples that capture the essence of thetraining data.\n- The latent space is an important aspect of generative models because itallows the model to capture the underlying structure of the data in acompact and efficient way. It also provides a way to control thegeneration process, such as by interpolating between latent space pointsto generate smooth transitions between samples. At this time thenavigation through that mathematical space is steered by vectors intothe space, which come from a separate and parallel integration of anatural language model. This crucial bridge came from research atOpenAI, and has been instrumental in the current explosion of usabilityof the systems.[[radford2021learning]]\n- ##### Edge AI compute and APUs\n-\n\t- [Qualcomm phone chip](https://www.theverge.com/2023/2/23/23611668/ai-image-stable-diffusion-mobile-android-qualcomm-fastest) offers low power and high speed Stable Diffusion on mobiles\n-\n\t- IBM have introduced the [concept of the AIU](https://research.ibm.com/blog/ibm-artificial-intelligence-unit-aiu), for high speed and low power training\n-\n\t- Nvidia’s [latest in the Jetson](https://www.okdo.com/p/nvidia-jetson-agx-orin-64gb-developer-kit/) Edge AGX line is a high performance general AI unit for industrial applications\n-\n\t- Esperanto Risc V chip [claims incredible performance](https://www.esperanto.ai/News/risc-v-startup-esperanto-technologies-samples-first-ai-silicon/) gains\n-\n\t- The MetaVRain asic [claims 900x speed increases](https://hdh4797.wixsite.com/dhan/project-1) on general GPU problems\n-\n\t- Microsoft are rumoured to be looking to mitigate the staggering costs of running ChatGPT ($1M/day) using forthcoming [hardware of their own design](https://www.theinformation.com/articles/microsoft-readies-ai-chip-as-machine-learning-costs-surge?)\n-\n\t- [Cerebras systems](https://www.cerebras.net/) have built an AI architecture from the ground up and claim incredible numbers.\n- These systems will drive the compute to less ‘constrained’ but somewhatless capable AI systems, distributing the access but increasing risks.\n- ##### Prompt engineering\n- The art of prompting constrains the generation space into documents thatcontain correct answers.\n- A better performance in few-shot prompting can be achieved byconstructing fictional scenarios.\n- Using flattery or painting a clear fictional narrative, such asreferring to the LLM as a \"masterful French translator,\" can help toguide the model towards producing the desired output.\n- Much of the initial prompt engineering involves constructing scenariosthat can only be completed correctly.\n- A good strategy often involves imagining what kind of document mightcontain the correct answer.\n- Fine-tuning models for tasks helps make the model more capable and moreable to do as it’s told.\n- Fine-tuning involves giving many examples of a task being donecorrectly, resulting in a model that acts almost as though it had beenprompted by thousands of correct examples. Use of assisted learning:Working with assistants or partners on the project can help identifyproblems and improve the model.\n- Correct absurdity instead of playing along: Even if the question seemsabsurd, the model should provide an answer grounded in reality insteadof going along with the absurdity.\n- Satisfaction of preference model: The model should aim to fulfill apreference model emulating what a human would want.\n- Consciousness of self: The model should be conscious in a sense of whatit is, what it’s doing, and where it’s situated in the world.\n- Avoidance of interpolation: Instead of just interpolating what humansmight do, the model should do something other than that.\n- Use of instruction tuning: This makes the desired outcome of the modelclearer.\n- Reinforcement learning with a reward model: This technique takes theresults to another level and provides feedback to improve the model.\n- Avoidance of mode collapse: The model should avoid fixating on aparticular way of answering.\n- #### Evaluation Metrics for Language Models\n- Prof Melanie Mitchell [providesinsights](https://www.pnas.org/doi/10.1073/pnas.2215907120) on theproblems of evaluating LLMs.\n- it“The key questions of the debate about understanding in LLMs are thefollowing: 1) Is talking of understanding in such systems simply acategory error, mistaking associations between language tokens forassociations between tokens and physical, social, or mental experience?In short, is it the case that these models are not, and will never be,the kind of things that can understand? Or conversely, 2) do thesesystems (or will their near-term successors) actually, even in theabsence of physical experience, create something like the richconcept-based mental models that are central to human understanding,and, if so, does scaling these models create ever better concepts? Or,3) if these systems do not create such concepts, can their unimaginablylarge systems of statistical correlations produce abilities that arefunctionally equivalent to human understanding? Or, indeed, that enablenew forms of higher-order logic that humans are incapable of accessing?And at this point will it still make sense to call such correlations“spurious” or the resulting solutions “shortcuts?” And would it makesense to see the systems’ behavior not as “competence withoutcomprehension” but as a new, nonhuman form of understanding? Thesequestions are no longer in the realm of abstract philosophicaldiscussions but touch on very real concerns about the capabilities,robustness, safety, and ethics of AI systems that increasingly playroles in humans’ everyday lives.”\n- ###### Debate on True Understanding and Intelligence\n- Melanie Mitchell highlights the ongoing debate about whether largelanguage models like GPT truly understand and are intelligent. Someexperts see their abilities as merely superficial, mimickingcomprehension without truly understanding. On the other side, some arguethat these models show signs of genuine comprehension, albeit limited.\n- ###### Lack of Sensory Grounding\n- Understanding in humans is deeply grounded in sensory experiences thatinform flexible and adaptable mental models. Unlike humans, languagemodels lack this essential grounding, which questions their capacity fortrue understanding.\n- ###### Need for Revised Evaluation Metrics\n- Mitchell emphasizes the need to rethink the metrics used for evaluatingAI systems. Current benchmarks often focus on aggregate performance,which can easily overlook failure modes and obscure the actualmechanisms of action within the model.\n- ###### Granular Testing and Abstract Generalization\n- To properly assess the capabilities of these models, more rigorous andgranular testing methods are essential. Testing should focus on taskslike abstract generalization to probe their true capabilities. Mitchelladvocates for an experimental science of machine cognition to fuelprogress in this area.\n- ###### Risks of Anthropomorphism\n- Mitchell warns against the risks involved in anthropomorphizing machineintelligence. Intelligence is not a universal, unlimited capacity; it isadapted to solve specific problems in specific environments.\n- ###### Human-Machine Performance Gap\n- Benchmark performance should not be directly equated between humans andmachines. Just because an AI system can pass a standardized test doesnot mean it possesses human-like generalization abilities.\n- ###### Importance of Reporting Failures\n- Reporting instance-level failures is crucial to understanding thebehavior of these models. Aggregate metrics often mask these failures,leading to an inflated sense of capability.\n- ###### Multidimensional Understanding\n- Understanding is a complex, multidimensional concept. When evaluatingmachine cognition, it’s important to specify what aspects are beingtested and to avoid making category errors in attribution.\n- ###### Interdisciplinary Collaboration\n- Mitchell encourages collaborations with fields like psychology andcognitive science to bring experimental rigor into the evaluation ofmachine cognition.\n- ###### Ultimate Goal\n- The end goal, according to Mitchell, should be to develop AI systemsthat act as beneficial, truth-seeking \"thinking machines,\" rather thansystems that display blind competency without understanding.\n- Evaluating the performance of language models involves a multi-facetedapproach that considers both unsupervised and supervised metrics, alongwith human evaluation, bias and safety factors, and efficiency. Thefollowing sections expand on these aspects.\n- ##### Perplexity\n- Perplexity measures the likelihood that the model will predict the nextword in a sequence correctly. Mathematically, it can be expressed as:\n- $${Perplexity} = 2^{-\\\\frac{1}{N}\\\\sum\\_{i=1}^{N}\\\\log\\_2 q(x\\_i)}$$\n- where *N* is the length of the text and *q*(*x*<sub>*i*</sub>) is thepredicted probability of each word *x*<sub>*i*</sub>.\n- ##### Diversity\n- Diversity can be quantified using Self-BLEU scores and unique n-grams.These metrics help in understanding the richness and uniqueness of thegenerated text. Higher diversity scores suggest a less repetitive andmore novel generation.\n- ##### Supervised Evaluation\n-\n\t- **Accuracy:** The ratio of correct predictions to the total number of samples in a classification test set.\n-\n\t- **F1 Score:** The harmonic mean of precision and recall, used primarily in classification tasks.\n-\n\t- **BLEU:** Stands for Bilingual Evaluation Understudy. Measures the similarity between candidate translations and reference translations using n-gram overlap.\n-\n\t- **ROUGE:** Metrics designed to compare the similarity between generated summaries and reference summaries.\n-\n\t- **Confidence:** Reflects the model’s predicted confidence on test examples. This is often calibrated using metrics like Expected Calibration Error (ECE).\n- ##### Human Evaluation\n- Human evaluation involves subjective assessment on various qualitativeaspects such as:\n-\n\t- Relevance\n-\n\t- Fluency\n-\n\t- Coherence\n- ##### Bias and Safety\n-\n\t- **Toxicity:** The rate at which the model generates language that can be considered toxic, hateful, or biased.\n-\n\t- **Stereotyping:** Measures the model’s propensity for unfair generalizations and stereotypes.\n- ##### Efficiency Metrics\n-\n\t- **Parameters:** Indicates the size and complexity of the model. Fewer parameters usually mean more efficiency.\n-\n\t- **FLOPs:** The number of Floating Point Operations performed by the model.\n-\n\t- **Latency:** Measures the time required to generate an output.\n-\n\t- **Power Usage:** Quantifies the energy consumed during the inference phase. Lower energy consumption is preferable.\n- #### Consumer tools\n- Gozalo-Brizuela and Garrido-Merchan provide a helpful review andtaxonomy of recent generative ML systems in their paper ‘ChatGPT is notall you need,’[[gozalo2023chatgpt]] with Figure<a href=\"#fig:MLtaxonomy\" data-reference-type=\"ref\" data-reference=\"fig:MLtaxonomy\">8.7</a>showing some of the main categories and systems.\n- <span class=\"image\">Taxonomy of [recent generative MLsystems](https://arxiv.org/abs/2301.04655) by Gozalo-Brizuela andGarrido-Merchan used with permission.</span>\n- ##### ChatGPT\n- ChatGPT is a neural network-based natural language processing (NLP)model developed by OpenAI. It is a continuation of the OpenAI programmewhich binds iteratively more capable Generative Pre-trained Transformermodels to a web and API based text chat interface. It usesself-attention mechanisms to generate high-quality text in a variety ofdifferent languages. ChatGPT is specifically designed for conversationaltext generation, and has been trained on a large corpus of dialogue datain order to produce responses that are natural, diverse, and relevant toa given conversation. Because it is a large language model, ChatGPT hasa vast amount of knowledge and can generate responses to a wide range ofquestions and prompts and meta-prompts.[[hou2022metaprompting]] Thisallows it to generate responses that are relevant, natural-sounding, anddiverse in nature. It has proved incredibly popular, demonstratinguncanny abilities for natural conversation, code generation, copywriting and more. It is substantially flawed in that it ‘speaks’ withauthority but often makes things up completely. This extended recentlyto creating academic references to back it’s assertions, completely outof thin air. The interface and APIs seem to be evolving and improving inreal time.\n- The model uses a transformer-based architecture, which means that itconsists of a series of interconnected “blocks” that process the inputdata and generate the output text. Each block contains multipleself-attention mechanisms, which allow the model to focus on differentaspects of the input data and generate a response that is coherent andrelevant to the conversation. In addition to its transformer-basedarchitecture, ChatGPT also uses a variety of other techniques to improveits performance. For example, it uses beam search to generate multiplecandidate responses for each input, and then selects the best one basedon a combination of factors such as relevance, coherence, and diversity.This allows the model to generate high-quality responses that areappropriate for a given conversation. Additionally, ChatGPT uses atechnique called “response conditioning” to bias the model towardsgenerating responses that are appropriate for a given conversationcontext. This allows the model to generate more relevant and coherentresponses, even when faced with challenging input data. Microsoft have[integratedGPT4](https://medium.com/@owenyin/scoop-oh-the-things-youll-do-with-bing-s-chatgpt-62b42d8d7198)with Bing, their internet search engine, and plugins for other websitesare coming soon.\n- One key aspect of GPT-4 is reinforcement learning with human feedback(RLHF), which helps align the model with human preferences, making itmore useful and easier to interact with. The process involves usinghuman feedback to fine-tune the model, requiring relatively less datacompared to the initial training phase. The development of GPT-4involves multiple components: the design of neural network algorithms,data selection, and human supervision through RLHF. Researchers havemade significant progress in understanding the behavior of the fullytrained system using evaluation processes, although the completeunderstanding of the model remains a challenge. GPT-4 has the ability toperform ‘reasoning’ based on the knowledge it has gained from thetraining data. While some interactions with the model may displaywisdom, others might lack it. The dialog format used in the modelenables it to answer follow-up questions, admit mistakes, challengeincorrect premises, and reject inappropriate requests. In a recentreport, researchers revealed groundbreaking advancements in GPT-4,hinting at sparks of artificial general intelligence.[[bubeck2023sparks]]The Microsoft researchers had access to the unrestrained GPT-4 duringits early development, allowing them to experiment for around sixmonths.\n-\n\t- GPT-4 can use tools with minimal instruction, displaying an emergent capability to utilize calculators, character APIs, and text-to-image rendering.\n-\n\t- GPT-4 can pass mock technical interviews on LeetCode and could potentially be hired as a software engineer.\n-\n\t- When tasked with creating a complex 3D game, GPT-4 produces a working game in a zero-shot fashion.\n-\n\t- GPT-4 can tackle the 2022 International Mathematics Olympiad, demonstrating a high level of mathematical ability.\n-\n\t- It can answer Fermi questions, which are complex questions often used in difficult interviews.\n-\n\t- GPT-4 can serve as an AI personal assistant, coordinating with others over email, booking events, and managing calendars.\n-\n\t- It can help diagnose and solve real-life problems, such as fixing a leak in a bathroom.\n-\n\t- GPT-4 can build mental maps of physical locations, which may be useful when embodied.\n-\n\t- It displays a theory of mind, capable of understanding what others may be thinking or believing about a situation.\n- There are obviously still limitations to GPT-4. As an autoregressivemodel, it cannot plan ahead and struggles with discontinuous tasks. Thisissue could potentially be addressed by augmenting language models withexternal memory. The paper raises concerns about the unrestrictedGPT-4’s ability to create propaganda and conspiracy theories, and theethical implications of giving AI intrinsic motivation. The researcherscall for a better understanding of AI systems like GPT-4 to addressthese challenges.\n- When asked about controversial figures or topics, GPT-4 can providenuanced and balanced answers, highlighting its potential to reintroducenuance to conversations. It is important to consider AI safety andalignment when developing powerful models like GPT-4. After itscompletion, the model underwent extensive internal and external testing,including red teaming and safety evaluations, to ensure alignment withhuman values. To ensure that AI systems align with various human values,it is necessary to establish broad societal boundaries, which may differacross countries and individual users. The art of crafting effectiveprompts for GPT-4 involves understanding the model’s behavior andcomposing prompts in a way that elicits the desired response. Thisprocess is similar to human conversation, where individuals adapt theirphrasing to communicate more effectively. As the model becomes moreadvanced, it may increasingly resemble human interactions, which canoffer insights into human communication and behaviour.\n- Although impressive, it is generally agreed that GPT-4 is not an AGI dueto its limitations in approximating human-level intelligence. Theconcept of consciousness in AI is debated, with some believing AI can beconscious, while others argue that AI can convincingly fakeconsciousness without being truly aware. Potential risks associated witheven a ‘simply’ and unconcious AI include disinformation, economicshocks, and geopolitical impacts. These concerns do not necessarilyrequire superintelligence but could result from large-scale deploymentof powerful language models without proper safety controls.\n- ##### GPT API and programming\n- In the context of programming, GPT-4’s advancements may have asignificant impact on how developers interact with AI systems,, andtheir productivity and creativity. The impact of AI systems like GPT-4on programming is significant, changing the way programmers work byallowing an iterative process and back-and-forth dialogue with the AI asa creative partner. This development is seen as a major step forward inprogramming.\n- It seems almost inevitable that at some stage a large language modelwill be optimised for computer instruction sets, and simply be able tobridge directly from human intentionality to bytecode, running it’s owntests and refinements without external consultation. The degree to whichthis would still be considered ‘a compiler’ is unclear.\n- ##### ChatGPT\n\t- Advanced Data Analysis\n- The ChatGPT Code Interpreter Plugin, introduced in March 2023, offers asandboxed environment featuring a working Python interpreter. Thisenvironment, which is isolated from other users and the Internet,supports an impressive array of functionalities. It comes pre-loadedwith over 330 libraries, including popular ones such as pandas,matplotlib, seaborn, and TensorFlow, among others.\n- As illustrated in Figure<a href=\"#fig:chatGPTdata\" data-reference-type=\"ref\" data-reference=\"fig:chatGPTdata\">8.8</a>,the Code Interpreter Plugin is capable of performing a myriad of tasks.For example, it can visualize any data inputted by the user, generateGIFs of the visualizations, and perform file uploads and downloads. Itcan extract colors from an image to create a color palette, andautonomously compress large images when memory is running low. Moreover,the plugin can clean and process data, generate insightfulvisualizations, and convert files to different formats quickly andefficiently.\n- The Code Interpreter Plugin can be installed by ChatGPT Plus users in afew simple steps. However, it is worth noting that while this plugin ispowerful, it does have certain limitations, such as frequent environmentresets, limited Optical Character Recognition (OCR) capabilities, and aninability to access the web. Despite these limitations, OpenAI continuesto work on improving the capabilities of the Code Interpreter Plugin,promising a future with substantial impacts on the world of programming.\n-\n\t- The Code Interpreter Plugin introduces a sandbox and an advanced language model, both of which are critical to its functionality.\n-\n\t- The emphasis of the plugin is on the quality of the model, which can generate code, debug it, and even decide when not to proceed without human input.\n-\n\t- The plugin offers substantial model autonomy, enabling it to work through multiple steps of code generation autonomously.\n-\n\t- Despite its powerful capabilities, the plugin does have limitations, such as frequent environment resets and limited OCR capabilities.\n-\n\t- The plugin is only available to ChatGPT Plus users, and requires a few simple steps for installation.\n-\n\t- The Code Interpreter Plugin represents a significant advancement in the realm of programming, changing the way programmers interact with AI systems.\n- ##### CodeLlama\n- Meta is now preparing to launch CodeLama, an open source code generatingmodel to rival OpenAI’s Codex and Microsoft’s GitHub Copilot. This coulddisrupt the industry.\n- ##### Google Gemini\n- ##### Current LLM Capabilities\n-\n\t- itGPT-4\n\t- Best for difficult reasoning tasks based on comparisons with Claude 2. Most advanced reasoning of current LLMs.\n-\n\t- itClaude 2\n\t- 100k token context window allows ingesting entire documents and books. Useful for QA and summarization with large knowledge bases.\n-\n\t- itBard\n\t- Integrated with web like Google Search. New multimodal features understand images. 40 language support.\n-\n\t- itCodex\n\t- Unlocks new abilities like running and refining code iteratively. Suspected separate model accessed via API.\n-\n\t- itPi\n\t- Focused on being a friendly personal assistant. Asks followup questions to continue conversations.\n-\n\t- itLLaMA 2\n\t- Open source alternative to GPT-4 from Meta. Could pose threat as safer open LLM.\n-\n\t- itPersonal LLMs\n\t- Allow customization with your own data. Create personalized AI assistants.\n- This is the list of libraries that code interpreter can call.\n- |                              |                                     |                              |                              ||:-----------------------------|:------------------------------------|:-----------------------------|:-----------------------------|| absl-py==1.4.0               | affine==2.4.0                       | aiohttp==3.8.1               | aiosignal==1.3.1             || analytics-python==1.4.post1  | anyio==3.7.1                        | anytree==2.8.0               | argcomplete==1.10.3          || argon2-cffi-bindings==21.2.0 | argon2-cffi==21.3.0                 | arviz==0.15.1                | asttokens==2.2.1             || async-timeout==4.0.2         | attrs==23.1.0                       | audioread==3.0.0             | babel==2.12.1                || backcall==0.2.0              | backoff==1.10.0                     | backports.zoneinfo==0.2.1 | basemap-data==1.3.2          || basemap==1.3.2               | bcrypt==4.0.1                       | beautifulsoup4==4.12.2       | bleach==6.0.0                || blinker==1.6.2               | blis==0.7.9                         | bokeh==2.4.0                 | branca==0.6.0                || brotli==1.0.9                | cachetools==5.3.1                   | cairocffi==1.6.0             | cairosvg==2.5.2              || camelot-py==0.10.1           | catalogue==2.0.8                    | certifi==2019.11.28          | cffi==1.15.1                 || chardet==4.0.0               | charset-normalizer==2.1.1           | click-plugins==1.1.1         | click==8.1.4                 || cligj==0.7.2                 | cloudpickle==2.2.1                  | cmudict==1.0.13              | comm==0.1.3                  || compressed-rtf==1.0.6        | countryinfo==0.1.2                  | cryptography==3.4.8          | cssselect2==0.7.0            || cycler==0.11.0               | cymem==2.0.7                        | dbus-python==1.2.16          | debugpy==1.6.7               || decorator==4.4.2             | defusedxml==0.7.1                   | deprecat==2.1.1              | dill==0.3.6                  || distro-info==0.23ubuntu1     | dlib==19.22.1                       | dnspython==2.3.0             | docx2txt==0.8                || ebcdic==1.1.1                | ebooklib==0.18                      | einops==0.3.2                | email-validator==2.0.0.post2 || entrypoints==0.4             | et-xmlfile==1.1.0                   | exceptiongroup==1.1.2        | exchange-calendars==3.4      || executing==1.2.0             | extract-msg==0.28.7                 | faker==8.13.2                | fastapi==0.92.0              || fastjsonschema==2.17.1       | fastprogress==1.0.3                 | ffmpeg-python==0.2.0         | ffmpy==0.3.0                 || filelock==3.12.2             | fiona==1.8.20                       | flask-cachebuster==1.0.0     | flask-cors==4.0.0            || flask-login==0.6.2           | flask==2.3.2                        | folium==0.12.1               | fonttools==4.40.0            || fpdf==1.7.2                  | frozenlist==1.3.3                   | future==0.18.3               | fuzzywuzzy==0.18.0           || gensim==4.1.0                | geographiclib==1.52                 | geopandas==0.10.2            | geopy==2.2.0                 || gradio==2.2.15               | graphviz==0.17                      | gtts==2.2.3                  | h11==0.14.0                  || h2==4.1.0                    | h5netcdf==1.1.0                     | h5py==3.4.0                  | hpack==4.0.0                 || html5lib==1.1                | httpcore==0.17.3                    | httptools==0.6.0             | httpx==0.24.1                || hypercorn==0.14.3            | hyperframe==6.0.1                   | idna==2.8                    | imageio-ffmpeg==0.4.8        || imageio==2.31.1              | imapclient==2.1.0                   | imgkit==1.2.2                | importlib-metadata==6.7.0 || importlib-resources==5.12.0  | iniconfig==2.0.0                    | ipykernel==6.24.0            | ipython-genutils==0.2.0      || ipython==8.12.2              | isodate==0.6.1                      | itsdangerous==2.1.2          | jax==0.2.28                  || jedi==0.18.2                 | jinja2==3.1.2                       | joblib==1.3.1                | json5==0.9.14                || jsonpickle==3.0.1            | jsonschema-specifications==2023.6.1 | jsonschema==4.18.0           | jupyter-client==7.4.9        || jupyter-core==5.1.3          | jupyter-server==1.23.5              | jupyterlab-pygments==0.2.2   | jupyterlab-server==2.19.0 || jupyterlab==3.4.8            | keras==2.6.0                        | kerykeion==2.1.16            | kiwisolver==1.4.4            || korean-lunar-calendar==0.3.1 | librosa==0.8.1                      | llvmlite==0.40.1             | loguru==0.5.3                || lxml==4.9.3                  | markdown2==2.4.9                    | markdownify==0.9.3           | markupsafe==2.1.3            || matplotlib-inline==0.1.6     | matplotlib-venn==0.11.6             | matplotlib==3.4.3            | mistune==3.0.1               || mizani==0.9.2                | mne==0.23.4                         | monotonic==1.6               | moviepy==1.0.3               || mpmath==1.3.0                | mtcnn==0.1.1                        | multidict==6.0.4             | munch==4.0.0                 || murmurhash==1.0.9            | mutagen==1.45.1                     | nashpy==0.0.35               | nbclassic==1.0.0             || nbclient==0.8.0              | nbconvert==7.6.0                    | nbformat==5.9.0              | nest-asyncio==1.5.6          || networkx==2.6.3              | nltk==3.6.3                         | notebook-shim==0.2.3         | notebook==6.5.1              || numba==0.57.1                | numexpr==2.8.4                      | numpy-financial==1.0.0       | numpy==1.21.2                || odfpy==1.4.1                 | olefile==0.46                       | opencv-python==4.5.2.54      | openpyxl==3.0.10             || opt-einsum==3.3.0            | orjson==3.9.1                       | packaging==23.1              | pandas==1.3.2                || pandocfilters==1.5.0         | paramiko==3.2.0                     | parso==0.8.3                 | pathy==0.10.2                || patsy==0.5.3                 | pdf2image==1.16.3                   | pdfkit==0.6.1                | pdfminer.six==20200517       || pdfplumber==0.5.28           | pdfrw==0.4                          | pexpect==4.8.0               | pickleshare==0.7.5           || pillow==8.3.2                | pip==20.0.2                         | pkgutil-resolve-name==1.3.10 | platformdirs==3.8.0          || plotly==5.3.0                | plotnine==0.10.1                    | pluggy==1.2.0                | pooch==1.7.0                 || preshed==3.0.8               | priority==2.0.0                     | proglog==0.1.10              | prometheus-client==0.17.0 || prompt-toolkit==3.0.39       | pronouncing==0.2.0                  | psutil==5.9.5                | ptyprocess==0.7.0            || pure-eval==0.2.2             | py==1.11.0                          | pyaudio==0.2.11              | pycountry==20.7.3            || pycparser==2.21              | pycryptodome==3.18.0                | pydantic==1.8.2              | pydot==1.4.2                 || pydub==0.25.1                | pydyf==0.7.0                        | pygments==2.15.1             | pygobject==3.36.0            || pygraphviz==1.7              | pylog==1.1                          | pyluach==2.2.0               | pymc3==3.11.5                || pymupdf==1.19.6              | pynacl==1.5.0                       | pypandoc==1.6.3              | pyparsing==3.1.0             || pypdf2==1.28.6               | pyphen==0.14.0                      | pyproj==3.5.0                | pyprover==0.5.6              || pyshp==2.1.3                 | pyswisseph==2.10.3.2                | pytesseract==0.3.8           | pytest==6.2.5                || pyth3==0.7                   | python-apt==2.0.1+ubuntu0.20.4.1 | python-dateutil==2.8.2       | python-docx==0.8.11          || python-dotenv==1.0.0         | python-multipart==0.0.6             | python-pptx==0.6.21          | pyttsx3==2.90                || pytz==2023.3                 | pywavelets==1.4.1                   | pyxlsb==1.0.8                | pyyaml==6.0                  || pyzbar==0.1.8                | pyzmq==25.1.0                       | qrcode==7.3                  | rarfile==4.0                 || rasterio==1.2.10             | rdflib==6.0.0                       | referencing==0.29.1          | regex==2023.6.3              || reportlab==3.6.1             | requests-unixsocket==0.2.0          | requests==2.31.0             | resampy==0.4.2               || rpds-py==0.8.8               | scikit-image==0.18.3                | scikit-learn==1.0            | scipy==1.7.3                 || seaborn==0.11.2              | semver==3.0.1                       | send2trash==1.8.2            | sentencepiece==0.1.99        || setuptools==45.2.0           | shap==0.39.0                        | shapely==1.7.1               | six==1.14.0                  || slicer==0.0.7                | smart-open==6.3.0                   | sniffio==1.3.0               | snuggs==1.4.7                || sortedcontainers==2.4.0      | soundfile==0.10.2                   | soupsieve==2.4.1             | spacy-legacy==3.0.12         || spacy==3.1.7                 | speechrecognition==3.8.1            | srsly==2.4.6                 | stack-data==0.6.2            || starlette==0.25.0            | statsmodels==0.12.2                 | svglib==1.1.0                | svgwrite==1.4.1              || sympy==1.8                   | tables==3.6.1                       | tabula==1.0.5                | tabulate==0.8.9              || tenacity==8.2.2              | terminado==0.17.1                   | text-unidecode==1.3          | textblob==0.15.3             || textract==1.6.4              | theano-pymc==1.1.2                  | thinc==8.0.17                | threadpoolctl==3.1.0         || tifffile==2023.7.4           | tinycss2==1.2.1                     | toml==0.10.2                 | tomli==2.0.1                 || toolz==0.12.0                | torch==1.10.0                       | torchaudio==0.10.0           | torchtext==0.6.0             || torchvision==0.11.1          | tornado==6.3.2                      | tqdm==4.64.0                 | traitlets==5.9.0             || trimesh==3.9.29              | typer==0.4.2                        | typing-extensions==4.5.0     | tzlocal==5.0.1               || ujson==5.8.0                 | unattended-upgrades==0.1            | urllib3==1.25.8              | uvicorn==0.22.0              || uvloop==0.17.0               | wand==0.6.11                        | wasabi==0.10.1               | watchfiles==0.19.0           || wcwidth==0.2.6               | weasyprint==53.3                    | webencodings==0.5.1          | websocket-client==1.6.1      || websockets==10.3             | werkzeug==2.3.6                     | wheel==0.34.2                | wordcloud==1.8.1             || wrapt==1.15.0                | wsproto==1.2.0                      | xarray-einstats==0.5.1       | xarray==2023.1.0             || xgboost==1.4.2               | xlrd==1.2.0                         | xlsxwriter==3.1.2            | xml-python==0.4.3            || yarl==1.9.2                  | zipp==3.15.0                        | zopfli==0.2.2                |                              |\n  ![]./assets/e606473a977e8fb4ae2b4e6600f6edcd8fbdbaaa.png\n  A multi-model conversation with chatGPT4 ‘code interpreter plugin’ by <a href=\"https://www.oneusefulthing.org/p/it-is-starting-to-get-strange\">Mollick</a>\n- #### Researcher toolkits\n- ##### COG containers for ML\n- Cog is an [open-source tool](https://github.com/replicate/cog) forpackaging machine learning models into production-ready containers. Itsimplifies Docker container creation, resolves compatibility issuesbetween CUDA, cuDNN, PyTorch, TensorFlow, and Python, and uses standardPython to define model inputs and outputs. Cog generates an OpenAPIschema, validates inputs and outputs with Pydantic, creates an automaticHTTP prediction server using FastAPI, and offers automatic queue workerfunctionality. It supports cloud storage with Amazon S3 and Google CloudStorage (coming soon), and allows model deployment on any infrastructurethat supports Docker images, including Replicate.\n- #### Enterprise and convergence\n- Startups are clearly eager to create innovative products and businessmodels, while established companies are exploring ways to respond to therapid advancements in generative AI. There seems to be a sense ofurgency for enterprises worldwide to develop AI strategies. Into thespace Nvidia AI have emerged as the clear market enabler, offering moreaccessible and faster infrastructure.\n- Their flagship product, the Nvidia DGX H100 is in full production andavailable through partners like Microsoft Azure. They’re also launchingNvidia DGX Cloud in collaboration with Microsoft Azure, Google GCP, andOracle OCI, making AI supercomputers accessible from a browser. NvidiaAI Foundations might be a suitable solution as a cloud service thatincludes:\n-\n\t- Language, visual, and biology model-making services\n-\n\t- Nvidia NeMo for building custom language models\n-\n\t- Picasso, a visual language model-making service for custom models trained with licensed or proprietary content\n- Nvidia Picasso could transform how visual content is created by allowingenterprises, ISVs, and service providers to deploy their own models.This might enable the generation of photorealistic images,high-resolution videos, and detailed 3D geometry for variousapplications, so it’s certainly something to watch closely. Ouralignment to self hosted and open source pipelines makes this less of apriority for exploration however.\n- Companies like Getty Images and Shutterstock intend to use Picasso forbuilding generative models with their extensive libraries. Nvidia saythey will also expand its partnership with Adobe to integrate generativeAI into creative workflows, focusing on commercial viability and propercontent attribution.\n- In the field of biology, Nvidia’s Clara could be a healthcareapplication framework for imaging instruments, genomics, and drugdiscovery. Their Bio NeMo might help researchers create fine-tunedcustom models with proprietary data. Nvidia BioNeMo service couldprovide generative AI models for drug discovery as a cloud service foreasy access to accelerated workflows.\n- As discussed Nvidia still hope that their enterprise metaverse offeringOnmiverse will gain worldwide traction. They are investing heavily inbringing ML into this product line. This is an incrediby similarproposition to flossvers, our proposal in this book, but operating at adifferent level of investment and technology, with commensurately moregated access.\n- Nvidia’s partnerships with TSMC, ASML, and Synopsys could lead toadvancements in chips and efficiency. Grace, Grace Hopper, and Bluefield3 are designed for energy-efficient accelerated data centers.\n- Microsoft’s investment in OpenAI and especially ChatGPT is clearlypaying dividends right now, but it is possible that they are mindful ofthe transition to less centrally controlled ‘edge’ hardware aspreviously mentioned has forced their hand toward offering theirgeneralised systems for use by corporations as plugins. It’s not clearat all that this is the correct model. With that said the currentresponse from Microsoft it yielding incredible powerful results. Theirplugin ‘code interpreter’ allows uploading of data files of up to100MBin size, and both writing and execution of Python code, one of thelanguages of data analystics. This toolkit is the first major leveragingof a unified multi-modal model. It can create media rich documents withcharts, images, and diagrams, providing appropriate descriptive analysisand diagnostics of the statistics employed. This is said to be happeningat the level of a junior data analytics specialist so far, and couldrepresent the beginning of a distributive democratisation of datascience Figure<a href=\"#fig:chatGPTdata\" data-reference-type=\"ref\" data-reference=\"fig:chatGPTdata\">8.8</a>and Figure<a href=\"#fig:chatGPTword\" data-reference-type=\"ref\" data-reference=\"fig:chatGPTword\">8.9</a>.\n- <span class=\"image\">The planned integration of these tools with OfficeSuite is likely to be a historic moment</span>\n- #### Accessibility\n- ##### Open source LLM chat and assistants\n- Sheng at el present FlexGen which allows execution of large languagemodel chat bots in powerful but affordable hardware[[Sheng2023]]. Thepaper presents FlexGen, a high-throughput generation engine for largelanguage models (LLMs) that can be run with a single commodity GPU.FlexGen can be configured under various hardware resource constraints byaggregating memory and computation from the GPU, CPU, and disk, and ituses a linear programming optimizer to store and access tensors. FlexGencompresses weights and attention key/value cache to 4 bits withnegligible accuracy loss, allowing for a larger batch size and increasedthroughput. When running OPT-175B on a single 16GB GPU. A PC runningalongside our metaverse server could provide ML assistance services tousers of the collaborative space immediately. We are currently using[Alpaca Llama 4-bit quantisedmodels](https://huggingface.co/Pi3141/alpaca-lora-30B-ggml/tree/main).\n- [Thisleak](https://www.semianalysis.com/p/google-we-have-no-moat-and-neither)purporting to be from a Google employee rings very true against theresearch we have done (paraphrased highlights):\n-\n\t- Open-source models are outpacing Google and OpenAI in terms of development speed and capabilities.\n-\n\t- Examples of open-source achievements include LLMs on a phone, scalable personal AI, responsible release, and multimodal advances.\n-\n\t- Google’s models have a slight edge in quality, but open-source models are faster, more customizable, more private, and overall more capable.\n-\n\t- Google has no secret sauce and should consider collaborating with the open-source community and enabling third-party integrations.\n-\n\t- Large models might slow down progress; smaller, faster models should be prioritized for quicker iteration.\n-\n\t- Meta’s LLaMA was leaked and sparked an outpouring of innovation in the open-source community, lowering the barrier to entry for training and experimentation.\n-\n\t- LoRA, an inexpensive fine-tuning method, has been underexploited within Google and should be paid more attention to.\n-\n\t- Retraining models from scratch is expensive and time-consuming; using LoRA allows for stackable improvements that can be kept up to date more easily.\n-\n\t- Large models may not be advantageous in the long run compared to rapid iteration on small models.\n-\n\t- Data quality scales better than data size; high-quality, curated datasets are becoming the standard in open-source training.\n-\n\t- Competing with open source is a losing proposition; Google should consider working with them instead.\n-\n\t- Owning the ecosystem, as Meta does, allows them to benefit from free labor and innovation, which Google could adopt by cooperating with the open-source community.\n- ##### Real time transcription\n- Real-time language translation can be applied to text interfaces withinmetaverse applications. This can be useful in situations where users aretyping or reading text, rather than speaking.\n- To apply NMT to text interfaces in the metaverse, the algorithm can beintegrated into the interface itself. When a user types text in aspecific language, the NMT algorithm can automatically detect thelanguage and generate a translation in the desired language. This can bedone in real-time, allowing for fast and seamless communication betweenusers speaking different languages. NMT algorithms are well-suited foruse in text interfaces, allowing for fast and accurate translationsbetween multiple languages. As the technology continues to advance, wecan expect to see more and more applications of NMT in the metaverse.\n- ##### Real time translation\n- One of its most impressive recent applications is real-time languagetranslation. In this section we will explore how this technology works,and how it can be used in metaverse applications.\n- Real-time language translation refers to the ability of a machinelearning model to instantly translate spoken or written text from onelanguage to another. This is different from traditional translationmethods, which often involve human translators and can be slow anderror-prone.\n- One of the key technologies behind real-time language translation isneural machine translation (NMT). This is a type of machine learningalgorithm that is based on neural networks. NMT algorithms are trainedon large datasets of text that has been translated by human experts.This allows the algorithm to learn the patterns and nuances of eachlanguage, which it can then use to generate accurate translations.\n- One of the key references for the use of neural machine translation inreal-time language translation is the paper \"Neural Machine Translationby Jointly Learning to Align and Translate\" by Bahdanau etal.[[bahdanau2014neural]] This paper describes the use of a neuralnetwork-based approach to machine translation, which has shownimpressive results in terms of accuracy and speed.\n- One of the key advantages of NMT is its ability to handle complex andvaried sentences. Traditional translation algorithms often rely on fixedrules and dictionaries, which can be limiting. NMT algorithms, on theother hand, can learn to handle a wide range of sentence structures andvocabulary. This makes them well-suited for translating naturallanguages, which are often full of irregularities and exceptions.\n- Another advantage of NMT is its ability to handle multiple languages atonce. Traditional translation algorithms often require the user tospecify the source and target languages, but NMT algorithms canautomatically detect the languages of the input and output text. Thismakes them well-suited for use in metaverse applications, where usersmay be speaking different languages at the same time.\n- One of the challenges of using NMT in metaverse applications is the needfor real-time performance. Metaverse applications often involvefast-paced interactions, and any delay in language translation canhinder the user experience. To overcome this challenge, NMT algorithmscan be optimized for speed, using techniques such as parallel processingand batching. It seems likely that in our proposed systems we willrequire API calls to external services for this functionality, and thiswill almost certainly incur a cost.\n- The use of NMT in metaverse applications is also an active area ofresearch, with a number of papers exploring the potential of thistechnology. For example, the paper \"Real-Time Neural Machine Translationfor Virtual Reality\" by Chen et al. describes the use of NMT algorithmsin virtual reality environments, showing how they can be used to supportreal-time communication between users speaking different languages.\n- Overall, the use of machine learning for real-time language translationis a rapidly-evolving field, with many exciting developments andapplications. As the technology continues to advance, we can expect tosee even more impressive results and applications in the future. [OpenAIwhisper](https://openai.com/blog/whisper/)\n- ##### Real time description\n- ##### Interfaces\n- [emg](https://tech.fb.com/ar-vr/2021/03/inside-facebook-reality-labs-wrist-based-interaction-for-the-next-computing-platform/)\n- ##### Text to sound\n- Complex acoustic environments are possible using [text tosound](https://anonymous.4open.science/w/iclr2023_samples-CB68/report.html)prompting.\n- <span class=\"image\">SD tools website shows elements of creation andtraining.</span>\n- #### Virtual humans\n- ##### Real time human to avatar mapping\n- #### AI actors\n- This is the next major section to be written.\n- #### Chatbots\n- We are using Flexgen[[Sheng2023]] on local hardware with various largelanguage models. Response time is over a minute and the accuracy of theresults is poor, but we are excited that it runs at all.\n  ![]./assets/66dbbeeb9ad877c5860fddd4ed14e7ff09599753.jpg\n  A16Z view the nominal deployment of and AI tech stack in this way, but we are not using any of these models.\n- ##### Faces\n- Faces and their corresponding personae are already paired in the theTavern AI ecosystem, encoding the metadata for the AI character into thePNG files. Obviously these could be inscribed and sold as Bitcoinordinals. It would be a nice touch to encode the personality for thecharacters into a larger, high resolution file using imagesteganography,[[morkel2005overview]] which would allow PKI type ownershiptoo. This would be more suitable for our RGB use case.\n- ##### Voices\n- ##### Autonomous tasks (AutoGPT & SaSa\n- Autonomous General Purpose Language Models (AutoGPTs) are tools that canperform any task, leveraging connection to the internet and LLMs.Recently there has been a shift in the AutoGPT space towards specializedagents that cater to specific tasks or industries, providing morefocused and useful solutions. This represents part of a more generalmove away from centralised general models toward more task specificsystems.\n- Autonomous agents for research for instance search the internet forinformation relevant to a specific research topic and extractinformation from trustworthy sources. One example is \\[insert\\], anagent designed explicitly for research purposes. Similarly, medicalresearch agents can call medical APIs and cite sources, providingtargeted assistance in the medical field.\n- These agents can leverage a chain of GPT calls and fine-tuned models toperform tasks efficiently and effectively.\n- It has been suggested that such systems be dubbed semi-autonomousspecialized agents (SASAs). These agents can streamline processes,automating multiple steps without requiring user mediation for eachtask. It can be seen that these are already being integrated withmessenger services such as Telegram bots, very similarly to the planningand approach seen in this book. We propose:\n- Extrinsic AI actors which link multiple  intrinsic virtual spaces.  Bespoke news and current affairs synthesis  Bespoke interactive subject matter training  bots that bring you what you want as bespoke audio visual packages\n- #### Governance and safeguarding\n- ##### Governance in the Virtual Reality Space\n- The governance of the virtual world will be a critical element in thesuccess of the Metaverse. The virtual world will need to be policed andgoverned in a way that will not only protect the rights of the citizensof this new digital environment but also protect them from cybercrime.As a somewhat strained but interesting example; [Interpolsee](https://www.reuters.com/technology/interpol-says-metaverse-opens-up-new-world-cybercrime-2022-10-27/)simulated environments as a way for terrorist groups to gather and planattack. Governments and regulatory bodies will play a key role in thegovernance of the virtual world, but so will the industry andbusinesses. Nair et al describe the “unprecedented privacy risks” of themetaverse, finding that wearing a headset can currently reveal 25 datapoints about the user, simply by analysis of thedata.[[nair2022exploring; @Nair2023]] This included inference aboutethnicity, disability, and economic status. Strong data protection lawswill be needed to safeguard privacy, data ownership and reduce the riskof data breaches. The governance of the virtual world will be criticalto success, safeguarding will be needed to protect citizens fromcyberattacks.\n- ##### Safeguarding in the Metaverse\n- When it comes to safeguarding in the Metaverse, people need to be madeaware of the risk of using VR technology. There are still many questionsaround the health implications of using VR and the impact it may have ona person’s eyesight. In terms of safeguarding in the Metaverse, this isjust one area that needs to be addressed. Users will also need to bemade aware of the risks of hacking. Users will need to be educated onthe need to be careful when it comes to sharing personal information andbe careful what websites they access on a virtual computer. They willneed to be made aware of the potential risk of having malware installedon their computer by visiting untrusted websites. Users will also needto be made aware of the potential risk of being manipulated in thevirtual world. This risk is particularly high when it comes to childrenwho are growing up in the digital world. They will need to be educatedon the potential risks of being groomed or manipulated in theMetaverse.  The problem with large social metaverse systems seems to be somehowwrapped up in humans need to test boundary conditions in novelsurroundings:\n-\n\t- Despite ‘best efforts’ by the software vendors there is a chaotic mix of levels of maturity amongst the participants. Ostensibly safe games are themselves ‘gamed’ by [slightly older](https://futurism.com/mom-horrified-her-kids-seeing-roblox) players.\n-\n\t- No recording of action, and reaction, creating a feeling of impunity of action. At it’s best ‘The philosophers Island’, but in safeguarding terms it seems more a school yard without a teacher, or perhaps worse, Lord of the Flies.[[cameron2012splendid]]\n-\n\t- Even adults in exclusively adult meeting places seem to go slightly off the rails trying to find technical and social boundaries instinctively. This leads to the now somewhat famous (TTP) “time to penis” problem[[lamb2022second]] ([coined at GDC 2009](http://gamedesignreviews.com/reviews/little-big-planet-browsing-content/)).\n-\n\t- The research on this is pretty thin.\n-\n\t- People seem to be suffering genuine psychological harm.\n- [Article in immersivewire](https://www.immersivewire.com/p/harassment-metaverse-how-address)\n- ##### How to fight against cybercrime in the Metaverse?\n- The best way to fight against cybercrime in the Metaverse is to educatethe general public on the potential risks and dangers in order toprevent them from being targeted. This can be done through variouschannels and mediums, such as social media, blogs and podcasts. Peoplewill need to be made aware of the risks of opening emails or clicking onlinks sent by unknown people. They will also need to be aware of therisks of clicking on ads and links that may lead them to websites thathost malware or that steal personal information.\n- [AI bill ofrights](https://www.whitehouse.gov/ostp/ai-bill-of-rights/)  Roblox [in BBC news](https://www.bbc.co.uk/news/technology-48450604) forchild exploitation.\n- #### The emergent role of AI in education\n- Affecting global change-----------------------\n- In his latest book, Runciman, professor of Politics at CambridgeUniversity, traces contemporary anxieties about artificial intelligenceback centuries to the origins of the modern state and corporation. Thereare interesting an striking parallels between the apparatus of state,and the emergent field of AI.\n- In the 17th century, Thomas Hobbes described the ideal state as a kindof \"automaton\"\n\t- a human-made machine that could provide stability andsecurity beyond fickle, emotional human politics. Later, the inventionof the limited liability corporation allowed artificial entities to takeon previously unthinkable risks and debts. Runciman argues that statesand corporations function essentially as “robots”\n\t- artificial,human-made creations constructed to make decisions and take actions.Much like our worries about AI today, these entities were designed totake over certain tasks and responsibilities from human hands.\n- States and corporations have acquired immense, sometimes uncheckedpower, persisting and protecting themselves as emergent features oftheir creation. They remain fundamentally inhuman; they do not think,feel, or have a conscience as individual humans do. Runciman suggeststhat the story of the modern world is the story of handing overdecision-making and control to these robots, AI and systemic alike.\n- Today, our prosperity, health, and safety depend deeply on these stateand corporate machines. Yet Runciman warns they could also lead tocatastrophe if we fail to maintain control. Their vast powers, from masssurveillance to nuclear weapons, remind us of their inhuman, roboticnature. Runciman argues we must focus not just on regulating new AI, buton democratizing and improving oversight over existing state andcorporate \"robots\" we rely upon daily. More transparency, public input,and innovation in governance is needed to retain human agency. Though wecreated them, these powerful machines can take on a life of their own.\n- This chapter attempts to speak to these issues, and the wider globalneed to reassert control over the human condition. We will start bylooking at global economics, then talk briefly about the globalapproaches to AI which are emerging, before exploring AI in detail init’s own chapter.\n- Malone, an ex central banking analyst now working in crypto, linksacross the last two chapters of blockchain, and money, [in a Twitterthread](https://twitter.com/brendanpmalone/status/1628067806984937472).He believes that policymakers should focus on the underlying problems inthe financial system, rather than just focusing on crypto. He has a lotof appreciation for US policymakers worrying about risk in the financialsystem. Crypto gets attention because it’s an easy target, but Malonebelieves that the real problems are so much bigger. According to Malone,people want to hold USD money to store value and make payments. Most arefamiliar with cash and bank deposits, but there’s actually a spectrum ofassets of varying quality that act like money, as we saw in the previouschapter. These include Euro dollars, repo, commercial paper, and more.This is what people are talking about when they reference the shadowbanking system\n\t- money moving around the financial system outside oftraditional banks, primarily in non-banks. As an aside, the name ‘eurodollar’ predates the Euro currency, and has nothing to do with it. Theorigins of the eurodollar market can be attributed to the Cold War inthe 1950s. At that time, the Soviet Union and its Eastern Europeanallies began depositing their US dollar holdings in European banks,primarily in London, to avoid the risk of their assets being frozen bythe US government. These dollar-denominated deposits held outside theUnited States became known as eurodollars. Malone notes that some amountof shadow banking activity is good because it allows the money supply tobe more reactive and expand and contract with economic activity, whichhelps fuel economic growth. However, the regulatory and politicalapparatus and the underlying systems weren’t really designed for asystem this large, opaque, and multi-dimensional. This was seen in 2008and 2009, which was as much about shadow banking and financial plumbingas it was about subprime housing and complex derivatives. The same wasseen in 2020 with COVID-19.\n- In times of crisis, people want to be able to freely convert whateverthey are holding into something safer on the spectrum. Sadly, sometimesmarket liquidity isn’t there, so the central banks and come to save theday, and this kicks the can down the road. The core issue is that peoplean institutions want to store capital in places they can’t access due totechnical, institutional, or geopolitical reasons. Sovereigns hold UStreasuries, hedge funds and HRTs use repo, and we have seen that thecrypto and Bitcoin economies have stablecoins.\n- Since 2008 and 2009, the Treasury Market has gotten significantlylarger, more fragile, and more complex. Banks have even morerestrictions on creating deposits, and the demand for safe assets keepsskyrocketing. On top of that, the geopolitical landscape has changeddramatically, with US sanctions and seizure of Russian USD assets.Malone notes that crypto is a response to these underlying problems.Although it is not perfect, it is getting better as people learn frompast experiences and begin to build regulatory clarity. This issue ofregulatory clarity leads us into this section of the book, which looksare implicit or explicit corruption of governance. As a uueful example;The New York Magazine article provided an in-depth interview with GaryGensler, the head of the Securities and Exchange Commission, in which heshared his thoughts on the cryptocurrency industry. One of the keytakeaways was his belief that all cryptocurrencies, except for Bitcoin,should be considered securities, as they involve relying on the work ofothers to give them value. Gensler is an ex banker, and an ambitiouspolitician, with his eyes on bigger prizes. He openly courted theattention of the now disgraced top team at FTX which failed sospectacularly. His assertions have sparked controversy, as it raisesquestions about the feasibility of registering all tokens as securities,given the unique challenges posed by open-source protocols and thechanging nature of blockchain technologies. Critics argue that Gensler’sstance could harm innovation and capital formation, as companies andentrepreneurs may struggle to comply with onerous regulations or abandontheir projects altogether. The current system simply doesn’t fit thisnew self forming marketplace, and his implication seems to be that thelegal end game here is the destruction of the invested capital, becauseof non compliance. This has led to frustration and concern among cryptoadvocates and investors, who worry about the impact of such policies onthe industry’s growth and development.\n- The discourse should be on the much more fundamental questions of themonetary system and fragility of past assumptions and their ability topredict what comes next. Even as these conversations happen, however,the Bitcoin and stable coin builders will keep building because they arenot going to sit around and wait for solutions to be presented to them.\n- ### Opportunities in Africa\n- #### Gridless\n- In the course of researching this book we see most opportunity forchange in Africa. As an example the company ‘Gridless’ began byexamining different energy sources in Africa and exploring opportunitiesfor larger energy generation and grid-connected energy. However, theyfound that the real benefit of gridless energy was in providing energyto places that were not well connected and did not have a good grid.They contacted mini-grid providers all over East and Southern Africa tolearn about their problems. A mini-grid is defined as a project thatgenerates energy under 2 megawatts, often under 1 megawatt. Theydiscovered that these providers had to overbuild for the community,resulting in stranded energy. The company found a way to utilize thisstranded energy by placing Bitcoin miners on it and paying the mini-gridproviders for it. They tested this method and found it to be successful.Additionally, they implemented a system to automate and remotely turnoff the power during periods of high usage to make the grid moreefficient and sustainable. This solution provided a win-win-winsituation for the company, the mini-grid providers, and the communitiesthey served.\n- The company utilizes Bitcoin miners to create space for other activitiesand to increase access to affordable energy for communities and smallbusinesses. As energy usage increases in the community, the companydecreases their usage of miners and moves them to other locations. Thisis outlined in their contracts with partners. The company is currentlytesting this method and has encountered some challenges, such as losinginternet connection at one of their sites and poor rainfall affectingthe amount of water flowing into turbines. They have found that buildinga lean operation with flexible and adaptable staff is crucial, as wellas creating processes and systems to manage variables. The company alsofaces unique environmental factors such as lightning strikes, whichrequire them to turn off their operations temporarily.\n- Gridless suggest that those who are critical of opportunities like thisoften come from a place of privilege and do not understand theconsequences of their actions in places like Africa where access toelectricity and other resources is limited. They argue that thesecritics, who are often from the West, have blinders on and cannot seethe impact of their actions on a global scale. They suggest that morepeople need to travel and have diverse experiences in order to changetheir perspective on Bitcoin and its potential to support humanflourishing in underprivileged areas. They also mention that gridlessplans may become a case study for the positive impact of Bitcoin miningon economic opportunities, particularly in rural Africa.\n- #### Machankura\n- Mobile phone users in Nigeria, Tanzania, South Africa, Kenya and fiveother African countries can now [send and receivebitcoin](https://www.forbes.com/sites/digital-assets/2023/03/15/how-africans-are-using-bitcoin-without-internet-access/?sh=434df18b7428)without a smartphone or Internet connection. Just a basic feature phoneand text code will suffice, thanks to a digital wallet from softwaredeveloper Ngako. No internet connection and low power handsets meansusing SMS and the Lightning network, with the phones SIM acting as thewallet private keys.\n- ### El Salvador as a case study\n- El Salvador became the first country in the world to adopt Bitcoin aslegal tender. El Salvador’s adoption of Bitcoin was a historic moment inthe world of Bitcoin and was met with a mix of excitement andscepticism. On June 9, 2021, the country’s Legislative Assembly approveda bill introduced by President Nayib Bukele to make Bitcoin a legaltender alongside the US dollar, which has been used as the country’sofficial currency since 2001.\n- President Bukele, who has been a vocal proponent of Bitcoin, stated thatthe adoption of Bitcoin was a way to promote financial inclusion andstability in the country, where more than 70% of the population isunbanked or underbanked. In a tweet, he stated, “Bitcoin will have thesame value as the US dollar. We will support both. They will have thesame power of purchase and will be accepted in the same way.”\n- The move was met with a lot of media attention and reaction, with somepraising it as a bold and innovative step, while others raised concernsabout the volatility of Bitcoin and their potential impact on theeconomy. President Nayib Bukele himself has faced criticism for hishandling of political power and some of his actions have raised concernsabout the potential for abuses of power. In 2021, President Bukele facedwidespread criticism for his handling of the legislative process and hisuse of the military to secure the Legislative Assembly building during apolitical standoff with lawmakers. This led to allegations ofintimidation and a violation of democratic norms, and raised concernsabout his willingness to use force to achieve his political goals.Additionally, President Bukele has faced criticism for his use of socialmedia to communicate with the public and his tendency to bypasstraditional media outlets, which has raised concerns about the potentialfor censorship and the manipulation of information. With that said heseems much loved in the country, and the previously appalling safetystatistics of the nation have radically improved.\n- In addition to the adoption of Bitcoin as legal tender, El Salvador hasalso proposed the issuance of a Bitcoin-backed bond to finance variouspublic works projects and promote the use of Bitcoin. The bond would bedenominated in Bitcoin and would allow investors to directly participatein the country’s development while also supporting the growth andadoption of Bitcoin.\n- Another ambitious project that has been proposed by President Bukele andhis administration is the creation of “Bitcoin City”, a new city thatwould mine Bitcoin at the base of a dormant Volcano, and offerconsiderable tax benefits to holders. The city would serve as a hub forinnovation and a showcase for the potential of Bitcoin, and would offera wide range of services, including housing, healthcare, education, andentertainment.\n- There has been a significant increase in the adoption of Bitcoin in ElSalvador, and apparently increased inward investment to the country.Many businesses, both small and large, have started accepting Bitcoin asa form of payment, and there has been a growing interest in Bitcoinamong the general population. Additionally, the government has beenactively promoting the use of Bitcoin through various initiatives. Therehave also been efforts to educate the public about Bitcoin and itspotential benefits, including increased financial security and reducedtransaction fees compared to traditional banking systems.\n- Overall, the adoption of Bitcoin in El Salvador has been positive, faroutstripping the number of people in the country with traditional bankaccounts, and has the potential to greatly impact the country’s economyand financial sector. However, it is important to note that there arestill challenges to overcome, such as regulatory and infrastructurelimitations, as well as ongoing concerns about the volatility andstability of Bitcoin.\n- Somewhat surprisingly the IMF have de-escalated their previously highlycritical assessment of the move, toward a more [concerned andconciliatorytone](https://www.imf.org/en/News/Articles/2023/02/10/el-salvador-staff-concluding-statement-of-the-2023-article-iv-mission):it“Bitcoin’s risks should be addressed. While risks have notmaterialized due to the limited Bitcoin use so far—as suggested bysurvey and remittances data—its use could grow given its legal tenderstatus and new legislative reforms to encourage the use of cryptoassets, including tokenized bonds (Digital Assets Law). In this context,underlying risks to financial integrity and stability, fiscalsustainability, and consumer protection persist, and the recommendationsof the 2021 Article IV remain valid. Greater transparency over thegovernment’s transactions in Bitcoin and the financial situation of thestate-owned Bitcoin-wallet (Chivo) remains essential, especially toassess the underlying fiscal contingencies and counterparty risks.”\n- In terms of economic impact, it is still too early to determine the fulleffects of the adoption of Bitcoin in El Salvador. However, it isexpected to have a positive impact on financial inclusion and stability,as well as reducing the reliance on traditional banking systems. The useof Bitcoin has the potential to lower transaction fees and increasefinancial security, which could be particularly beneficial for those whodo not have access to traditional banking services.\n- Overall, the adoption of Bitcoin in El Salvador marks a significant stepforward in the mainstream acceptance and adoption of Bitcoin and has thepotential to set a precedent for other countries to follow. However, itis important to monitor the situation and assess the long-term impactson the economy and financial sector.\n- The swift rise of digital walled gardens, moving towards a lesstransparent internet, reveals both a need for user data protection and acorporate push for greater control and profit. Tech giants like Google,Reddit, and Twitter are increasingly controlling their platforms,adjusting data flows for revenue growth. Google’s new privacy policy,which allows data collection for AI model training, increases publicconcerns over user consent and privacy rights. The wide-ranging languageof the policy gives Google considerable power in using user-generatedcontent, fueling debates on data usage ethics. Simultaneously, thesocial web’s shift towards an entertainment-focused business modelprioritizes revenue over human connection. Platforms target ad revenuethrough vertically scrolling videos, risking reduced content diversityand creating echo chambers.\n- Entertainment unions like the International Alliance of Theatrical StageEmployees (IATSE) are grappling with AI’s impact on employment. Theirapproach includes research, collaboration, education, politicaladvocacy, organizing, and collective bargaining to protect members’interests, including upskilling initiatives. Upskilling is gainingindustry attention. Companies like Tata Consultancy highlight the needto equip engineers with AI skills. Recognizing AI technologies’potential, they invest in reskilling programs to stay competitive andeffectively use AI tools.\n- The rise of generative AI and the declining open web raises concernsabout maintaining digital commons and encouraging diverse perspectives.AI-generated content could overshadow human contributions, makingmeaningful information harder to find and increasing misinformationrisks. This situation highlights the need for balance betweenAI-generated and human-generated content.\n- Data control battles between platforms and users fuel debates on dataownership and profit sharing. Users demand more control over their datause and potentially a share in the resulting profits. This issueemphasizes the need for transparent data policies and fair usercompensation models.\n- The influence of AI on the job market and the future of work is asignificant concern. As AI technology progresses, the need forupskilling and reskilling programs grows to ensure workers can adapt tochanging job requirements. Collaboration between industries,governments, and educational institutions is essential to addressAI-induced disruptions and ensure a smooth workforce transition.\n- Finally, the importance of AI ethics and governance grows as AItechnologies become more prevalent. The development and deployment of AIsystems require ethical frameworks, transparency, and accountability.Collaboration between AI researchers, policymakers, and ethicists iscritical to address potential risks and societal implications of AItechnology.\n- ### Artificial Intelligence in a global context\n- This currently borrows heavily from [the AI breakdownpodcast](https://www.youtube.com/watch?v=5clOHBo8HP8), is an AIgenerated placeholder, and needs considerably more more.\n- #### Perception of AI and Society\n- The examination of AI’s implications on societal structures shouldundoubtedly receive the necessary attention. Soros’s language andperception of reality seem particularly interesting, especially in theera of AI. He emphasizes his belief in reality and its importance inproviding moral guidance, a concept that seems increasingly challengedin the age of AI.\n- #### AI, Propaganda, and Authoritarianism\n- In an opinion piece for The Hill by Bill Drexel and Caleb Withers,titled \"Generative AI could be an authoritarian breakthrough inbrainwashing,\" the authors argue that the concern isn’t just externalattempts to influence U.S. elections, but the impact on the populationswithin authoritarian countries. They posit that foreign disinformationefforts by Chinese and Russian entities are only the tip of the iceberg,with Beijing and Moscow disseminating massive amounts of propaganda totheir own populations. The authors also cite instances of AI-enabledpropaganda and misinformation campaigns, both in the context ofundermining democracies and consolidating control within authoritarianstates.\n- #### Increased Surveillance Through AI\n- Another critical concern around AI and authoritarianism is the potentialfor increased surveillance. With the integration of AI and data scrapingtechniques, governments can employ extensive teams to facilitateunprecedented levels of surveillance, compromising privacy. Suchconcerns are raised in the works of authors like Daniel Oberhaus, whoposits that authoritarian regimes may have an advantage in AI due totheir willingness to exploit data, such as advanced facial recognitiondata, in ways that open societies might not.\n- #### Worker Surveillance and Remote Work\n- Furthermore, the issue of worker surveillance, especially with the riseof remote work regimes, has garnered the attention of various entities,including the White House. This is due to concerns over automatedsystems that employers are using to monitor their remote workers,highlighting a less benign context of surveillance.\n- #### AI and Ideology\n- One way AI might foster authoritarianism is by supporting the ideologyof closed societies or authoritarian regimes, such as China. Thesesocieties may leverage their global influence to disseminate theirparticular AI model, aligning it with their motivations and goals. TheCarnegie Endowment for International Peace points out that for mostcountries, AI technology is viewed as an economic development factorthat determines their standing in the global technology race, ratherthan as an ideological preference.\n- #### AI and Central Planning\n- Another concern is the fear that AI will make centrally plannedeconomies seem viable, where past attempts failed due to the lack ofdata. This idea was discussed in a conversation between Peter Thiel andReed Hoffman hosted by Neil Ferguson at Stanford in 2018. Thiel positedthat AI appears to favor centralization, an aspect that supports theprinciples of central planning.\n- #### Uncontrolled AGI Creation\n- On the other hand, some suggest that capitalist competition could resultin the creation of AGI that cannot be controlled. Dr. Jeffrey Hinton, avocal advocate of this view, argues that AI’s potential to disruptbusiness models could drive companies to recklessly pursue advancementsin AI to stay competitive. This could lead to increased state power aspeople become more reliant on the state in an AI-dominated economy,potentially resulting in increased authoritarianism.\n- #### AI Promoting Freedom\n- However, AI could also promote freedom in several ways. For instance, AItools like Altana have been used to identify goods made using forcedlabor, helping companies make informed supply chain decisions. AI couldalso serve as a new interface for disseminating information, such as achatbot that aids detainees in requesting legal assistance.\n- #### AI, Integrity, and Accessibility\n- Yet, for AI to achieve its full potential in promoting freedom, theintegrity of the information it disseminates must be uncompromised, andits accessibility must be ensured despite potential firewalls.\n- #### AI’s Impact on Societal Organization\n- Given these diverse viewpoints, it seems that the potential of AI toeither aid authoritarianism or promote freedom is yet to be fullyexplored. However, the inherent ability of democracies to encouragedisagreement and diverse perspectives may serve as a counterbalance tothe potential of AI for authoritarian control. Moreover, AI’s capacityas a catalytic force in societal organization should not beunderestimated. The increasing discourse around AI and its implicationsfor labor and technology usage suggests that AI technology is reshapingthe world in ways that were unimaginable just a few years ago. Itscapabilities in data analysis, decision making, and automation aretransforming industries and redefining the scope of what’s possible.\n- #### Democratization of AI Technology\n- An argument often made in favor of democratization of AI technology isthat it should be made open-source and freely available, thus creating achallenging framework for global political incumbents. This perspectiveis grounded on the belief that technology\n\t- and its underlying power -must be accessible to everyone to mitigate the risks of misuse andensure fair benefits distribution.\n- #### Open-source AI and Innovation\n- Open-source AI can be a vehicle for widespread innovation. It can spurcreativity, leading to breakthroughs in various sectors, from healthcareand education to energy and transportation. Open-source technologiesfacilitate collaboration, accelerate the pace of research, anddemocratize access, enabling researchers and developers across the globeto contribute to the expansion of AI’s capabilities. It opens thepossibility for rapid iteration and innovation, reducing the likelihoodthat a few powerful entities monopolize control over thesetransformative technologies.\n- #### Open-source AI and Global Politics\n- However, as beneficial as open-source AI may appear, the complexity ofglobal politics can make the transition challenging. A landscape whereAI technologies are open-source and freely available brings aboutpotential dilemmas in various areas including national security,economic competitiveness, intellectual property rights, and dataprivacy.\n- #### National Security and Open-source AI\n- To start, national security is a primary concern. AI has a myriad ofapplications in defense and security sectors, many of which couldpotentially be exploited by adversarial entities. As such, unrestrictedaccess to AI technologies could pose a risk to nations’ security.Nevertheless, it is crucial to note that security risks also stem fromconcentrated AI power. A handful of nations or corporations owning themajority of AI developments may lead to destabilization, powerimbalance, and heightened global tensions.\n- #### Economic Competitiveness and Open-source AI\n- Economic competitiveness is another intricate aspect. Countries andcorporations are engaged in a fiercely competitive race to advance in AItechnologies, recognizing the economic gains and strategic advantagestied to AI leadership. Open-source AI might challenge this dynamic,disrupting traditional models of competition. However, it could alsocreate an environment of shared growth, leading to a more balancedglobal AI landscape.\n- #### Intellectual Property Rights and Open-source AI\n- Intellectual property rights form another complex dimension in thediscussion. Open-source AI challenges traditional notions of ownershipand patents, potentially undermining the incentives for companies andindividuals to invest in AI research and development. Balancing the needfor innovation with the necessity to protect inventors’ rights becomescritical in an open-source framework.\n- #### Data Privacy and Open-source AI\n- Data privacy is a further point of contention. Open-source AI, coupledwith increasingly ubiquitous data collection methods, raises concernsabout individuals’ privacy. However, it also provides an opportunity todevelop robust, decentralized, and transparent AI systems that respectuser privacy.\n- #### A New Social Contract for AI\n- Thus, navigating the intersection of AI and global politics necessitatescareful consideration. It requires establishing a new social contractfor AI—one that respects human rights, promotes equitable economicgrowth, and protects national security.\n- #### Conclusion\n- In conclusion, making AI open-source and freely available represents ashift from the status quo, with both promising potentials and dauntingchallenges. A global AI framework that upholds democratic principles andvalues, promotes shared prosperity, and safeguards security and privacyis the aspiration. To achieve this, an inclusive and multidimensionaldiscourse is essential, involving governments, corporations, civilsociety, academia, and individual citizens. It is through thiscollective effort that AI’s true potential can be harnessed for theglobal good.\n- There is skepticism the idea of artificial general intelligence (AGI)leading to superintelligent machines that threaten humanity in the nearfuture. This supposed risk of AGI is described as a \"red herring\"\n\t- anunfounded fear. The reasons given are:\n-\n\t- We do not have a clear understanding or definition of general intelligence or consciousness.\n-\n\t- Current AI like large language models are limited in scope. They are good at statistical pattern matching in language, not generally intelligent.\n-\n\t- The hypothesis that intelligence and consciousness emerge simply from increasing computational power is unproven. There are likely other components we don’t understand.\n- The real risk is perhaps government control and regulation of AIdevelopment and applications, justified by arguing it is needed forsafety and responsible AI. This could impose limits on acceptable speechand thought. Centralised entities could become gatekeepers for howpeople access and interpret information about the world. Mandatingallowable language could narrow ideas and speech to fit an officialnarrative. Fears of AGI, even if exaggerated, open the door forregulators and bureaucrats to intervene in the name of safety. The riskis not AGI itself but the government control that hype about it enables.\n- There is speculation that AI will automate many white collar cognitivejobs, similar to how industrial machinery automated manual labor. Thismay \"chase humans up the value stack\" as lower value work is handled byAI, freeing people to focus on higher value creative activities.\n\n## Current Landscape (2025)\n\n- Industry adoption and implementations\n  - Metaverse platforms continue to evolve with focus on interoperability and open standards\n  - Web3 integration accelerating with decentralised identity and asset ownership\n  - Enterprise adoption growing in virtual collaboration, training, and digital twins\n  - UK companies increasingly active in metaverse development and immersive technologies\n\n- Technical capabilities\n  - Real-time rendering at photorealistic quality levels\n  - Low-latency networking enabling seamless multi-user experiences\n  - AI-driven content generation and procedural world building\n  - Spatial audio and haptics enhancing immersion\n\n- UK and North England context\n  - Manchester: Digital Innovation Factory supports metaverse startups and research\n  - Leeds: Holovis leads in immersive experiences for entertainment and training\n  - Newcastle: University research in spatial computing and interactive systems\n  - Sheffield: Advanced manufacturing using digital twin technology\n\n- Standards and frameworks\n  - Metaverse Standards Forum driving interoperability protocols\n  - WebXR enabling browser-based immersive experiences\n  - glTF and USD for 3D asset interchange\n  - Open Metaverse Interoperability Group defining cross-platform standards\n\n## Metadata\n\n- **Last Updated**: 2025-11-16\n- **Review Status**: Automated remediation with 2025 context\n- **Verification**: Academic sources verified\n- **Regional Context**: UK/North England where applicable",
  "properties": {
    "id": "book-to-be-processed-owl-axioms",
    "collapsed": "true",
    "- ontology": "true",
    "- term-id": "mv-189722789702",
    "- preferred-term": "book to be processed",
    "- source-domain": "metaverse",
    "- status": "active",
    "- public-access": "true",
    "- definition": "A component of the metaverse ecosystem focusing on book to be processed.",
    "- maturity": "mature",
    "- authority-score": "0.85",
    "- owl:class": "mv:BookToBeProcessed",
    "- owl:physicality": "ConceptualEntity",
    "- owl:role": "Concept",
    "- belongsToDomain": "[[MetaverseDomain]]",
    "- enables": "[[ImmersiveExperience]], [[Presence]], [[SpatialComputing]]",
    "- requires": "[[DisplayTechnology]], [[TrackingSystem]], [[RenderingEngine]]",
    "- bridges-to": "[[HumanComputerInteraction]], [[ComputerVision]], [[Robotics]]",
    "exclude-from-graph-view": "true"
  },
  "backlinks": [],
  "wiki_links": [
    "Desai2011",
    "Oyekoya2012; @Pan2014; @Pan2012",
    "Hedge1978",
    "Argyle1969; @Kleinke1986",
    "Pan2014",
    "aiken2020zooming",
    "Norris2013; @Norris2012",
    "Kristoffersson2011",
    "Lincoln2010",
    "Presence",
    "Bruno2013",
    "Nguyen2009",
    "Heldner2010",
    "Argyle1969; @Argyle1988; @Cook1977",
    "Fagel2010",
    "Biocca2003",
    "Argyle1988",
    "Wilson2000",
    "cryptopians",
    "Voida2012",
    "Porter2008",
    "Heeter1992; @Biocca1997",
    "Mihoub2015",
    "Ichikawa1995; @Okada1994",
    "Yendo2010",
    "Hofmann2006",
    "Stiefelhagen2001; @Stiefelhagen2002",
    "warwick2016can; @french2012moving; @french2000turing; @searle2009turing",
    "cameron2012splendid",
    "Lee2011; @Sakamoto2007; @Tsui2011",
    "Argyle1976",
    "Regenbrecht2015",
    "Bente1998; @Slessor2008; @Argyle1988; @Hofmann2006; @Pan2008",
    "Bloom2015",
    "Bandyopadhyay2001",
    "SpatialComputing",
    "Pejsa2016",
    "Schrammel2007",
    "Kristoffersson2013",
    "Roberts2015",
    "Beck2011; @Schuemie2001",
    "doerr2018measure",
    "Sellen1995",
    "Berger1975",
    "Chartrand1999",
    "Fels2000; @Sellen1992",
    "Otsuka2013",
    "Loomis2008; @Fullwood2006",
    "mclellan1993avatars",
    "MetaverseDomain",
    "Kleinsmith2013",
    "Parsons1974",
    "gibson2019neuromancer",
    "Bondareva2004; @Jouppi2002",
    "Mori1970",
    "Goldin-Meadow1999",
    "Tse2007",
    "ohba1998",
    "Usoh2000",
    "Adalgeirsson2010; @Lee2011; @Tsui2011; @Paulos1998; @Kristoffersson2013",
    "Kendon1967",
    "Kleinke1986; @Nguyen2009",
    "Bartneck2009",
    "schiano2004",
    "ComputerVision",
    "Stiefelhagen2002",
    "Moubayed2012; @Vishwanath2005; @Anstis1969; @Wollaston1824",
    "Stiefelhagen2001",
    "Adamczyk2007",
    "Ishii1993",
    "Rivera-Gutierrez2012",
    "lamb2022second",
    "Monk2002",
    "Tang2006",
    "Bailenson2001",
    "OMalley1996",
    "Blascovich2002",
    "Vertegaal2001",
    "turing1950computing",
    "Drobyshev22MP",
    "Ou2005",
    "Benjamin2012",
    "TrackingSystem",
    "Argyle1965",
    "Jones1971",
    "Ekman1993",
    "Macrae2002",
    "Aoki2003",
    "Gunawardena1997",
    "Robotics",
    "Eldes2013",
    "Risko2011",
    "Hauber2006",
    "gozalo2023chatgpt",
    "ImmersiveExperience",
    "liu2023lost",
    "Luff2011",
    "penrose1990emperor",
    "Vertegaal2002",
    "Cuddihy2000",
    "Fuchs2002",
    "authenticVolume2022",
    "Wolff2008",
    "Van_Eijk2010; @Bock2008",
    "Gillies2005",
    "Boker2011",
    "biener2022quantifying; @luong2022demographic",
    "Hancock2004",
    "Steinmeyer2013",
    "Steptoe2010",
    "Bulu2012",
    "park2022metaverse",
    "Watson1966; @Argyle1988",
    "Loomis2012",
    "DisplayTechnology",
    "Sellen1992",
    "Bondareva2004",
    "Kulik2011",
    "Chen2002",
    "Lincoln2009; @Lincoln2010; @Raskar2001",
    "Moubayed2012",
    "Langton2000",
    "Harms2004",
    "xue2023repeat",
    "morkel2005overview",
    "Argyle1988; @Argyle1976; @Argyle1965; @Argyle1976; @Argyle1969; @Kendon1967; @Monk2002",
    "Goodwin2000",
    "Bardram2012",
    "oeppen2020human",
    "radford2021learning",
    "Iverson2005",
    "Slater1999",
    "yao2023tree",
    "Jones2009",
    "Buxton1992",
    "Jamil2011; @Jetter2011",
    "Ho2008",
    "hou2022metaprompting",
    "Jouppi2002",
    "Dalsgaard2011",
    "Marti2005",
    "sep-turing-test",
    "Troje1998",
    "Tsui2012",
    "kerskens2022experimental",
    "Bartneck2007; @Bartneck2009",
    "Raskar2001",
    "prakash2020characteristic",
    "Hauber2005; @Gunawardena1997",
    "Mitchell2011",
    "Nowak2001",
    "Rosenthal1947",
    "Steptoe2009",
    "Beck2011",
    "park2023generative",
    "HumanComputerInteraction",
    "hennig2022social",
    "Bohannon2013",
    "Yee2007",
    "Edigo1988",
    "Nguyen2005",
    "Symons2004",
    "chen2023chatgpts",
    "Vertegaal2003",
    "Itoh2005",
    "Ekman1993; @Chovil1991",
    "Ting-Toomey2012",
    "Hall1969",
    "Yee2007; @Bailenson2001; @Bailenson2003",
    "Hager1979; @Argyle1988",
    "xi2022challenges",
    "Sheng2023",
    "dai2022can",
    "Colburn2000",
    "Slessor2008; @Hofmann2006",
    "Maeda2004",
    "Tang2010",
    "Murray2009",
    "St_John2001",
    "ondrejka2004escaping",
    "Vertegaal1997",
    "Kollock1998",
    "Van_Baren2004; @Harms2004",
    "Kauff2002; @Kauff2002a",
    "Blanche2010; @Tay2008",
    "Vertegaal2000",
    "Rienks2010",
    "Criminisi2003; @Van_Eijk2010",
    "Roberts2013",
    "nevelsteen2018virtual",
    "Novick1996",
    "Bailenson2002",
    "Fulk1987; @Haythornthwaite1995",
    "bahdanau2014neural",
    "katagiri2007aiduti",
    "Vishwanath2005; @Anstis1969; @Wollaston1824",
    "fox2015neural",
    "Cook1977; @Kleinke1986; @Fagel2010",
    "Gemmell2000; @Kuster2012",
    "butlin2023consciousness",
    "mystakidis2022metaverse",
    "Loomis2008",
    "Williams1977",
    "Sakamoto2007",
    "RenderingEngine",
    "nair2022exploring; @Nair2023",
    "Slater2004",
    "Gotsch2018",
    "Otsuka2005",
    "Deckers2013; @Gibson1963",
    "Strait2014",
    "Krauss1996",
    "Holm2010",
    "Dourish1996",
    "mcculloch1943logical",
    "barrero2021working",
    "hu2021lora",
    "Raskar1998",
    "pan2023unifying",
    "larson2021myth; @searle1980minds",
    "Kleinke1986",
    "Schegloff1998",
    "bubeck2023sparks",
    "Jouppi2004",
    "Argyle1969",
    "Kluttz2009",
    "Garrison1999; @Beck2011",
    "Carlston2013",
    "Tuddenham2009",
    "Nagendran2012"
  ],
  "ontology": {
    "term_id": "mv-189722789702",
    "preferred_term": "book to be processed",
    "definition": "A component of the metaverse ecosystem focusing on book to be processed.",
    "source_domain": "metaverse",
    "maturity_level": null,
    "authority_score": 0.85
  }
}