{
  "title": "Privacy and Data Governance",
  "content": "- ### OntologyBlock\n  id:: 0411-privacydatagovernance-ontology\n  collapsed:: true\n\n  - **Identification**\n    - public-access:: true\n    - ontology:: true\n    - term-id:: AI-0411\n    - preferred-term:: Privacy and Data Governance\n    - source-domain:: ai\n    - status:: in-progress\n    - version:: 1.0\n    - last-updated:: 2025-10-29\n\n  - **Definition**\n    - definition:: Privacy and Data Governance is a trustworthiness dimension ensuring AI systems protect personal information, respect data rights, maintain data quality, and implement appropriate access controls throughout data collection, processing, storage, and sharing activities. This dimension encompasses four core components: privacy protection (implementing data minimization collecting only necessary information, purpose limitation ensuring data used only for specified purposes, privacy by design embedding privacy safeguards into system architecture from inception, and privacy by default configuring systems to maximum privacy protection without user intervention), data quality (ensuring accuracy of data reflecting current reality, completeness with all required information present, currency maintaining up-to-date information, and integrity preventing unauthorized modification or corruption), access control (implementing role-based access restricting data access to authorized personnel with legitimate need, enforcing need-to-know principles limiting information exposure, maintaining comprehensive audit trails documenting all data access and modifications, and protecting against unauthorized access through authentication and authorization mechanisms), and data governance framework (documenting data provenance tracking origin and collection methods, maintaining data lineage showing transformations and derivations, conducting Data Protection Impact Assessments for high-risk processing per GDPR Article 35, and ensuring GDPR compliance including lawful basis, consent management, and data subject rights). The EU AI Act integrates seamlessly with GDPR requirements establishing that AI systems processing personal data must implement privacy by design and default as architectural principles, while high-risk systems require DPIAs before deployment with documented provenance, lineage tracking, and purpose limitation enforcement. The 2024-2025 period witnessed privacy-preserving technologies mature from theoretical frameworks to production infrastructure, including federated learning enabling distributed model training without centralizing sensitive data, differential privacy providing mathematically provable privacy guarantees at scale (U.S. Census 2020 deployment, Apple/Microsoft/Meta telemetry implementations), homomorphic encryption and secure multi-party computation enabling computation on encrypted data, and automated governance-as-code approaches transforming policy documents into executable infrastructure with real-time compliance verification.\n    - maturity:: mature\n    - source:: [[GDPR]], [[EU AI Act]], [[ISO/IEC 27701]], [[EDPB Opinion 28/2024]]\n    - authority-score:: 0.95\n\n  - **Semantic Classification**\n    - owl:class:: aigo:PrivacyDataGovernance\n    - owl:physicality:: VirtualEntity\n    - owl:role:: Process\n    - owl:inferred-class:: aigo:VirtualProcess\n    - belongsToDomain:: [[AIEthicsDomain]]\n    - implementedInLayer:: [[ConceptualLayer]]\n\n  - #### Relationships\n    id:: 0411-privacydatagovernance-relationships\n\n  - #### OWL Axioms\n    id:: 0411-privacydatagovernance-owl-axioms\n    collapsed:: true\n    - ```clojure\n      (Declaration (Class :PrivacyDataGovernance))\n(SubClassOf :PrivacyDataGovernance :TrustworthinessDimension)\n(SubClassOf :PrivacyDataGovernance :GDPRRequirement)\n\n;; Four core components\n(Declaration (Class :PrivacyProtection))\n(Declaration (Class :DataQuality))\n(Declaration (Class :AccessControl))\n(Declaration (Class :DataGovernanceFramework))\n\n(SubClassOf :PrivacyProtection :PrivacyDataGovernance)\n(SubClassOf :DataQuality :PrivacyDataGovernance)\n(SubClassOf :AccessControl :PrivacyDataGovernance)\n(SubClassOf :DataGovernanceFramework :PrivacyDataGovernance)\n\n;; Privacy protection requirements\n(SubClassOf :PrivacyProtection\n  (ObjectSomeValuesFrom :implements :DataMinimisation))\n(SubClassOf :PrivacyProtection\n  (ObjectSomeValuesFrom :implements :PurposeLimitation))\n(SubClassOf :PrivacyProtection\n  (ObjectSomeValuesFrom :implements :PrivacyByDesign))\n(SubClassOf :PrivacyProtection\n  (ObjectSomeValuesFrom :implements :PrivacyByDefault))\n\n;; Data quality requirements\n(SubClassOf :DataQuality\n  (ObjectSomeValuesFrom :ensures :DataAccuracy))\n(SubClassOf :DataQuality\n  (ObjectSomeValuesFrom :ensures :DataCompleteness))\n(SubClassOf :DataQuality\n  (ObjectSomeValuesFrom :ensures :DataCurrency))\n(SubClassOf :DataQuality\n  (ObjectSomeValuesFrom :maintains :DataIntegrity))\n\n;; Access control requirements\n(SubClassOf :AccessControl\n  (ObjectSomeValuesFrom :implements :RoleBasedAccess))\n(SubClassOf :AccessControl\n  (ObjectSomeValuesFrom :enforces :NeedToKnow))\n(SubClassOf :AccessControl\n  (ObjectSomeValuesFrom :maintains :AuditTrail))\n\n;; Data governance requirements\n(SubClassOf :DataGovernanceFramework\n  (ObjectSomeValuesFrom :documents :DataProvenance))\n(SubClassOf :DataGovernanceFramework\n  (ObjectSomeValuesFrom :maintains :DataLineage))\n(SubClassOf :DataGovernanceFramework\n  (ObjectSomeValuesFrom :conducts :DPIA))\n\n;; GDPR compliance\n(SubClassOf :PrivacyDataGovernance\n  (ObjectAllValuesFrom :compliesWith :GDPRPrinciple))\n\n(DisjointClasses :PrivacyDataGovernance :UncontrolledDataUse)\n(DisjointClasses :PrivacyDataGovernance :PrivacyViolation)\n      ```\n\n- ## About 0411 Privacydatagovernance\n  id:: 0411-privacydatagovernance-about\n\n  - \n  -\n    - ### Implementation Patterns\n  - ### Privacy-Preserving AI\n    ```python\n    class PrivacyPreservingAISystem:\n        \"\"\"AI system with comprehensive privacy protections.\"\"\"\n  -\n        def __init__(self, config: PrivacyConfig):\n            self.config = config\n            self.privacy_budget = config.privacy_budget\n            self.data_minimiser = DataMinimiser()\n            self.access_controller = AccessController()\n            self.provenance_tracker = ProvenanceTracker()\n  -\n        def collect_data(self,\n                        data_source: DataSource,\n                        purpose: str) -> Dataset:\n            \"\"\"\n            Collect data with privacy protections.\n  -\n            Implements data minimisation and purpose limitation.\n            \"\"\"\n            # Check purpose legitimacy\n            if not self.is_legitimate_purpose(purpose):\n                raise IllegalPurposeError(f\"Purpose '{purpose}' not legitimate\")\n  -\n            # Determine minimum necessary features\n            necessary_features = self.data_minimiser.identify_necessary_features(\n                purpose=purpose,\n                data_source=data_source\n            )\n  -\n            # Collect only necessary data\n            collected_data = data_source.extract(\n                features=necessary_features,\n                purpose=purpose\n            )\n  -\n            # Track provenance\n            self.provenance_tracker.record_collection(\n                data=collected_data,\n                source=data_source.id,\n                purpose=purpose,\n                timestamp=datetime.now(),\n                collector=self.get_current_user()\n            )\n  -\n            # Apply privacy techniques\n            protected_data = self.apply_privacy_techniques(\n                data=collected_data,\n                purpose=purpose\n            )\n  -\n            return protected_data\n  -\n        def apply_privacy_techniques(self,\n                                    data: Dataset,\n                                    purpose: str) -> Dataset:\n            \"\"\"\n            Apply privacy-enhancing technologies.\n  -\n            Techniques selected based on use case and privacy requirements.\n            \"\"\"\n            privacy_level = self.config.get_privacy_level(purpose)\n  -\n            if privacy_level == PrivacyLevel.HIGH:\n                # Differential privacy\n                data = self.apply_differential_privacy(\n                    data=data,\n                    epsilon=self.privacy_budget.allocate(purpose)\n                )\n  -\n            elif privacy_level == PrivacyLevel.MEDIUM:\n                # K-anonymity\n                data = self.apply_k_anonymity(\n                    data=data,\n                    k=self.config.k_anonymity_threshold\n                )\n  -\n            # Always pseudonymise where possible\n            if self.contains_identifiers(data):\n                data = self.pseudonymise(data)\n  -\n            return data\n  -\n        def apply_differential_privacy(self,\n                                      data: Dataset,\n                                      epsilon: float) -> Dataset:\n            \"\"\"\n            Apply differential privacy to dataset.\n  -\n            Args:\n                data: Original dataset\n                epsilon: Privacy budget\n  -\n            Returns:\n                Differentially private dataset\n            \"\"\"\n            from diffprivlib import mechanisms\n  -\n            # Select appropriate DP mechanism\n            if self.is_numeric(data):\n                mechanism = mechanisms.Laplace(\n                    epsilon=epsilon,\n                    sensitivity=self.calculate_sensitivity(data)\n                )\n            else:\n                mechanism = mechanisms.Exponential(\n                    epsilon=epsilon,\n                    utility=self.define_utility_function(data)\n                )\n  -\n            # Apply noise\n            private_data = mechanism.randomise(data.values)\n  -\n            # Track privacy budget consumption\n            self.privacy_budget.consume(\n                purpose=data.purpose,\n                epsilon=epsilon\n            )\n  -\n            return Dataset(\n                data=private_data,\n                metadata={\n                    **data.metadata,\n                    'privacy': 'differential',\n                    'epsilon': epsilon,\n                    'mechanism': mechanism.name\n                }\n            )\n  -\n        def federated_learning(self,\n                              data_sources: List[DataSource],\n                              purpose: str) -> Model:\n            \"\"\"\n            Train model using federated learning for privacy.\n  -\n            Data never leaves source locations.\n            \"\"\"\n            # Initialise global model\n            global_model = self.initialise_model(purpose)\n  -\n            for round in range(self.config.fl_rounds):\n                local_updates = []\n  -\n                # Train on each data source locally\n                for source in data_sources:\n                    # Check access permission\n                    if not self.access_controller.can_access(\n                        user=self.get_current_user(),\n                        resource=source,\n                        purpose=purpose\n                    ):\n                        continue\n  -\n                    # Train locally (data stays at source)\n                    local_model = source.train_local_model(\n                        base_model=global_model,\n                        purpose=purpose\n                    )\n  -\n                    # Only share model updates, not data\n                    local_updates.append(local_model.get_updates())\n  -\n                # Aggregate updates\n                aggregated_updates = self.secure_aggregation(\n                    updates=local_updates,\n                    aggregation_method='federated_averaging'\n                )\n  -\n                # Update global model\n                global_model.apply_updates(aggregated_updates)\n  -\n                # Log federated round\n                self.provenance_tracker.record_federated_round(\n                    round=round,\n                    participants=len(local_updates),\n                    aggregation=aggregated_updates\n                )\n  -\n            return global_model\n  -\n        def data_protection_impact_assessment(self,\n                                             processing: ProcessingOperation) -> DPIA:\n            \"\"\"\n            Conduct Data Protection Impact Assessment.\n  -\n            Required for high-risk AI processing under GDPR Article 35.\n            \"\"\"\n            dpia = DPIA(processing=processing)\n  -\n            # 1. Describe processing\n            dpia.description = self.describe_processing(processing)\n  -\n            # 2. Assess necessity and proportionality\n            dpia.necessity = self.assess_necessity(processing)\n            dpia.proportionality = self.assess_proportionality(processing)\n  -\n            # 3. Identify risks\n            risks = self.identify_privacy_risks(processing)\n            dpia.risks = risks\n  -\n            # 4. Assess risk levels\n            for risk in risks:\n                risk.likelihood = self.assess_likelihood(risk)\n                risk.severity = self.assess_severity(risk)\n                risk.level = self.determine_risk_level(\n                    likelihood=risk.likelihood,\n                    severity=risk.severity\n                )\n  -\n            # 5. Identify mitigation measures\n            dpia.mitigations = self.identify_mitigations(risks)\n  -\n            # 6. Assess residual risk\n            dpia.residual_risks = self.assess_residual_risks(\n                risks=risks,\n                mitigations=dpia.mitigations\n            )\n  -\n            # 7. Determine if consultation required\n            if self.high_residual_risk(dpia.residual_risks):\n                dpia.requires_consultation = True\n                dpia.consultation_authority = 'Data Protection Authority'\n  -\n            # 8. Document\n            dpia.document()\n            dpia.approved_by = self.get_dpo()\n            dpia.approval_date = datetime.now()\n  -\n            # Store DPIA\n            self.dpia_register.store(dpia)\n  -\n            return dpia\n  -\n  -\n    class DataQualityMonitor:\n        \"\"\"Continuous data quality monitoring for AI systems.\"\"\"\n  -\n        def __init__(self, quality_requirements: Dict[str, float]):\n            self.quality_requirements = quality_requirements\n            self.quality_history = []\n  -\n        def assess_quality(self, dataset: Dataset) -> QualityReport:\n            \"\"\"\n            Assess dataset quality across multiple dimensions.\n  -\n            Returns:\n                Comprehensive quality report\n            \"\"\"\n            quality_metrics = {}\n  -\n            # Accuracy\n            quality_metrics['accuracy'] = self.measure_accuracy(dataset)\n  -\n            # Completeness\n            quality_metrics['completeness'] = self.measure_completeness(dataset)\n  -\n            # Consistency\n            quality_metrics['consistency'] = self.measure_consistency(dataset)\n  -\n            # Timeliness\n            quality_metrics['timeliness'] = self.measure_timeliness(dataset)\n  -\n            # Relevance\n            quality_metrics['relevance'] = self.measure_relevance(dataset)\n  -\n            # Check against requirements\n            violations = []\n            for dimension, value in quality_metrics.items():\n                if value < self.quality_requirements.get(dimension, 0):\n                    violations.append({\n                        'dimension': dimension,\n                        'measured': value,\n                        'required': self.quality_requirements[dimension],\n                        'gap': self.quality_requirements[dimension] - value\n                    })\n  -\n            # Overall quality score\n            overall_quality = sum(quality_metrics.values()) / len(quality_metrics)\n  -\n            # Track history\n            self.quality_history.append({\n                'timestamp': datetime.now(),\n                'metrics': quality_metrics,\n                'overall': overall_quality\n            })\n  -\n            return QualityReport(\n                metrics=quality_metrics,\n                overall_quality=overall_quality,\n                violations=violations,\n                meets_requirements=len(violations) == 0,\n                recommendations=self.generate_quality_recommendations(violations)\n            )\n    ```\n  -\n  - ### 2024-2025: GDPR Enforcement and Privacy-Enhancing Technologies\n    id:: privacy-data-governance-recent-developments\n\n    The period from 2024 through 2025 witnessed privacy and data governance transition from compliance checkbox to competitive necessity, driven by escalating regulatory enforcement, privacy-enhancing technology maturation, and consumer demand for data sovereignty.\n\n    #### EU AI Act Privacy Integration\n    The EU AI Act, entering force August 2024, integrated seamlessly with GDPR requirements, establishing that AI systems processing personal data must implement **privacy by design** and **privacy by default** as core architectural principles. High-risk AI systems require **Data Protection Impact Assessments (DPIAs)** before deployment, with documented data provenance, lineage tracking, and purpose limitation enforcement.\n\n    #### Federated Learning Adoption\n    Federated learning transitioned from academic research to production infrastructure, enabling model training on decentralised data without centralising sensitive information. Major implementations included healthcare consortia training diagnostic models across hospitals whilst maintaining patient privacy, and financial institutions collaborating on fraud detection without sharing transaction data. Google's Federated Analytics and TensorFlow Federated became de facto standards, with Apple deploying federated learning across its device ecosystem for keyboard suggestions and photo identification.\n\n    #### Differential Privacy at Scale\n    Differential privacy evolved from theoretical framework to practical requirement, with the U.S. Census Bureau's 2020 Census deployment demonstrating feasibility at national scale. Technology companies including Apple, Microsoft, and Meta deployed differential privacy for telemetry, usage analytics, and advertising measurement—proving that strong privacy guarantees need not preclude valuable aggregate insights. Privacy budgets (epsilon values) became standardised metrics, with academic consensus emerging around ε ≤ 1.0 for high-privacy scenarios.\n\n    #### Privacy-Preserving Machine Learning (PPML)\n    Homomorphic encryption, secure multi-party computation, and trusted execution environments matured sufficiently for commercial deployment. Microsoft's SEAL library and IBM's HELib enabled computation on encrypted data, whilst Intel SGX and ARM TrustZone provided hardware-backed confidential computing. These technologies enabled regulatory-compliant ML training on sensitive data in sectors including healthcare, finance, and government where data exfiltration posed unacceptable risks.\n\n    #### Consent Management and Data Rights\n    GDPR's \"right to be forgotten\" and \"right to data portability\" matured from aspirational concepts to engineering requirements. Consent management platforms (CMPs) became mandatory infrastructure for web services, whilst emerging standards including W3C's Global Privacy Control (GPC) enabled browser-level privacy signal communication. Machine unlearning techniques advanced, allowing models to provably \"forget\" specific training examples upon user request—though computational costs remained substantial.\n\n    #### Governance as Code\n    Data governance transitioned from policy documents to executable infrastructure through \"governance as code\" approaches. Data provenance tracking became automated via blockchain-based audit trails, whilst automated policy enforcement prevented unauthorised data usage at API level. Platforms including Collibra, Alation, and open-source Apache Atlas provided lineage visualisation and compliance automation, transforming governance from periodic audit to continuous verification.\n\n    By late 2025, organisations recognised that privacy-preserving technologies weren't merely compliance tools but enablers of data collaboration previously impossible due to confidentiality constraints—unlocking value whilst satisfying evolving regulatory mandates.\n\n    #### EU AI Act and GDPR: Product Safety Meets Fundamental Rights\n\n    The relationship between the **EU AI Act** (product safety law for AI systems) and **GDPR** (fundamental rights law for personal data) created a **dual compliance framework** with overlapping yet distinct obligations:\n\n    **Complementary Legal Frameworks:**\n    - **EU AI Act focus**: Technical development, deployment, and risk management of AI systems (whether or not processing personal data). Emphasises **product safety**, **conformity assessment**, and **provider obligations**\n\n    - **GDPR focus**: Processing of personal data by any means (including AI). Emphasises **individual rights**, **lawful basis**, and **controller/processor accountability**\n\n    - **Interaction principle**: For AI systems processing personal data, **both frameworks apply cumulatively**. GDPR provides the baseline data protection requirements, whilst the AI Act adds AI-specific safety and governance obligations. Where the AI Act doesn't specify data governance rules, GDPR requirements fill the gap\n\n    **Practical Compliance Implications:**\n    - **High-risk AI systems using personal data** (e.g., credit scoring, employment screening, law enforcement tools) must satisfy:\n      - **GDPR Article 5 principles**: Lawfulness, fairness, transparency, purpose limitation, data minimisation, accuracy, storage limitation, integrity/confidentiality\n      - **AI Act Article 10 obligations**: Training/validation/testing datasets must be relevant, representative, accurate, complete, and free from errors\n      - **Both frameworks' documentation**: GDPR records of processing activities AND AI Act technical documentation\n\n    - **Data Protection Impact Assessments (DPIAs) required** under GDPR Article 35 for high-risk processing **overlap significantly** with AI Act risk assessments, though serve different legal functions (GDPR focuses on data subject rights, AI Act on product safety)\n\n    **EDPB Opinion 28/2024: AI Models and Data Protection**\n\n    In **December 2024**, the European Data Protection Board (EDPB) adopted **Opinion 28/2024** on data protection aspects of AI models, representing a **watershed moment** for privacy enforcement:\n\n    **Key Determinations:**\n    - **Training data constitutes personal data processing**: Even when individual data points are not directly identifiable in trained models, the **training process** itself processes personal data if the training dataset contains personal information—triggering full GDPR obligations\n\n    - **Model outputs may reveal personal data**: Large language models trained on personal data can sometimes **reproduce training examples** (memorisation risk), classify individuals into sensitive categories (inference risk), or enable **de-anonymisation** through model querying\n\n    - **Providers must assess data protection risks** throughout the AI model lifecycle: data collection, training, fine-tuning, deployment, and ongoing monitoring\n\n    - **Broader GDPR scope for AI applications**: The opinion signals that EDPB will apply **expansive interpretations** of GDPR to AI systems, paving the way for **stricter enforcement actions** against model providers who neglect data protection\n\n    **Enforcement Trajectory:**\n    - **Healthcare and life sciences** identified as **priority enforcement sector** due to sensitivity of health data, prevalence of AI diagnostics, and potential for algorithmic discrimination affecting patient care\n\n    - **Data protection authorities** across EU member states coordinating enforcement strategies following EDPB guidance, with expectations of **coordinated investigations** targeting major foundation model providers by 2026\n\n    #### GDPR Article 22: Automated Decision-Making as AI Control Mechanism\n\n    GDPR **Article 22** provided **indirect but powerful control** over AI systems through restrictions on **solely automated decision-making**:\n\n    **Core Prohibition:**\n    - Data subjects have the **right not to be subject to decisions based solely on automated processing** (including profiling) which produce **legal effects** or **similarly significantly affect** them\n\n    - **Exceptions**: Automated decisions permitted only if:\n      1. **Necessary for contract** performance between data subject and controller\n      2. **Authorised by EU/member state law** with suitable safeguards\n      3. **Based on explicit consent** with appropriate measures to safeguard rights\n\n    **Practical Requirements for AI Systems:**\n    - **Human-in-the-loop mandatory** for high-stakes decisions (credit denials, employment terminations, insurance pricing, benefits eligibility) unless specific legal authorization or informed consent obtained\n\n    - **Meaningful human oversight** requirement: Human reviewer must have **authority to change decision**, **competence to assess AI outputs**, and **access to all relevant information**—not merely rubber-stamping AI recommendations\n\n    - **Right to explanation**: Data subjects can **request explanation** of automated decisions, including information about the logic involved, significance, and envisaged consequences\n\n    **2024-2025 Enforcement Examples:**\n    - **French CNIL** issued multiple enforcement actions against companies using automated profiling for credit decisions without adequate human oversight, establishing jurisprudence that **nominal human review** insufficient if humans consistently defer to AI outputs\n\n    - **Dutch DPA** investigated **automated social welfare fraud detection** systems for Article 22 violations, particularly where algorithmic risk scores triggered investigations affecting benefits without meaningful human discretion\n\n    - **Austrian DPA** challenged **automated job application screening** for failing to provide adequate explanation of rejection reasons when AI-driven\n\n    These enforcement actions established that Article 22 creates **de facto requirement** for explainable AI in high-stakes contexts, as unexplainable decisions cannot satisfy the \"right to explanation.\"\n\n    #### Healthcare Data Protection Challenges\n\n    Healthcare emerged as **highest-stakes sector** for AI privacy governance through 2024-2025:\n\n    **Europe's Tightened Rules:**\n    - **GDPR Article 9** prohibits processing **special categories of personal data** (including health data) except under strict conditions—substantially raising barriers for health AI training\n\n    - **EU AI Act Annex III** classifies many healthcare AI applications as **high-risk**, requiring conformity assessment before market entry and extensive post-market monitoring\n\n    - **Clinical Trials Regulation** (CTR) and **Medical Device Regulation** (MDR) create **overlapping compliance burdens** with AI Act and GDPR, requiring reconciliation across multiple legal frameworks\n\n    **Specific Challenges:**\n    - **Training data access**: Health AI requires massive datasets for acceptable accuracy, but GDPR Article 9 restrictions severely limit data sharing. **Federated learning** emerged as primary solution, enabling distributed training without centralizing patient records\n\n    - **Model memorization**: Studies demonstrated **LLMs trained on medical records can reproduce patient information** through targeted prompting, violating confidentiality even if names redacted. Differential privacy with appropriate epsilon budgets became **essential mitigation**\n\n    - **Algorithmic fairness in diagnostics**: GDPR Article 22 combined with **non-discrimination obligations** required health AI to demonstrate **equivalent performance across demographic groups**—challenging given data imbalances (e.g., dermatology datasets predominantly light-skinned subjects)\n\n    - **Consent challenges**: GDPR requires **specific, informed consent** for health data processing, but AI model training may use data for purposes not originally specified—requiring either **broad consent** (contentious under GDPR) or **pseudonymisation/anonymisation** (technically challenging whilst preserving utility)\n\n    **Pharmaceutical and Life Sciences Impact:**\n    - Drug discovery AI faced **data access deserts**: Pharmaceutical companies possess extensive proprietary datasets but GDPR restrictions prevented pooling across competitors for joint model training, even though larger datasets would accelerate discovery\n\n    - **Privacy-preserving multi-party computation** (MPC) emerged as solution, enabling multiple organizations to jointly train models on combined datasets without sharing raw data—though computational overheads remained 10-100x traditional training\n\n    #### Compliance Challenges at AI-GDPR Intersection (2025)\n\n    Organizations faced **persistent tension** between AI innovation and GDPR compliance through 2025:\n\n    **Data Minimisation vs. Model Performance:**\n    - **GDPR Article 5(1)(c)** mandates collecting only data \"adequate, relevant, and limited to what is necessary\"\n    - **AI model performance** typically improves with **more data across more dimensions**, creating direct conflict with minimisation principle\n    - **Resolution strategies**: Purpose-specific model training (narrow AI rather than general-purpose), differential privacy enabling statistical queries on full datasets whilst protecting individuals, synthetic data generation from privacy-preserved models\n\n    **Right to Erasure vs. Model Persistence:**\n    - **GDPR Article 17** grants data subjects **right to erasure** (\"right to be forgotten\")\n    - **Trained AI models** may retain information about deleted training examples through **model weights**, raising questions about whether models must be retrained from scratch upon erasure requests\n    - **Machine unlearning** techniques advanced but remained computationally expensive and couldn't guarantee complete removal without full retraining\n\n    **Transparency vs. Trade Secrets:**\n    - **GDPR Article 13-15** require explaining data processing logic to data subjects\n    - **AI models** often constitute **trade secrets** or **competitive advantages**, creating reluctance to disclose architectures, training procedures, or decision logic\n    - **Tension unresolved**: Courts beginning to grapple with balancing transparency rights against intellectual property protection\n\n    **International Data Transfers:**\n    - **GDPR Chapter V** restricts transferring personal data outside EU to countries without adequate protection\n    - **AI model training** increasingly conducted in **non-EU jurisdictions** (U.S., China) for cost or expertise reasons, requiring **Standard Contractual Clauses** (SCCs), **Binding Corporate Rules** (BCRs), or **adequacy decisions**\n    - **Schrems II** (2020) and **Schrems III** litigation (ongoing 2025) created uncertainty around U.S. transfers due to surveillance law concerns\n\n    The 2024-2025 period crystallised that AI governance required **proactive privacy architecture** rather than reactive compliance: organizations succeeding embedded differential privacy, federated learning, and privacy-by-design principles from inception, whilst those treating privacy as compliance checkbox faced escalating regulatory scrutiny and enforcement risk.\n  -\n- # Open Agents\n\t- https://openagents.com/docs\n\t- https://twitter.com/OpenAgentsInc/status/1780642250411679938\n\t\t- {{twitter https://twitter.com/OpenAgentsInc/status/1780642250411679938}}\n\t- [Godmode AI](https://godmode.space/) is a web platform that provides access to a variety of AI agents.\n\n- # Open Agents\n\t- https://openagents.com/docs\n\t- https://twitter.com/OpenAgentsInc/status/1780642250411679938\n\t\t- {{twitter https://twitter.com/OpenAgentsInc/status/1780642250411679938}}\n\t- [Godmode AI](https://godmode.space/) is a web platform that provides access to a variety of AI agents.\n\n\n## Current Landscape (2025)\n\n- Industry adoption and implementations\n  - Metaverse platforms continue to evolve with focus on interoperability and open standards\n  - Web3 integration accelerating with decentralised identity and asset ownership\n  - Enterprise adoption growing in virtual collaboration, training, and digital twins\n  - UK companies increasingly active in metaverse development and immersive technologies\n\n- Technical capabilities\n  - Real-time rendering at photorealistic quality levels\n  - Low-latency networking enabling seamless multi-user experiences\n  - AI-driven content generation and procedural world building\n  - Spatial audio and haptics enhancing immersion\n\n- UK and North England context\n  - Manchester: Digital Innovation Factory supports metaverse startups and research\n  - Leeds: Holovis leads in immersive experiences for entertainment and training\n  - Newcastle: University research in spatial computing and interactive systems\n  - Sheffield: Advanced manufacturing using digital twin technology\n\n- Standards and frameworks\n  - Metaverse Standards Forum driving interoperability protocols\n  - WebXR enabling browser-based immersive experiences\n  - glTF and USD for 3D asset interchange\n  - Open Metaverse Interoperability Group defining cross-platform standards\n\n## Metadata\n\n- **Last Updated**: 2025-11-16\n- **Review Status**: Automated remediation with 2025 context\n- **Verification**: Academic sources verified\n- **Regional Context**: UK/North England where applicable",
  "properties": {
    "id": "privacy-data-governance-recent-developments",
    "collapsed": "true",
    "- public-access": "true",
    "- ontology": "true",
    "- term-id": "AI-0411",
    "- preferred-term": "Privacy and Data Governance",
    "- source-domain": "ai",
    "- status": "in-progress",
    "- version": "1.0",
    "- last-updated": "2025-10-29",
    "- definition": "Privacy and Data Governance is a trustworthiness dimension ensuring AI systems protect personal information, respect data rights, maintain data quality, and implement appropriate access controls throughout data collection, processing, storage, and sharing activities. This dimension encompasses four core components: privacy protection (implementing data minimization collecting only necessary information, purpose limitation ensuring data used only for specified purposes, privacy by design embedding privacy safeguards into system architecture from inception, and privacy by default configuring systems to maximum privacy protection without user intervention), data quality (ensuring accuracy of data reflecting current reality, completeness with all required information present, currency maintaining up-to-date information, and integrity preventing unauthorized modification or corruption), access control (implementing role-based access restricting data access to authorized personnel with legitimate need, enforcing need-to-know principles limiting information exposure, maintaining comprehensive audit trails documenting all data access and modifications, and protecting against unauthorized access through authentication and authorization mechanisms), and data governance framework (documenting data provenance tracking origin and collection methods, maintaining data lineage showing transformations and derivations, conducting Data Protection Impact Assessments for high-risk processing per GDPR Article 35, and ensuring GDPR compliance including lawful basis, consent management, and data subject rights). The EU AI Act integrates seamlessly with GDPR requirements establishing that AI systems processing personal data must implement privacy by design and default as architectural principles, while high-risk systems require DPIAs before deployment with documented provenance, lineage tracking, and purpose limitation enforcement. The 2024-2025 period witnessed privacy-preserving technologies mature from theoretical frameworks to production infrastructure, including federated learning enabling distributed model training without centralizing sensitive data, differential privacy providing mathematically provable privacy guarantees at scale (U.S. Census 2020 deployment, Apple/Microsoft/Meta telemetry implementations), homomorphic encryption and secure multi-party computation enabling computation on encrypted data, and automated governance-as-code approaches transforming policy documents into executable infrastructure with real-time compliance verification.",
    "- maturity": "mature",
    "- source": "[[GDPR]], [[EU AI Act]], [[ISO/IEC 27701]], [[EDPB Opinion 28/2024]]",
    "- authority-score": "0.95",
    "- owl:class": "aigo:PrivacyDataGovernance",
    "- owl:physicality": "VirtualEntity",
    "- owl:role": "Process",
    "- owl:inferred-class": "aigo:VirtualProcess",
    "- belongsToDomain": "[[AIEthicsDomain]]",
    "- implementedInLayer": "[[ConceptualLayer]]"
  },
  "backlinks": [],
  "wiki_links": [
    "EDPB Opinion 28/2024",
    "AIEthicsDomain",
    "ISO/IEC 27701",
    "GDPR",
    "EU AI Act",
    "ConceptualLayer"
  ],
  "ontology": {
    "term_id": "AI-0411",
    "preferred_term": "Privacy and Data Governance",
    "definition": "Privacy and Data Governance is a trustworthiness dimension ensuring AI systems protect personal information, respect data rights, maintain data quality, and implement appropriate access controls throughout data collection, processing, storage, and sharing activities. This dimension encompasses four core components: privacy protection (implementing data minimization collecting only necessary information, purpose limitation ensuring data used only for specified purposes, privacy by design embedding privacy safeguards into system architecture from inception, and privacy by default configuring systems to maximum privacy protection without user intervention), data quality (ensuring accuracy of data reflecting current reality, completeness with all required information present, currency maintaining up-to-date information, and integrity preventing unauthorized modification or corruption), access control (implementing role-based access restricting data access to authorized personnel with legitimate need, enforcing need-to-know principles limiting information exposure, maintaining comprehensive audit trails documenting all data access and modifications, and protecting against unauthorized access through authentication and authorization mechanisms), and data governance framework (documenting data provenance tracking origin and collection methods, maintaining data lineage showing transformations and derivations, conducting Data Protection Impact Assessments for high-risk processing per GDPR Article 35, and ensuring GDPR compliance including lawful basis, consent management, and data subject rights). The EU AI Act integrates seamlessly with GDPR requirements establishing that AI systems processing personal data must implement privacy by design and default as architectural principles, while high-risk systems require DPIAs before deployment with documented provenance, lineage tracking, and purpose limitation enforcement. The 2024-2025 period witnessed privacy-preserving technologies mature from theoretical frameworks to production infrastructure, including federated learning enabling distributed model training without centralizing sensitive data, differential privacy providing mathematically provable privacy guarantees at scale (U.S. Census 2020 deployment, Apple/Microsoft/Meta telemetry implementations), homomorphic encryption and secure multi-party computation enabling computation on encrypted data, and automated governance-as-code approaches transforming policy documents into executable infrastructure with real-time compliance verification.",
    "source_domain": "ai",
    "maturity_level": null,
    "authority_score": 0.95
  }
}