{
  "title": "Fairness Metrics",
  "content": "- ### OntologyBlock\n  id:: 0377-fairness-metrics-ontology\n  collapsed:: true\n\n  - **Identification**\n    - public-access:: true\n    - ontology:: true\n    - term-id:: AI-0377\n    - preferred-term:: Fairness Metrics\n    - source-domain:: ai\n    - status:: approved\n    - version:: 1.0\n    - last-updated:: 2025-11-16\n\n  - **Definition**\n    - definition:: Fairness Metrics are quantitative measures and mathematical frameworks used to evaluate and ensure equitable treatment across different demographic groups in AI systems. These metrics provide objective, measurable criteria to assess whether an algorithmic system produces disparate impacts, maintains statistical parity, or achieves equalized odds across protected attributes such as race, gender, age, or disability status. Key fairness metrics include demographic parity (equal positive prediction rates across groups), equalized odds (equal true positive and false positive rates), equal opportunity (equal true positive rates), and predictive parity (equal precision across groups). The selection and application of fairness metrics depends on the specific context, stakeholder values, and regulatory requirements, as different metrics can conflict and no single metric satisfies all fairness criteria simultaneously. Implementation requires confusion matrix analysis, statistical testing, and careful consideration of base rate differences between groups, as formalized in IEEE P7003-2021 and NIST SP 1270 guidelines for algorithmic fairness assessment.\n    - maturity:: mature\n    - source:: [[IEEE P7003-2021]], [[ISO/IEC TR 24027]], [[NIST SP 1270]]\n    - authority-score:: 0.95\n\n  - **Semantic Classification**\n    - owl:class:: aigo:FairnessMetrics\n    - owl:physicality:: VirtualEntity\n    - owl:role:: Process\n    - owl:inferred-class:: aigo:VirtualProcess\n    - belongsToDomain:: [[AIEthicsDomain]]\n    - implementedInLayer:: [[ConceptualLayer]]\n\n  - #### Relationships\n    id:: 0377-fairness-metrics-relationships\n    - is-part-of:: [[Algorithmic Fairness]], [[AI Ethics]], [[Bias Detection]]\n    - requires:: [[Confusion Matrix]], [[Statistical Testing]], [[Protected Attributes]]\n    - enables:: [[Bias Mitigation]], [[Fairness Auditing]], [[Regulatory Compliance]]\n    - related-to:: [[AI Safety Research]], [[Value Alignment]], [[AI Trustworthiness]], [[Algorithmic Accountability]]\n    - measured-by:: [[Fairness Auditing Tools]]\n    - depends-on:: [[IEEE P7003-2021]], [[ISO/IEC TR 24027]], [[NIST SP 1270]]\n\n  - #### OWL Axioms\n    id:: 0377-fairness-metrics-owl-axioms\n    collapsed:: true\n    - ```clojure\n      (Declaration (Class :FairnessMetric))\n(SubClassOf :FairnessMetric :EvaluationMetric)\n(SubClassOf :FairnessMetric :EthicalAIComponent)\n\n(AnnotationAssertion rdfs:label :FairnessMetric\n  \"Fairness Metric\"@en)\n(AnnotationAssertion rdfs:comment :FairnessMetric\n  \"Quantitative measures for assessing algorithmic fairness across protected groups, including demographic parity, equalized odds, and equality of opportunity.\"@en)\n(AnnotationAssertion :dcterms:source :FairnessMetric\n  \"IEEE P7003-2021, ISO/IEC TR 24027:2021, NIST SP 1270\")\n\n;; Object Properties\n(Declaration (ObjectProperty :measures))\n(ObjectPropertyDomain :measures :FairnessMetric)\n(ObjectPropertyRange :measures :AlgorithmicFairness)\n\n(Declaration (ObjectProperty :detectsBias))\n(ObjectPropertyDomain :detectsBias :FairnessMetric)\n(ObjectPropertyRange :detectsBias :ProtectedAttribute)\n\n(Declaration (ObjectProperty :appliesTo))\n(ObjectPropertyDomain :appliesTo :FairnessMetric)\n(ObjectPropertyRange :appliesTo :AIModel)\n\n(Declaration (ObjectProperty :requiresConfusionMatrix))\n(SubObjectPropertyOf :requiresConfusionMatrix :dependsOn)\n\n;; Data Properties\n(Declaration (DataProperty :hasValueRange))\n(DataPropertyAssertion :hasValueRange :FairnessMetric\n  \"[0,1] for most metrics\"^^xsd:string)\n\n(Declaration (DataProperty :hasThreshold))\n(DataPropertyDomain :hasThreshold :FairnessMetric)\n(DataPropertyRange :hasThreshold xsd:decimal)\n\n(Declaration (DataProperty :requiresGroundTruth))\n(DataPropertyAssertion :requiresGroundTruth :FairnessMetric\n  \"true\"^^xsd:boolean)\n\n;; Subclass Definitions\n(Declaration (Class :DemographicParity))\n(SubClassOf :DemographicParity :FairnessMetric)\n(AnnotationAssertion rdfs:comment :DemographicParity\n  \"P(Ŷ=1|A=0) = P(Ŷ=1|A=1) where A is protected attribute and Ŷ is prediction\"@en)\n\n(Declaration (Class :EqualizedOdds))\n(SubClassOf :EqualizedOdds :FairnessMetric)\n(AnnotationAssertion rdfs:comment :EqualizedOdds\n  \"P(Ŷ=1|A=0,Y=y) = P(Ŷ=1|A=1,Y=y) for y ∈ {0,1}\"@en)\n\n(Declaration (Class :EqualOpportunity))\n(SubClassOf :EqualOpportunity :FairnessMetric)\n(AnnotationAssertion rdfs:comment :EqualOpportunity\n  \"P(Ŷ=1|A=0,Y=1) = P(Ŷ=1|A=1,Y=1) - equal true positive rates\"@en)\n\n(Declaration (Class :PredictiveParity))\n(SubClassOf :PredictiveParity :FairnessMetric)\n(AnnotationAssertion rdfs:comment :PredictiveParity\n  \"P(Y=1|Ŷ=1,A=0) = P(Y=1|Ŷ=1,A=1) - equal precision across groups\"@en)\n\n;; Disjoint Classes\n(DisjointClasses :DemographicParity :EqualizedOdds :EqualOpportunity)\n\n;; Domain Constraints\n(SubClassOf :FairnessMetric\n  (ObjectSomeValuesFrom :measures :AlgorithmicFairness))\n(SubClassOf :FairnessMetric\n  (ObjectSomeValuesFrom :detectsBias :ProtectedAttribute))\n(SubClassOf :FairnessMetric\n  (DataSomeValuesFrom :hasThreshold xsd:decimal))\n      ```\n\n- ## About Fairness Metrics\n  id:: 0377-fairness-metrics-about\n\n  Fairness Metrics provide quantitative frameworks for evaluating algorithmic equity across demographic groups. These mathematical measures are essential for detecting and mitigating bias in AI systems, ensuring compliance with regulatory frameworks such as the [[EU AI Act]], [[IEEE P7003-2021]], and [[NIST AI Risk Management Framework]].\n\n  ### Core Fairness Metrics\n\n  - **[[Demographic Parity]]**: Equal positive prediction rates across groups: P(Ŷ=1|A=0) = P(Ŷ=1|A=1)\n  - **[[Equalized Odds]]**: Equal true positive and false positive rates: P(Ŷ=1|A=0,Y=y) = P(Ŷ=1|A=1,Y=y)\n  - **[[Equal Opportunity]]**: Equal true positive rates across groups\n  - **[[Predictive Parity]]**: Equal precision across demographic groups\n  - **[[Calibration]]**: Predicted probabilities match actual outcomes across groups\n\n  ### Application Domains\n\n  - **Criminal Justice**: Risk assessment tools, recidivism prediction\n  - **Financial Services**: Credit scoring, loan approval systems\n  - **Healthcare**: Diagnosis algorithms, treatment recommendations\n  - **Employment**: Hiring algorithms, performance evaluation\n  - **Education**: Admissions systems, grading automation\n\n  ### Implementation Challenges\n\n  Fairness metrics often conflict with one another - achieving one form of fairness may preclude others. The **impossibility theorem** (Kleinberg et al., 2017) demonstrates that demographic parity, equalized odds, and predictive parity cannot be simultaneously satisfied except in trivial cases. This requires careful stakeholder engagement to determine which fairness criteria align with societal values and regulatory requirements.\n\n  ### UK Context\n\n  The [[UK AI Regulation]] emphasises fairness assessment through the [[ICO AI Auditing Framework]] and [[BSI ADS standards]]. UK organisations implementing fairness metrics include:\n\n  - **[[NHS AI Lab]]**: Fairness testing for medical diagnosis algorithms\n  - **[[Financial Conduct Authority]]**: Credit decisioning fairness requirements\n  - **[[University of Oxford]]**: Research on fairness metric selection and tradeoffs\n  - **Manchester AI Ethics Hub**: Regional fairness auditing initiatives\n\n  ### 2024-2025 Developments\n\n  - Integration with [[Large Language Models]] fairness testing\n  - [[Intersectional Fairness]] metrics accounting for multiple protected attributes\n  - [[Causal Fairness]] frameworks using causal inference\n  - [[Dynamic Fairness]] metrics for evolving populations\n  - [[Differential Privacy]] integration for fairness with privacy guarantees",
  "properties": {
    "id": "0377-fairness-metrics-about",
    "collapsed": "true",
    "- public-access": "true",
    "- ontology": "true",
    "- term-id": "AI-0377",
    "- preferred-term": "Fairness Metrics",
    "- source-domain": "ai",
    "- status": "approved",
    "- version": "1.0",
    "- last-updated": "2025-11-16",
    "- definition": "Fairness Metrics are quantitative measures and mathematical frameworks used to evaluate and ensure equitable treatment across different demographic groups in AI systems. These metrics provide objective, measurable criteria to assess whether an algorithmic system produces disparate impacts, maintains statistical parity, or achieves equalized odds across protected attributes such as race, gender, age, or disability status. Key fairness metrics include demographic parity (equal positive prediction rates across groups), equalized odds (equal true positive and false positive rates), equal opportunity (equal true positive rates), and predictive parity (equal precision across groups). The selection and application of fairness metrics depends on the specific context, stakeholder values, and regulatory requirements, as different metrics can conflict and no single metric satisfies all fairness criteria simultaneously. Implementation requires confusion matrix analysis, statistical testing, and careful consideration of base rate differences between groups, as formalized in IEEE P7003-2021 and NIST SP 1270 guidelines for algorithmic fairness assessment.",
    "- maturity": "mature",
    "- source": "[[IEEE P7003-2021]], [[ISO/IEC TR 24027]], [[NIST SP 1270]]",
    "- authority-score": "0.95",
    "- owl:class": "aigo:FairnessMetrics",
    "- owl:physicality": "VirtualEntity",
    "- owl:role": "Process",
    "- owl:inferred-class": "aigo:VirtualProcess",
    "- belongsToDomain": "[[AIEthicsDomain]]",
    "- implementedInLayer": "[[ConceptualLayer]]",
    "- is-part-of": "[[Algorithmic Fairness]], [[AI Ethics]], [[Bias Detection]]",
    "- requires": "[[Confusion Matrix]], [[Statistical Testing]], [[Protected Attributes]]",
    "- enables": "[[Bias Mitigation]], [[Fairness Auditing]], [[Regulatory Compliance]]",
    "- related-to": "[[AI Safety Research]], [[Value Alignment]], [[AI Trustworthiness]], [[Algorithmic Accountability]]",
    "- measured-by": "[[Fairness Auditing Tools]]",
    "- depends-on": "[[IEEE P7003-2021]], [[ISO/IEC TR 24027]], [[NIST SP 1270]]"
  },
  "backlinks": [],
  "wiki_links": [
    "Algorithmic Accountability",
    "Large Language Models",
    "Dynamic Fairness",
    "Fairness Auditing Tools",
    "ICO AI Auditing Framework",
    "Regulatory Compliance",
    "NIST SP 1270",
    "Intersectional Fairness",
    "Predictive Parity",
    "Financial Conduct Authority",
    "Equal Opportunity",
    "Statistical Testing",
    "NHS AI Lab",
    "Protected Attributes",
    "BSI ADS standards",
    "AI Ethics",
    "Equalized Odds",
    "University of Oxford",
    "Calibration",
    "ISO/IEC TR 24027",
    "AIEthicsDomain",
    "Bias Detection",
    "NIST AI Risk Management Framework",
    "Bias Mitigation",
    "Confusion Matrix",
    "ConceptualLayer",
    "Algorithmic Fairness",
    "Value Alignment",
    "UK AI Regulation",
    "Fairness Auditing",
    "Differential Privacy",
    "Demographic Parity",
    "AI Trustworthiness",
    "AI Safety Research",
    "EU AI Act",
    "IEEE P7003-2021",
    "Causal Fairness"
  ],
  "ontology": {
    "term_id": "AI-0377",
    "preferred_term": "Fairness Metrics",
    "definition": "Fairness Metrics are quantitative measures and mathematical frameworks used to evaluate and ensure equitable treatment across different demographic groups in AI systems. These metrics provide objective, measurable criteria to assess whether an algorithmic system produces disparate impacts, maintains statistical parity, or achieves equalized odds across protected attributes such as race, gender, age, or disability status. Key fairness metrics include demographic parity (equal positive prediction rates across groups), equalized odds (equal true positive and false positive rates), equal opportunity (equal true positive rates), and predictive parity (equal precision across groups). The selection and application of fairness metrics depends on the specific context, stakeholder values, and regulatory requirements, as different metrics can conflict and no single metric satisfies all fairness criteria simultaneously. Implementation requires confusion matrix analysis, statistical testing, and careful consideration of base rate differences between groups, as formalized in IEEE P7003-2021 and NIST SP 1270 guidelines for algorithmic fairness assessment.",
    "source_domain": "ai",
    "maturity_level": null,
    "authority_score": 0.95
  }
}