{
  "title": "Visual Servoing",
  "content": "- ### OntologyBlock\n  id:: rb-0065-visual-servoing-ontology\n  collapsed:: true\n\t- **Identification**\n\t  - term-id:: RB-0065\n\t  - preferred-term:: Visual Servoing\n\t  - source-domain:: robotics\n\t  - status:: mature\n\t  - public-access:: true\n\t  - owl:class:: rb:VisualServoing\n\t  - owl:physicality:: ConceptualEntity\n\t  - owl:role:: ControlMethod\n\t  - belongsToDomain:: [[RoboticsDomain]]\n\n\t- **Definition**\n\t  - definition:: **Visual Servoing** is a closed-loop robot control technique that uses real-time visual feedback from cameras to guide robot motion towards desired configurations or tasks. The method computes control signals directly from visual features extracted from camera images, enabling precise positioning and tracking without explicit knowledge of object positions in world coordinates. Visual servoing systems fall into two primary categories: **Position-Based Visual Servoing (PBVS)** reconstructs 3D pose from visual features and controls the robot in Cartesian space, while **Image-Based Visual Servoing (IBVS)** directly uses 2D image features to compute control velocities, avoiding pose estimation errors. Camera configurations include **eye-in-hand** (camera mounted on robot end-effector) and **eye-to-hand** (fixed external camera), each offering distinct advantages for workspace coverage and occlusion handling. The control law relies on the **image Jacobian** or **interaction matrix** that maps joint velocities to image feature velocities, typically computed through analytical models or numerical approximation. Feature extraction employs corner detectors (Harris, FAST), blob detectors, or learned features (CNN-based descriptors) tracked across frames using optical flow or template matching algorithms. Applications span precision assembly in manufacturing (sub-millimetre alignment), minimally invasive surgery (tool guidance), automated inspection (quality control), and agricultural robotics (fruit picking). UK research institutions including Imperial College's Robot Vision Group, University of Manchester's Centre for Robotics and AI, and Sheffield Robotics actively advance visual servoing through hybrid force-vision control, deep learning feature representations, and adaptive control under varying illumination. Standards ISO 8373:2021 defines visual servoing within robot control systems, while IEEE research establishes stability analysis and performance benchmarks. The technique addresses uncertainties in robot kinematics, camera calibration errors, and dynamic environments through adaptive gain scheduling, robust feature selection, and predictive control strategies. Advances integrate tactile feedback, multi-camera fusion, and event-based vision sensors for high-speed manipulation tasks requiring sub-10ms control loops.\n\t  - maturity:: mature\n\t  - source:: [[IEEE Transactions on Robotics]], [[Hutchinson et al. 1996 - A Tutorial on Visual Servo Control]], [[Corke 2017 - Robotics, Vision and Control]], [[ISO 8373:2021]]\n\t  - authority-score:: 0.90\n\t  - version:: 2.0.0\n\t  - last-updated:: 2025-11-18\n\n\t- **Relationships**\n\t  - is-part-of:: [[Robot Control]], [[Computer Vision]], [[Sensor-Based Control]]\n\t  - requires:: [[Camera]], [[Image Processing]], [[Jacobian Matrix]], [[Feature Extraction]], [[Real-Time Control]]\n\t  - enables:: [[Precision Assembly]], [[Surgical Robotics]], [[Automated Inspection]], [[Mobile Manipulation]]\n\t  - related-to:: [[Motion Planning]], [[Calibration]], [[Machine Vision]], [[Adaptive Control]]\n\t  - implements:: [[Closed-Loop Control]], [[Feedback Systems]]\n\n\t- **OWL Axioms**\n\t  - ```clojure\n\t    (Declaration (Class :VisualServoing))\n\t    (SubClassOf :VisualServoing :ControlMethod)\n\t    (SubClassOf :VisualServoing (ObjectSomeValuesFrom :requiresSensor :Camera))\n\t    (SubClassOf :VisualServoing (ObjectSomeValuesFrom :usesAlgorithm :ImageProcessing))\n\n\t    (AnnotationAssertion rdfs:label :VisualServoing \"Visual Servoing\"@en)\n\t    (AnnotationAssertion rdfs:comment :VisualServoing\n\t      \"Closed-loop robot control using real-time visual feedback from cameras\"@en)\n\t    (AnnotationAssertion :termID :VisualServoing \"RB-0065\"^^xsd:string)\n\t    (AnnotationAssertion :authorityScore :VisualServoing \"0.90\"^^xsd:decimal)\n\n\t    (Declaration (ObjectProperty :hasConfiguration))\n\t    (ObjectPropertyDomain :hasConfiguration :VisualServoing)\n\t    (ObjectPropertyRange :hasConfiguration :CameraConfiguration)\n\n\t    (Declaration (Class :PBVS))\n\t    (Declaration (Class :IBVS))\n\t    (SubClassOf :PBVS :VisualServoing)\n\t    (SubClassOf :IBVS :VisualServoing)\n\t    (DisjointClasses :PBVS :IBVS)\n\n\t    (Declaration (DataProperty :hasControlFrequency))\n\t    (DataPropertyDomain :hasControlFrequency :VisualServoing)\n\t    (DataPropertyRange :hasControlFrequency xsd:decimal)\n\t    ```\n\n\t- **Technical Specifications**\n\t  - control-methods:: [[Position-Based Visual Servoing (PBVS)]], [[Image-Based Visual Servoing (IBVS)]], [[Hybrid Visual Servoing]]\n\t  - camera-configurations:: [[Eye-in-Hand]], [[Eye-to-Hand]], [[Multi-Camera Systems]]\n\t  - feature-types:: [[Point Features]], [[Line Features]], [[Contour Features]], [[Learned Features]]\n\t  - tracking-algorithms:: [[Optical Flow]], [[Template Matching]], [[Feature Descriptor Matching]], [[Kalman Filtering]]\n\t  - control-frequency:: 10-1000 Hz\n\t  - typical-accuracy:: 0.1-5 mm (task-dependent)\n\n- ## About Visual Servoing\n\n\t- ### Primary Definition\n\t**Visual Servoing** is a closed-loop robot control technique using real-time visual feedback from cameras to guide robot motion, enabling precise positioning through either position-based (PBVS) or image-based (IBVS) control strategies.\n\n\t- ### Technical Architecture\n\n\t  **Control Approaches**\n\t  1. **Position-Based Visual Servoing (PBVS)**\n\t     - Reconstructs 3D pose from visual features\n\t     - Controls robot in Cartesian space\n\t     - Requires accurate camera calibration\n\t     - Subject to pose estimation errors\n\n\t  2. **Image-Based Visual Servoing (IBVS)**\n\t     - Direct control from 2D image features\n\t     - Avoids explicit 3D reconstruction\n\t     - Robust to calibration errors\n\t     - May exhibit image space singularities\n\n\t  3. **Hybrid Approaches**\n\t     - Combine PBVS and IBVS advantages\n\t     - Partition control across task dimensions\n\t     - 2.5D visual servoing frameworks\n\n\t  **Camera Configurations**\n\t  - **Eye-in-Hand**: Camera mounted on robot end-effector\n\t    - Better workspace coverage during motion\n\t    - Potential self-occlusion issues\n\t    - Requires end-effector space for mounting\n\n\t  - **Eye-to-Hand**: Fixed external camera\n\t    - Larger field of view\n\t    - No moving camera calibration errors\n\t    - Limited workspace coverage\n\n\t  **Jacobian Matrix Computation**\n\t  - Maps joint velocities to image feature velocities\n\t  - Analytical computation from camera model and robot kinematics\n\t  - Numerical approximation through finite differences\n\t  - Adaptive estimation for uncertain parameters\n\t  - Pseudo-inverse for redundant systems\n\n\t- ### Feature Extraction and Tracking\n\n\t  **Classical Features**\n\t  - Harris corner detector (rotation invariant)\n\t  - FAST corner detection (real-time performance)\n\t  - SIFT/SURF descriptors (scale invariant)\n\t  - Canny edge detection (contour features)\n\t  - Blob detection (centroid tracking)\n\n\t  **Tracking Algorithms**\n\t  - Lucas-Kanade optical flow (dense tracking)\n\t  - KLT tracker (sparse feature tracking)\n\t  - Template matching (cross-correlation)\n\t  - Particle filters (non-Gaussian noise)\n\t  - Kalman filtering (state estimation)\n\n\t  **Modern Approaches**\n\t  - CNN-based feature learning (SuperPoint, R2D2)\n\t  - Deep descriptors (learned invariance)\n\t  - Event-based vision (microsecond latency)\n\t  - Semantic segmentation features (object-level control)\n\n\t- ### Applications and Use Cases\n\n\t  **Manufacturing and Assembly**\n\t  - Peg-in-hole insertion (sub-millimetre tolerance)\n\t  - PCB component placement (0.1mm accuracy)\n\t  - Wire harness assembly (flexible object handling)\n\t  - Quality inspection (defect detection)\n\t  - Welding seam tracking (real-time path correction)\n\n\t  **Medical Robotics**\n\t  - Minimally invasive surgery (tool guidance)\n\t  - Endoscopic procedures (trocar port alignment)\n\t  - Needle insertion (tissue deformation compensation)\n\t  - Retinal surgery (micrometre precision)\n\t  - Robotic ultrasound (force-vision control)\n\n\t  **Service and Field Robotics**\n\t  - Agricultural picking (fruit harvesting)\n\t  - Bin picking (cluttered environments)\n\t  - Infrastructure inspection (crack detection)\n\t  - Underwater manipulation (turbid conditions)\n\t  - Aerial manipulation (moving platform stabilisation)\n\n\t- ### UK Research and Industry Context\n\n\t  **Leading Research Institutions**\n\t  - **Imperial College London**: Robot Vision Group\n\t    - Hybrid force-vision control\n\t    - Dense visual servoing with RGB-D\n\t    - Deformable object manipulation\n\n\t  - **University of Manchester**: Centre for Robotics and AI\n\t    - Multi-camera visual servoing\n\t    - Adaptive control under lighting variations\n\t    - Industrial automation applications\n\n\t  - **Sheffield Robotics**: University of Sheffield\n\t    - Nuclear decommissioning robotics\n\t    - Event-based visual servoing\n\t    - Extreme environment applications\n\n\t  - **University of Oxford**: Oxford Robotics Institute\n\t    - SLAM-based visual servoing\n\t    - Long-term visual navigation\n\t    - Outdoor mobile manipulation\n\n\t  **Industry Applications**\n\t  - Rolls-Royce: Jet engine inspection robotics\n\t  - BAE Systems: Automated composite layup\n\t  - Ocado Technology: Warehouse picking systems\n\t  - CMR Surgical: Versius surgical robot platform\n\t  - Shadow Robot Company: Dexterous manipulation research\n\n\t- ### Standards and References\n\n\t  **International Standards**\n\t  - ISO 8373:2021: Robotics vocabulary (control system definitions)\n\t  - ISO/TS 15066:2016: Collaborative robots (safety with vision systems)\n\t  - IEC 61496: Safety of machinery (vision-based protective devices)\n\n\t  **Foundational Literature**\n\t  1. **Hutchinson, S., Hager, G.D., Corke, P.I. (1996)**\n\t     - \"A Tutorial on Visual Servo Control\"\n\t     - IEEE Transactions on Robotics and Automation, 12(5):651-670\n\t     - Definitive tutorial establishing terminology and taxonomy\n\n\t  2. **Chaumette, F., Hutchinson, S. (2006)**\n\t     - \"Visual Servo Control, Part I: Basic Approaches\"\n\t     - IEEE Robotics & Automation Magazine, 13(4):82-90\n\t     - Modern review of classical methods\n\n\t  3. **Corke, P. (2017)**\n\t     - \"Robotics, Vision and Control: Fundamental Algorithms in MATLAB\"\n\t     - Springer Tracts in Advanced Robotics, 2nd Edition\n\t     - Comprehensive textbook with software tools\n\n\t  4. **Marchand, E., Spindler, F., Chaumette, F. (2005)**\n\t     - \"ViSP for Visual Servoing: A Generic Software Platform\"\n\t     - IEEE Robotics & Automation Magazine, 12(4):40-52\n\t     - Open-source implementation reference\n\n\t- ### Technical Challenges and Solutions\n\n\t  **Stability and Convergence**\n\t  - Local minima in image space (IBVS)\n\t  - Camera singularities (loss of degrees of freedom)\n\t  - Solutions: Task sequencing, switching control laws, global planners\n\n\t  **Robustness Issues**\n\t  - Lighting variations (adaptive thresholding, normalisation)\n\t  - Occlusions (multi-camera fusion, predictive tracking)\n\t  - Feature correspondence (RANSAC outlier rejection)\n\t  - Motion blur (event cameras, high-frame-rate imaging)\n\n\t  **Performance Optimisation**\n\t  - Control loop latency (GPU acceleration, optimised code)\n\t  - Jacobian conditioning (damped least-squares, SVD analysis)\n\t  - Convergence rate (adaptive gains, optimal control)\n\t  - Workspace constraints (joint limit avoidance, null-space control)\n\n\t  **Calibration Requirements**\n\t  - Hand-eye calibration (eye-in-hand systems)\n\t  - Intrinsic camera parameters (lens distortion)\n\t  - Robot kinematic model accuracy\n\t  - Solutions: Self-calibration, online estimation, kinematic learning\n\n\t- ### Advanced Techniques (2025 State-of-the-Art)\n\n\t  **Deep Learning Integration**\n\t  - Learned visual features (end-to-end visuomotor policies)\n\t  - Sim-to-real transfer (domain randomisation)\n\t  - Imitation learning from demonstrations\n\t  - Reinforcement learning for adaptive control\n\n\t  **Multi-Modal Fusion**\n\t  - Vision-force-tactile integration\n\t  - RGB-D visual servoing (depth-enhanced control)\n\t  - Thermal imaging (transparent object handling)\n\t  - Event-based vision (1 kHz control loops)\n\n\t  **Distributed Systems**\n\t  - Multi-robot visual servoing (formation control)\n\t  - Cloud-based computation (offloaded processing)\n\t  - Edge computing (latency reduction)\n\t  - 5G-enabled teleoperation (remote visual servoing)\n\n\t- ### Performance Metrics and Benchmarks\n\n\t  **Quantitative Measures**\n\t  - Positioning accuracy: 0.1-5 mm (application-dependent)\n\t  - Control frequency: 10-1000 Hz (sensor-limited)\n\t  - Convergence time: Task complexity and feature distance\n\t  - Trajectory smoothness: Jerk minimisation, energy efficiency\n\n\t  **Benchmark Datasets**\n\t  - ACRV Picking Benchmark (cluttered scene manipulation)\n\t  - RAVEN surgical tasks (precision medical applications)\n\t  - Assembly tasks from World Robot Summit\n\t  - Custom calibration and tracking datasets\n\n\t- ### Future Research Directions\n\n\t  **Emerging Trends**\n\t  1. **Cognitive Visual Servoing**\n\t     - Semantic scene understanding\n\t     - Task and motion planning integration\n\t     - High-level reasoning with low-level control\n\n\t  2. **Collaborative Systems**\n\t     - Human-robot shared visual servoing\n\t     - Intention prediction from vision\n\t     - Safe interaction in dynamic environments\n\n\t  3. **Extreme Environments**\n\t     - Underwater robotics (turbidity, refractive effects)\n\t     - Space applications (illumination extremes)\n\t     - Nuclear decommissioning (radiation-hard sensors)\n\n\t  4. **Certification and Safety**\n\t     - Formal verification methods\n\t     - Safety-critical visual servoing\n\t     - Redundant sensing for fault tolerance\n\t     - Standards for autonomous systems\n\n\t- ### Implementation Frameworks and Tools\n\n\t  **Open-Source Libraries**\n\t  - ViSP (Visual Servoing Platform): C++ library with Python bindings\n\t  - ROS MoveIt: Integration with motion planning\n\t  - OpenCV: Computer vision primitives\n\t  - Peter Corke's Robotics Toolbox: MATLAB implementation\n\n\t  **Commercial Systems**\n\t  - Cognex vision-guided robotics\n\t  - Keyence robot vision systems\n\t  - MVTec HALCON (machine vision software)\n\t  - ABB Integrated Vision (industrial applications)\n\n\t- ### Original Content\n\t  collapsed:: true\n\t\t- ```\n# RB-0065: Visual Servoing\n\n\t\t  ## Metadata\n\t\t  - **Term ID**: RB-0065\n\t\t  - **Term Type**: Core Concept\n\t\t  - **Classification**: Control Systems\n\t\t  - **Priority**: 1 (Foundational)\n\t\t  - **Authority Score**: 0.90\n\t\t  - **ISO Reference**: ISO 8373:2021\n\t\t  - **Version**: 2.0.0\n\t\t  - **Last Updated**: 2025-11-18\n\n\t\t  ## Definition\n\n\t\t  ### Primary Definition\n\t\t  **Visual Servoing** - Closed-loop robot control using real-time visual feedback from cameras to guide robot motion towards desired configurations or tasks.\n\n\t\t  ### Standards Context\n\t\t  Defined according to ISO 8373:2021 and IEEE Transactions on Robotics foundational literature.\n\n\t\t  ### Key Characteristics\n\t\t  1. Real-time visual feedback control\n\t\t  2. Position-based (PBVS) or image-based (IBVS) approaches\n\t\t  3. Eye-in-hand or eye-to-hand configurations\n\t\t  4. Jacobian matrix computation for velocity mapping\n\t\t  5. Applications in assembly, surgery, inspection\n\n\t\t  ## Formal Ontology (OWL Functional Syntax)\n\n\t\t  ```clojure\n\t\t  (Declaration (Class :VisualServoing))\n\t\t  (SubClassOf :VisualServoing :ControlMethod)\n\t\t  (SubClassOf :VisualServoing (ObjectSomeValuesFrom :requiresSensor :Camera))\n\n\t\t  (AnnotationAssertion rdfs:label :VisualServoing \"Visual Servoing\"@en)\n\t\t  (AnnotationAssertion rdfs:comment :VisualServoing\n\t\t    \"Closed-loop robot control using real-time visual feedback\"@en)\n\t\t  (AnnotationAssertion :termID :VisualServoing \"RB-0065\"^^xsd:string)\n\n\t\t  (Declaration (ObjectProperty :hasConfiguration))\n\t\t  (ObjectPropertyDomain :hasConfiguration :VisualServoing)\n\n\t\t  (Declaration (Class :PBVS))\n\t\t  (Declaration (Class :IBVS))\n\t\t  (SubClassOf :PBVS :VisualServoing)\n\t\t  (SubClassOf :IBVS :VisualServoing)\n\t\t  ```\n\n\t\t  ## Relationships\n\n\t\t  ### Parent Classes\n\t\t  - `ControlMethod`: Primary classification\n\t\t  - `SensorBasedControl`: Feedback-based systems\n\n\t\t  ### Related Concepts\n\t\t  - Computer Vision\n\t\t  - Robot Kinematics\n\t\t  - Real-Time Control\n\t\t  - Feature Tracking\n\n\t\t  ## Use Cases\n\n\t\t  ### Industrial Applications\n\t\t  1. Precision assembly (peg-in-hole)\n\t\t  2. Quality inspection (defect detection)\n\t\t  3. Welding seam tracking\n\n\t\t  ### Medical Applications\n\t\t  1. Minimally invasive surgery\n\t\t  2. Endoscopic procedures\n\t\t  3. Needle insertion guidance\n\n\t\t  ### Research Applications\n\t\t  1. Mobile manipulation\n\t\t  2. Deformable object handling\n\t\t  3. Extreme environment robotics\n\n\t\t  ## Standards References\n\n\t\t  ### Primary Standards\n\t\t  1. **ISO 8373:2021**: Robotics vocabulary\n\t\t  2. **ISO/TS 15066:2016**: Collaborative robots\n\t\t  3. **IEEE Transactions on Robotics**: Research standards\n\n\t\t  ## Validation Criteria\n\n\t\t  ### Conformance Requirements\n\t\t  1. ✓ Real-time control loop (10+ Hz)\n\t\t  2. ✓ Stable convergence behaviour\n\t\t  3. ✓ Documented accuracy metrics\n\t\t  4. ✓ Safety compliance demonstrated\n\t\t  5. ✓ Robustness to environmental variations\n\n\t\t  ## Implementation Notes\n\n\t\t  ### Design Considerations\n\t\t  - Camera selection and placement\n\t\t  - Feature selection and tracking\n\t\t  - Jacobian computation method\n\t\t  - Control law design (gains, stability)\n\t\t  - Calibration procedures\n\n\t\t  ### Common Patterns\n\t\t  ```yaml\n\t\t  implementation:\n\t\t    control_method: IBVS\n\t\t    camera_config: eye_in_hand\n\t\t    features: corner_points\n\t\t    control_frequency: 100Hz\n\t\t    framework: ViSP\n\t\t  ```\n\n\t\t  ## Cross-References\n\n\t\t  ### Robotics Domain Integration\n\t\t  - Motion planning systems\n\t\t  - Force control integration\n\t\t  - Multi-sensor fusion\n\n\t\t  ### Domain Ontologies\n\t\t  - Computer vision ontologies\n\t\t  - Control systems ontologies\n\t\t  - Manufacturing ontologies\n\n\t\t  ## Future Directions\n\n\t\t  ### Emerging Trends\n\t\t  1. Deep learning feature representations\n\t\t  2. Event-based vision sensors\n\t\t  3. Multi-modal sensor fusion\n\t\t  4. Cognitive visual servoing\n\t\t  5. Safety certification standards\n\n\t\t  ---\n\n\t\t  **Version History**\n\t\t  - 2.0.0 (2025-11-18): Comprehensive enrichment with technical details\n\t\t  - 1.0.0 (2025-10-28): Initial foundational definition\n\n\t\t  **Contributors**: Robotics Ontology Working Group\n\t\t  **License**: CC BY 4.0\n\t\t  **Namespace**: `https://metaverse-ontology.org/robotics/RB-0065`\n\n\t\t  ```\n\n## Current Landscape (2025)\n\n- Industry adoption and implementations\n  - Visual servoing widely deployed in manufacturing (automotive assembly, electronics manufacturing)\n  - Medical robotics companies integrate vision-guided manipulation (da Vinci, Versius systems)\n  - Warehouse automation relies on vision for picking and place operations (Ocado, Amazon Robotics)\n  - UK robotics sector growing in nuclear decommissioning and offshore inspection applications\n  - Deep learning transforming feature extraction with learned representations\n\n- Technical capabilities\n  - Control frequencies reaching 1 kHz with event-based cameras\n  - Sub-millimetre accuracy in controlled industrial environments\n  - RGB-D cameras enable dense visual servoing with depth information\n  - GPU acceleration enables real-time processing of high-resolution imagery\n  - Multi-camera systems provide robustness to occlusions\n\n- UK and North England context\n  - Imperial College London: Leading research in hybrid visual-force control\n  - University of Manchester: Centre for Robotics and AI advancing adaptive visual servoing\n  - Sheffield Robotics: Nuclear robotics with vision-guided manipulation\n  - Rolls-Royce: Jet engine inspection using visual servoing\n  - Shadow Robot Company: Dexterous manipulation research platforms\n\n- Standards and frameworks\n  - ISO 8373:2021 defines visual servoing within robot control terminology\n  - ViSP (Visual Servoing Platform) provides open-source implementation\n  - ROS MoveIt integrates visual servoing with motion planning\n  - IEEE Transactions on Robotics establishes performance benchmarks\n  - Safety standards (ISO/TS 15066) address vision-based collaborative systems\n\n## Metadata\n\n- **Last Updated**: 2025-11-18\n- **Review Status**: Comprehensive enrichment with technical depth\n- **Verification**: IEEE standards and foundational literature verified\n- **Regional Context**: UK robotics research institutions and industry\n- **Authority Score**: 0.90\n- **Version**: 2.0.0",
  "properties": {
    "id": "rb-0065-visual-servoing-ontology",
    "collapsed": "true",
    "- term-id": "RB-0065",
    "- preferred-term": "Visual Servoing",
    "- source-domain": "robotics",
    "- status": "mature",
    "- public-access": "true",
    "- owl:class": "rb:VisualServoing",
    "- owl:physicality": "ConceptualEntity",
    "- owl:role": "ControlMethod",
    "- belongsToDomain": "[[RoboticsDomain]]",
    "- definition": "**Visual Servoing** is a closed-loop robot control technique that uses real-time visual feedback from cameras to guide robot motion towards desired configurations or tasks. The method computes control signals directly from visual features extracted from camera images, enabling precise positioning and tracking without explicit knowledge of object positions in world coordinates. Visual servoing systems fall into two primary categories: **Position-Based Visual Servoing (PBVS)** reconstructs 3D pose from visual features and controls the robot in Cartesian space, while **Image-Based Visual Servoing (IBVS)** directly uses 2D image features to compute control velocities, avoiding pose estimation errors. Camera configurations include **eye-in-hand** (camera mounted on robot end-effector) and **eye-to-hand** (fixed external camera), each offering distinct advantages for workspace coverage and occlusion handling. The control law relies on the **image Jacobian** or **interaction matrix** that maps joint velocities to image feature velocities, typically computed through analytical models or numerical approximation. Feature extraction employs corner detectors (Harris, FAST), blob detectors, or learned features (CNN-based descriptors) tracked across frames using optical flow or template matching algorithms. Applications span precision assembly in manufacturing (sub-millimetre alignment), minimally invasive surgery (tool guidance), automated inspection (quality control), and agricultural robotics (fruit picking). UK research institutions including Imperial College's Robot Vision Group, University of Manchester's Centre for Robotics and AI, and Sheffield Robotics actively advance visual servoing through hybrid force-vision control, deep learning feature representations, and adaptive control under varying illumination. Standards ISO 8373:2021 defines visual servoing within robot control systems, while IEEE research establishes stability analysis and performance benchmarks. The technique addresses uncertainties in robot kinematics, camera calibration errors, and dynamic environments through adaptive gain scheduling, robust feature selection, and predictive control strategies. Advances integrate tactile feedback, multi-camera fusion, and event-based vision sensors for high-speed manipulation tasks requiring sub-10ms control loops.",
    "- maturity": "mature",
    "- source": "[[IEEE Transactions on Robotics]], [[Hutchinson et al. 1996 - A Tutorial on Visual Servo Control]], [[Corke 2017 - Robotics, Vision and Control]], [[ISO 8373:2021]]",
    "- authority-score": "0.90",
    "- version": "2.0.0",
    "- last-updated": "2025-11-18",
    "- is-part-of": "[[Robot Control]], [[Computer Vision]], [[Sensor-Based Control]]",
    "- requires": "[[Camera]], [[Image Processing]], [[Jacobian Matrix]], [[Feature Extraction]], [[Real-Time Control]]",
    "- enables": "[[Precision Assembly]], [[Surgical Robotics]], [[Automated Inspection]], [[Mobile Manipulation]]",
    "- related-to": "[[Motion Planning]], [[Calibration]], [[Machine Vision]], [[Adaptive Control]]",
    "- implements": "[[Closed-Loop Control]], [[Feedback Systems]]",
    "- control-methods": "[[Position-Based Visual Servoing (PBVS)]], [[Image-Based Visual Servoing (IBVS)]], [[Hybrid Visual Servoing]]",
    "- camera-configurations": "[[Eye-in-Hand]], [[Eye-to-Hand]], [[Multi-Camera Systems]]",
    "- feature-types": "[[Point Features]], [[Line Features]], [[Contour Features]], [[Learned Features]]",
    "- tracking-algorithms": "[[Optical Flow]], [[Template Matching]], [[Feature Descriptor Matching]], [[Kalman Filtering]]",
    "- control-frequency": "10-1000 Hz",
    "- typical-accuracy": "0.1-5 mm (task-dependent)"
  },
  "backlinks": [],
  "wiki_links": [
    "Camera",
    "Kalman Filtering",
    "RoboticsDomain",
    "ISO 8373:2021",
    "Eye-to-Hand",
    "Jacobian Matrix",
    "Feature Descriptor Matching",
    "Feature Extraction",
    "Multi-Camera Systems",
    "Computer Vision",
    "Precision Assembly",
    "Contour Features",
    "Template Matching",
    "Image Processing",
    "Line Features",
    "IEEE Transactions on Robotics",
    "Position-Based Visual Servoing (PBVS)",
    "Point Features",
    "Hybrid Visual Servoing",
    "Learned Features",
    "Image-Based Visual Servoing (IBVS)",
    "Surgical Robotics",
    "Calibration",
    "Robot Control",
    "Mobile Manipulation",
    "Feedback Systems",
    "Closed-Loop Control",
    "Eye-in-Hand",
    "Real-Time Control",
    "Hutchinson et al. 1996 - A Tutorial on Visual Servo Control",
    "Sensor-Based Control",
    "Motion Planning",
    "Optical Flow",
    "Automated Inspection",
    "Machine Vision",
    "Adaptive Control",
    "Corke 2017 - Robotics, Vision and Control"
  ],
  "ontology": {
    "term_id": "RB-0065",
    "preferred_term": "Visual Servoing",
    "definition": "**Visual Servoing** is a closed-loop robot control technique that uses real-time visual feedback from cameras to guide robot motion towards desired configurations or tasks. The method computes control signals directly from visual features extracted from camera images, enabling precise positioning and tracking without explicit knowledge of object positions in world coordinates. Visual servoing systems fall into two primary categories: **Position-Based Visual Servoing (PBVS)** reconstructs 3D pose from visual features and controls the robot in Cartesian space, while **Image-Based Visual Servoing (IBVS)** directly uses 2D image features to compute control velocities, avoiding pose estimation errors. Camera configurations include **eye-in-hand** (camera mounted on robot end-effector) and **eye-to-hand** (fixed external camera), each offering distinct advantages for workspace coverage and occlusion handling. The control law relies on the **image Jacobian** or **interaction matrix** that maps joint velocities to image feature velocities, typically computed through analytical models or numerical approximation. Feature extraction employs corner detectors (Harris, FAST), blob detectors, or learned features (CNN-based descriptors) tracked across frames using optical flow or template matching algorithms. Applications span precision assembly in manufacturing (sub-millimetre alignment), minimally invasive surgery (tool guidance), automated inspection (quality control), and agricultural robotics (fruit picking). UK research institutions including Imperial College's Robot Vision Group, University of Manchester's Centre for Robotics and AI, and Sheffield Robotics actively advance visual servoing through hybrid force-vision control, deep learning feature representations, and adaptive control under varying illumination. Standards ISO 8373:2021 defines visual servoing within robot control systems, while IEEE research establishes stability analysis and performance benchmarks. The technique addresses uncertainties in robot kinematics, camera calibration errors, and dynamic environments through adaptive gain scheduling, robust feature selection, and predictive control strategies. Advances integrate tactile feedback, multi-camera fusion, and event-based vision sensors for high-speed manipulation tasks requiring sub-10ms control loops.",
    "source_domain": "robotics",
    "maturity_level": null,
    "authority_score": 0.9
  }
}