{
  "id": "Fairness",
  "title": "Fairness",
  "content": "- ### OntologyBlock\n  id:: fairness-ontology\n  collapsed:: true\n\t- ontology:: true\n\t- term-id:: AI-0065\n\t- preferred-term:: Fairness\n\t- status:: draft\n\t- public-access:: true\n\t- definition:: The property of an AI system whereby it produces equitable outcomes and avoids creating or reinforcing unjustifiable disparities across different demographic groups or individuals, measured through various mathematical definitions and ethical principles.\n\t- #### Relationships\n\t  id:: fairness-relationships\n\t  collapsed:: true\n\t\t- is-subclass-of:: [[Metaverse]]\n\t\t- requires:: [[Bias Detection]]\n\t\t- requires:: [[Bias Mitigation]]\n\t\t- enables:: [[Non-discrimination]]\n\t\t- enables:: [[Equal Treatment]]\n\n## Formal Specification\n\n```yaml\nterm: Fairness\ndefinition: \"Equitable treatment and outcomes across groups and individuals in AI systems\"\ndomain: AI Ethics\ntype: Quality Attribute\ndimensions:\n  - individual_fairness\n  - group_fairness\n  - procedural_fairness\n  - distributive_fairness\nmetrics:\n  - demographic_parity\n  - equalized_odds\n  - equal_opportunity\n  - calibration\n  - individual_fairness_metrics\nprotected_attributes: [race, gender, age, disability, religion, nationality]\n```\n\n## Formal Ontology\n\n```clojure\n(Declaration (Class :Fairness))\n(SubClassOf :Fairness :QualityAttribute)\n(SubClassOf :Fairness :TrustworthinessDimension)\n\n;; Fairness types\n(Declaration (Class :IndividualFairness))\n(Declaration (Class :GroupFairness))\n(Declaration (Class :ProceduralFairness))\n(Declaration (Class :DistributiveFairness))\n\n(SubClassOf :IndividualFairness :Fairness)\n(SubClassOf :GroupFairness :Fairness)\n(SubClassOf :ProceduralFairness :Fairness)\n(SubClassOf :DistributiveFairness :Fairness)\n\n;; Core properties\n(Declaration (ObjectProperty :evaluatedByFairnessMetric))\n(Declaration (ObjectProperty :protectsAttribute))\n(Declaration (ObjectProperty :preventsDiscriminationAgainst))\n\n;; Fairness metrics\n(Declaration (Class :FairnessMetric))\n(Declaration (Class :DemographicParity))\n(Declaration (Class :EqualizedOdds))\n(Declaration (Class :EqualOpportunity))\n(Declaration (Class :Calibration))\n\n(SubClassOf :DemographicParity :FairnessMetric)\n(SubClassOf :EqualizedOdds :FairnessMetric)\n(SubClassOf :EqualOpportunity :FairnessMetric)\n(SubClassOf :Calibration :FairnessMetric)\n\n;; Property characteristics\n(ObjectPropertyDomain :evaluatedByFairnessMetric :AISystem)\n(ObjectPropertyRange :evaluatedByFairnessMetric :FairnessMetric)\n(ObjectPropertyDomain :protectsAttribute :FairnessPolicy)\n(ObjectPropertyRange :protectsAttribute :ProtectedAttribute)\n\n;; Data properties for fairness metrics\n(Declaration (DataProperty :demographicParityDifference))\n(DataPropertyDomain :demographicParityDifference :AISystem)\n(DataPropertyRange :demographicParityDifference xsd:float)\n(AnnotationAssertion rdfs:comment :demographicParityDifference\n  \"Absolute difference in positive prediction rates across groups (0.0 to 1.0)\"^^xsd:string)\n\n(Declaration (DataProperty :disparateImpact))\n(DataPropertyDomain :disparateImpact :AISystem)\n(DataPropertyRange :disparateImpact xsd:float)\n(AnnotationAssertion rdfs:comment :disparateImpact\n  \"Ratio of positive rates (80% rule: >= 0.8 considered fair)\"^^xsd:string)\n\n(Declaration (DataProperty :equalizedOddsDifference))\n(DataPropertyDomain :equalizedOddsDifference :AISystem)\n(DataPropertyRange :equalizedOddsDifference xsd:float)\n\n(Declaration (DataProperty :fairnessScore))\n(DataPropertyDomain :fairnessScore :AISystem)\n(DataPropertyRange :fairnessScore xsd:float)\n(AnnotationAssertion rdfs:comment :fairnessScore\n  \"Composite fairness score from 0.0 (unfair) to 1.0 (fair)\"^^xsd:string)\n\n;; Relationships\n(SubClassOf :Fairness\n  (ObjectSomeValuesFrom :opposes :Bias))\n(SubClassOf :Fairness\n  (ObjectSomeValuesFrom :opposes :HarmfulBias))\n(SubClassOf :Fairness\n  (ObjectSomeValuesFrom :requires :BiasDetection))\n(SubClassOf :Fairness\n  (ObjectSomeValuesFrom :requires :BiasMitigation))\n\n;; Disjointness axioms\n(DisjointClasses :Fairness :Bias)\n\n;; Standards alignment\n(AnnotationAssertion dcterms:source :Fairness\n  \"ISO/IEC TR 24027:2021\"^^xsd:string)\n(AnnotationAssertion dcterms:source :Fairness\n  \"NIST AI RMF 1.0\"^^xsd:string)\n```\n\n## Authoritative References\n\n### Primary Sources\n\n1. **ISO/IEC TR 24027:2021** - Information technology — Artificial intelligence (AI) — Bias in AI systems and AI aided decision making\n   - Section 5: \"Fairness in AI\"\n   - Defines fairness concepts and metrics\n   - Source: ISO/IEC JTC 1/SC 42\n\n2. **NIST AI Risk Management Framework (AI RMF 1.0)**, January 2023\n   - Section 2.2: \"Fair — with harmful bias managed\"\n   - Multiple fairness definitions and context-dependency\n   - Source: National Institute of Standards and Technology\n\n3. **EU AI Act** (Regulation 2024/1689), June 2024\n   - Article 10: \"Data and data governance\" (bias mitigation)\n   - Recital 44: Fairness requirements\n   - Source: European Parliament and Council\n\n### Supporting Standards\n\n4. **ISO/IEC 23894:2023** - Guidance on risk management\n   - Section 7.5: \"Fairness considerations in risk management\"\n\n5. **Mehrabi, N., et al. (2021)** - \"A Survey on Bias and Fairness in Machine Learning\"\n   - *ACM Computing Surveys*, 54(6), 1-35\n   - Comprehensive fairness taxonomy\n\n## Key Characteristics\n\n### Types of Fairness\n\n#### 1. Group Fairness (Statistical Parity)\n\n**Demographic Parity**\n- Equal positive prediction rates across groups\n- $$P(\\hat{Y}=1|A=0) = P(\\hat{Y}=1|A=1)$$\n- Where A is protected attribute (e.g., gender)\n\n**Example**: Equal loan approval rates for men and women\n\n**Equalized Odds**\n- Equal true positive and false positive rates across groups\n- $$P(\\hat{Y}=1|Y=1,A=0) = P(\\hat{Y}=1|Y=1,A=1)$$\n- $$P(\\hat{Y}=1|Y=0,A=0) = P(\\hat{Y}=1|Y=0,A=1)$$\n\n**Example**: Recidivism prediction equally accurate for all races\n\n**Equal Opportunity**\n- Equal true positive rates (recall) across groups\n- $$P(\\hat{Y}=1|Y=1,A=0) = P(\\hat{Y}=1|Y=1,A=1)$$\n\n**Example**: Qualified candidates equally likely to be hired regardless of gender\n\n**Calibration**\n- Predicted probabilities match actual outcomes across groups\n- $$P(Y=1|\\hat{Y}=p,A=0) = P(Y=1|\\hat{Y}=p,A=1)$$\n\n**Example**: 70% risk score means 70% probability for all groups\n\n#### 2. Individual Fairness\n\n**Similarity-Based Fairness**\n- Similar individuals receive similar outcomes\n- Dwork et al.: \"Treat similar individuals similarly\"\n- Lipschitz condition: $$d(f(x_1), f(x_2)) \\leq L \\cdot d(x_1, x_2)$$\n\n**Example**: Two applicants with same qualifications get similar loan terms\n\n**Counterfactual Fairness**\n- Decision unchanged if protected attribute were different\n- Kusner et al.: Causal definition of fairness\n\n**Example**: Loan decision same regardless of applicant's race\n\n#### 3. Procedural Fairness\n\n- Transparency in decision-making process\n- Ability to contest decisions\n- Right to explanation\n- Human oversight and appeal mechanisms\n\n#### 4. Distributive Fairness\n\n- Equitable distribution of benefits and burdens\n- Consideration of existing inequalities\n- Justice in outcomes\n\n## Fairness Metrics\n\n### Classification Metrics\n\n1. **Demographic Parity Difference (DPD)**\n   ```\n   DPD = |P(Ŷ=1|A=0) - P(Ŷ=1|A=1)|\n   ```\n   - Range: [0, 1]\n   - Threshold: Often < 0.1 for \"fairness\"\n\n2. **Disparate Impact (DI)**\n   ```\n   DI = P(Ŷ=1|A=0) / P(Ŷ=1|A=1)\n   ```\n   - Range: [0, ∞]\n   - 80% rule: DI ≥ 0.8 (EEOC guideline)\n\n3. **Equalized Odds Difference (EOD)**\n   ```\n   EOD = |TPR₀ - TPR₁| + |FPR₀ - FPR₁|\n   ```\n\n4. **Average Odds Difference (AOD)**\n   ```\n   AOD = (|TPR₀ - TPR₁| + |FPR₀ - FPR₁|) / 2\n   ```\n\n### Ranking Metrics\n\n1. **Normalized Discounted Cumulative Gain (NDCG) Parity**\n2. **Exposure Parity**\n3. **Representation Metrics**\n\n### Impossibility Theorems\n\n**Key Insight**: Multiple fairness definitions cannot be simultaneously satisfied (except in trivial cases)\n\n**Chouldechova (2017)**: Cannot simultaneously achieve:\n- Calibration\n- Equal false positive rates\n- Equal false negative rates\n\n**Kleinberg et al. (2017)**: Cannot simultaneously satisfy:\n- Calibration\n- Balance for positive class\n- Balance for negative class\n\n**Implication**: Fairness requires trade-offs and context-specific choices\n\n## Relationships\n\n- **Component Of**: AI Trustworthiness (AI-0061)\n- **Opposed To**: Bias (AI-0067), Harmful Bias (AI-0084)\n- **Enables**: Non-discrimination, Equal Treatment\n- **Requires**: Bias Detection, Bias Mitigation\n- **Related To**: Accountability (AI-0068), Transparency (AI-0062)\n\n## Sources of Unfairness\n\n### Data-Related\n\n1. **Historical Bias**\n   - Past discrimination in training data\n   - Example: Hiring data reflects historical gender imbalance\n\n2. **Representation Bias**\n   - Underrepresentation of certain groups\n   - Example: Facial recognition trained primarily on one ethnicity\n\n3. **Measurement Bias**\n   - Systematic errors in data collection\n   - Example: Arrest records as proxy for crime (subject to policing bias)\n\n4. **Aggregation Bias**\n   - Inappropriate data aggregation\n   - Example: One-size-fits-all model for diverse populations\n\n### Algorithm-Related\n\n1. **Feature Selection Bias**\n   - Use of proxy variables for protected attributes\n   - Example: Zip code as proxy for race\n\n2. **Optimization Bias**\n   - Objective function doesn't include fairness\n   - Optimization for majority group\n\n3. **Evaluation Bias**\n   - Metrics evaluated on non-representative data\n   - Fairness not measured\n\n### Deployment-Related\n\n1. **Automation Bias**\n   - Over-reliance on system outputs\n   - Reduced human oversight\n\n2. **Feedback Loops**\n   - System reinforces existing biases\n   - Example: Predictive policing creates more arrests in over-policed areas\n\n3. **Population Shift**\n   - Model deployed on different population than training\n   - Fairness not maintained across contexts\n\n## Bias Mitigation Strategies\n\n### Pre-Processing (Data)\n\n1. **Re-sampling**\n   - Oversample minority groups\n   - Undersample majority groups\n   - Synthetic data generation (SMOTE)\n\n2. **Re-weighting**\n   - Assign weights to instances\n   - Balance group representation\n\n3. **Data Augmentation**\n   - Generate additional examples\n   - Balance across attributes\n\n4. **Fair Representation Learning**\n   - Learn bias-free representations\n   - Remove protected attribute information\n\n### In-Processing (Algorithm)\n\n1. **Fairness Constraints**\n   - Add fairness as optimization constraint\n   - Constrained optimization\n\n2. **Adversarial Debiasing**\n   - Train model to be invariant to protected attributes\n   - Adversarial network removes bias\n\n3. **Prejudice Remover**\n   - Regularization term for fairness\n   - Penalize discrimination\n\n4. **Fair Regularization**\n   - Add fairness penalty to loss function\n   - Balance accuracy and fairness\n\n### Post-Processing (Outputs)\n\n1. **Threshold Optimization**\n   - Group-specific decision thresholds\n   - Achieve demographic parity or equalized odds\n\n2. **Calibration**\n   - Adjust probabilities across groups\n   - Ensure calibration fairness\n\n3. **Reject Option Classification**\n   - Withhold predictions near decision boundary\n   - Request human review for borderline cases\n\n## Domain-Specific Fairness\n\n### Employment\n\n- **Regulation**: Equal Employment Opportunity laws\n- **Metric**: 80% rule (disparate impact)\n- **Approach**: Remove protected attributes, monitor outcomes\n- **Example**: Resume screening systems\n\n### Credit and Lending\n\n- **Regulation**: Equal Credit Opportunity Act, Fair Lending laws\n- **Metric**: Demographic parity in approval rates\n- **Approach**: Explainable models, disparate impact testing\n- **Example**: Credit scoring, loan approval\n\n### Criminal Justice\n\n- **Regulation**: Constitutional protections, due process\n- **Metric**: Equalized odds (equal accuracy across races)\n- **Approach**: Transparent risk scores, human oversight\n- **Example**: Recidivism prediction (COMPAS controversy)\n\n### Healthcare\n\n- **Regulation**: Anti-discrimination laws, medical ethics\n- **Metric**: Equal opportunity (equal benefit)\n- **Approach**: Clinical validation across populations\n- **Example**: Diagnostic algorithms, treatment recommendations\n\n### Education\n\n- **Regulation**: Title IX, disability rights laws\n- **Metric**: Individual fairness, equal opportunity\n- **Approach**: Personalized learning, accommodation\n- **Example**: Admissions, student assessment\n\n## Regulatory Requirements\n\n### EU AI Act\n\n**Article 10: Data and Data Governance**\n- Training datasets free from bias\n- Examination of possible biases\n- Appropriate data governance measures\n\n**Recital 44**: Fairness as core requirement for high-risk systems\n\n### GDPR\n\n**Article 22**: Automated Decision-Making\n- Right not to be subject to solely automated decisions\n- Safeguards including right to explanation\n\n**Recital 71**: Protection against discriminatory effects\n\n### US Equal Employment Opportunity Commission (EEOC)\n\n**Four-Fifths Rule (80% Rule)**\n- Selection rate for protected group ≥ 80% of highest group\n- Adverse impact threshold\n\n## Challenges and Debates\n\n### Philosophical Tensions\n\n1. **Formal vs. Substantive Fairness**\n   - Mathematical definitions vs. justice principles\n   - Metrics vs. meaningful equity\n\n2. **Individual vs. Group Fairness**\n   - Treating individuals fairly vs. achieving group parity\n   - Tensions between approaches\n\n3. **Fairness vs. Accuracy**\n   - Performance costs of fairness constraints\n   - When to trade accuracy for fairness\n\n4. **Procedural vs. Outcome Fairness**\n   - Fair process vs. fair results\n   - Which takes priority?\n\n### Practical Challenges\n\n1. **Defining Protected Groups**\n   - Which attributes to protect\n   - Intersectionality (multiple protected attributes)\n   - Proxy variables\n\n2. **Choosing Fairness Metric**\n   - Context-dependent appropriateness\n   - Impossibility of satisfying all metrics\n   - Stakeholder disagreement\n\n3. **Measuring Fairness**\n   - Data availability for subgroups\n   - Statistical significance\n   - Temporal stability\n\n4. **Balancing Multiple Objectives**\n   - Accuracy, fairness, privacy, interpretability\n   - Multi-objective optimization\n   - Pareto frontiers\n\n## Best Practices\n\n1. **Multi-Stakeholder Input**\n   - Include affected communities\n   - Diverse development teams\n   - External audits\n\n2. **Context-Appropriate Metrics**\n   - Choose fairness definition based on domain\n   - Document rationale for metric selection\n   - Consider multiple metrics\n\n3. **Intersectional Analysis**\n   - Examine fairness across intersections\n   - Not just single protected attributes\n   - Example: Black women, elderly disabled\n\n4. **Continuous Monitoring**\n   - Track fairness metrics over time\n   - Detect fairness degradation\n   - Feedback loop management\n\n5. **Transparency and Documentation**\n   - Document fairness considerations\n   - Report fairness metrics\n   - Enable external review\n\n6. **Human Oversight**\n   - Human-in-the-loop for critical decisions\n   - Appeal mechanisms\n   - Contestability\n\n7. **Proactive Bias Testing**\n   - Test before deployment\n   - Red-team for fairness\n   - Adversarial testing\n\n## Tools and Frameworks\n\n1. **AI Fairness 360 (IBM)**\n   - 70+ fairness metrics\n   - 10+ bias mitigation algorithms\n   - Open source\n\n2. **Fairlearn (Microsoft)**\n   - Fairness assessment\n   - Mitigation algorithms\n   - Integration with scikit-learn\n\n3. **What-If Tool (Google)**\n   - Visual fairness exploration\n   - Counterfactual analysis\n   - Interactive interface\n\n4. **Aequitas (University of Chicago)**\n   - Fairness auditing\n   - Bias report generation\n   - Multiple metrics\n\n## Related Terms\n\n- **AI Trustworthiness** (AI-0061)\n- **Bias** (AI-0067)\n- **Harmful Bias** (AI-0084)\n- **Accountability** (AI-0068)\n- **Transparency** (AI-0062)\n- **Non-discrimination** (AI-0038)\n\n## Version History\n\n- **1.0** (2025-10-27): Initial definition based on ISO/IEC TR 24027:2021 and NIST AI RMF\n\n---\n\n*This definition reflects the multifaceted and context-dependent nature of fairness in AI systems, acknowledging both mathematical formalizations and ethical considerations.*\n\n## 2024-2025: From Theory to Regulatory Mandate\nid:: fairness-recent-developments\n\nThe period from 2024 to 2025 represented a pivotal transition whereby algorithmic fairness evolved from an academic and voluntary concern into a **legally mandated requirement** across multiple jurisdictions, whilst simultaneously revealing the profound practical challenges of implementing fairness metrics in production AI systems at scale.\n\n### Regulatory Codification of Fairness\n\nThe **EU AI Act**, which entered into force on 1st August 2024, established the world's first comprehensive legal framework explicitly requiring fairness in high-risk AI systems. **Article 10** mandates that training datasets for high-risk systems must be \"free from errors and complete\" and subject to \"appropriate data governance and management practices.\" Crucially, **Recital 44** identifies fairness as a \"core requirement\" for high-risk AI, with system providers facing penalties up to **€35 million or 7% of worldwide annual turnover** for non-compliance.\n\nThe **Colorado AI Act**, enacted on 17th May 2024 and effective February 2026, became the first U.S. state law mandating fairness assessments for \"high-risk\" automated decision systems in employment, education, financial services, healthcare, housing, and legal contexts. The Act requires **algorithmic discrimination impact assessments** and affords consumers the right to opt out of profiling for consequential decisions.\n\n**New York City's Local Law 144**, which took effect in July 2023 but saw active enforcement intensify throughout 2024, requires annual **bias audits** of automated employment decision tools, with results publicly disclosed, making NYC the first U.S. jurisdiction to mandate fairness auditing with transparency requirements.\n\n### Fairness-Accuracy Trade-offs in Practice\n\nWhilst fairness metrics have been well-theorised, 2024-2025 revealed the stark **operational challenges** of implementing fairness constraints in production systems. Research demonstrated that achieving demographic parity or equalised odds often requires **significant accuracy degradation**, particularly for minority groups the fairness constraints aim to protect.\n\nA significant debate emerged around **fairness gerrymandering**: organisations selecting specific fairness metrics that produce favourable scores whilst ignoring metrics that would reveal bias. For instance, a hiring algorithm might satisfy demographic parity (equal hiring rates across groups) whilst violating equalised odds (unequal accuracy across groups), and organisations might publicly report only the favourable metric.\n\nThe **impossibility theorems** established by Chouldechova (2017) and Kleinberg et al. (2017) moved from theoretical curiosities to practical dilemmas. Organisations discovered they could not simultaneously satisfy calibration, balance for positive class, and balance for negative class except in trivial cases, forcing explicit **value judgements** about which fairness definition to prioritise.\n\n### ISO Fairness Standards Adoption\n\n**ISO/IEC 42001:2023**, published in December 2023, gained widespread adoption throughout 2024 as the first international standard for AI management systems, with fairness and non-discrimination as core requirements. **ISO/IEC TR 24027:2021**, focusing specifically on bias in AI systems, became the de facto reference for fairness metric selection and bias mitigation strategies.\n\nMany organisations pursuing ISO 42001 certification discovered that achieving **demonstrable fairness** required substantial architectural changes, not merely post-processing adjustments. This drove adoption of **fairness-aware machine learning** techniques during model training rather than relying solely on threshold adjustments after training.\n\n### Intersectional Fairness and Compounding Bias\n\n2024 witnessed increased focus on **intersectional fairness**—examining AI system performance across intersections of multiple protected attributes (e.g., Black women, elderly disabled persons). Research consistently demonstrated that examining single attributes in isolation masked substantial disparities at intersections.\n\nFairness tools such as **AI Fairness 360**, **Fairlearn**, and **Aequitas** added intersectional analysis capabilities, allowing practitioners to compute fairness metrics across multiple demographic intersections simultaneously. However, intersectional analysis exacerbated the **small sample problem**: many intersectional subgroups had insufficient data for statistically meaningful fairness assessment, creating a tension between comprehensiveness and statistical validity.\n\n### Contextual and Cultural Fairness\n\nThe global deployment of AI systems in 2024-2025 highlighted that fairness is **culturally contextual**. What constitutes \"fair\" treatment varies across societies, legal traditions, and cultural values. For instance, demographic parity is legally required in some EU contexts, whilst U.S. law emphasises individual merit and may reject group-based fairness metrics as discriminatory.\n\nThis created challenges for multinational AI systems: a single model deployed globally might satisfy fairness requirements in one jurisdiction whilst violating them in another. Some organisations developed **region-specific models** with different fairness constraints, whilst others sought \"universal\" fairness criteria that satisfied multiple jurisdictions simultaneously (often at the cost of reduced accuracy).\n\n### Automated Fairness Monitoring\n\nBy late 2024, **continuous fairness monitoring** became a standard practice for high-risk AI systems. Rather than one-time fairness audits, organisations implemented real-time dashboards tracking fairness metrics in production, with automated alerts when metrics degraded below thresholds.\n\nThis shift was driven by recognition that fairness is not static: models degrade over time due to **distribution shift**, **feedback loops**, and **changing demographics**. What was fair at deployment may become unfair months later without ongoing vigilance.\n\n### The Fairness Washing Problem\n\nRegulators and civil society organisations raised concerns about **fairness washing**—superficial claims of fairness without substantive evidence. In response, the EU AI Act's **Article 13** requires **technical documentation** demonstrating how fairness was achieved, whilst NYC's bias audit law requires specific methodological disclosure.\n\nAcademic research in 2025 proposed **fairness certification schemes** analogous to security certifications, with independent third parties verifying fairness claims against standardised criteria, though widespread adoption remained nascent.\n\t- maturity:: draft\n\t- owl:class:: mv:Fairness\n\t- owl:physicality:: ConceptualEntity\n\t- owl:role:: Concept\n\t- belongsToDomain:: [[MetaverseDomain]]\n\t- #### Relationships\n\t  id:: fairness-relationships\n\t\t- is-subclass-of:: [[Metaverse]]\n\t\t- requires:: [[Bias Detection]], [[Bias Mitigation]]\n\t\t- enables:: [[Non-discrimination]], [[Equal Treatment]]",
  "backlinks": [
    "AI Model Card",
    "Stakeholder",
    "AI Governance Principle",
    "Loss Function"
  ],
  "wiki_links": [
    "Equal Treatment",
    "Bias Mitigation",
    "Bias Detection",
    "MetaverseDomain",
    "Metaverse",
    "Non-discrimination"
  ],
  "ontology": {
    "term_id": "AI-0065",
    "preferred_term": "Fairness",
    "alt_terms": [],
    "iri": "http://narrativegoldmine.com/metaverse#Fairness",
    "source_domain": null,
    "domain": "ai",
    "domain_full_name": "Artificial Intelligence",
    "definition": "The property of an AI system whereby it produces equitable outcomes and avoids creating or reinforcing unjustifiable disparities across different demographic groups or individuals, measured through various mathematical definitions and ethical principles.",
    "scope_note": null,
    "status": "draft",
    "maturity": "draft",
    "version": null,
    "public_access": true,
    "last_updated": null,
    "authority_score": null,
    "quality_score": null,
    "cross_domain_links": null,
    "owl_class": "mv:Fairness",
    "owl_physicality": "ConceptualEntity",
    "owl_role": "Concept",
    "owl_inferred_class": null,
    "is_subclass_of": [
      "Metaverse"
    ],
    "has_part": [],
    "is_part_of": [],
    "requires": [
      "Bias Detection",
      "Bias Mitigation"
    ],
    "depends_on": [],
    "enables": [
      "Non-discrimination",
      "Equal Treatment"
    ],
    "relates_to": [],
    "bridges_to": [],
    "bridges_from": [],
    "domain_extensions": {},
    "belongs_to_domain": [
      "MetaverseDomain"
    ],
    "implemented_in_layer": [],
    "source": [],
    "other_relationships": {
      "belongsToDomain": [
        "MetaverseDomain"
      ]
    },
    "validation": {
      "is_valid": false,
      "errors": [
        "Missing required property: source-domain",
        "Missing required property: last-updated",
        "owl:class namespace 'mv' doesn't match source-domain 'ai'"
      ]
    }
  }
}