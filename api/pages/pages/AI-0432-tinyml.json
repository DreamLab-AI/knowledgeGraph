{
  "id": "AI-0432-tinyml",
  "title": "TinyML",
  "content": "- ### OntologyBlock\n  id:: tinyml-(machine-learning-on-microcontrollers)-(ai-0432)-ontology\n  collapsed:: true\n\t- ontology:: true\n\t- term-id:: AI-0432\n\t- preferred-term:: TinyML\n\t- source-domain:: ai\n\t- status:: in\n\t- public-access:: true\n\t- definition:: TinyML is machine learning deployment on extremely resource-constrained microcontrollers with memory measured in kilobytes (typically 256KB RAM, 1MB flash storage), power consumption in milliwatts (under 1mW idle, a few mW active), and processing measured in MHz (typically 16-80MHz ARM Cortex-M cores), enabling intelligent inference at the extreme edge in battery-powered IoT devices, wearables, and embedded sensors. This paradigm enables always-on intelligent sensing at ultra-low power enabling applications infeasible with cloud connectivity including keyword spotting wake-word detection consuming under 1mW enabling multi-year battery life, gesture recognition processing accelerometer data locally for responsive interaction, anomaly detection in industrial sensors identifying equipment failures without connectivity, audio event classification recognizing sounds like glass breaking or baby crying for home automation, and predictive maintenance on rotating machinery analyzing vibration patterns to predict bearing failures. Implementation requires aggressive model optimization through INT8 quantization representing weights and activations in 8-bit integers (4x compression versus FP32), extreme pruning removing 70-95% of model weights while maintaining acceptable accuracy, knowledge distillation training compact student models mimicking larger teacher models, and architecture search discovering efficient neural architectures (MobileNet, EfficientNet variants) tailored for resource constraints. Key constraints include memory footprint where entire model must fit in RAM with typical limit 100KB for weights plus activation memory, computational budget constrained to deliver real-time inference within 10-50ms on CPUs without hardware accelerators, energy per inference typically 0.5mJ enabling 10,000+ inferences per mAh battery capacity, and fixed-point arithmetic as floating-point operations prohibitively expensive requiring software emulation or absent from hardware entirely. The TinyML ecosystem comprises frameworks including TensorFlow Lite for Microcontrollers (Google) supporting ARM Cortex-M deployment, Edge Impulse providing end-to-end workflow from data collection to deployment, uTensor enabling neural network inference on mbed-OS devices, and CMSIS-NN providing optimized neural network kernels for ARM Cortex-M processors, while benchmarks from MLPerf Tiny establish standardized metrics for comparing inference latency, accuracy, and energy consumption across TinyML implementations, with typical results showing 10ms keyword spotting inference consuming 0.5mJ on Cortex-M4 processors.\n\t- source:: [[TensorFlow Lite Micro]], [[TinyML Foundation]], [[MLPerf Tiny]]\n\t- maturity:: mature\n\t- owl:class:: ai:TinyML\n\t- owl:physicality:: VirtualEntity\n\t- owl:role:: Process\n\t- owl:inferred-class:: aigo:VirtualProcess\n\t- belongsToDomain:: [[AIEthicsDomain]]\n\t- implementedInLayer:: [[ConceptualLayer]]",
  "backlinks": [],
  "wiki_links": [
    "MLPerf Tiny",
    "TensorFlow Lite Micro",
    "ConceptualLayer",
    "AIEthicsDomain",
    "TinyML Foundation"
  ],
  "ontology": {
    "term_id": "AI-0432",
    "preferred_term": "TinyML",
    "alt_terms": [],
    "iri": "http://narrativegoldmine.com/ai#TinyML",
    "source_domain": "ai",
    "domain": "ai",
    "domain_full_name": "Artificial Intelligence",
    "definition": "TinyML is machine learning deployment on extremely resource-constrained microcontrollers with memory measured in kilobytes (typically 256KB RAM, 1MB flash storage), power consumption in milliwatts (under 1mW idle, a few mW active), and processing measured in MHz (typically 16-80MHz ARM Cortex-M cores), enabling intelligent inference at the extreme edge in battery-powered IoT devices, wearables, and embedded sensors. This paradigm enables always-on intelligent sensing at ultra-low power enabling applications infeasible with cloud connectivity including keyword spotting wake-word detection consuming under 1mW enabling multi-year battery life, gesture recognition processing accelerometer data locally for responsive interaction, anomaly detection in industrial sensors identifying equipment failures without connectivity, audio event classification recognizing sounds like glass breaking or baby crying for home automation, and predictive maintenance on rotating machinery analyzing vibration patterns to predict bearing failures. Implementation requires aggressive model optimization through INT8 quantization representing weights and activations in 8-bit integers (4x compression versus FP32), extreme pruning removing 70-95% of model weights while maintaining acceptable accuracy, knowledge distillation training compact student models mimicking larger teacher models, and architecture search discovering efficient neural architectures (MobileNet, EfficientNet variants) tailored for resource constraints. Key constraints include memory footprint where entire model must fit in RAM with typical limit 100KB for weights plus activation memory, computational budget constrained to deliver real-time inference within 10-50ms on CPUs without hardware accelerators, energy per inference typically 0.5mJ enabling 10,000+ inferences per mAh battery capacity, and fixed-point arithmetic as floating-point operations prohibitively expensive requiring software emulation or absent from hardware entirely. The TinyML ecosystem comprises frameworks including TensorFlow Lite for Microcontrollers (Google) supporting ARM Cortex-M deployment, Edge Impulse providing end-to-end workflow from data collection to deployment, uTensor enabling neural network inference on mbed-OS devices, and CMSIS-NN providing optimized neural network kernels for ARM Cortex-M processors, while benchmarks from MLPerf Tiny establish standardized metrics for comparing inference latency, accuracy, and energy consumption across TinyML implementations, with typical results showing 10ms keyword spotting inference consuming 0.5mJ on Cortex-M4 processors.",
    "scope_note": null,
    "status": "in",
    "maturity": "mature",
    "version": null,
    "public_access": true,
    "last_updated": null,
    "authority_score": null,
    "quality_score": null,
    "cross_domain_links": null,
    "owl_class": "ai:TinyML",
    "owl_physicality": "VirtualEntity",
    "owl_role": "Process",
    "owl_inferred_class": "aigo:VirtualProcess",
    "is_subclass_of": [],
    "has_part": [],
    "is_part_of": [],
    "requires": [],
    "depends_on": [],
    "enables": [],
    "relates_to": [],
    "bridges_to": [],
    "bridges_from": [],
    "domain_extensions": {},
    "belongs_to_domain": [
      "AIEthicsDomain"
    ],
    "implemented_in_layer": [
      "ConceptualLayer"
    ],
    "source": [
      "TensorFlow Lite Micro",
      "TinyML Foundation",
      "MLPerf Tiny"
    ],
    "validation": {
      "is_valid": false,
      "errors": [
        "Missing required property: last-updated",
        "Missing required property: is-subclass-of (at least one parent class)"
      ]
    }
  }
}