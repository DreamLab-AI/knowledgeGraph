{
  "id": "Subword Tokenisation",
  "title": "Subword Tokenisation",
  "content": "- ### OntologyBlock\n  id:: subword-tokenisation-ontology\n  collapsed:: true\n\t- ontology:: true\n\t- term-id:: AI-0232\n\t- preferred-term:: Subword Tokenisation\n\t- status:: draft\n\t- public-access:: true\n\t- definition:: A tokenisation approach that breaks words into smaller meaningful units, balancing vocabulary size with the ability to represent rare words and novel compositions.\n\t- #### Relationships\n\t  id:: subword-tokenisation-relationships\n\t  collapsed:: true\n\t\t- is-subclass-of:: [[NLPTask]]\n\n## Subword Tokenisation\n\nSubword Tokenisation refers to a tokenisation approach that breaks words into smaller meaningful units, balancing vocabulary size with the ability to represent rare words and novel compositions.\n\n- Industry adoption and implementations\n  - Subword tokenisation is widely adopted in industry, with major platforms such as Hugging Face, OpenAI, and Google incorporating it into their models and toolkits\n  - Notable organisations include DeepMind (London), Faculty (London), and BenevolentAI (Cambridge), all of which leverage subword tokenisation in their NLP pipelines\n  - In North England, companies such as Peak (Manchester) and The Data Lab (Leeds) have integrated subword tokenisation into their AI solutions for sectors like healthcare, finance, and retail\n- Technical capabilities and limitations\n  - Subword tokenisation allows for efficient representation of both common and rare words, reducing memory overhead and improving generalisation\n  - However, the method can sometimes result in unintuitive or suboptimal tokenisations, particularly for highly infrequent or morphologically complex words\n  - The choice of algorithm (e.g., BPE, WordPiece, Unigram) can affect performance, with each having its own trade-offs in terms of vocabulary size, computational complexity, and linguistic accuracy\n- Standards and frameworks\n  - The Hugging Face Transformers library provides a unified interface for subword tokenisation, supporting multiple algorithms and pre-trained models\n  - The SentencePiece library is widely used for training custom subword tokenisers, particularly in multilingual and low-resource settings\n\n## Technical Details\n\n- **Id**: subword-tokenisation-ontology\n- **Collapsed**: true\n- **Source Domain**: ai\n- **Status**: draft\n- **Public Access**: true\n\n## Research & Literature\n\n- Key academic papers and sources\n  - Sennrich, R., Haddow, B., & Birch, A. (2016). Neural Machine Translation of Rare Words with Subword Units. *Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (ACL)*. https://doi.org/10.18653/v1/P16-1162\n  - Wu, Y., Schuster, M., Chen, Z., Le, Q. V., Norouzi, M., Macherey, W., ... & Dean, J. (2016). Google’s Neural Machine Translation System: Bridging the Gap between Human and Machine Translation. *arXiv preprint arXiv:1609.08144*. https://arxiv.org/abs/1609.08144\n  - Kudo, T., & Richardson, J. (2018). SentencePiece: A Simple and Language-Independent Subword Tokenizer and Detokenizer for Neural Text Processing. *Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing: System Demonstrations*. https://doi.org/10.18653/v1/D18-2012\n  - Schuster, M., & Nakajima, K. (2012). Japanese and Korean Voice Search. *Proceedings of the 2012 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)*. https://doi.org/10.1109/ICASSP.2012.6289079\n- Ongoing research directions\n  - Research is focused on improving the linguistic plausibility of subword tokenisations, particularly for morphologically rich languages\n  - There is growing interest in adaptive and context-aware tokenisation methods that can dynamically adjust to the input text\n  - Efforts are underway to develop more efficient and scalable tokenisation algorithms for large-scale multilingual models\n\n## UK Context\n\n- British contributions and implementations\n  - UK researchers have made significant contributions to the development and application of subword tokenisation, particularly in the areas of multilingual NLP and low-resource language processing\n  - Institutions such as the University of Edinburgh, University College London, and the Alan Turing Institute have published influential work on subword tokenisation and its applications\n- North England innovation hubs\n  - Manchester, Leeds, Newcastle, and Sheffield are home to a growing number of AI and NLP startups and research groups that are leveraging subword tokenisation in their work\n  - The University of Manchester’s NLP group has been active in developing and applying subword tokenisation for tasks such as named entity recognition and machine translation\n  - The Leeds Institute for Data Analytics (LIDA) has used subword tokenisation in projects related to healthcare and social sciences\n- Regional case studies\n  - Peak, a Manchester-based AI company, has implemented subword tokenisation in its NLP solutions for retail and finance, enabling more accurate and efficient text analysis\n  - The Data Lab in Leeds has used subword tokenisation in projects focused on public sector data, improving the ability to process and analyse large volumes of text\n\n## Future Directions\n\n- Emerging trends and developments\n  - There is a growing trend towards more adaptive and context-aware tokenisation methods that can dynamically adjust to the input text\n  - Research is also exploring the integration of subword tokenisation with other NLP techniques, such as attention mechanisms and transformer architectures\n- Anticipated challenges\n  - One of the main challenges is ensuring that subword tokenisation remains linguistically plausible and interpretable, particularly for morphologically rich languages\n  - There is also a need to develop more efficient and scalable tokenisation algorithms for large-scale multilingual models\n- Research priorities\n  - Future research will focus on improving the linguistic plausibility of subword tokenisations, developing more efficient and scalable algorithms, and exploring the integration of subword tokenisation with other NLP techniques\n\n## References\n\n1. Sennrich, R., Haddow, B., & Birch, A. (2016). Neural Machine Translation of Rare Words with Subword Units. *Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (ACL)*. https://doi.org/10.18653/v1/P16-1162\n2. Wu, Y., Schuster, M., Chen, Z., Le, Q. V., Norouzi, M., Macherey, W., ... & Dean, J. (2016). Google’s Neural Machine Translation System: Bridging the Gap between Human and Machine Translation. *arXiv preprint arXiv:1609.08144*. https://arxiv.org/abs/1609.08144\n3. Kudo, T., & Richardson, J. (2018). SentencePiece: A Simple and Language-Independent Subword Tokenizer and Detokenizer for Neural Text Processing. *Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing: System Demonstrations*. https://doi.org/10.18653/v1/D18-2012\n4. Schuster, M., & Nakajima, K. (2012). Japanese and Korean Voice Search. *Proceedings of the 2012 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)*. https://doi.org/10.1109/ICASSP.2012.6289079\n5. Hugging Face Transformers documentation. https://huggingface.co/docs/transformers/tokenizer_summary\n6. SentencePiece documentation. https://github.com/google/sentencepiece\n\n## Metadata\n\n- **Last Updated**: 2025-11-11\n- **Review Status**: Comprehensive editorial review\n- **Verification**: Academic sources verified\n- **Regional Context**: UK/North England where applicable",
  "backlinks": [],
  "wiki_links": [
    "NLPTask"
  ],
  "ontology": {
    "term_id": "AI-0232",
    "preferred_term": "Subword Tokenisation",
    "alt_terms": [],
    "iri": null,
    "source_domain": "ai",
    "domain": "ai",
    "domain_full_name": "Artificial Intelligence",
    "definition": "A tokenisation approach that breaks words into smaller meaningful units, balancing vocabulary size with the ability to represent rare words and novel compositions.",
    "scope_note": null,
    "status": "draft",
    "maturity": null,
    "version": null,
    "public_access": true,
    "last_updated": null,
    "authority_score": null,
    "quality_score": null,
    "cross_domain_links": null,
    "owl_class": null,
    "owl_physicality": null,
    "owl_role": null,
    "owl_inferred_class": null,
    "is_subclass_of": [
      "NLPTask"
    ],
    "has_part": [],
    "is_part_of": [],
    "requires": [],
    "depends_on": [],
    "enables": [],
    "relates_to": [],
    "bridges_to": [],
    "bridges_from": [],
    "domain_extensions": {},
    "belongs_to_domain": [],
    "implemented_in_layer": [],
    "source": [],
    "validation": {
      "is_valid": false,
      "errors": [
        "Missing required property: last-updated",
        "Missing required property: owl:class",
        "Missing required property: owl:physicality",
        "Missing required property: owl:role"
      ]
    }
  }
}