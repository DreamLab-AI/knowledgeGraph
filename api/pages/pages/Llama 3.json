{
  "id": "Llama 3",
  "title": "Llama 3",
  "content": "- ## Llama 3 overview\n\t- Meta (formerly Facebook) has recently released LLaMA-3,  a series of foundational large language models (LLMs) that aim to advance AI research while remaining more accessible in terms of computational requirements.\n\t- ### Performance\n\t\t- LLaMA-3 models demonstrate competitive performance on various language modeling tasks, showcasing significant improvements over previous open weights models.\n\t\t- Integrated free across their social media platforms\n\t\t- Fast image generation, can be fine tunes for creatives.\n\t\t- [Introducing New AI Experiences Across Our Family of Apps and Devices | Meta (fb.com)](https://about.fb.com/news/2023/09/introducing-ai-powered-assistants-characters-and-creative-tools/)\n\t\t- [What’s up with Llama 3? Arena data analysis | LMSYS Org](https://lmsys.org/blog/2024-05-08-llama3/)\n\t\t- ![](https://lmsys.org/images/blog/llama3/topic_win_rate.png){:width 500}\n\t\t-\n\t- ### Efficiency\n\t\t- LLaMA-3 models are notably smaller than comparable LLMs. They were more efficient to train.\n\t\t- Sizes are currently:\n\t\t\t- 7B parameters, outperforming the previous 70B models on some metrics\n\t\t\t- 70B parameters, approaching or exceeding some closed source online models\n\t\t\t- 400B parameters (still in training), expected to outperform SOTA models\n\t\t- This efficiency allows them to run on less powerful hardware, broadening accessibility for researchers.\n\t\t\t- They can be fine tuned more easily.\n\t- ### Bias and Safety\n\t\t- Meta has taken active steps to assess and mitigate potential biases and harmful outputs. This is usually \"undone\" by the community at some stage for performance gains, raising important questions.\n\t- ### Open-Source Focus\n\t\t- Meta's release of the LLaMA-3 weights and code under a non-commercial license fosters transparency and encourages research collaboration.\n\t- ![1713640453190.jpeg](../assets/1713640453190_1713644111547_0.jpeg)\n- ## Acceptable Use Policy\n\t- **Open Source Controversy**\n\t\t- Llama 3 is claimed to be open source but faces criticisms.\n\t\t- Licence restrictions may not meet the [[Open Source]] Initiative's definition.\n\t\t- Restrictions on free use, modification, and redistribution.\n\t- **Acceptable Use Policy (AUP)**\n\t\t- Applicable to Llama 2, but also underpins Llama 3 license.\n\t\t- Prohibits:\n\t\t\t- Law violations.\n\t\t\t- Infringement of third-party rights.\n\t\t\t- Misuse of sensitive information.\n\t\t- Emphasizes compliance across jurisdictions.\n\t\t- Lacks specifics on consequences for policy violations.\n\t- **Intellectual Property Rights**\n\t\t- Grants non-exclusive, worldwide, non-transferable, royalty-free license.\n\t\t- Users can utilize and modify Llama Materials under conditions.\n\t\t- Meta retains ownership of foundational technology and Llama Materials.\n\t\t- Licensees own derivative works but with commercial use restrictions.\n\t- **Legal Implications**\n\t\t- Termination of license if licensee sues Meta for IP infringement.\n\t\t- Mandatory indemnification of Meta against third-party claims related to user's use of Llama.\n\t- **License and AUP Dynamics**\n\t\t- Unilateral updates to AUP by Meta.\n\t\t- Compliance with extensive laws and regulations required.\n\t\t- Significant operational constraints implied.\n\t\t- Liability limitations and warranty disclaimers shift risk to users.\n- # Fine-Tunes of Merit\n\t- [cognitivecomputations/dolphin-2.9-llama3-8b · Hugging Face](https://huggingface.co/cognitivecomputations/dolphin-2.9-llama3-8b)\n\t-",
  "backlinks": [],
  "wiki_links": [
    "Open Source"
  ]
}