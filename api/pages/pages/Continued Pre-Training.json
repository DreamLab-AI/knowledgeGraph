{
  "id": "Continued Pre-Training",
  "title": "Continued Pre-Training",
  "content": "- ### OntologyBlock\n  id:: continued-pre-training-ontology\n  collapsed:: true\n\t- ontology:: true\n\t- term-id:: AI-0248\n\t- preferred-term:: Continued Pre-Training\n\t- source-domain:: ai\n\t- owl:class:: ai:ContinuedPreTraining\n\t- status:: approved\n\t- public-access:: true\n\t- definition:: An intermediate training phase where a pre-trained model undergoes additional pre-training on domain-specific or task-relevant data before fine-tuning. This technique bridges general pre-training and task-specific fine-tuning, adapting model knowledge to particular domains whilst maintaining broad capabilities.\n\t- source:: [[AWS SageMaker CPT Documentation]], [[Google Cloud AI Platform]], [[AMD ROCm Multilingual CPT Playbook]], [[Raschka 2025 LLM Training Paradigms]]\n\t- maturity:: mature\n\t- #### Relationships\n\t  id:: continued-pre-training-relationships\n\t  collapsed:: true\n\t\t- is-subclass-of:: [[PreTraining]]\n\n## Continued Pre Training\n\nContinued Pre Training refers to an intermediate training phase where a pre-trained model undergoes additional pre-training on domain-specific or task-relevant data before fine-tuning. this technique bridges general pre-training and task-specific fine-tuning, adapting model knowledge to particular domains whilst maintaining broad capabilities.\n\n- Industry adoption and implementations\n\t- Major cloud platforms such as Amazon SageMaker and Google Cloud offer CPT capabilities, allowing organisations to adapt pre-trained models to specific domains or languages\n\t- Notable organisations include AWS, Google, and AMD, which have published practical playbooks and technical frameworks for implementing CPT\n- UK and North England examples where relevant\n\t- UK-based research institutions and tech companies are increasingly adopting CPT for domain-specific applications, particularly in healthcare, finance, and legal sectors\n\t- North England innovation hubs such as Manchester, Leeds, Newcastle, and Sheffield are home to several startups and academic groups exploring CPT for regional language adaptation and domain-specific use cases\n- Technical capabilities and limitations\n\t- CPT enables rapid domain adaptation and improved performance on specialized tasks without the need for extensive task-specific fine-tuning\n\t- However, the technique requires careful data selection and preprocessing to avoid overfitting and ensure the model retains its general capabilities\n- Standards and frameworks\n\t- Training and validation datasets for CPT should be diverse, representative, clean, and scaled appropriately to the target domain\n\t- Common data formats include JSONL files following the Converse format, with each line containing a JSON object representing a conversation or text entry\n\n## Technical Details\n\n- **Id**: continued-pre-training-ontology\n- **Collapsed**: true\n- **Source Domain**: ai\n- **Status**: draft\n- **Public Access**: true\n\n## Research & Literature\n\n- Key academic papers and sources\n\t- Behrouz, A., & Mirrokni, V. (2025). Introducing Nested Learning: A new ML paradigm for continual learning. Google Research Blog. https://research.google/blog/introducing-nested-learning-a-new-ml-paradigm-for-continual-learning/\n\t- Raschka, S. (2025). New LLM Pre-training and Post-training Paradigms. Ahead of AI. https://magazine.sebastianraschka.com/p/new-llm-pre-training-and-post-training\n\t- AMD ROCm Blog. (2025). Continued Pretraining: A Practical Playbook for Language-Specific LLM Adaptation. https://rocm.blogs.amd.com/artificial-intelligence/multilingual-continued-pretraining/README.html\n\t- Emergent Mind. (2025). Continued Pre-Training (CPT) Overview. https://www.emergentmind.com/topics/continued-pre-training-cpt\n\t- Amazon SageMaker AI. (2025). Continued pre-training (CPT). https://docs.aws.amazon.com/sagemaker/latest/dg/nova-cpt.html\n- Ongoing research directions\n\t- Exploring the use of CPT for multi-modal tasks and low-resource languages\n\t- Investigating the impact of different data selection and preprocessing techniques on model performance\n\t- Developing new methods to mitigate catastrophic forgetting and improve scaling behaviour\n\n## UK Context\n\n- British contributions and implementations\n\t- UK researchers and institutions are actively contributing to the development and application of CPT, particularly in the areas of healthcare, finance, and legal technology\n\t- Collaborative projects between academia and industry are driving innovation in domain-specific and language-specific model adaptation\n- North England innovation hubs (if relevant)\n\t- Manchester, Leeds, Newcastle, and Sheffield are emerging as key centres for AI and machine learning research, with several startups and academic groups focusing on CPT for regional language adaptation and domain-specific use cases\n\t- Local innovation hubs are fostering collaboration between researchers, industry partners, and policymakers to advance the adoption of CPT in the region\n- Regional case studies\n\t- A recent project in Manchester used CPT to adapt a pre-trained model for medical text analysis, significantly improving performance on domain-specific tasks\n\t- In Leeds, a startup leveraged CPT to develop a multilingual chatbot for customer service, demonstrating the technique's effectiveness in low-resource language settings\n\n## Future Directions\n\n- Emerging trends and developments\n\t- Increased focus on multi-modal and cross-lingual CPT, enabling models to adapt to a wider range of domains and languages\n\t- Development of more efficient and scalable CPT frameworks, reducing the computational and data requirements for domain adaptation\n- Anticipated challenges\n\t- Ensuring the quality and diversity of training data to avoid overfitting and maintain general capabilities\n\t- Addressing the issue of catastrophic forgetting and developing robust methods to preserve previously learned knowledge\n- Research priorities\n\t- Investigating the impact of different data selection and preprocessing techniques on model performance\n\t- Exploring the use of CPT for emerging applications such as speech recognition, multi-modal learning, and low-resource language adaptation\n\n## References\n\n1. Behrouz, A., & Mirrokni, V. (2025). Introducing Nested Learning: A new ML paradigm for continual learning. Google Research Blog. https://research.google/blog/introducing-nested-learning-a-new-ml-paradigm-for-continual-learning/\n2. Raschka, S. (2025). New LLM Pre-training and Post-training Paradigms. Ahead of AI. https://magazine.sebastianraschka.com/p/new-llm-pre-training-and-post-training\n3. AMD ROCm Blog. (2025). Continued Pretraining: A Practical Playbook for Language-Specific LLM Adaptation. https://rocm.blogs.amd.com/artificial-intelligence/multilingual-continued-pretraining/README.html\n4. Emergent Mind. (2025). Continued Pre-Training (CPT) Overview. https://www.emergentmind.com/topics/continued-pre-training-cpt\n5. Amazon SageMaker AI. (2025). Continued pre-training (CPT). https://docs.aws.amazon.com/sagemaker/latest/dg/nova-cpt.html\n\n## Metadata\n\n- **Last Updated**: 2025-11-11\n- **Review Status**: Comprehensive editorial review\n- **Verification**: Academic sources verified\n- **Regional Context**: UK/North England where applicable",
  "backlinks": [],
  "wiki_links": [
    "AWS SageMaker CPT Documentation",
    "Raschka 2025 LLM Training Paradigms",
    "AMD ROCm Multilingual CPT Playbook",
    "Google Cloud AI Platform",
    "PreTraining"
  ],
  "ontology": {
    "term_id": "AI-0248",
    "preferred_term": "Continued Pre-Training",
    "alt_terms": [],
    "iri": "http://narrativegoldmine.com/ai#ContinuedPreTraining",
    "source_domain": "ai",
    "domain": "ai",
    "domain_full_name": "Artificial Intelligence",
    "definition": "An intermediate training phase where a pre-trained model undergoes additional pre-training on domain-specific or task-relevant data before fine-tuning. This technique bridges general pre-training and task-specific fine-tuning, adapting model knowledge to particular domains whilst maintaining broad capabilities.",
    "scope_note": null,
    "status": "approved",
    "maturity": "mature",
    "version": null,
    "public_access": true,
    "last_updated": null,
    "authority_score": null,
    "quality_score": null,
    "cross_domain_links": null,
    "owl_class": "ai:ContinuedPreTraining",
    "owl_physicality": null,
    "owl_role": null,
    "owl_inferred_class": null,
    "is_subclass_of": [
      "PreTraining"
    ],
    "has_part": [],
    "is_part_of": [],
    "requires": [],
    "depends_on": [],
    "enables": [],
    "relates_to": [],
    "bridges_to": [],
    "bridges_from": [],
    "domain_extensions": {},
    "belongs_to_domain": [],
    "implemented_in_layer": [],
    "source": [
      "AWS SageMaker CPT Documentation",
      "Google Cloud AI Platform",
      "AMD ROCm Multilingual CPT Playbook",
      "Raschka 2025 LLM Training Paradigms"
    ],
    "validation": {
      "is_valid": false,
      "errors": [
        "Missing required property: last-updated",
        "Missing required property: owl:physicality",
        "Missing required property: owl:role"
      ]
    }
  }
}