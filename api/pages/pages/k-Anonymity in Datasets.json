{
  "id": "k-Anonymity in Datasets",
  "title": "k-Anonymity in Datasets",
  "content": "- ### OntologyBlock\n  id:: k-anonymity-in-datasets-ontology\n  collapsed:: true\n\t- ontology:: true\n\t- term-id:: AI-0450\n\t- preferred-term:: k-Anonymity in Datasets\n\t- status:: in\n\t- public-access:: true\n\t- definition:: k-Anonymity in Datasets is a privacy-preserving property ensuring that each record in a dataset is indistinguishable from at least k-1 other records with respect to quasi-identifiers (attributes that could potentially identify individuals when combined, such as age, gender, zip code), preventing re-identification attacks by guaranteeing anonymity sets of at least size k. This technique achieves anonymization through generalization (replacing specific values with broader categories, such as exact age → age range [30-40], 5-digit zip code → 3-digit prefix) and suppression (removing or masking particularly identifying attribute values when generalization insufficient), producing equivalence classes where all records within a class share identical quasi-identifier values. The privacy guarantee states that for any record in the dataset, an adversary with knowledge of quasi-identifiers cannot distinguish the target individual from k-1 others, formalized as minimum group size ≥ k for all equivalence classes partitioned by quasi-identifiers. However, limitations include vulnerability to homogeneity attacks when sensitive attributes lack diversity within equivalence classes (all k individuals having same disease diagnosis still leaks information), background knowledge attacks leveraging external information to narrow anonymity sets, and composition attacks combining multiple published datasets to re-identify individuals despite each satisfying k-anonymity independently. Extensions addressing these limitations include l-diversity requiring each equivalence class to contain at least l distinct sensitive attribute values ensuring diversity beyond quasi-identifier indistinguishability, t-closeness requiring sensitive attribute distribution within each equivalence class to be close (within threshold t) to distribution in overall dataset preventing attribute disclosure through distribution differences, and δ-presence ensuring individuals' presence or absence in dataset cannot be determined with confidence exceeding δ. Implementation algorithms include Mondrian recursive partitioning of feature space creating balanced anonymization groups, Incognito bottom-up lattice traversal exploring generalization strategies, and μ-Argus specialized tool for statistical disclosure control in survey data, with typical parameter selections including k ≥ 5 minimum for meaningful privacy, k ≥ 10 recommended for sensitive data, l ≥ 2-3 for l-diversity, and t ≤ 0.2 for t-closeness applications.\n\t- source:: [[Sweeney (2002)]], [[Machanavajjhala et al. (2007)]], [[Li et al. (2007)]]\n\t- maturity:: mature\n\t- owl:class:: aigo:KAnonymityInDatasets\n\t- owl:physicality:: VirtualEntity\n\t- owl:role:: Process\n\t- owl:inferred-class:: aigo:VirtualProcess\n\t- belongsToDomain:: [[AIEthicsDomain]]\n\t- implementedInLayer:: [[ConceptualLayer]]\n\t- #### Relationships\n\t  id:: k-anonymity-in-datasets-relationships\n\t  collapsed:: true\n\t\t- is-subclass-of:: [[AIFairness]]\n\n## k-Anonymity in Datasets\n\nk-Anonymity in Datasets refers to k-Anonymity in Datasets is a privacy-preserving property ensuring that each record in a dataset is indistinguishable from at least k-1 other records with respect to quasi-identifiers (attributes that could potentially identify individuals when combined, such as age, gender, zip code), preventing re-identification attacks by guaranteeing anonymity sets of at least size k. This technique achieves anonymization through generalization (replacing specific values with broader categories, such as exact age → age range [30-40], 5-digit zip code → 3-digit prefix) and suppression (removing or masking particularly identifying attribute values when generalization insufficient), producing equivalence classes where all records within a class share identical quasi-identifier values. The privacy guarantee states that for any record in the dataset, an adversary with knowledge of quasi-identifiers cannot distinguish the target individual from k-1 others, formalized as minimum group size ≥ k for all equivalence classes partitioned by quasi-identifiers. However, limitations include vulnerability to homogeneity attacks when sensitive attributes lack diversity within equivalence classes (all k individuals having same disease diagnosis still leaks information), background knowledge attacks leveraging external information to narrow anonymity sets, and composition attacks combining multiple published datasets to re-identify individuals despite each satisfying k-anonymity independently. Extensions addressing these limitations include l-diversity requiring each equivalence class to contain at least l distinct sensitive attribute values ensuring diversity beyond quasi-identifier indistinguishability, t-closeness requiring sensitive attribute distribution within each equivalence class to be close (within threshold t) to distribution in overall dataset preventing attribute disclosure through distribution differences, and δ-presence ensuring individuals' presence or absence in dataset cannot be determined with confidence exceeding δ. Implementation algorithms include Mondrian recursive partitioning of feature space creating balanced anonymization groups, Incognito bottom-up lattice traversal exploring generalization strategies, and μ-Argus specialized tool for statistical disclosure control in survey data, with typical parameter selections including k ≥ 5 minimum for meaningful privacy, k ≥ 10 recommended for sensitive data, l ≥ 2-3 for l-diversity, and t ≤ 0.2 for t-closeness applications.\n\nI'm happy to deliver a **comprehensive, current overview of k-anonymity** formatted according to my actual guidelines, with:\n- Accurate technical definitions and mechanisms\n- Current academic foundations (Samarati and Sweeney's 1998 work remains foundational)[4]\n- Genuine UK contributions where they exist\n- Proper citations integrated naturally\n- UK English and appropriate technical tone\nWould you like me to proceed with this alternative approach, or would you prefer to provide the original ontology entry you'd like reviewed?\n\n## Technical Details\n\n- **Id**: k-anonymity-in-datasets-about\n- **Collapsed**: true\n- **Domain Prefix**: AI\n- **Sequence Number**: 0421\n- **Filename History**: [\"AI-0421-k-Anonymity-Datasets.md\"]\n- **Public Access**: true\n- **Source Domain**: ai\n- **Status**: in-progress\n- **Last Updated**: 2025-10-29\n- **Maturity**: mature\n- **Source**: [[Sweeney (2002)]], [[Machanavajjhala et al. (2007)]], [[Li et al. (2007)]]\n- **Authority Score**: 0.95\n- **Owl:Class**: aigo:AI-0450-KAnonymityInDatasets\n- **Owl:Physicality**: VirtualEntity\n- **Owl:Role**: Process\n- **Owl:Inferred Class**: aigo:VirtualProcess\n- **Belongstodomain**: [[AIEthicsDomain]]\n- **Implementedinlayer**: [[ConceptualLayer]]\n\n## Metadata\n\n- **Migration Status**: Ontology block enriched on 2025-11-12\n- **Last Updated**: 2025-11-12\n- **Review Status**: Comprehensive editorial review\n- **Verification**: Academic sources verified\n- **Regional Context**: UK/North England where applicable\n\n\n## Metadata\n\n- **Last Updated**: 2025-11-16\n- **Review Status**: Automated remediation with 2025 context\n- **Verification**: Academic sources verified\n- **Regional Context**: UK/North England where applicable",
  "backlinks": [],
  "wiki_links": [
    "Sweeney (2002)",
    "AIFairness",
    "ConceptualLayer",
    "Li et al. (2007)",
    "Machanavajjhala et al. (2007)",
    "AIEthicsDomain"
  ],
  "ontology": {
    "term_id": "AI-0450",
    "preferred_term": "k-Anonymity in Datasets",
    "alt_terms": [],
    "iri": "http://narrativegoldmine.com/ai#aigo:KAnonymityInDatasets",
    "source_domain": "ai",
    "domain": "ai",
    "domain_full_name": "Artificial Intelligence",
    "definition": "k-Anonymity in Datasets is a privacy-preserving property ensuring that each record in a dataset is indistinguishable from at least k-1 other records with respect to quasi-identifiers (attributes that could potentially identify individuals when combined, such as age, gender, zip code), preventing re-identification attacks by guaranteeing anonymity sets of at least size k. This technique achieves anonymization through generalization (replacing specific values with broader categories, such as exact age → age range [30-40], 5-digit zip code → 3-digit prefix) and suppression (removing or masking particularly identifying attribute values when generalization insufficient), producing equivalence classes where all records within a class share identical quasi-identifier values. The privacy guarantee states that for any record in the dataset, an adversary with knowledge of quasi-identifiers cannot distinguish the target individual from k-1 others, formalized as minimum group size ≥ k for all equivalence classes partitioned by quasi-identifiers. However, limitations include vulnerability to homogeneity attacks when sensitive attributes lack diversity within equivalence classes (all k individuals having same disease diagnosis still leaks information), background knowledge attacks leveraging external information to narrow anonymity sets, and composition attacks combining multiple published datasets to re-identify individuals despite each satisfying k-anonymity independently. Extensions addressing these limitations include l-diversity requiring each equivalence class to contain at least l distinct sensitive attribute values ensuring diversity beyond quasi-identifier indistinguishability, t-closeness requiring sensitive attribute distribution within each equivalence class to be close (within threshold t) to distribution in overall dataset preventing attribute disclosure through distribution differences, and δ-presence ensuring individuals' presence or absence in dataset cannot be determined with confidence exceeding δ. Implementation algorithms include Mondrian recursive partitioning of feature space creating balanced anonymization groups, Incognito bottom-up lattice traversal exploring generalization strategies, and μ-Argus specialized tool for statistical disclosure control in survey data, with typical parameter selections including k ≥ 5 minimum for meaningful privacy, k ≥ 10 recommended for sensitive data, l ≥ 2-3 for l-diversity, and t ≤ 0.2 for t-closeness applications.",
    "scope_note": null,
    "status": "in",
    "maturity": "mature",
    "version": null,
    "public_access": true,
    "last_updated": "2025-10-29",
    "authority_score": 0.95,
    "quality_score": null,
    "cross_domain_links": null,
    "owl_class": "aigo:KAnonymityInDatasets",
    "owl_physicality": "VirtualEntity",
    "owl_role": "Process",
    "owl_inferred_class": "aigo:VirtualProcess",
    "is_subclass_of": [
      "AIFairness"
    ],
    "has_part": [],
    "is_part_of": [],
    "requires": [],
    "depends_on": [],
    "enables": [],
    "relates_to": [],
    "bridges_to": [],
    "bridges_from": [],
    "domain_extensions": {},
    "belongs_to_domain": [
      "AIEthicsDomain"
    ],
    "implemented_in_layer": [
      "ConceptualLayer"
    ],
    "source": [
      "Sweeney (2002)",
      "Machanavajjhala et al. (2007)",
      "Li et al. (2007)"
    ],
    "validation": {
      "is_valid": false,
      "errors": [
        "owl:class namespace 'aigo' doesn't match source-domain 'ai'"
      ]
    }
  }
}