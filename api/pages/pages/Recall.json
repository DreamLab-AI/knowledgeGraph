{
  "id": "Recall",
  "title": "Recall",
  "content": "- ### OntologyBlock\n  id:: recall-ontology\n  collapsed:: true\n\t- ontology:: true\n\t- term-id:: AI-0109\n\t- preferred-term:: Recall\n\t- source-domain:: metaverse\n\t- status:: draft\n\t- public-access:: true\n\n\n\n\n### OWL Classification\n\t- owl:class:: mv:Recall\n\t- owl:physicality:: ConceptualEntity\n\t- owl:role:: Concept\n\n### Domain & Architecture\n\t- belongsToDomain:: [[MetaverseDomain]]\n\t- maturity:: draft\n\n### Relationships\nid:: recall-relationships\n\t\t- is-subclass-of:: [[Metaverse]]\n\n## Mathematical Definition\n\n$$\\text{Recall} = \\frac{TP}{TP + FN} = \\frac{TP}{\\text{All Actual Positives}}$$\n\nWhere:\n- **TP** (True Positives): Correctly identified positive instances\n- **FN** (False Negatives): Missed positive instances (Type II errors)\n\nAlso known as **Sensitivity**, **True Positive Rate (TPR)**, or **Hit Rate**.\n\n## Context and Significance\n\nRecall answers the question: \"Out of all actual positive cases, how many did the model find?\" This metric is essential in scenarios where missing positive cases is particularly costly or dangerous—such as disease screening (missing cancer cases), security threat detection (missing threats), or quality control (missing defects). High recall ensures comprehensive detection of positive instances, though it says nothing about how many negative instances are incorrectly flagged (that's related to precision and specificity).\n\nRecall trades off with precision: achieving 100% recall is trivial (predict every instance as positive) but results in terrible precision. The challenge lies in maintaining high recall whilst managing false positive rates, with application-specific requirements determining the appropriate balance.\n\n## Key Characteristics\n\n- **False negative focus**: Emphasises minimising missed positive cases\n- **Completeness metric**: Measures thoroughness of positive case detection\n- **Trade-off with precision**: Improving recall often reduces precision\n- **Threshold-sensitive**: For probabilistic classifiers, varies with classification threshold\n- **Class-specific**: Typically calculated per class in multi-class settings\n- **Imbalance-affected**: Can be high in imbalanced datasets by predicting majority class\n\n## Appropriate Use Cases\n\n**When Recall is Critical:**\n- Medical disease screening: Catching all potential cases paramount\n- Security threat detection: Missing threats has severe consequences\n- Fraud detection (certain contexts): Identifying all fraud attempts critical\n- Manufacturing defect detection: Missing defects compromises safety\n- Search and rescue operations: Finding all victims essential\n- Legal compliance: Identifying all relevant documents or transactions\n\n**When to Prioritise Precision Instead:**\n- Spam filtering: Blocking legitimate emails more harmful than missing spam\n- Content moderation (certain contexts): Over-removal damages user experience\n- Quality control (certain contexts): False rejections costly\n\n**When to Balance Both:**\n- Use F1 score or other balanced metrics\n- Adjust threshold to application-specific precision-recall trade-off\n\n## Relationships\n\n- **Component of**: Model Performance assessment\n- **Calculated from**: Confusion Matrix (TP and FN)\n- **Synonym for**: Sensitivity, True Positive Rate (TPR)\n- **Complement of**: False Negative Rate (FNR = 1 - Recall)\n- **Trades off with**: Precision\n- **Harmonised in**: F1 Score (harmonic mean of precision and recall)\n- **Visualised via**: Precision-Recall Curve, ROC Curve (recall as y-axis)\n- **Paired with**: Specificity (for medical/diagnostic contexts)\n- **Used in**: Model Evaluation, threshold selection, performance monitoring\n- **Reported in**: Model Cards, clinical validation reports, audit documentation\n\n## Examples and Applications\n\n1. **Cancer Screening Test**: Recall of 95% means test identifies 95 out of 100 actual cancer cases, missing 5—high recall critical for early detection despite false positives requiring follow-up\n2. **Airport Security Screening**: Threat detection with 99.9% recall catches 999 out of 1,000 actual threats—extremely high recall necessary despite inconvenience of false alarms (low precision acceptable)\n3. **Email Spam Filter**: Spam detection with 85% recall catches 85 out of 100 spam emails, allowing 15 through—lower recall acceptable as users can delete spam, but high precision critical to avoid filtering legitimate mail\n4. **Manufacturing Defect Detection**: Quality control with 92% recall identifies 92 out of 100 defective products—remaining 8% reach customers, requiring balance with inspection costs (precision)\n\n## Calculation and Implementation\n\n**Standard Calculation:**\n```python\nfrom sklearn.metrics import recall_score\n\nrecall = recall_score(y_true, y_pred)\n# For multi-class: specify average parameter\n# 'micro', 'macro', 'weighted', or None for per-class\n```\n\n**Manual Calculation:**\n```python\ntrue_positives = sum((y_true == 1) & (y_pred == 1))\nfalse_negatives = sum((y_true == 1) & (y_pred == 0))\nrecall = true_positives / (true_positives + false_negatives)\n```\n\n**Threshold Optimization:**\nFor probabilistic classifiers, recall varies with classification threshold:\n```python\nfrom sklearn.metrics import precision_recall_curve\n\nprecisions, recalls, thresholds = precision_recall_curve(y_true, y_scores)\n# Select threshold based on minimum recall requirement\n```\n\n## Implementation Considerations\n\n**Best Practices:**\n- Report recall alongside precision and F1 score\n- Calculate per-class recall in multi-class problems\n- Use precision-recall curves to select appropriate thresholds\n- Disaggregate recall across demographic groups for fairness assessment\n- Establish minimum acceptable recall based on false negative costs\n- Monitor recall trends over time to detect degradation\n- In medical contexts, report sensitivity (recall) with specificity\n\n**Common Pitfalls:**\n- Achieving high recall by predicting everything as positive (ignoring precision)\n- Not accounting for class imbalance when interpreting recall\n- Using micro-averaging in multi-class settings, obscuring per-class performance\n- Failing to adjust thresholds for deployment context\n- Ignoring recall variations across demographic subgroups (fairness issue)\n\n**Precision-Recall Trade-off Management:**\n- Use precision-recall curve to visualise trade-off across thresholds\n- Set threshold based on relative costs of false positives vs. false negatives\n- Consider cascaded classifiers: high-recall initial filter, high-precision refinement\n- Employ cost-sensitive learning to optimise application-specific objectives\n- Use calibration to improve reliability of probability estimates for threshold setting\n\n## Variants and Related Metrics\n\n**Micro-averaged Recall** (multi-class): Aggregate TP and FN across classes\n$$\\text{Recall}_{\\text{micro}} = \\frac{\\sum_i TP_i}{\\sum_i (TP_i + FN_i)}$$\n\n**Macro-averaged Recall** (multi-class): Average of per-class recalls\n$$\\text{Recall}_{\\text{macro}} = \\frac{1}{n}\\sum_i \\text{Recall}_i$$\n\n**Weighted Recall**: Recall averaged across classes weighted by support\n\n**Recall@K**: Proportion of relevant items in top K recommendations (ranking tasks)\n\n**Sensitivity Analysis**: In medical contexts, often reported as sensitivity with confidence intervals\n\n## ISO/IEC and Standards Alignment\n\n**ISO/IEC 25059** (Quality Model for AI Systems):\n- Recall as metric for functional completeness\n- Coverage of actual positive cases\n\n**ISO/IEC 25024** (Data Quality Metrics):\n- Recall in context of output completeness measurement\n\n## NIST AI RMF Integration\n\n**MEASURE Function**:\n- MEASURE-2.2: Appropriate metrics including recall selected based on application risks\n- MEASURE-2.3: Recall measured across different contexts and subgroups\n- Recall critical for Safety (detecting hazards) and Reliability trustworthiness characteristics\n\n## Medical and Diagnostic Context\n\nIn medical and diagnostic testing, recall (sensitivity) is conventionally reported alongside **specificity** (true negative rate):\n\n**Sensitivity (Recall)**: Ability to correctly identify those with condition\n**Specificity**: Ability to correctly identify those without condition\n$$\\text{Specificity} = \\frac{TN}{TN + FP}$$\n\nTogether, sensitivity and specificity provide comprehensive picture of diagnostic test performance.\n\n## Related Terms\n\n- [[Model Performance]]: Broader concept including recall\n- [[Precision]]: Complementary metric for prediction reliability\n- [[F1 Score]]: Harmonic mean balancing precision and recall\n- [[Accuracy]]: Overall correctness metric\n- [[Confusion Matrix]]: Source of recall calculation\n- [[Precision-Recall Curve]]: Visualisation of precision-recall trade-off\n- [[ROC Curve]]: Visualisation using recall (TPR) and FPR\n- [[Sensitivity]]: Synonym for recall\n- [[True Positive Rate]]: Synonym for recall\n- [[False Negative]]: Missed positives affecting recall\n\n## References\n\n1. Powers, D.M.W., *Evaluation: From Precision, Recall and F-Measure to ROC, Informedness, Markedness & Correlation* (2011)\n2. Davis, J. & Goadrich, M., *The Relationship Between Precision-Recall and ROC Curves*, ICML (2006)\n3. ISO/IEC 25059, *Software engineering — Systems and software Quality Requirements and Evaluation (SQuaRE) — Quality model for AI systems*\n4. Saito, T. & Rehmsmeier, M., *The Precision-Recall Plot Is More Informative than the ROC Plot When Evaluating Binary Classifiers on Imbalanced Datasets*, PLOS ONE (2015)\n\n## Formal Ontology\n\n<details>\n<summary>OWL Functional Syntax</summary>\n\n```clojure\n;; Class Declaration\n(Declaration (Class :Recall))\n(SubClassOf :Recall :PerformanceMetric)\n(SubClassOf :Recall :ClassificationMetric)\n\n;; Synonyms\n(Declaration (Class :Sensitivity))\n(Declaration (Class :TruePositiveRate))\n(Declaration (Class :HitRate))\n(EquivalentClasses :Recall :Sensitivity :TruePositiveRate :HitRate)\n\n;; Core relationships\n(SubClassOf :Recall\n  (ObjectSomeValuesFrom :measuresPerformanceOf :MachineLearningModel))\n(SubClassOf :Recall\n  (ObjectSomeValuesFrom :calculatedFrom :ConfusionMatrix))\n(SubClassOf :Recall\n  (ObjectSomeValuesFrom :tradeOffWith :Precision))\n(SubClassOf :Recall\n  (ObjectSomeValuesFrom :harmonisedIn :F1Score))\n(SubClassOf :Recall\n  (ObjectSomeValuesFrom :visualisedVia :ROCCurve))\n\n;; Metric properties\n(Declaration (DataProperty :hasRecallValue))\n(DataPropertyDomain :hasRecallValue :Recall)\n(DataPropertyRange :hasRecallValue xsd:float)\n(FunctionalDataProperty :hasRecallValue)\n\n(Declaration (DataProperty :truePositiveCount))\n(DataPropertyDomain :truePositiveCount :Recall)\n(DataPropertyRange :truePositiveCount xsd:integer)\n\n(Declaration (DataProperty :falseNegativeCount))\n(DataPropertyDomain :falseNegativeCount :Recall)\n(DataPropertyRange :falseNegativeCount xsd:integer)\n\n;; Complement relationship\n(Declaration (Class :FalseNegativeRate))\n(Declaration (ObjectProperty :complementOf))\n(SubClassOf :Recall\n  (ObjectSomeValuesFrom :complementOf :FalseNegativeRate))\n\n;; Value constraints\n(SubClassOf :Recall\n  (DataPropertyRestriction\n    :hasRecallValue\n    (MinInclusiveDataRange 0.0^^xsd:float)\n    (MaxInclusiveDataRange 1.0^^xsd:float)))\n\n;; Use case focus\n(Declaration (Class :FalseNegativeMinimisation))\n(SubClassOf :Recall\n  (ObjectSomeValuesFrom :optimisesFor :FalseNegativeMinimisation))\n\n;; Critical applications\n(Declaration (Class :MedicalDiagnostics))\n(Declaration (Class :SecurityThreatDetection))\n(Declaration (Class :DefectDetection))\n(Declaration (ObjectProperty :criticalFor))\n\n(SubClassOf :Recall\n  (ObjectUnionValuesFrom :criticalFor\n    :MedicalDiagnostics :SecurityThreatDetection :DefectDetection))\n\n;; Medical context pairing\n(Declaration (Class :Specificity))\n(Declaration (ObjectProperty :pairedWith))\n(SubClassOf :Recall\n  (ObjectSomeValuesFrom :pairedWith :Specificity))\n\n;; Annotations\n(AnnotationAssertion rdfs:label :Recall \"Recall\"@en)\n(AnnotationAssertion rdfs:label :Recall \"Sensitivity\"@en)\n(AnnotationAssertion rdfs:label :Recall \"True Positive Rate\"@en)\n(AnnotationAssertion rdfs:comment :Recall\n  \"A classification performance metric representing the proportion of actual positive instances that an artificial intelligence model correctly identifies.\"@en)\n(AnnotationAssertion dcterms:source :Recall <https://www.iso.org/standard/74438.html>)\n(AnnotationAssertion :termID :Recall \"AI-0109\"^^xsd:string)\n(AnnotationAssertion :mathematicalFormula :Recall\n  \"TP / (TP + FN)\"^^xsd:string)\n\n;; ISO/IEC alignment\n(AnnotationAssertion :alignedWith :Recall :ISO25059)\n(AnnotationAssertion :alignedWith :Recall :ISO25024)\n(AnnotationAssertion :alignedWith :Recall :NISTAIRFM_MEASURE22)\n```\n\n</details>\n\n## See Also\n\n- [[Model Performance]]\n- [[Precision]]\n- [[F1 Score]]\n- [[Accuracy]]\n- [[Confusion Matrix]]\n- [[Precision-Recall Curve]]\n- [[ROC Curve]]\n- [[Sensitivity]]\n- [[Specificity]]\n- [[True Positive Rate]]\n\t- maturity:: draft\n\t- owl:class:: mv:Recall\n\t- owl:physicality:: ConceptualEntity\n\t- owl:role:: Concept\n\t- belongsToDomain:: [[MetaverseDomain]]\n\t- #### Relationships\n\t  id:: recall-relationships\n\t\t- is-subclass-of:: [[Metaverse]]",
  "backlinks": [
    "Accuracy",
    "Precision"
  ],
  "wiki_links": [
    "Sensitivity",
    "ROC Curve",
    "Accuracy",
    "Metaverse",
    "Model Performance",
    "False Negative",
    "Specificity",
    "MetaverseDomain",
    "Precision-Recall Curve",
    "Precision",
    "Confusion Matrix",
    "F1 Score",
    "True Positive Rate"
  ],
  "ontology": {
    "term_id": "AI-0109",
    "preferred_term": "Recall",
    "alt_terms": [],
    "iri": "http://narrativegoldmine.com/metaverse#Recall",
    "source_domain": "metaverse",
    "domain": "metaverse",
    "domain_full_name": "",
    "definition": null,
    "scope_note": null,
    "status": "draft",
    "maturity": "draft",
    "version": null,
    "public_access": true,
    "last_updated": null,
    "authority_score": null,
    "quality_score": null,
    "cross_domain_links": null,
    "owl_class": "mv:Recall",
    "owl_physicality": "ConceptualEntity",
    "owl_role": "Concept",
    "owl_inferred_class": null,
    "is_subclass_of": [
      "Metaverse"
    ],
    "has_part": [],
    "is_part_of": [],
    "requires": [],
    "depends_on": [],
    "enables": [],
    "relates_to": [],
    "bridges_to": [],
    "bridges_from": [],
    "domain_extensions": {},
    "belongs_to_domain": [
      "MetaverseDomain",
      "MetaverseDomain"
    ],
    "implemented_in_layer": [],
    "source": [],
    "validation": {
      "is_valid": false,
      "errors": [
        "Missing required property: last-updated",
        "Missing required property: definition",
        "owl:class namespace 'mv' doesn't match source-domain 'metaverse'"
      ]
    }
  }
}