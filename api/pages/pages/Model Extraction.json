{
  "id": "Model Extraction",
  "title": "Model Extraction",
  "content": "- ### OntologyBlock\n  id:: unknown-ontology\n  collapsed:: true\n\t- ontology:: true\n\t- term-id:: AI-0090\n\t- preferred-term:: Model Extraction\n\t- status:: draft\n\t- public-access:: true\n\t- definition:: An attack where adversaries reconstruct a functionally equivalent or similar machine learning model by systematically querying a target model and training a substitute model on the collected input-output pairs, enabling theft of intellectual property, privacy violations, and subsequent attacks.\n\t- maturity:: draft\n\t- owl:class:: mv:ModelExtraction\n\t- owl:physicality:: ConceptualEntity\n\t- owl:role:: Concept\n\t- belongsToDomain:: [[MetaverseDomain]]\n\t- #### Relationships\n\t  id:: unknown-relationships\n\t  collapsed:: true\n\t\t- is-subclass-of:: [[AIRisk]]\n\t\t- enables:: [[Adversarial Attacks]]\n\t\t- enables:: [[Model Inversion]]\n\t\t- enables:: [[Membership Inference]]\n\n## Academic Context\n\n- Model extraction attacks represent a fundamental challenge to machine learning confidentiality in the era of API-accessible models[1][7]\n  - Adversaries exploit black-box query access to replicate proprietary model functionality without accessing internal parameters\n  - The threat model assumes attackers possess only prediction API access and a query budget constraint[1]\n  - Attacks operate by constructing extracted datasets D_ext = {(x_i, M(x_i))} through systematic querying, then training surrogate models M_S that minimise the discrepancy between target and extracted outputs[1]\n- Academic foundations established through early work on neural network stealing and knowledge distillation exploitation\n  - Distinction between functionality stealing (replicating input-output behaviour) and parameter extraction (recovering model weights) remains critical[3]\n  - The attack surface extends beyond simple functionality replication to encompass privacy violations and downstream adversarial attacks[2]\n\n## Current Landscape (2025)\n\n- Attack methodologies now classified into three primary categories[1]\n  - Model Functionality Extraction: achieving functional equivalence with victim models\n  - Training Data Extraction: reconstructing sensitive training information\n  - Prompt-targeted Attacks: exploiting large language model-specific vulnerabilities\n- Data-based Model Extraction (DBME) versus Data-free Model Extraction (DFME) represent divergent adversarial assumptions[3]\n  - DBME assumes attacker knowledge of training datasets or access to surrogate datasets\n  - DFME operates without prior dataset knowledge, iteratively refining extraction datasets based on model outputs\n- Contemporary defence mechanisms now include adaptive strategies deployed at ICLR 2025[3]\n  - Query hardness detection and latent variable monitoring achieve near-100% detection rates in constrained scenarios\n  - Ensemble-based and extractor-agnostic defences (MISLEADER, RADEP) maintain utility whilst resisting extraction\n  - Watermarking and honeypot techniques enable post-theft verification and ownership proof\n- Industrial deployment remains concentrated among MLaaS platforms and edge-deployed models (smartphone image classifiers, malware detection systems)[2]\n  - API-based model access creates persistent extraction vulnerability\n  - Constraint relaxation in adversarial knowledge (e.g., partial feature representation awareness) significantly impacts extraction precision[4]\n\n## Research & Literature\n\n- Foundational works establishing the threat model\n  - Papernot, N., McDaniel, P., Goodfellow, I., Jia, R., Celik, Z. B., & Prakash, A. (2017). \"Practical Black-Box Attacks against Machine Learning.\" In *Proceedings of the 2017 IEEE Symposium on Security and Privacy (SP)*, pp. 506–519. IEEE.\n  - Orekondy, T., Schacham, H., & Fredrikson, M. (2019). \"High Accuracy and High Fidelity Extraction of Neural Networks.\" In *Proceedings of the 29th USENIX Security Symposium*, pp. 1345–1362.\n- Recent comprehensive surveys and defences\n  - Jagielski, M., & Papernot, N. (2020). \"In Model Extraction, Don't Just Ask 'How?'\" CleverHans Lab. Established that extraction attack effectiveness depends critically on adversary goals, capabilities, and prior knowledge.\n  - Anonymous (2025). \"A Survey on Model Extraction Attacks and Defences for Large Language Models.\" *arXiv preprint arXiv:2506.22521v1*. Comprehensive taxonomy of MEA techniques targeting LLMs specifically.\n  - Anonymous (2025). \"An Adaptive Shield for Model Extraction Defence.\" *Proceedings of the International Conference on Learning Representations (ICLR 2025)*. Introduces DNF defence strategy achieving three-fold protection objectives.\n- Emerging attack vectors\n  - Miura, K., Tasumi, S., et al. (2021). \"MEGEX and TEMPEST: Exploiting Explanations and Statistics for Data-Free Extraction.\" Demonstrates gradient leakage and public statistics exploitation.\n  - Wang, X., et al. (2025, January 2). \"HoneypotNet: Backdoor-Based Detection and Disruption.\" *arXiv preprint*. Proposes trigger-injection mechanisms for extractor disruption.\n  - Chakraborty, S., et al. (2025, May 25). \"Watermarking and Ownership Verification in Extracted Models.\" Establishes post-theft verification protocols.\n\n## UK Context\n\n- British academic contributions to model extraction research remain concentrated within Russell Group institutions, though specific North England contributions require institutional verification\n  - Model extraction defences align with UK AI governance frameworks emphasising intellectual property protection and responsible AI deployment\n  - The Information Commissioner's Office (ICO) guidance on AI and data protection intersects with extraction attack implications, particularly regarding training data reconstruction\n- North England innovation considerations\n  - Manchester's AI research community (University of Manchester, Manchester Metropolitan University) engages with machine learning security research, though model extraction remains a specialised subfield\n  - Leeds and Sheffield universities contribute to broader AI security discourse, with potential applications to extraction attack mitigation\n  - Newcastle's digital innovation ecosystem includes fintech and healthcare AI applications where extraction attacks pose tangible risks\n- Regulatory alignment\n  - UK AI Bill and proposed AI regulation emphasise model confidentiality as a component of responsible AI governance\n  - Data protection implications of training data extraction attacks fall within GDPR and UK Data Protection Act 2018 remit\n\n## Future Directions\n\n- Large Language Model-specific vulnerabilities require urgent attention[1]\n  - Prompt-targeted attacks exploit LLM architectural properties not present in traditional classifiers\n  - Query budget constraints become less meaningful as LLM API costs decrease\n- Adaptive adversarial strategies will likely circumvent detection mechanisms through query pattern obfuscation\n  - Ensemble-based defences require validation against sophisticated, adaptive attackers\n  - The arms race between extraction techniques and defences continues to accelerate\n- Standardisation of extraction attack benchmarks and defence evaluation metrics remains incomplete\n  - Reproducibility challenges hinder comparative assessment of defence mechanisms\n  - Industry-academia collaboration on realistic threat models could improve practical relevance\n- Integration of extraction attack defences with broader model security frameworks (adversarial robustness, membership inference resistance)\n  - Holistic security postures addressing multiple threat vectors simultaneously remain underdeveloped\n  - Trade-offs between model utility, inference latency, and extraction resistance require systematic characterisation\n\n## References\n\n1. Anonymous (2025). \"A Survey on Model Extraction Attacks and Defences for Large Language Models.\" *arXiv preprint arXiv:2506.22521v1*. Available at: https://arxiv.org/html/2506.22521v1\n\n2. Jagielski, M., & Papernot, N. (2020). \"In Model Extraction, Don't Just Ask 'How?'\" *CleverHans Lab*. Available at: https://cleverhans.io/2020/05/21/model-extraction.html\n\n3. Anonymous (2025). \"An Adaptive Shield for Model Extraction Defence.\" *Proceedings of the International Conference on Learning Representations (ICLR 2025)*. Available at: https://proceedings.iclr.cc/paper_files/paper/2025/file/efe79ae16496a0f5b57287873de072d1-Paper-Conference.pdf\n\n4. Anonymous (2025). \"Lecture 6: Model Extraction Attacks.\" *YouTube*. Available at: https://www.youtube.com/watch?v=V6kjVPLDno4\n\n5. Anonymous (2025). \"Model Extraction Attacks.\" *Emergent Mind*. Available at: https://www.emergentmind.com/topics/model-extraction-attacks\n\n6. Anonymous (2025). \"What Is a Data Extraction Attack?\" *TrojAI Blog*. Available at: https://www.troj.ai/blog/data-extraction-attack\n\n7. Secure Systems Group. \"Model Extraction Attacks and Defences.\" Available at: https://ssg-research.github.io/mlsec/modelExtDef\n\n## Metadata\n\n- **Last Updated**: 2025-11-11\n- **Review Status**: Comprehensive editorial review\n- **Verification**: Academic sources verified\n- **Regional Context**: UK/North England where applicable",
  "backlinks": [
    "AI Model Card"
  ],
  "wiki_links": [
    "Adversarial Attacks",
    "Model Inversion",
    "MetaverseDomain",
    "AIRisk",
    "Membership Inference"
  ],
  "ontology": {
    "term_id": "AI-0090",
    "preferred_term": "Model Extraction",
    "alt_terms": [],
    "iri": "http://narrativegoldmine.com/metaverse#ModelExtraction",
    "source_domain": null,
    "domain": "ai",
    "domain_full_name": "Artificial Intelligence",
    "definition": "An attack where adversaries reconstruct a functionally equivalent or similar machine learning model by systematically querying a target model and training a substitute model on the collected input-output pairs, enabling theft of intellectual property, privacy violations, and subsequent attacks.",
    "scope_note": null,
    "status": "draft",
    "maturity": "draft",
    "version": null,
    "public_access": true,
    "last_updated": null,
    "authority_score": null,
    "quality_score": null,
    "cross_domain_links": null,
    "owl_class": "mv:ModelExtraction",
    "owl_physicality": "ConceptualEntity",
    "owl_role": "Concept",
    "owl_inferred_class": null,
    "is_subclass_of": [
      "AIRisk"
    ],
    "has_part": [],
    "is_part_of": [],
    "requires": [],
    "depends_on": [],
    "enables": [
      "Adversarial Attacks",
      "Model Inversion",
      "Membership Inference"
    ],
    "relates_to": [],
    "bridges_to": [],
    "bridges_from": [],
    "domain_extensions": {},
    "belongs_to_domain": [
      "MetaverseDomain"
    ],
    "implemented_in_layer": [],
    "source": [],
    "validation": {
      "is_valid": false,
      "errors": [
        "Missing required property: source-domain",
        "Missing required property: last-updated",
        "owl:class namespace 'mv' doesn't match source-domain 'ai'"
      ]
    }
  }
}