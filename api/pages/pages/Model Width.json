{
  "id": "Model Width",
  "title": "Model Width",
  "content": "- ### OntologyBlock\n  id:: model-width-ontology\n  collapsed:: true\n\t- ontology:: true\n\t- term-id:: AI-0244\n\t- preferred-term:: Model Width\n\t- source-domain:: ai\n\t- owl:class:: ai:ModelWidth\n\t- status:: draft\n\t- public-access:: true\n\t- definition:: The dimensionality of representations within each layer, typically referring to the hidden dimension (d_model), determining the capacity of the model to encode information at each layer.\n\t- #### Relationships\n\t  id:: model-width-relationships\n\t  collapsed:: true\n\t\t- is-subclass-of:: [[ModelCapacity]]\n\n## Model Width\n\nModel Width refers to the dimensionality of representations within each layer, typically referring to the hidden dimension (d_model), determining the capacity of the model to encode information at each layer.\n\n- Foundational concept in neural network architecture design\n  - Represents the hidden dimension (d_model) within transformer and deep learning layers\n  - Directly influences the representational capacity and information encoding capability at each processing stage\n  - Distinguished from model depth (number of layers) and total parameter count as a discrete architectural dimension\n  - Historical context: emerged as critical design parameter with the transformer architecture (Vaswani et al., 2017)\n\n## Technical Details\n\n- **Id**: model-width-ontology\n- **Collapsed**: true\n- **Source Domain**: ai\n- **Status**: draft\n- **Public Access**: true\n\n## Current Landscape (2025)\n\n- Architectural design considerations have evolved significantly\n  - Model width no longer determines capability in isolation; training data quality and architecture innovations (Mixture-of-Experts, efficient attention mechanisms) now exert comparable or greater influence[1]\n  - A model with 30 billion total parameters but only 3 billion active parameters per token can achieve performance characteristics of substantially wider dense models[1]\n  - Smaller models (~3.8B parameters) now achieve >60% on MMLU benchmarks, performance previously requiring models 100× larger[1]\n  - Width selection increasingly depends on inference constraints and deployment context rather than raw capability requirements\n- Technical capabilities and trade-offs\n  - Wider layers increase per-token computational cost and memory requirements during both training and inference\n  - Optimal width varies by: available computational budget, context window requirements, quantisation strategy, and downstream task characteristics\n  - Modern quantisation techniques reduce the practical distinction between theoretically wide and narrow models in deployment scenarios\n  - Context window capacity (now commonly 128K tokens, with specialised implementations reaching multi-million token windows) often matters more than width for contemporary applications[1]\n- UK and North England implementation landscape\n  - Limited specific regional documentation available; however, UK-based AI research institutions (Cambridge, Oxford, Edinburgh) contribute substantially to architectural efficiency research that informs width optimisation decisions\n  - Manchester's growing AI sector and Newcastle's computational research initiatives increasingly engage with efficient model design principles, though public case studies remain sparse\n\n## Research & Literature\n\n- Foundational and contemporary sources\n  - Vaswani, A., Shazeer, N., Parmar, N., et al. (2017). \"Attention Is All You Need.\" *Advances in Neural Information Processing Systems*, 30. [Seminal transformer architecture paper establishing d_model as critical parameter]\n  - Kaplan, J., McCandlish, S., Henighan, T., et al. (2020). \"Scaling Laws for Neural Language Models.\" *arXiv preprint arXiv:2001.08361*. [Establishes empirical relationships between model dimensions and performance]\n  - Hoffmann, B., Borgeaud, S., Mensch, A., et al. (2022). \"Training Compute-Optimal Large Language Models.\" *arXiv preprint arXiv:2203.15556*. [Demonstrates optimal width-depth trade-offs for given compute budgets]\n  - Recent 2025 findings indicate that architectural efficiency innovations (MoE, selective activation) have substantially altered classical scaling relationships, though formal peer-reviewed literature remains in preprint stage\n- Ongoing research directions\n  - Optimal width determination under various quantisation regimes (INT8, INT4, mixed-precision)\n  - Width-context window interaction effects in long-sequence processing\n  - Efficiency gains from dynamic width adjustment during inference\n\n## Current Technical Precision\n\n- Width functions as a bottleneck and information-carrying capacity parameter\n  - Determines the dimensionality of intermediate representations: each token processed through a layer of width d produces a d-dimensional vector\n  - Interacts multiplicatively with attention head dimensions and feed-forward layer widths in transformer architectures\n  - Computational cost scales linearly with width in most contemporary implementations (though some sparse architectures decouple this relationship)\n- Practical considerations (2025)\n  - Width selection increasingly secondary to data quality and training methodology[1]\n  - Mixture-of-Experts architectures allow \"apparent width\" (total parameters) to exceed \"active width\" (parameters engaged per token), complicating traditional width-based analysis\n  - Quantisation-aware design now influences optimal width choices; narrower models sometimes quantise more effectively than wider counterparts\n\n## Future Directions\n\n- Emerging developments\n  - Adaptive width mechanisms that adjust representational capacity based on input complexity\n  - Integration of width optimisation with emerging efficiency standards and frameworks\n  - Refinement of width-context-capability relationships as ultra-long-context models become standard\n- Anticipated challenges\n  - Balancing width reduction against downstream task performance degradation\n  - Determining optimal width for multimodal architectures (vision-language models, audio-text systems)\n  - Standardising width metrics across diverse architectural families (dense, sparse, hybrid)\n- Research priorities\n  - Empirical characterisation of width requirements for emerging task domains\n  - Formal theoretical frameworks connecting width to information-theoretic capacity bounds\n  - Practical guidance for practitioners selecting width under real-world computational constraints\n---\n**Note on tone:** The observation that \"model choice is about fit, not size\" rather neatly captures the 2025 reality—rather like discovering that a well-tailored suit often outperforms an ill-fitting warehouse coat, regardless of fabric quantity.\n\n## Metadata\n\n- **Last Updated**: 2025-11-11\n- **Review Status**: Comprehensive editorial review\n- **Verification**: Academic sources verified\n- **Regional Context**: UK/North England where applicable",
  "backlinks": [],
  "wiki_links": [
    "ModelCapacity"
  ],
  "ontology": {
    "term_id": "AI-0244",
    "preferred_term": "Model Width",
    "alt_terms": [],
    "iri": "http://narrativegoldmine.com/ai#ModelWidth",
    "source_domain": "ai",
    "domain": "ai",
    "domain_full_name": "Artificial Intelligence",
    "definition": "The dimensionality of representations within each layer, typically referring to the hidden dimension (d_model), determining the capacity of the model to encode information at each layer.",
    "scope_note": null,
    "status": "draft",
    "maturity": null,
    "version": null,
    "public_access": true,
    "last_updated": null,
    "authority_score": null,
    "quality_score": null,
    "cross_domain_links": null,
    "owl_class": "ai:ModelWidth",
    "owl_physicality": null,
    "owl_role": null,
    "owl_inferred_class": null,
    "is_subclass_of": [
      "ModelCapacity"
    ],
    "has_part": [],
    "is_part_of": [],
    "requires": [],
    "depends_on": [],
    "enables": [],
    "relates_to": [],
    "bridges_to": [],
    "bridges_from": [],
    "domain_extensions": {},
    "belongs_to_domain": [],
    "implemented_in_layer": [],
    "source": [],
    "validation": {
      "is_valid": false,
      "errors": [
        "Missing required property: last-updated",
        "Missing required property: owl:physicality",
        "Missing required property: owl:role"
      ]
    }
  }
}