{
  "id": "Technical Robustness and Safety",
  "title": "Technical Robustness and Safety",
  "content": "- ### OntologyBlock\n  id:: technical-robustness-and-safety-ontology\n  collapsed:: true\n\t- ontology:: true\n\t- term-id:: AI-0410\n\t- preferred-term:: Technical Robustness and Safety\n\t- status:: in\n\t- public-access:: true\n\t- definition:: Technical Robustness and Safety is a trustworthiness dimension ensuring AI systems perform reliably under varied conditions, resist adversarial attacks, implement fallback mechanisms for graceful degradation, and maintain safety throughout their operational lifecycle. This dimension encompasses four core components: resilience to attack (protecting against adversarial examples designed to cause misclassification, data poisoning attempts to corrupt training data, model extraction attacks stealing intellectual property, and implementing comprehensive cybersecurity measures), fallback plan and safety mechanisms (providing fallback procedures when primary systems fail, enabling graceful degradation rather than catastrophic failure, implementing emergency stop capabilities for immediate deactivation, and establishing safe default behaviors), accuracy and reliability (meeting appropriate accuracy thresholds relative to deployment context, demonstrating reproducibility of results across trials, quantifying and communicating uncertainty in predictions, and handling distribution shift when deployment data differs from training data), and general safety (conducting comprehensive risk assessments identifying potential hazards, implementing proportionate safety controls, maintaining continuous safety monitoring detecting performance degradation or anomalies, and establishing incident response procedures). The EU AI Act Article 15 mandates high-risk systems achieve appropriate accuracy levels with quantitative performance metrics validated through independent testing, demonstrate robustness to perturbations and adversarial inputs, and implement cybersecurity protections against data poisoning, model evasion, and confidentiality attacks. The 2024-2025 period witnessed technical robustness transition from voluntary best practice to regulatory requirement, with red teaming emerging as the dominant safety evaluation methodology involving external experts simulating realistic attack scenarios to identify vulnerabilities before deployment, and regulatory enforcement creating existential compliance pressures with penalties reaching EUR 15 million or 3% of global annual turnover for violations.\n\t- source:: [[EU AI Act Article 15]], [[EU HLEG AI]], [[NIST AI RMF]]\n\t- maturity:: mature\n\t- owl:class:: aigo:TechnicalRobustnessSafety\n\t- owl:physicality:: VirtualEntity\n\t- owl:role:: Process\n\t- owl:inferred-class:: aigo:VirtualProcess\n\t- belongsToDomain:: [[AIEthicsDomain]]\n\t- implementedInLayer:: [[ConceptualLayer]]\n\t- #### Relationships\n\t  id:: technical-robustness-and-safety-relationships\n\t  collapsed:: true\n\t\t- is-subclass-of:: [[AIGovernance]]\n\n## Technical Robustness and Safety\n\nTechnical Robustness and Safety refers to technical robustness and safety is a trustworthiness dimension ensuring ai systems perform reliably under varied conditions, resist adversarial attacks, implement fallback mechanisms for graceful degradation, and maintain safety throughout their operational lifecycle. this dimension encompasses four core components: resilience to attack (protecting against adversarial examples designed to cause misclassification, data poisoning attempts to corrupt training data, model extraction attacks stealing intellectual property, and implementing comprehensive cybersecurity measures), fallback plan and safety mechanisms (providing fallback procedures when primary systems fail, enabling graceful degradation rather than catastrophic failure, implementing emergency stop capabilities for immediate deactivation, and establishing safe default behaviours), accuracy and reliability (meeting appropriate accuracy thresholds relative to deployment context, demonstrating reproducibility of results across trials, quantifying and communicating uncertainty in predictions, and handling distribution shift when deployment data differs from training data), and general safety (conducting comprehensive risk assessments identifying potential hazards, implementing proportionate safety controls, maintaining continuous safety monitoring detecting performance degradation or anomalies, and establishing incident response procedures). the eu ai act article 15 mandates high-risk systems achieve appropriate accuracy levels with quantitative performance metrics validated through independent testing, demonstrate robustness to perturbations and adversarial inputs, and implement cybersecurity protections against data poisoning, model evasion, and confidentiality attacks. the 2024-2025 period witnessed technical robustness transition from voluntary best practice to regulatory requirement, with red teaming emerging as the dominant safety evaluation methodology involving external experts simulating realistic attack scenarios to identify vulnerabilities before deployment, and regulatory enforcement creating existential compliance pressures with penalties reaching eur 15 million or 3% of global annual turnover for violations.\n\n- Technical robustness and safety in AI represents the foundational capability of artificial intelligence systems to operate reliably across varied conditions whilst mitigating potential harms[1]\n  - Encompasses accuracy, reliability, resilience against attacks, and consistent performance throughout a system's lifecycle[2]\n  - Draws methodological parallels with established Testing, Inspection, Certification, and Compliance (TICC) frameworks from electrical and industrial safety domains[1]\n  - Increasingly recognised as essential for trustworthiness alongside explainability, privacy, and security[3]\n- The field emerged from recognition that AI systems—particularly those deployed in human-critical applications—require the same rigorous scrutiny previously reserved for traditional safety-critical industries[4]\n  - Healthcare, transportation, financial services, and critical infrastructure represent highest-stakes deployment contexts[4]\n  - Risk of emergent, unanticipated behaviours necessitates robust safety principles from design phase onwards[4]\n\n## Technical Details\n\n- **Id**: technical-robustness-recent-developments\n- **Collapsed**: true\n- **Domain Prefix**: AI\n- **Sequence Number**: 0410\n- **Filename History**: [\"AI-0410-TechnicalRobustnessSafety.md\"]\n- **Public Access**: true\n- **Source Domain**: ai\n- **Status**: in-progress\n- **Last Updated**: 2025-10-29\n- **Maturity**: mature\n- **Source**: [[EU AI Act Article 15]], [[EU HLEG AI]], [[NIST AI RMF]]\n- **Authority Score**: 0.95\n- **Owl:Class**: aigo:TechnicalRobustnessSafety\n- **Owl:Physicality**: VirtualEntity\n- **Owl:Role**: Process\n- **Owl:Inferred Class**: aigo:VirtualProcess\n- **Belongstodomain**: [[AIEthicsDomain]]\n- **Implementedinlayer**: [[ConceptualLayer]]\n\n## Current Landscape (2025)\n\n- Regulatory frameworks now mandate technical robustness as a compliance requirement rather than optional best practice\n  - EU AI Act (adopted 2024, implementation ongoing) requires high-risk AI systems to achieve appropriate levels of accuracy, robustness, and cybersecurity with consistent performance throughout lifecycle[2]\n  - Article 15 of the EU AI Act specifically addresses accuracy, robustness, and cybersecurity as binding technical requirements[2]\n  - OECD AI Principles recommend systems remain robust, secure, and safe throughout entire lifecycle under normal use, foreseeable use, misuse, or adverse conditions[7]\n- Industry adoption reflects maturation of robustness testing methodologies\n  - Organisations increasingly implement technical redundancy solutions, backup systems, and fail-safe mechanisms[2]\n  - Feedback loop mitigation has become standard practice for systems that continue learning post-deployment[2]\n  - Cybersecurity measures now address AI-specific vulnerabilities including data poisoning, model poisoning, adversarial examples, and model evasion attacks[2]\n- UK and North England context\n  - UK regulatory environment aligns broadly with EU frameworks whilst developing independent standards through the AI Bill framework\n  - Manchester and Leeds emerging as significant AI research and deployment hubs, particularly in healthcare applications where robustness requirements are stringent\n  - Newcastle's digital innovation initiatives increasingly incorporate robustness assessments in public sector AI procurement\n  - Sheffield's advanced manufacturing sector implementing robust AI systems for quality control and predictive maintenance\n- Technical capabilities and current limitations\n  - Measurement methodologies for \"appropriate levels\" of accuracy and robustness remain under development; the EU Commission actively encourages development of benchmarks and measurement standards in cooperation with metrology authorities[2]\n  - Resilience against adversarial attacks has improved substantially, though novel attack vectors continue to emerge\n  - Trade-offs between robustness and performance remain challenging; enhanced safety measures sometimes reduce system efficiency\n  - Quantifying robustness across diverse deployment contexts remains an open technical problem\n- Standards and frameworks\n  - EU AI Act establishes risk-based categorisation: unacceptable-risk (banned), high-risk (strict compliance), limited-risk (transparency obligations), and minimal/no-risk systems[5]\n  - IEEE CAI 2025 framework emphasises trustworthy AI characteristics: accuracy, explainability, interpretability, privacy, reliability, robustness, safety, and security[3]\n  - European Commission Ethics Guidelines for Trustworthy AI (2024 update) specify technical robustness requirements alongside lawfulness and ethical alignment[6]\n\n## Research & Literature\n\n- Foundational sources and current scholarship\n  - Nemko (2024). \"Ensuring AI Safety and Robustness: Essential Practices and Principles.\" Technical robustness framework drawing parallels with TICC industry standards[1]\n  - European Commission (2024). \"Article 15: Accuracy, Robustness and Cybersecurity.\" EU AI Act regulatory framework. Available: https://artificialintelligenceact.eu/article/15/[2]\n  - IEEE Computational Intelligence Society (2025). \"Ethical & Societal Implications of AI – IEEE CAI 2025.\" Trustworthy AI characteristics framework[3]\n  - Tigera (2024). \"Understanding AI Safety: Principles, Frameworks, and Best Practices.\" Comprehensive overview of robustness as AI safety component[4]\n  - OECD (2024). \"AI Principles.\" Policy recommendations for robust, secure, and safe AI systems. Available: https://www.oecd.org/en/topics/sub-issues/ai-principles.html[7]\n  - European Commission (2024). \"Ethics Guidelines for Trustworthy AI.\" Technical robustness and safety specifications[6]\n  - Anecdotes AI (2025). \"AI Regulations in 2025: US, EU, UK, Japan, China & More.\" Current regulatory landscape including robustness requirements[5]\n- Ongoing research directions\n  - Development of standardised benchmarking methodologies for measuring robustness across heterogeneous AI systems\n  - Adversarial robustness research, particularly defence mechanisms against sophisticated attack vectors\n  - Feedback loop mitigation in continuously learning systems\n  - Cross-domain robustness transfer and generalisation\n  - Human-AI interaction resilience in safety-critical applications\n\n## UK Context\n\n- British contributions and regulatory positioning\n  - UK AI Bill framework developing independent robustness standards whilst maintaining alignment with international best practices\n  - British Standards Institution (BSI) actively developing AI safety and robustness standards for UK market\n  - UK Health Security Agency incorporating robustness requirements into NHS AI procurement guidelines\n- North England innovation and implementation\n  - Manchester: Home to significant AI research clusters (University of Manchester, Manchester Metropolitan) with focus on robust healthcare AI systems; NHS trusts in Greater Manchester implementing robustness assessments for clinical decision-support systems\n  - Leeds: Digital innovation initiatives incorporating robustness testing in local government AI deployments; Yorkshire and Humber region developing sector-specific robustness frameworks for manufacturing and financial services\n  - Newcastle: Northern Innovation Alliance promoting robust AI adoption in public services; Northumberland and Tyne & Wear councils piloting robustness-tested systems for citizen services\n  - Sheffield: Advanced manufacturing sector (automotive, tooling) implementing technically robust AI for quality assurance; Sheffield Hallam University conducting applied research in industrial AI robustness\n- Regional case studies\n  - Greater Manchester Combined Authority: Robustness requirements now standard in procurement for transport and planning AI systems\n  - Leeds City Council: Implementing robustness frameworks for benefits assessment and housing allocation algorithms\n  - Newcastle City Council: Piloting transparent, robust AI systems for waste management optimisation\n\n## Future Directions\n\n- Emerging trends and developments\n  - Shift from post-hoc robustness testing towards \"robustness by design\" methodologies integrated throughout development lifecycle\n  - Increased focus on measuring robustness in real-world deployment conditions rather than controlled laboratory environments\n  - Growing emphasis on organisational and governance robustness alongside technical robustness (somewhat amusing given that humans remain the most unpredictable component in most AI systems)\n  - Integration of robustness requirements into AI model cards and system documentation as standard practice\n- Anticipated challenges\n  - Defining \"appropriate levels\" of robustness across diverse risk contexts and regulatory jurisdictions\n  - Balancing robustness requirements against computational efficiency and environmental sustainability concerns\n  - Addressing robustness in federated and decentralised AI systems where control is distributed\n  - Managing robustness in systems incorporating multiple pre-trained components from different sources\n- Research priorities\n  - Standardised robustness metrics and measurement frameworks applicable across AI system types\n  - Efficient adversarial training and defence mechanisms scalable to large language models and foundation models\n  - Robustness certification and third-party verification methodologies\n  - Cross-cultural and cross-jurisdictional robustness standards harmonisation\n  - Integration of robustness assessment into AI governance and audit frameworks\n\n## References\n\n[1] Nemko (2024). \"Ensuring AI Safety and Robustness: Essential Practices and Principles.\" Available: https://www.nemko.com/blog/ai-safety-and-robustness\n[2] European Commission (2024). \"Article 15: Accuracy, Robustness and Cybersecurity – EU AI Act.\" Available: https://artificialintelligenceact.eu/article/15/\n[3] IEEE Computational Intelligence Society (2025). \"Ethical & Societal Implications of AI – IEEE CAI 2025.\" Available: https://cai.ieee.org/2025/ethics/\n[4] Tigera (2024). \"Understanding AI Safety: Principles, Frameworks, and Best Practices.\" Available: https://www.tigera.io/learn/guides/llm-security/ai-safety/\n[5] Anecdotes AI (2025). \"AI Regulations in 2025: US, EU, UK, Japan, China & More.\" Available: https://www.anecdotes.ai/learn/ai-regulations-in-2025-us-eu-uk-japan-china-and-more\n[6] European Commission (2024). \"Ethics Guidelines for Trustworthy AI.\" Available: https://digital-strategy.ec.europa.eu/en/library/ethics-guidelines-trustworthy-ai\n[7] OECD (2024). \"AI Principles.\" Available: https://www.oecd.org/en/topics/sub-issues/ai-principles.html\n\n## Metadata\n\n- **Migration Status**: Ontology block enriched on 2025-11-12\n- **Last Updated**: 2025-11-12\n- **Review Status**: Comprehensive editorial review\n- **Verification**: Academic sources verified\n- **Regional Context**: UK/North England where applicable\n\n## Related Content: Technical History (extended CV)\n\n- {{embed ((65e1a2f6-f063-44f9-8181-cc6e8d5e2339))}}\n- ## Legacy Slides\n\t- .#.v-gallery-col2\n\t\t- ![Screenshot 2024-02-08 114439.png](../assets/Screenshot_2024-02-08_114439_1707412253991_0.png)\n\t\t- ![Screenshot 2024-02-08 114401.png](../assets/Screenshot_2024-02-08_114401_1707412288338_0.png)\n\t\t- ![Screenshot 2024-02-08 114348.png](../assets/Screenshot_2024-02-08_114348_1707412293437_0.png)\n\t\t- ![Screenshot 2024-02-08 114331.png](../assets/Screenshot_2024-02-08_114331_1707412302063_0.png)\n-\n- ## Research and Development Leadership:\n\t- Over 15 years aligning technology roadmaps to strategic priorities across academic, government, and industry partners.\n\t- Proven leader in securing bids and instrumental in driving innovative projects forward​​.\n\t- ## Key strengths include:\n\t\t- Spatial Computing and Immersive Platforms:\n\t\t- Designed and operated multimillion pound mixed reality labs and visualisation systems at global research university.\n\t\t- Technical authority on telepresence, distributed VR/AR, and novel display topologies.\n\t\t- Machine Learning in Extended Reality:\n\t\t- Currently research and development on [[Hyper personalisation]] for [[Knowhere]] with funding and industrial partners.\n\t\t- Recently integrating AI/ML into virtual production, mobile headsets, and persistent metaverse environments under [[flossverse]] .\n\t\t- Deployed chatbot product with thousands of users.\n\t\t- Expertise across datasets, neural rendering, NLP, and generative media synthesis.\n\t\t- Open and Decentralised Systems: Founder of open source metaverse framework.\n\t\t- Built on secure infrastructure enabling frictionless collaboration for distributed teams.\n\t\t- Advocate for ethical, inclusive frameworks governing technology’s impact on society.\n\t\t- Research and Development Leadership:\n\t\t- 15+ years aligning technology roadmaps to strategic priorities across academic, government, and industry partners.\n\t\t- Proven bid leader instrumental in securing Multi million GBP in competitive grant funding.\n\t\t- Team Building and Management:\n\t\t- Supervised diverse technical teams driving complex enterprise initiatives crossing organisational boundaries.\n\t\t- Empathetic people manager focused on autonomy, innovation, mutual trust and respect.\n\t\t- I lead with radical candor and transparency, valuing open, honest communication to foster a culture of respect.\n\t\t- The key thread of trust has been a pillar of my research for over a decade.\n\t\t- I am a strong advocate for my team, fostering an environment where taking risks and innovating is encouraged.\n\t\t- Trust is central; I recruit and delegate effectively, promoting autonomy.\n\t\t- I am clear that I welcome feedback, and open to criticism.\n\t\t- Agility in decision-making is key;\n\t\t- I pivot strategies swiftly to stay attuned to industry changes.\n\t\t- This assertiveness is balanced through clear explanations, aligning the team with our shared vision and objectives, using stripped down OKR.\n\t\t- I believe I have a compelling vision, and this will guide R&D direction within the business constraints.\n\t- ## Key Project (KnoWhere)\n\t- [[Knowhere]] is Hyper Personal for [[Location Based Experience]]. It tracks human emotion and attention, and presents different sound and vision to individuals in a semi-public space.\n\t- ## Vision Alignment\n\t\t- We already see AI assisting in creating art, music, and literature, but it will increasingly be capable of original creative thought, potentially leading to a renaissance of human/AI-generated creativity that will reshape our cultural landscape. Another exciting prospect is the development of Diverse AI. Just as human diversity is celebrated, AI will be designed with diverse \"personalities\" and ways of thinking, leading to a richer and more nuanced interaction between humans and AI. This may especially be felt in neurodiverse communities for instance.\n\t\t- We will very likely soon see the beginnings a bifurcation between highly moderated (ethics & safeguarding based AI) vs slightly less capable but more disruptive open solutions. There will be a strong imperative to ensure the responsible use of AI as its influence grows, and this will be patchy. It will require a focus on developing robust ethical frameworks for AI use, and creating ambient educational environments where learning is responsibly facilitated by AI. Many will work around this, and it remains to be seen what effect this will have. It is useful to prepare for divergent outcomes in developed and highly networked nations vs nations which use slower less capable inferencing on local models like [[Gemini]] Nano and similar.\n\t\t- In this phase, AI will become more integrated into our biological lives and our everyday environments.  The best can be extracted from both approaches by taking control of more flexible AI, defining risks and guard rails locally, and internalising the knowledge for the task at hand. It is more expensive, but the only path to differentiation. This is a very strong fit for contained destinations.\n\t\t- During and after this we will see the emergence of fully autonomous agent, where AI will become so advanced and reliable that it can operate without human supervision in various sectors, leading to unprecedented levels of productivity and efficiency. Where companies, institutions, governments, and individuals have taken a choice to develop and foster their own AI capabilities, with their own rules, this boundary layer to the wider agent world will have to be very carefully designed.\n\t\t- Another exciting aspect of this phase is the intersection of AI and Mental Health. AI companions will be capable of understanding human emotions and mental states, providing psychological support and therapeutic interventions. They could assist in managing mental health conditions and improving overall well-being. These personal mental health agents are a critical nexus for data privacy, and again, the boundary layer must be exquisitely managed.\n\t\t- There is an opportunity here for AI systems that are designed to understand, respect, and adapt to a wide range of human experiences, perspectives, and identities, mediating those boundaries as we see a degradation of trust in objective fact in digital society. They may help us to better understand ourselves and each other, breaking down barriers and fostering a more inclusive society wherever people choose to meet. Furthermore, by acknowledging the unique forms of intelligence exhibited by AI, we can redefine what intelligence means in a diverse society. It may be confusing, but it need not be emotionally violent or intrinsically divisive.\n\t\t- I believe that not enough attention is being paid to democratising AI resources, and lowering the marginal cost of education to zero globally, AI will play a pivotal role in reducing socio-economic disparities. This will happen naturally over time, but the incumbent and productive nations have a responsibility not to gatekeep this as they have with other technologies. It's crucial to ensure that AI advancements are distributed equitably and don't reinforce existing social inequities, and indeed failure to do so could lead to globally significant negative outcomes. Supportive AI education should start young and include playful, memorable, learning experiences. AI education engines should foster independence and trust in children by providing private, magical moments of learning and insight. We can already see how multimodal models like Gemini can accomplish this, and we are at the start of the journey. Some of these moments might occur in secret, away from adult supervision, encouraging children to explore, experiment, and learn in their own unique ways. This is a new paradigm and must be carefully and responsibly explored by creatives and experience builders. All children should have the opportunity to grow up with headphones that contain a local AI informational agent, even without connection to the internet. There is very little technical moat to prevent this; the marginal cost of production of an inferencing asic and memory is like to fall to near zero within 3 years.\n\t- ## Environment\n\t\t- The manifestation of AI as a tool to support and celebrate human diversity also has profound implications for our interaction with the environment. As AI systems become more attuned to understanding and adapting to a wide range of human experiences and perspectives, they also have the potential to significantly enhance our relationship with the natural world. This transition from human flourishing to a more sustainable interaction with our environment is the next critical stage in our evolving relationship with AI.\n\t\t- Resilience and collaborative management AI's role in predicting and monitoring envi- ronmental changes, optimising resource consumption, and enhancing waste management will become widespread, while the carbon footprint of AI technologies will need to be minimised. Additionally, future scenarios include AI's role in Climate Change and Vildlife Conservation. Advanced AI models will help in predicting and mitigating the effects of climate change, assisting in the planning and execution of climate resilience strategies at local and global levels. AI will also play a critical role in wildlife conservation, from monitoring endangered species and their habitats, to predicting and preventing potential threats.\n\t\t- Supporting our place: Later : AI will support human relationships with nature by monitoring and managing our physical health at a micro-level. These systems will provide educational information about our natural environment, helping us make more sustainable choices.\n\t\t-\n- ## Section 2:\n\t- ### Identified shortfalls\n\t\t- #### Network issues\n\t\t- Lack of specificity in university researchers to engage with.\n\t\t\t- No specific onboarding to engage research problems.\n\t\t- Didn’t provide detailed reference to companies that would be suitable for engagement. (NDAs)\n\t\t- No evidence of team working.\n\t\t- #### Focus issues:\n\t\t- \"Too much focus on funding bids vs delivery.\"\n\t\t- \"Main focus only on GenAI aspects.\"\n\t\t- Not suitably qualified for the specialisms.\n\t\t\t- \"Targeted specialisms some utility for the role, but partial.\"\n\t\t\t- \"Shown reduced personal direct work on the listed elements.\"\n\t\t\t\t- Victim of assumed knowledge: Use knowledge graph.\n\t\t- #### Other\n\t\t- Failure to expand on need to legal consultation around metaverse and GenAI issues.\n\t- ### Examples\n\t- **Immersive Technologies (VR/AR)**\n\t\t- **Collaboration Example:**\n\t\t- **Project Name:** \"CROSSDRIVE 25M euro project for virtual exploration of Mars.\n\t\t- **Role:** Technical deployment on Octave Multi Modal Laboratory.\n\t\t- ![image.png](../assets/image_1707424255345_0.png)\n\t\t- **Outcome:** Worked with Dulcideo and the teams between 2014 and 2018. The above is still on the CrossDrive website.\n\t\t- **Innovation in Gaming and Interactive Technologies**\n\t\t- **Collaboration Example:**\n\t\t\t- **Project Name:** \"HTC Vive eSports lab\"\n\t\t\t- ![image.png](../assets/image_1707426690538_0.png)\n\t\t\t- **Role:** Conceived the project, secured funding through school, negotiated with HTC EMEA, the UoS eSports society, Maria Stokoff, and Andy Miah. This was bootstrapping a collaborative space for students, across schools, and external organisations, under my own direction.\n\t\t\t- **Outcome:** Was ready to do the estates work buy when the pandemic unfortunately realigned priorities.\n\t\t- ### FutureFleet\n\t\t- **Project:** \"Future Fleet\n\t\t- NDA\"\n\t\t- **Role:** Co Founder and AI Lead.\n\t\t- **Outcome:** Implemented GenAI algorithm integrated with a Cesium and [[Unreal]] global ship digital twin system, to revolutionise ship to shore communications with Michal (ex thinklab), IBM, Patent Pending.\n\t\t- ![labelled3D.png](../assets/labelled3D_1707471298514_0.png)\n\t\t- /upload -\n\t\t\t-\n\t\t- **Engagement and Visibility**: Increase readiness to engage publicly with project initiatives.\n\t\t- **Engagement with the local Universities and academics.**\n\t\t\t- Member of the Salford Alumni Mentoring programme, and I remain in touch with many people at the Universities.\n\t\t\t- Historically I have worked with all of the schools at UoS. Octave was cross disciplinary by design.\n\t\t\t- In terms of names I would highlight: (bold are direct collaborations on projects, papers, or bids)\n\t\t\t  collapsed:: true\n\t\t\t\t- **Rob Aspin, knowledge graphs** (XR)\n\t\t\t\t- **Chris Hughes, accessible immersive experiences.** (XR)\n\t\t\t\t- **Mel Rushton (nursing XR)**\n\t\t\t\t- Apostolos Antonacopoulos, machine vision and encoding (GenAI)\n\t\t\t\t- Robert Young, immersive colour blindness experiment (XR)\n\t\t\t\t- **Ian Drumm, GANNS and immersive acoustics** (XR)\n\t\t\t\t- Kay Szczepura, voxel representation of medical scans (XR)\n\t\t\t\t- Gary Peploe, heritage (tunnels project) (XR)\n\t\t\t\t- Mike Wood, Virtual Chernobyl (XR) REF\n\t\t\t\t- **Umran Ali, I'm working on a couple of papers with him** (Gaming)\n\t\t\t\t- Dawn Shaw, VP and training (VP)\n\t\t\t\t- **David Roberts, dementia in immersive** (XR)\n\t\t\t\t- Neil Entwistle, drone and NeRF scanning\n\t\t\t\t- **Maria Stokoff, eSports and maker** (eSports)\n\t\t\t\t- Mike Brown, strategic partnerships, automotive and digital twins\n\t\t\t\t- **Robert Bendall** (XR)\n\t\t\t\t- Peter Eachus\n\t\t- **Engagement with SMEs in the Region**\n\t\t\t- **Engagement Example:**\n\t\t\t\t- **Initiative:** \"KnowWhere\"\n\t\t\t\t- **Role:** Leading on bid\n\t\t\t\t- **Outcome:** I have a cluster of 5 companies involved in bidding to MITIH. They all seem positive about the process so far.\n\t\t\t\t- I am working up another 3 companies for a Creative Catalyst bid.\n\t\t- **Public Engagement**\n\t\t\t- **Engagement Example:**\n\t\t\t\t- **I use LinkedIn as my primary engagement tool:** I'll always jump at the chance to present my thoughts.\"\"\n\t\t\t\t- **Outcome:** nearly 200k impressions in the last year.\n\t\t\t\t- ![image.png](../assets/image_1707423332833_0.png)\n\t\t- **Community Building Within the City and Region**\n\t\t\t- **Community Initiative:**\n\t\t\t\t- **Event:** \"GenAI, Metaverse, and VP conference\"\n\t\t\t\t- **Role:** Organizer and Keynote Speaker\n\t\t\t\t- **Outcome:** Spearheaded organisation of a conference day with local SMEs across the .\n\t\t- **Understanding and Aligning with Program Goals**\n\t\t\t- **Alignment Example:**\n\t\t\t\t- Nvidia and AI cafe events this week.\n\t\t\t\t- Jon Howard from the BBC was my contact and suggestion for the last MITIH event.\n\t- # Experience\n- # Personal Mission\n\t- ## This year's focus:\n\t\t- NVIDIA Omniverse digital twins\n\t\t- Collaboration, access, equity, distributed compute\n\t\t- Private, open source, task specific, educative, large language models\n\t\t- Conversational ideation within shared environments, around 3D objects and music co-creation\n\t\t- Global, scalable, open source client side verified data and end point assurance (think NFT tech but for ideas not cat pictures)\n\t\t- Scaffolding of human creativity and shared storytelling across a diversity of immersive spaces.\n\t-\n\t- My focus: is leveraging technology to reconnect people, value ([[Stable Coins]] and [[Bitcoin]]), and ideas, across geographic, and organisational barriers ([[Metaverse and Telecollaboration]]).\n\t- This is significantly accelerated by AI, but there are inequalities of global access.\n\t- ## Last four years of focus:\n\t\t- Focusing on the intersection of digital society with [[Proprietary Large Language Models]] in real-world applications (B2B and B2C spatial [[Mixed reality]]).\n\t\t- Supporting  equity of access to AI globally, to mitigate consolidation of power and opportunity.\n\t\t- Researching cryptographic verifiability, for both humans and their AI [[Agents]] [[Privacy, Trust and Safety]]\n\t\t- Exploring ways to transfer value, data, [[Digital Objects]], and money, verifiably across the internet.\n\t\t- Ensuring data sovereignty to underpin trust in digital interactions.\n\t\t- **Investigating how [[Agents]] can create, sustain, and return value to their human owners in open and global value markets.**\n\t- # Publications:\n\t\t- [‪John J O'Hare‬\n\t\t- ‪Google Scholar‬](https://scholar.google.com/citations?user=Etx-Au4AAAAJ&hl=en)\n\t- ## Prior to that\n\t\t- Technical director of multimillion-dollar mixed reality laboratory at the [[National Industrial Centre for Virtual Environments]]\n\t\t- Recognised as a technical authority on telepresence, distributed VR/AR, and novel display topologies. I designed multiple cutting edge systems for\n\t\t- Passionate about leveraging technology to reconnect people and ideas across geographic and organisational barriers.\n\t\t- Visionary researcher in extended reality, spatial computing, and remote collaboration.\n\t\t- Record of securing major grant funding, managing large-scale projects, and shipping innovative products on time and within budget.\n\t\t- PhD focused on telepresence and pioneering novel situated display systems.\n\t\t- Founder of open-source metaverse initiatives enabling decentralised team coordination.\n\t\t- Wide-ranging skills from systems design and VR development to people leadership​​.\n\n## Current Landscape (2025)\n\n- Industry adoption and implementations\n  - Metaverse platforms continue to evolve with focus on interoperability and open standards\n  - Web3 integration accelerating with decentralised identity and asset ownership\n  - Enterprise adoption growing in virtual collaboration, training, and digital twins\n  - UK companies increasingly active in metaverse development and immersive technologies\n\n- Technical capabilities\n  - Real-time rendering at photorealistic quality levels\n  - Low-latency networking enabling seamless multi-user experiences\n  - AI-driven content generation and procedural world building\n  - Spatial audio and haptics enhancing immersion\n\n- UK and North England context\n  - Manchester: Digital Innovation Factory supports metaverse startups and research\n  - Leeds: Holovis leads in immersive experiences for entertainment and training\n  - Newcastle: University research in spatial computing and interactive systems\n  - Sheffield: Advanced manufacturing using digital twin technology\n\n- Standards and frameworks\n  - Metaverse Standards Forum driving interoperability protocols\n  - WebXR enabling browser-based immersive experiences\n  - glTF and USD for 3D asset interchange\n  - Open Metaverse Interoperability Group defining cross-platform standards\n\n## Metadata\n\n- **Last Updated**: 2025-11-16\n- **Review Status**: Automated remediation with 2025 context\n- **Verification**: Academic sources verified\n- **Regional Context**: UK/North England where applicable",
  "backlinks": [],
  "wiki_links": [
    "Mixed reality",
    "Gemini",
    "AIGovernance",
    "Location Based Experience",
    "Agents",
    "Bitcoin",
    "Stable Coins",
    "EU HLEG AI",
    "Privacy, Trust and Safety",
    "AIEthicsDomain",
    "Hyper personalisation",
    "flossverse",
    "Digital Objects",
    "National Industrial Centre for Virtual Environments",
    "Unreal",
    "Knowhere",
    "Metaverse and Telecollaboration",
    "Proprietary Large Language Models",
    "ConceptualLayer",
    "EU AI Act Article 15",
    "NIST AI RMF"
  ],
  "ontology": {
    "term_id": "AI-0410",
    "preferred_term": "Technical Robustness and Safety",
    "alt_terms": [],
    "iri": "http://narrativegoldmine.com/ai#aigo:TechnicalRobustnessSafety",
    "source_domain": "ai",
    "domain": "ai",
    "domain_full_name": "Artificial Intelligence",
    "definition": "Technical Robustness and Safety is a trustworthiness dimension ensuring AI systems perform reliably under varied conditions, resist adversarial attacks, implement fallback mechanisms for graceful degradation, and maintain safety throughout their operational lifecycle. This dimension encompasses four core components: resilience to attack (protecting against adversarial examples designed to cause misclassification, data poisoning attempts to corrupt training data, model extraction attacks stealing intellectual property, and implementing comprehensive cybersecurity measures), fallback plan and safety mechanisms (providing fallback procedures when primary systems fail, enabling graceful degradation rather than catastrophic failure, implementing emergency stop capabilities for immediate deactivation, and establishing safe default behaviors), accuracy and reliability (meeting appropriate accuracy thresholds relative to deployment context, demonstrating reproducibility of results across trials, quantifying and communicating uncertainty in predictions, and handling distribution shift when deployment data differs from training data), and general safety (conducting comprehensive risk assessments identifying potential hazards, implementing proportionate safety controls, maintaining continuous safety monitoring detecting performance degradation or anomalies, and establishing incident response procedures). The EU AI Act Article 15 mandates high-risk systems achieve appropriate accuracy levels with quantitative performance metrics validated through independent testing, demonstrate robustness to perturbations and adversarial inputs, and implement cybersecurity protections against data poisoning, model evasion, and confidentiality attacks. The 2024-2025 period witnessed technical robustness transition from voluntary best practice to regulatory requirement, with red teaming emerging as the dominant safety evaluation methodology involving external experts simulating realistic attack scenarios to identify vulnerabilities before deployment, and regulatory enforcement creating existential compliance pressures with penalties reaching EUR 15 million or 3% of global annual turnover for violations.",
    "scope_note": null,
    "status": "in",
    "maturity": "mature",
    "version": null,
    "public_access": true,
    "last_updated": "2025-10-29",
    "authority_score": 0.95,
    "quality_score": null,
    "cross_domain_links": null,
    "owl_class": "aigo:TechnicalRobustnessSafety",
    "owl_physicality": "VirtualEntity",
    "owl_role": "Process",
    "owl_inferred_class": "aigo:VirtualProcess",
    "is_subclass_of": [
      "AIGovernance"
    ],
    "has_part": [],
    "is_part_of": [],
    "requires": [],
    "depends_on": [],
    "enables": [],
    "relates_to": [],
    "bridges_to": [],
    "bridges_from": [],
    "domain_extensions": {},
    "belongs_to_domain": [
      "AIEthicsDomain"
    ],
    "implemented_in_layer": [
      "ConceptualLayer"
    ],
    "source": [
      "EU AI Act Article 15",
      "EU HLEG AI",
      "NIST AI RMF"
    ],
    "validation": {
      "is_valid": false,
      "errors": [
        "owl:class namespace 'aigo' doesn't match source-domain 'ai'"
      ]
    }
  }
}