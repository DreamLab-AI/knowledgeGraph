{
  "id": "Pre-Training",
  "title": "Pre Training",
  "content": "- ### OntologyBlock\n  id:: pre-training-ontology\n  collapsed:: true\n\t- ontology:: true\n\t- term-id:: AI-0247\n\t- preferred-term:: Pre Training\n\t- status:: draft\n\t- public-access:: true\n\t- definition:: The initial training phase where a model learns general representations from large amounts of unlabelled or weakly labelled data before being adapted to specific tasks. Pre-training establishes foundational knowledge that can be transferred across multiple downstream applications.\n\t- #### Relationships\n\t  id:: pre-training-relationships\n\t  collapsed:: true\n\t\t- is-subclass-of:: [[ModelTraining]]\n\n## Academic Context\n\nPre-training revolutionised natural language processing and computer vision by enabling models to learn rich, transferable representations from vast amounts of data without task-specific labels.\n\n**Primary Sources**:\n- Devlin et al., \"BERT\", arXiv:1810.04805 (2018)\n- Radford et al., \"Improving Language Understanding by Generative Pre-Training\" (2018)\n\n## Key Characteristics\n\n- Uses large-scale unlabelled or weakly labelled data\n- Learns general representations and patterns\n- Precedes task-specific fine-tuning\n- Computationally intensive (requires significant resources)\n- Creates foundation for transfer learning\n\n## Technical Details\n\n**Pre-training Objectives**:\n- **Language Models**: Next token prediction (GPT)\n- **Masked Language Models**: Predict masked tokens (BERT)\n- **Contrastive Learning**: Align related samples (CLIP)\n- **Denoising**: Reconstruct corrupted inputs\n\n**Typical Process**:\n1. Collect large-scale training corpus\n2. Define self-supervised learning objective\n3. Train model on general data\n4. Save pre-trained weights\n5. Use as initialisation for fine-tuning\n\n## Usage in AI/ML\n\n\"Pre-training on vast amounts of general-domain data is followed by domain adaptation and fine-tuning steps.\"\n\nApplications:\n- Foundation for all modern large language models\n- Basis for vision-language models (CLIP, ALIGN)\n- Transfer learning across domains\n- Few-shot and zero-shot learning capabilities\n\n## Related Concepts\n\n- **Fine-Tuning**: Subsequent adaptation to specific tasks\n- **Transfer Learning**: Knowledge transfer paradigm\n- **Self-Supervised Learning**: Learning without explicit labels\n- **Foundation Model**: Large-scale pre-trained model\n- **Continued Pre-Training**: Additional pre-training on domain data\n\n## Pre-training Loss\n\nThe loss function value during pre-training serves as a predictor of downstream task performance and emergent capabilities. Research shows models exhibit emergent abilities when pre-training loss falls below specific thresholds.\n\n## Historical Development\n\n- Pre-2018: Task-specific training dominated\n- 2018: BERT and GPT demonstrate pre-training power\n- 2019-2020: Pre-training becomes standard practice\n- 2020+: Scaling laws drive ever-larger pre-training\n- 2023+: Trillion-token pre-training regimes\n\n## Significance\n\nPre-training fundamentally changed AI development by enabling knowledge reuse across tasks, dramatically reducing data requirements for specific applications whilst improving performance.\n\n## OWL Functional Syntax\n\n```clojure\n(Declaration (Class :PreTraining))\n(SubClassOf :PreTraining :TrainingTechnique)\n(SubClassOf :PreTraining\n  (ObjectSomeValuesFrom :trainsOn :LargeScaleUnlabelledData))\n(SubClassOf :PreTraining\n  (ObjectSomeValuesFrom :learns :GeneralRepresentation))\n(SubClassOf :PreTraining\n  (ObjectSomeValuesFrom :precedes :FineTuning))\n(SubClassOf :PreTraining\n  (ObjectSomeValuesFrom :enables :TransferLearning))\n(SubClassOf :PreTraining\n  (ObjectSomeValuesFrom :uses :SelfSupervisedLearning))\n(SubClassOf :PreTraining\n  (ObjectSomeValuesFrom :creates :FoundationModel))\n\n(AnnotationAssertion rdfs:comment :PreTraining\n  \"Initial training phase where models learn general representations from vast amounts of unlabelled data before task-specific adaptation\"@en)\n(AnnotationAssertion :hasAcademicSource :PreTraining\n  \"Devlin et al., BERT, arXiv:1810.04805 (2018); Radford et al., GPT (2018)\")\n```\n\n## UK English Notes\n\n- \"Pre-training\" (hyphenated)\n- \"Unlabelled data\" (not \"unlabeled\")\n- \"Generalisation\" in related contexts\n\n**Last Updated**: 2025-10-27\n**Verification Status**: Verified against BERT and GPT foundational papers\n\t- maturity:: draft\n\t- owl:class:: mv:PreTraining\n\t- owl:physicality:: ConceptualEntity\n\t- owl:role:: Concept\n\t- belongsToDomain:: [[MetaverseDomain]]",
  "backlinks": [],
  "wiki_links": [
    "ModelTraining",
    "MetaverseDomain"
  ],
  "ontology": {
    "term_id": "AI-0247",
    "preferred_term": "Pre Training",
    "alt_terms": [],
    "iri": "http://narrativegoldmine.com/metaverse#PreTraining",
    "source_domain": null,
    "domain": "ai",
    "domain_full_name": "Artificial Intelligence",
    "definition": "The initial training phase where a model learns general representations from large amounts of unlabelled or weakly labelled data before being adapted to specific tasks. Pre-training establishes foundational knowledge that can be transferred across multiple downstream applications.",
    "scope_note": null,
    "status": "draft",
    "maturity": "draft",
    "version": null,
    "public_access": true,
    "last_updated": null,
    "authority_score": null,
    "quality_score": null,
    "cross_domain_links": null,
    "owl_class": "mv:PreTraining",
    "owl_physicality": "ConceptualEntity",
    "owl_role": "Concept",
    "owl_inferred_class": null,
    "is_subclass_of": [
      "ModelTraining"
    ],
    "has_part": [],
    "is_part_of": [],
    "requires": [],
    "depends_on": [],
    "enables": [],
    "relates_to": [],
    "bridges_to": [],
    "bridges_from": [],
    "domain_extensions": {},
    "belongs_to_domain": [
      "MetaverseDomain"
    ],
    "implemented_in_layer": [],
    "source": [],
    "other_relationships": {
      "belongsToDomain": [
        "MetaverseDomain"
      ]
    },
    "validation": {
      "is_valid": false,
      "errors": [
        "Missing required property: source-domain",
        "Missing required property: last-updated",
        "owl:class namespace 'mv' doesn't match source-domain 'ai'"
      ]
    }
  }
}