{
  "id": "Interpretability",
  "title": "Interpretability",
  "content": "- ### OntologyBlock\n  id:: interpretability-ontology\n  collapsed:: true\n\t- ontology:: true\n\t- term-id:: AI-0064\n\t- preferred-term:: Interpretability\n\t- status:: draft\n\t- public-access:: true\n\t- definition:: The degree to which a human can understand the internal mechanics, decision-making processes, and cause-effect relationships within an AI system, independent of external explanation tools.\n\t- #### Relationships\n\t  id:: interpretability-relationships\n\t  collapsed:: true\n\t\t- is-subclass-of:: [[ArtificialIntelligence]]\n\t\t- enables:: [[Trust]]\n\t\t- enables:: [[Model Validation]]\n\t\t- enables:: [[Debugging]]\n\n## OWL Formal Semantics\n\n```clojure\n;; OWL Functional Syntax\n\n(Declaration (Class :Interpretability))\n\n;; Annotations\n(AnnotationAssertion rdfs:label :Interpretability \"Interpretability\"@en)\n(AnnotationAssertion rdfs:comment :Interpretability \"The degree to which a human can understand the internal mechanics, decision-making processes, and cause-effect relationships within an AI system, independent of external explanation tools.\"@en)\n\n;; Data Properties\n(AnnotationAssertion dcterms:identifier :Interpretability \"AI-0064\"^^xsd:string)\n(DataPropertyAssertion :isAITechnology :Interpretability \"true\"^^xsd:boolean)\n```\n\n## Formal Specification\n\n```yaml\nterm: Interpretability\ndefinition: \"Inherent comprehensibility of AI system mechanisms and decision processes\"\ndomain: AI System Design\ntype: Quality Attribute\ncharacteristics:\n  - intrinsic_understandability\n  - mechanism_transparency\n  - causal_clarity\n  - human_comprehension\nmodel_types:\n  high_interpretability: [linear_regression, decision_trees, rule_based]\n  low_interpretability: [deep_neural_networks, ensemble_methods, black_box]\nscope:\n  - global_interpretability\n  - local_interpretability\n  - modular_interpretability\n```\n\n## Authoritative References\n\n### Primary Sources\n\n1. **ISO/IEC TR 24029-1:2021** - Assessment of the robustness of neural networks — Part 1: Overview\n   - Section 4.3.1: \"Interpretability\"\n   - \"The degree to which a human can consistently predict the model's result\"\n   - Source: ISO/IEC JTC 1/SC 42\n\n2. **NIST AI Risk Management Framework (AI RMF 1.0)**, January 2023\n   - Section 2.2: \"Explainable and Interpretable\"\n   - Interpretability as understanding internal functions\n   - Source: National Institute of Standards and Technology\n\n3. **Rudin, C. (2019)** - \"Stop Explaining Black Box Machine Learning Models for High Stakes Decisions and Use Interpretable Models Instead\"\n   - *Nature Machine Intelligence*, 1, 206-215\n   - Foundational argument for interpretable models\n\n### Supporting Standards\n\n4. **ISO/IEC 23894:2023** - Guidance on risk management\n   - Section 7.4.4: \"Interpretability in risk assessment\"\n\n5. **Doshi-Velez, F. & Kim, B. (2017)** - \"Towards A Rigorous Science of Interpretable Machine Learning\"\n   - Comprehensive taxonomy of interpretability\n   - arXiv:1702.08608\n\n## Key Characteristics\n\n### Dimensions of Interpretability\n\n1. **Transparency**\n   - Algorithmic transparency: How the algorithm works\n   - Decomposability: Understanding each component\n   - Simulatability: Can a human simulate the model?\n\n2. **Comprehensibility**\n   - Feature interpretability\n   - Parameter interpretability\n   - Decision boundary clarity\n\n3. **Complexity**\n   - Model size (number of parameters)\n   - Computational depth\n   - Rule/decision path length\n\n### Levels of Interpretability\n\n1. **Global Interpretability**\n   - Understanding entire model logic\n   - Overall decision-making process\n   - Complete model behaviour\n\n2. **Local Interpretability**\n   - Understanding individual predictions\n   - Instance-specific reasoning\n   - Decision for single input\n\n3. **Modular Interpretability**\n   - Understanding specific components\n   - Subsystem comprehension\n   - Layer or module-level insight\n\n## Interpretability Spectrum\n\n### High Interpretability Models\n\n1. **Linear Regression**\n   ```\n   y = β₀ + β₁x₁ + β₂x₂ + ... + βₙxₙ\n   ```\n   - **Interpretability**: Each coefficient shows feature contribution\n   - **Limitation**: Assumes linear relationships\n\n2. **Decision Trees**\n   ```\n   IF (credit_score > 700) AND (income > 50000)\n       THEN approve_loan\n   ELSE deny_loan\n   ```\n   - **Interpretability**: Clear if-then rules\n   - **Limitation**: Can become very large\n\n3. **Rule-Based Systems**\n   ```\n   Rule 1: IF age < 25 THEN risk = \"high\"\n   Rule 2: IF accidents > 2 THEN risk = \"high\"\n   ```\n   - **Interpretability**: Explicit logical rules\n   - **Limitation**: May not capture complex patterns\n\n4. **GAMs** (Generalized Additive Models)\n   ```\n   g(E[y]) = β₀ + f₁(x₁) + f₂(x₂) + ... + fₙ(xₙ)\n   ```\n   - **Interpretability**: Visualise each feature's effect\n   - **Limitation**: No feature interactions\n\n### Low Interpretability Models\n\n1. **Deep Neural Networks**\n   - Millions of parameters\n   - Non-linear transformations\n   - Distributed representations\n   - **Challenge**: Internal mechanism opaque\n\n2. **Random Forests**\n   - Ensemble of many trees\n   - Aggregated decisions\n   - **Challenge**: No single decision path\n\n3. **Gradient Boosting Machines**\n   - Sequential ensemble\n   - Complex feature interactions\n   - **Challenge**: Cumulative complexity\n\n4. **Support Vector Machines (nonlinear)**\n   - Kernel transformations\n   - High-dimensional space\n   - **Challenge**: Decision boundary in transformed space\n\n## Interpretability vs. Explainability\n\n| Interpretability | Explainability |\n|------------------|----------------|\n| **Nature**: Intrinsic property | **Nature**: External addition |\n| **Timing**: Design-time | **Timing**: Post-hoc or runtime |\n| **Approach**: Model architecture | **Approach**: Explanation methods |\n| **Goal**: Understand mechanism | **Goal**: Justify outputs |\n| **Question**: \"How does it work?\" | **Question**: \"Why this result?\" |\n| **Dependency**: Model-inherent | **Dependency**: Explanation tool |\n\n## Relationships\n\n- **Component Of**: AI Trustworthiness (AI-0061)\n- **Related To**: Explainability (AI-0063), Transparency (AI-0062)\n- **Enables**: Model Validation, Debugging, Trust\n- **Trade-off With**: Model Complexity, Performance (sometimes)\n\n## Measuring Interpretability\n\n### Objective Measures\n\n1. **Model Complexity Metrics**\n   - Number of parameters\n   - Depth of network\n   - Number of rules/nodes\n   - Decision path length\n\n2. **Simulatability**\n   - Can human trace through model?\n   - Time to understand model\n   - Cognitive load assessment\n\n3. **Decomposability**\n   - Number of interpretable components\n   - Component interaction complexity\n   - Modularity score\n\n### Subjective Measures\n\n1. **Human Evaluation**\n   - User studies on comprehension\n   - Expert assessment\n   - Task-based evaluation\n\n2. **Predictive Accuracy of Human Simulation**\n   - Can humans predict model outputs?\n   - Agreement rate between human understanding and model\n\n3. **Trust Calibration**\n   - Appropriate trust levels\n   - Understanding of limitations\n   - Confidence alignment\n\n## Design Principles for Interpretable Models\n\n### Christoph Molnar's Criteria\n\n1. **Simplicity**\n   - Fewer features\n   - Fewer parameters\n   - Shorter decision paths\n\n2. **Decomposability**\n   - Individual components understandable\n   - Clear component roles\n   - Minimal interactions\n\n3. **Algorithmic Transparency**\n   - Known mathematical properties\n   - Provable characteristics\n   - Well-understood behaviour\n\n### Inherently Interpretable Architectures\n\n1. **Sparse Linear Models**\n   - LASSO regularization\n   - Feature selection\n   - Coefficient interpretation\n\n2. **Shallow Decision Trees**\n   - Depth limits (e.g., max depth 3-5)\n   - Minimum samples per leaf\n   - Pruning strategies\n\n3. **Rule Lists and Sets**\n   - Sequential rules\n   - Non-overlapping conditions\n   - Ordered decision logic\n\n4. **Prototype-Based Models**\n   - k-Nearest Neighbours\n   - Case-based reasoning\n   - Exemplar models\n\n5. **Attention-Based Models**\n   - Self-attention mechanisms\n   - Attention weight visualization\n   - Learned focus areas\n\n## Domain-Specific Interpretability\n\n### Healthcare\n\n- **Requirement**: Clinical interpretability\n- **Rationale**: Life-critical decisions, regulatory requirements\n- **Approach**: Inherently interpretable models or rigorous validation\n- **Example**: Risk scores with clear factor weights\n\n### Finance\n\n- **Requirement**: Regulatory compliance (e.g., SR 11-7)\n- **Rationale**: Fair lending laws, consumer protection\n- **Approach**: Explainable credit models\n- **Example**: Linear models or shallow trees for credit scoring\n\n### Criminal Justice\n\n- **Requirement**: Due process, constitutional protections\n- **Rationale**: Fundamental rights implications\n- **Approach**: Transparent risk assessments\n- **Example**: Point-based scoring systems\n\n### Autonomous Systems\n\n- **Requirement**: Safety validation\n- **Rationale**: Real-time decision verification\n- **Approach**: Verifiable controllers\n- **Example**: Formal methods, decision trees for critical functions\n\n## Trade-offs and Considerations\n\n### Accuracy vs. Interpretability\n\n**Traditional View**:\n- Complex models (neural networks) → Higher accuracy\n- Simple models (linear, trees) → Lower accuracy\n\n**Modern Research**:\n- For many problems, interpretable models achieve comparable accuracy\n- High-stakes domains should prioritize interpretability\n- \"Stop explaining black boxes, use interpretable models\" (Rudin, 2019)\n\n### When to Prioritize Interpretability\n\n1. **High-Stakes Decisions**\n   - Healthcare diagnosis\n   - Criminal sentencing\n   - Loan approvals\n\n2. **Safety-Critical Systems**\n   - Autonomous vehicles\n   - Medical devices\n   - Aviation systems\n\n3. **Regulated Domains**\n   - Fair lending\n   - Employment\n   - Insurance\n\n4. **Scientific Discovery**\n   - Understanding phenomena\n   - Hypothesis generation\n   - Knowledge extraction\n\n### When Complex Models May Be Acceptable\n\n1. **Low-Stakes Applications**\n   - Content recommendations\n   - Image search\n   - Translation\n\n2. **With Rigorous Validation**\n   - Extensive testing\n   - Post-hoc explanation validation\n   - Human oversight\n\n3. **Where Interpretable Models Fail**\n   - Proven accuracy gap\n   - Complex patterns require complexity\n   - Alternative safeguards in place\n\n## Technical Implementation\n\n### Designing Interpretable Neural Networks\n\n1. **Constrained Architectures**\n   ```python\n   class InterpretableNN:\n       def __init__(self, input_dim, hidden_dim):\n           # Shallow network\n           self.layer1 = nn.Linear(input_dim, hidden_dim)\n           self.layer2 = nn.Linear(hidden_dim, 1)\n           # Sparse connections\n           self.apply_sparsity_constraint()\n\n       def forward(self, x):\n           # Monotonic activation for interpretability\n           h = F.relu(self.layer1(x))\n           return self.layer2(h)\n   ```\n\n2. **Prototype-Based Deep Learning**\n   - ProtoPNet: Learns interpretable prototypes\n   - Decisions based on similarity to prototypes\n   - \"This looks like [prototype] therefore [class]\"\n\n3. **Attention Mechanisms**\n   - Visualise what model focuses on\n   - Weight-based interpretation\n   - Alignment with human reasoning\n\n### Interpretability-Enhancing Techniques\n\n1. **Feature Engineering**\n   - Use meaningful, interpretable features\n   - Avoid opaque transformations\n   - Domain-informed features\n\n2. **Regularization for Sparsity**\n   - L1 (LASSO) for feature selection\n   - Encourage zero coefficients\n   - Reduce model complexity\n\n3. **Monotonicity Constraints**\n   - Enforce logical relationships\n   - \"Higher income → higher credit score\"\n   - Align with domain knowledge\n\n4. **Structural Constraints**\n   - Limit model depth/width\n   - Impose tree structure\n   - Use additive models\n\n## Evaluation and Validation\n\n### Formal Verification\n\n1. **Completeness**\n   - All possible inputs covered\n   - No undefined behaviours\n   - Full decision space specified\n\n2. **Consistency**\n   - No contradictory rules\n   - Logical coherence\n   - Deterministic outputs\n\n3. **Soundness**\n   - Correct reasoning\n   - Valid inference\n   - Mathematically proven properties\n\n### Human-Centered Evaluation\n\n1. **User Studies**\n   - Task performance with model\n   - Comprehension tests\n   - Trust calibration\n\n2. **Expert Review**\n   - Domain expert validation\n   - Alignment with expert knowledge\n   - Identification of errors\n\n3. **Cognitive Walkthrough**\n   - Step-by-step decision tracing\n   - Understanding verification\n   - Complexity assessment\n\n## Best Practices\n\n1. **Start with Interpretable Models**\n   - Baseline with simple models\n   - Only increase complexity if necessary\n   - Document rationale for complexity\n\n2. **Use Domain Knowledge**\n   - Incorporate expert insights\n   - Align features with domain concepts\n   - Validate against domain understanding\n\n3. **Design for Users**\n   - Consider stakeholder technical level\n   - Match interpretability to use case\n   - Provide multiple views\n\n4. **Document Limitations**\n   - Be clear about what is not interpretable\n   - Explain assumptions\n   - Acknowledge uncertainty\n\n5. **Validate Understanding**\n   - Test human comprehension\n   - Verify correct interpretation\n   - Measure alignment with model behaviour\n\n6. **Iterative Refinement**\n   - Gather user feedback\n   - Improve based on comprehension issues\n   - Simplify where possible\n\n## Related Terms\n\n- **AI Trustworthiness** (AI-0061)\n- **Transparency** (AI-0062)\n- **Explainability** (AI-0063)\n- **Model Validation** (AI-0095)\n- **Accountability** (AI-0068)\n\n## Version History\n\n- **1.0** (2025-10-27): Initial definition based on ISO/IEC TR 24029-1:2021 and interpretability research\n\n---\n\n*This definition aligns with the growing consensus that interpretability should be a design goal for high-stakes AI systems.*\n\t- maturity:: draft\n\t- owl:class:: mv:Interpretability\n\t- owl:physicality:: ConceptualEntity\n\t- owl:role:: Concept\n\t- belongsToDomain:: [[MetaverseDomain]]\n\t- #### Relationships\n\t  id:: interpretability-relationships\n\t\t- is-subclass-of:: [[ArtificialIntelligence]]\n\t\t- enables:: [[Model Validation]], [[Debugging]], [[Trust]]",
  "backlinks": [],
  "wiki_links": [
    "Trust",
    "MetaverseDomain",
    "ArtificialIntelligence",
    "Debugging",
    "Model Validation"
  ],
  "ontology": {
    "term_id": "AI-0064",
    "preferred_term": "Interpretability",
    "alt_terms": [],
    "iri": "http://narrativegoldmine.com/metaverse#Interpretability",
    "source_domain": null,
    "domain": "ai",
    "domain_full_name": "Artificial Intelligence",
    "definition": "The degree to which a human can understand the internal mechanics, decision-making processes, and cause-effect relationships within an AI system, independent of external explanation tools.",
    "scope_note": null,
    "status": "draft",
    "maturity": "draft",
    "version": null,
    "public_access": true,
    "last_updated": null,
    "authority_score": null,
    "quality_score": null,
    "cross_domain_links": null,
    "owl_class": "mv:Interpretability",
    "owl_physicality": "ConceptualEntity",
    "owl_role": "Concept",
    "owl_inferred_class": null,
    "is_subclass_of": [
      "ArtificialIntelligence"
    ],
    "has_part": [],
    "is_part_of": [],
    "requires": [],
    "depends_on": [],
    "enables": [
      "Trust",
      "Model Validation",
      "Debugging"
    ],
    "relates_to": [],
    "bridges_to": [],
    "bridges_from": [],
    "domain_extensions": {},
    "belongs_to_domain": [
      "MetaverseDomain"
    ],
    "implemented_in_layer": [],
    "source": [],
    "other_relationships": {
      "belongsToDomain": [
        "MetaverseDomain"
      ]
    },
    "validation": {
      "is_valid": false,
      "errors": [
        "Missing required property: source-domain",
        "Missing required property: last-updated",
        "owl:class namespace 'mv' doesn't match source-domain 'ai'"
      ]
    }
  }
}