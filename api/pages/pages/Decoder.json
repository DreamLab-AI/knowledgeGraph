{
  "id": "Decoder",
  "title": "Decoder",
  "content": "- ### OntologyBlock\n  id:: decoder-ontology\n  collapsed:: true\n\t- ontology:: true\n\t- term-id:: AI-0206\n\t- preferred-term:: Decoder\n\t- source-domain:: metaverse\n\t- status:: draft\n\t- public-access:: true\n\n\n\n\n### OWL Classification\n\t- owl:class:: mv:Decoder\n\t- owl:physicality:: ConceptualEntity\n\t- owl:role:: Concept\n\n### Domain & Architecture\n\t- belongsToDomain:: [[MetaverseDomain]]\n\t- maturity:: draft\n\n### Relationships\n\n## Characteristics\n\n- **Autoregressive Generation**: Generates tokens sequentially\n- **Masked Self-Attention**: Prevents attending to future positions\n- **Cross-Attention**: Attends to encoder representations\n- **Causal Structure**: Maintains left-to-right generation order\n\n## Academic Foundations\n\n**Primary Source**: Vaswani et al., \"Attention Is All You Need\", arXiv:1706.03762 (2017)\n\n**Architecture**: Each decoder layer contains masked self-attention, encoder-decoder cross-attention, and a feed-forward network with residual connections.\n\n## Technical Context\n\nThe decoder generates output sequences autoregressively, attending to both previously generated tokens (via masked self-attention) and the encoder's output (via cross-attention). GPT-style models use decoder-only architecture without cross-attention.\n\n## Ontological Relationships\n\n- **Broader Term**: Transformer Architecture Component\n- **Related Terms**: Encoder, Cross-Attention, Causal Attention\n- **Examples**: GPT, GPT-2, GPT-3 (decoder-only models)\n\n## Usage Context\n\n\"The transformer decoder uses masked self-attention and cross-attention to generate output sequences autoregressively.\"\n\n## OWL Functional Syntax\n\n```clojure\n(Declaration (Class :Decoder))\n(AnnotationAssertion rdfs:label :Decoder \"Decoder\"@en)\n(AnnotationAssertion rdfs:comment :Decoder\n  \"Component that generates output sequence autoregressively using masked self-attention and cross-attention.\"@en)\n(AnnotationAssertion :hasSource :Decoder\n  \"Vaswani et al., 'Attention Is All You Need', arXiv:1706.03762 (2017)\"@en)\n\n;; Taxonomic relationships\n(SubClassOf :Decoder :TransformerArchitectureComponent)\n\n;; Structural composition\n(SubClassOf :Decoder\n  (ObjectSomeValuesFrom :consistsOfLayers :DecoderLayer))\n(SubClassOf :Decoder\n  (ObjectMinCardinality 1 :consistsOfLayers :DecoderLayer))\n\n;; Decoder layer structure\n(Declaration (Class :DecoderLayer))\n(SubClassOf :DecoderLayer\n  (ObjectSomeValuesFrom :contains :MaskedSelfAttention))\n(SubClassOf :DecoderLayer\n  (ObjectSomeValuesFrom :contains :CrossAttention))\n(SubClassOf :DecoderLayer\n  (ObjectSomeValuesFrom :contains :FeedForwardNetwork))\n(SubClassOf :DecoderLayer\n  (ObjectSomeValuesFrom :uses :ResidualConnection))\n(SubClassOf :DecoderLayer\n  (ObjectSomeValuesFrom :uses :LayerNormalisation))\n\n;; Attention mechanisms\n(SubClassOf :Decoder\n  (ObjectSomeValuesFrom :implementsMechanism :CausalAttention))\n(SubClassOf :Decoder\n  (ObjectSomeValuesFrom :implementsMechanism :CrossAttention))\n\n;; Generation characteristics\n(DataPropertyAssertion :isAutoregressive :Decoder \"true\"^^xsd:boolean)\n(DataPropertyAssertion :generatesSequentially :Decoder \"true\"^^xsd:boolean)\n(DataPropertyAssertion :maintainsCausalStructure :Decoder \"true\"^^xsd:boolean)\n\n;; Typical configurations\n(DataPropertyAssertion :typicalLayerCount :Decoder \"6\"^^xsd:integer)\n(DataPropertyAssertion :largeModelLayerCount :Decoder \"24\"^^xsd:integer)\n\n;; Decoder-only models (without cross-attention)\n(Declaration (Class :DecoderOnlyModel))\n(SubClassOf :GPT :DecoderOnlyModel)\n(SubClassOf :GPT2 :DecoderOnlyModel)\n(SubClassOf :GPT3 :DecoderOnlyModel)\n(SubClassOf :GPT4 :DecoderOnlyModel)\n\n(AnnotationAssertion rdfs:comment :DecoderOnlyModel\n  \"Transformer models using only masked self-attention without encoder cross-attention\"@en)\n\n;; Functional properties\n(SubClassOf :Decoder\n  (ObjectSomeValuesFrom :attendsTo :EncoderOutput))\n(SubClassOf :Decoder\n  (ObjectSomeValuesFrom :attendsTo :PreviouslyGeneratedTokens))\n(SubClassOf :Decoder\n  (ObjectSomeValuesFrom :generates :OutputSequence))\n```\n\n## References\n\n- Vaswani, A., et al. (2017). \"Attention Is All You Need\". arXiv:1706.03762\n- Radford, A., et al. (2018). \"Improving Language Understanding by Generative Pre-Training\"\n\n---\n\n*Ontology Term managed by AI-Grounded Ontology Working Group*\n*UK English Spelling Standards Applied*\n\t- maturity:: draft\n\t- owl:class:: mv:Decoder\n\t- owl:physicality:: ConceptualEntity\n\t- owl:role:: Concept\n\t- is-subclass-of:: [[ArtificialIntelligence]]\n\t- belongsToDomain:: [[MetaverseDomain]]",
  "backlinks": [],
  "wiki_links": [
    "MetaverseDomain",
    "ArtificialIntelligence"
  ],
  "ontology": {
    "term_id": "AI-0206",
    "preferred_term": "Decoder",
    "alt_terms": [],
    "iri": "http://narrativegoldmine.com/metaverse#Decoder",
    "source_domain": "metaverse",
    "domain": "metaverse",
    "domain_full_name": "",
    "definition": null,
    "scope_note": null,
    "status": "draft",
    "maturity": "draft",
    "version": null,
    "public_access": true,
    "last_updated": null,
    "authority_score": null,
    "quality_score": null,
    "cross_domain_links": null,
    "owl_class": "mv:Decoder",
    "owl_physicality": "ConceptualEntity",
    "owl_role": "Concept",
    "owl_inferred_class": null,
    "is_subclass_of": [],
    "has_part": [],
    "is_part_of": [],
    "requires": [],
    "depends_on": [],
    "enables": [],
    "relates_to": [],
    "bridges_to": [],
    "bridges_from": [],
    "domain_extensions": {},
    "belongs_to_domain": [
      "MetaverseDomain",
      "MetaverseDomain"
    ],
    "implemented_in_layer": [],
    "source": [],
    "validation": {
      "is_valid": false,
      "errors": [
        "Missing required property: last-updated",
        "Missing required property: definition",
        "Missing required property: is-subclass-of (at least one parent class)",
        "owl:class namespace 'mv' doesn't match source-domain 'metaverse'"
      ]
    }
  }
}