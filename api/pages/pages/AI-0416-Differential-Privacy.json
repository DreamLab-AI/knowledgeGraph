{
  "id": "AI-0416-Differential-Privacy",
  "title": "Differential Privacy",
  "content": "- ### OntologyBlock\n  id:: differential-privacy-ontology\n  collapsed:: true\n\t- ontology:: true\n\t- term-id:: AI-0416\n\t- preferred-term:: Differential Privacy\n\t- source-domain:: ai\n\t- status:: in\n\t- public-access:: true\n\t- definition:: Differential Privacy is a mathematical framework providing provable privacy guarantees by adding carefully calibrated noise to data queries or model outputs, ensuring that the presence or absence of any single individual's data has negligible impact on analysis results. This technique provides formal privacy protection through the epsilon (ε) parameter quantifying privacy loss, where smaller ε values indicate stronger privacy guarantees (typically ε ≤ 1.0 for high-privacy scenarios), with differential privacy satisfied when for all datasets D1 and D2 differing by one record and all possible outputs S, P(M(D1) ∈ S) ≤ exp(ε) × P(M(D2) ∈ S). Implementation mechanisms include the Laplace mechanism adding noise proportional to query sensitivity for numeric queries, the Gaussian mechanism suitable for more complex settings with delta (δ) parameter allowing negligible probability of privacy breach, the exponential mechanism for non-numeric outputs selecting results proportional to their utility, and composition theorems tracking cumulative privacy loss across multiple queries (sequential composition where total ε_total = Σε_i, advanced composition providing tighter bounds). The 2024-2025 period witnessed differential privacy evolve from theoretical framework to practical requirement with the U.S. Census Bureau's 2020 Census deployment demonstrating feasibility at national scale, technology companies including Apple, Microsoft, and Meta deploying differential privacy for telemetry and usage analytics proving strong privacy need not preclude valuable aggregate insights, and academic consensus emerging around epsilon budgets with ε ≤ 1.0 for high-privacy scenarios. Applications span statistical databases enabling privacy-preserving aggregate statistics, machine learning protecting training data through differentially private stochastic gradient descent (DP-SGD), and federated learning scenarios adding noise to model updates before aggregation, though challenges include computational overhead of noise addition, utility degradation particularly for complex queries or small datasets, and privacy budget exhaustion requiring careful allocation across queries.\n\t- source:: [[Dwork et al. (2006)]], [[U.S. Census 2020]], [[Apple Differential Privacy]]\n\t- maturity:: mature\n\t- owl:class:: ai:DifferentialPrivacy\n\t- owl:physicality:: VirtualEntity\n\t- owl:role:: Process\n\t- owl:inferred-class:: aigo:VirtualProcess\n\t- belongsToDomain:: [[AIEthicsDomain]]\n\t- implementedInLayer:: [[ConceptualLayer]]\n\n## Metadata\n\n- **Last Updated**: 2025-11-16\n- **Review Status**: Automated remediation with 2025 context\n- **Verification**: Academic sources verified\n- **Regional Context**: UK/North England where applicable",
  "backlinks": [],
  "wiki_links": [
    "Dwork et al. (2006)",
    "U.S. Census 2020",
    "Apple Differential Privacy",
    "ConceptualLayer",
    "AIEthicsDomain"
  ],
  "ontology": {
    "term_id": "AI-0416",
    "preferred_term": "Differential Privacy",
    "alt_terms": [],
    "iri": "http://narrativegoldmine.com/ai#DifferentialPrivacy",
    "source_domain": "ai",
    "domain": "ai",
    "domain_full_name": "Artificial Intelligence",
    "definition": "Differential Privacy is a mathematical framework providing provable privacy guarantees by adding carefully calibrated noise to data queries or model outputs, ensuring that the presence or absence of any single individual's data has negligible impact on analysis results. This technique provides formal privacy protection through the epsilon (ε) parameter quantifying privacy loss, where smaller ε values indicate stronger privacy guarantees (typically ε ≤ 1.0 for high-privacy scenarios), with differential privacy satisfied when for all datasets D1 and D2 differing by one record and all possible outputs S, P(M(D1) ∈ S) ≤ exp(ε) × P(M(D2) ∈ S). Implementation mechanisms include the Laplace mechanism adding noise proportional to query sensitivity for numeric queries, the Gaussian mechanism suitable for more complex settings with delta (δ) parameter allowing negligible probability of privacy breach, the exponential mechanism for non-numeric outputs selecting results proportional to their utility, and composition theorems tracking cumulative privacy loss across multiple queries (sequential composition where total ε_total = Σε_i, advanced composition providing tighter bounds). The 2024-2025 period witnessed differential privacy evolve from theoretical framework to practical requirement with the U.S. Census Bureau's 2020 Census deployment demonstrating feasibility at national scale, technology companies including Apple, Microsoft, and Meta deploying differential privacy for telemetry and usage analytics proving strong privacy need not preclude valuable aggregate insights, and academic consensus emerging around epsilon budgets with ε ≤ 1.0 for high-privacy scenarios. Applications span statistical databases enabling privacy-preserving aggregate statistics, machine learning protecting training data through differentially private stochastic gradient descent (DP-SGD), and federated learning scenarios adding noise to model updates before aggregation, though challenges include computational overhead of noise addition, utility degradation particularly for complex queries or small datasets, and privacy budget exhaustion requiring careful allocation across queries.",
    "scope_note": null,
    "status": "in",
    "maturity": "mature",
    "version": null,
    "public_access": true,
    "last_updated": null,
    "authority_score": null,
    "quality_score": null,
    "cross_domain_links": null,
    "owl_class": "ai:DifferentialPrivacy",
    "owl_physicality": "VirtualEntity",
    "owl_role": "Process",
    "owl_inferred_class": "aigo:VirtualProcess",
    "is_subclass_of": [],
    "has_part": [],
    "is_part_of": [],
    "requires": [],
    "depends_on": [],
    "enables": [],
    "relates_to": [],
    "bridges_to": [],
    "bridges_from": [],
    "domain_extensions": {},
    "belongs_to_domain": [
      "AIEthicsDomain"
    ],
    "implemented_in_layer": [
      "ConceptualLayer"
    ],
    "source": [
      "Dwork et al. (2006)",
      "U.S. Census 2020",
      "Apple Differential Privacy"
    ],
    "validation": {
      "is_valid": false,
      "errors": [
        "Missing required property: last-updated",
        "Missing required property: is-subclass-of (at least one parent class)"
      ]
    }
  }
}