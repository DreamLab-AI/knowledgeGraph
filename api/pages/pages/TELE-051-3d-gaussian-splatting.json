{
  "id": "TELE-051-3d-gaussian-splatting",
  "title": "3D Gaussian Splatting",
  "content": "- ### OntologyBlock\n  id:: gaussian-splatting-ontology\n  collapsed:: true\n\t- ontology:: true\n\t- term-id:: TELE-051\n\t- preferred-term:: 3D Gaussian Splatting\n\t- source-domain:: tc\n\t- owl:class:: tc:ThreeDGaussianSplatting\n\t- status:: active\n\t- public-access:: true\n\t- definition:: \"A neural rendering technique that represents 3D scenes as collections of millions of 3D Gaussian primitives with learnable positions, colours, opacities, and covariances, enabling photorealistic real-time rendering at 100+ frames per second through GPU-accelerated rasterisation, revolutionising telepresence and immersive collaboration with unprecedented visual fidelity.\"\n\t- maturity:: developing\n\t- owl:physicality:: ConceptualEntity\n\t- owl:role:: Process\n\t- #### Relationships\n\t  id:: gaussian-splatting-relationships\n\t  collapsed:: true\n\t\t- is-subclass-of:: [[NeuralRendering]]\n\t\t- is-subclass-of:: [[TELE-050-neural-rendering-telepresence]]\n\t\t- is-subclass-of:: [[3DReconstruction]]\n\t\t- requires:: [[MultiViewImages]]\n\t\t- requires:: [[DifferentiableRendering]]\n\t\t- requires:: [[GPUAcceleration]]\n\t\t- requires:: [[GradientDescent]]\n\t\t- enables:: [[PhotorealisticTelepresence]]\n\t\t- enables:: [[NovelViewSynthesis]]\n\t\t- enables:: [[RealTimeRendering]]\n\t\t- related-to:: [[TELE-053-volumetric-video-conferencing]]\n\t\t- related-to:: [[TELE-052-neural-radiance-fields]]\n\t\t- related-to:: [[TELE-060-instant-ngp]]\n\n## Definition\n\n**3D Gaussian Splatting** is a breakthrough neural rendering method published at SIGGRAPH 2023 by Kerbl et al., representing 3D scenes as explicit collections of anisotropic 3D Gaussian distributions rather than implicit neural networks. Each Gaussian primitive encodes a 3D position (mean), colour, opacity, and 3√ó3 covariance matrix defining its shape and orientation in space. Rendering involves \"splatting\" these Gaussians onto the image plane through differentiable rasterisation, achieving photorealistic quality at 100-300 frames per second on consumer GPUs‚Äîover 100√ó faster than Neural Radiance Fields ([[TELE-052-neural-radiance-fields]]) whilst matching or exceeding visual fidelity.\n\nThe technique trains by optimising Gaussian parameters (positions, colours, covariances) to match input multi-view photographs through gradient descent, starting with sparse 3D point clouds from Structure-from-Motion (SfM). Gaussians are adaptively split, cloned, or pruned during optimisation to capture fine detail (hair strands, foliage) or remove redundancy. The explicit scene representation enables real-time rendering through GPU rasterisation pipelines, unlocking applications in [[TELE-020-virtual-reality-telepresence]], [[TELE-053-volumetric-video-conferencing]], and immersive telepresence where photorealistic environments must render at 90+ FPS for comfortable VR.\n\n## Current Landscape (2025)\n\n3D Gaussian Splatting has rapidly transitioned from academic novelty to production deployment, with major telepresence platforms integrating the technology for photorealistic avatars and environments.\n\n**Adoption Statistics**:\n- 67% of neural rendering research papers (2024-2025) employ Gaussian splatting variants (arXiv analysis)\n- Meta, Apple, Niantic incorporate Gaussian splatting in AR/VR pipelines\n- 14,000+ GitHub stars on official implementation (most-starred graphics paper 2023)\n- Consumer apps (Luma AI, PolyCam) enable smartphone Gaussian capture\n\n**Technology Capabilities (2025)**:\n- **Training Time**: 30 minutes for room-scale scene on RTX 4090 (vs. 24 hours for NeRF)\n- **Rendering Speed**: 150-300 FPS at 1080p resolution\n- **Quality**: PSNR 30-35 dB (comparable to NeRF, exceeding mesh-based methods)\n- **Scene Size**: Millions of Gaussians represent entire buildings\n\n**UK Context**:\n- **Luma AI** (London office): Develops NeRF-to-Gaussian conversion tools\n- **PolyCam** (UK users): Gaussian splatting mode in 3D scanning app\n- **University of Oxford**: Research on dynamic Gaussian splatting for moving objects\n- **Imperial College London**: Compression techniques for streaming Gaussian scenes\n\n## Technical Details\n\n### Scene Representation\n\nEach 3D Gaussian primitive ùí¢·µ¢ defined by:\n- **Mean Œº·µ¢ ‚àà ‚Ñù¬≥**: 3D position in world space\n- **Covariance Œ£·µ¢ ‚àà ‚Ñù¬≥À£¬≥**: Defines ellipsoidal shape/orientation\n- **Colour c·µ¢ ‚àà ‚Ñù¬≥** (or spherical harmonics for view-dependent appearance)\n- **Opacity Œ±·µ¢ ‚àà [0,1]**: Transparency\n\nGaussian function: G(x) = exp(-¬Ω(x-Œº)·µÄŒ£‚Åª¬π(x-Œº))\n\n### Rendering Pipeline\n\n1. **Projection**: Transform 3D Gaussians to 2D image space\n   - Project mean Œº·µ¢ via camera matrix\n   - Approximate 2D covariance via Jacobian of projection\n\n2. **Sorting**: Order Gaussians by depth (painter's algorithm with Œ±-blending)\n\n3. **Rasterisation**: For each pixel, accumulate colours of overlapping Gaussians\n   - Front-to-back traversal with early stopping when opacity saturates\n   - GPU-accelerated parallel processing\n\n4. **Output**: Photorealistic rendered image from novel viewpoint\n\n### Optimisation\n\n**Input**: 50-200 multi-view photographs with camera poses (from SfM)\n\n**Initialisation**: Sparse 3D point cloud ‚Üí one Gaussian per point\n\n**Loss Function**: L1 + SSIM (Structural Similarity Index) between rendered and ground truth images\n\n**Optimisation**:\n- Stochastic gradient descent with Adam optimiser\n- 30,000 iterations (~30 minutes on RTX 4090)\n- Adaptive density control: split under-reconstructed regions, prune low-opacity Gaussians\n\n**Result**: Millions of optimised Gaussians encoding scene\n\n### Advantages Over NeRF\n\n| Aspect | Gaussian Splatting | Neural Radiance Fields ([[TELE-052-neural-radiance-fields]]) |\n|--------|-------------------|--------------------------------------------------------------|\n| **Rendering Speed** | 100-300 FPS | 0.1-1 FPS (real-time variants: 30 FPS) |\n| **Training Time** | 30 minutes | 12-48 hours |\n| **Quality** | Photorealistic (30-35 dB PSNR) | Photorealistic (30-36 dB PSNR) |\n| **Representation** | Explicit (Gaussians) | Implicit (MLP weights) |\n| **Memory** | 100-500 MB per scene | 10-50 MB (more compact) |\n| **Editability** | Easy (move/delete Gaussians) | Difficult (retrain network) |\n\n## Applications in Telepresence\n\n### Photorealistic Virtual Environments ([[TELE-020-virtual-reality-telepresence]])\n- Scan real office spaces with smartphones (50-100 photos)\n- Train Gaussian scene in 30 minutes\n- Render in VR at 90 FPS for telepresence meetings\n- Example: Meta Horizon Workrooms experimenting with Gaussian environments (2025)\n\n### Volumetric Video Conferencing ([[TELE-053-volumetric-video-conferencing]])\n- Capture participant with multi-camera rig (6-12 cameras)\n- Real-time Gaussian optimisation (30 Hz update rate)\n- Stream compressed Gaussians to remote clients\n- Render photorealistic avatar from any angle\n- Example: Microsoft Mesh exploring dynamic Gaussian avatars\n\n### Virtual Tourism\n- Museums digitise exhibits with Gaussian scans\n- Remote visitors navigate photorealistic 3D environments\n- Example: Luma AI captures heritage sites for virtual tours\n\n### Remote Site Inspection\n- Construction sites scanned with drones\n- Engineers inspect progress remotely in photorealistic 3D\n- Example: UK engineering firms use PolyCam for site documentation\n\n## Technical Challenges and Solutions\n\n### Challenge: Large File Sizes\n- **Problem**: Millions of Gaussians ‚Üí 500 MB+ per scene\n- **Solution**: Neural compression, quantisation (reduce to 50-100 MB)\n- **Research**: Compact 3DGS, EAGLES (entropy-aware compression)\n\n### Challenge: Dynamic Scenes\n- **Problem**: Original technique assumes static scenes\n- **Solution**: 4D Gaussian splatting (add time dimension), deformable Gaussians\n- **Research**: Dynamic 3DGS, 4DGaussians (moving people, avatars)\n\n### Challenge: Training Data Requirements\n- **Problem**: Needs 50-200 high-quality photos with accurate poses\n- **Solution**: Structure-from-Motion automation, smartphone capture apps\n- **Tools**: COLMAP (SfM), Luma AI app, PolyCam\n\n### Challenge: Real-Time Streaming\n- **Problem**: 500 MB scenes unsuitable for network streaming\n- **Solution**: Progressive transmission (coarse-to-fine), level-of-detail rendering\n- **Research**: Streamable Gaussians, LoD-GS\n\n## Future Directions\n\n**Near-Term (2025-2027)**:\n- Real-time Gaussian capture from single RGB-D camera (iPhone LiDAR)\n- Compression to <50 MB per scene for mobile deployment\n- Integration into WebXR standard (browser-based Gaussian rendering)\n\n**Medium-Term (2027-2030)**:\n- Photorealistic full-body Gaussian avatars updating at 60 Hz\n- Gaussian-based telepresence as default in Meta/Apple platforms\n- Semantic Gaussians (each primitive labelled: \"table\", \"wall\", etc.)\n\n**Long-Term (2030+)**:\n- Neural codecs compressing Gaussians 100√ó further\n- Light-field displays rendering Gaussians holographically (no headset)\n- Real-time global illumination in Gaussian scenes (ray tracing)\n\n## Related Concepts\n\n- [[TELE-050-neural-rendering-telepresence]]\n- [[TELE-052-neural-radiance-fields]]\n- [[TELE-053-volumetric-video-conferencing]]\n- [[TELE-020-virtual-reality-telepresence]]\n- [[TELE-060-instant-ngp]]\n\n## Academic References\n\n1. Kerbl, B., Kopanas, G., Leimk√ºhler, T., & Drettakis, G. (2023). \"3D Gaussian Splatting for Real-Time Radiance Field Rendering\". *ACM Transactions on Graphics (SIGGRAPH)*, 42(4), 1-14.\n2. Luiten, J., et al. (2023). \"Dynamic 3D Gaussians: Tracking by Persistent Dynamic View Synthesis\". *arXiv preprint*.\n3. Niedermayr, S., et al. (2024). \"Compressed 3D Gaussian Splatting for Accelerated Novel View Synthesis\". *CVPR 2024*.\n\n## Open-Source Implementations\n\n- **Official**: https://github.com/graphdeco-inria/gaussian-splatting\n- **Nerfstudio**: Gaussian splatting module in unified NeRF framework\n- **gsplat**: PyTorch library for differentiable Gaussian rasterisation\n- **WebGL Viewer**: Real-time browser-based Gaussian rendering\n\n## Metadata\n\n- **Term-ID**: TELE-051\n- **Last Updated**: 2025-11-16\n- **Maturity**: Developing\n- **Authority Score**: 0.91\n- **UK Context**: High (Luma AI, university research)\n- **Cross-Domain**: Bridges to AI, Metaverse\n\n## Related Content: Gaussian splatting and Similar\n\npublic:: true\n\n- #Public page automatically published\n- {{video https://www.youtube.com/watch?v=lG3g8mYKfqU}}\n- # Gaussian Splatting\n\t- [Instantsplat: Unbounded Sparse-view Pose-free Gaussian Splatting in 40 Seconds](https://instantsplat.github.io/)\n\t- [The Rise Of 3D Gaussian Splatting: What Is It And How Is It Changing The Immersive Media Industry? ‚Äî Magnopus](https://www.magnopus.com/blog/the-rise-of-3d-gaussian-splatting)\n\t- 4D [[Gaussian splatting and Similar]] [with time domain](https://github.com/hustvl/4DGaussians)\n\t- [[Gaussian splatting and Similar]] [gsgen](https://github.com/gsgen3d/gsgen)\n\t- Room scale [[Gaussian splatting and Similar]] technique for single lens (#SLAM) [[Scene Capture and Reconstruction]]  [Gaussian-SLAM: Photo-realistic Dense SLAM with Gaussian Splatting (vladimiryugay.github.io)](https://vladimiryugay.github.io/gaussian_slam/)\n\t- [Mip-Splatting (niujinshuchong.github.io)](https://niujinshuchong.github.io/mip-splatting/) reduced artefacts in [[Gaussian splatting and Similar]]\n\t- [Gaussian-SLAM: Photo-realistic Dense SLAM with Gaussian Splatting (vladimiryugay.github.io)](https://vladimiryugay.github.io/gaussian_slam/)\n\t- GaussianDiffusion: 3D Gaussian Splatting for Denoising Diffusion Probabilistic Models with Structured Noise\n\t\t- logseq://graph/researchpapers?block-id=6579a51f-5e6d-4570-903f-9458f84e845f\n\t- Gaussian [[SLAM]] rooms scale scanning\n\t\t- logseq://graph/researchpapers?block-id=6579a880-ce7f-4a79-b3d3-9135ff4348b3\n\t\t- [Gaussian Splatting SLAM (rmurai.co.uk)](https://rmurai.co.uk/projects/GaussianSplattingSLAM/)  is near real-time\n\t- [Paper page TRIPS: Trilinear Point Splatting for Real-Time Radiance Field Rendering (huggingface.co)](https://huggingface.co/papers/2401.06003)\n\t- [Deblurring 3D Gaussian Splatting (benhenryl.github.io)](https://benhenryl.github.io/Deblurring-3D-Gaussian-Splatting/)\n\t- [huggingface/gsplat.js: JavaScript Gaussian Splatting library. (github.com)](https://github.com/huggingface/gsplat.js/)\n\t- [[Gaussian splatting and Similar]] in Houdini\n\t- [Triplane Meets Gaussian Splatting: Fast and Generalizable Single-View 3D Reconstruction with Transformers (zouzx.github.io)](https://zouzx.github.io/TriplaneGaussian/) understandable [[3D and 4D]] from [[Gaussian splatting and Similar]]\n\t- [dynamic3dgaussians.github.io](https://dynamic3dgaussians.github.io/) using a multi [[Motion Capture]] dome and [[Gaussian splatting and Similar]] for 6DOF [[Human tracking and SLAM capture]]\n\t- [LangSplat: 3D Language Gaussian Splatting](https://langsplat.github.io/)\n- # NeRFs\n\t- MobileNeRF: This approach adapts NeRFs for mobile devices by exploiting the polygon rasterization pipeline for efficient neural field rendering. It achieves very fast rendering times (0.016-0.017s) but requires long training times[](https://spectrum.ieee.org/ai-graphics-neural-rendering).\n\t- MobileR2L: This method uses a full CNN-based neural light field model with a super-resolution model in its second stage. It achieves real-time inference on mobile devices while maintaining high image quality, rendering a 1008x756 image of real 3D scenes in 18.04ms on an iPhone 13[](https://spectrum.ieee.org/ai-graphics-neural-rendering).\n\t- Instant NGP (Neural Graphics Primitives): Developed by NVIDIA, this technique significantly speeds up the training and rendering of NeRFs, allowing for near-instantaneous scene reconstruction[](https://github.com/weihaox/awesome-neural-rendering/blob/master/docs/INTRODUCTION-AND-SURVEY.md).\n\t- Plenoxels (Plenoptic Voxels): This method replaces neural networks with a sparse 3D grid of spherical harmonics, enabling faster training and competitive quality compared to NeRFs[](https://github.com/weihaox/awesome-neural-rendering/blob/master/docs/INTRODUCTION-AND-SURVEY.md).\n\t- NGLOD (Neural Geometric Level of Detail): This approach combines neural implicit representations with explicit geometric representations, allowing for multi-resolution rendering and faster training[](https://arxiv.org/abs/2402.00028).\n\t- NeRF-MAE (Masked AutoEncoders for NeRFs): This technique applies the concept of masked autoencoders to NeRFs for self-supervised 3D representation learning, potentially improving generalization and efficiency[](https://ideas-ncbr.pl/en/research/neural-rendering/).\n\t- ## NeRFs vs Hardware Acceleration\n\t\t- old page, needs [[Update Cycle]]\n\t\t- [Neural Rendering and Its Hardware Acceleration: A Review (arxiv.org)](https://arxiv.org/html/2402.00028v1)\n\t\t- | Paper                                  | Neural Network Type | Residual Layer | Concatenation Layer | Suitability for Low-end Mobile Hardware |\n\t\t  |----------------------------------------|---------------------|----------------|---------------------|----------------------------------------|\n\t\t  | GIRAFFE                                | MLP, CNN            | Required       | Required            | 7                                      |\n\t\t  | Render Net                             | MLP, CNN            | Not Required   | Required            | 6                                      |\n\t\t  | Neural Voxel Renderer                  | MLP, CNN            | Not Required   | Required            | 5                                      |\n\t\t  | Neural Volumes                         | MLP, CNN            | Not Required   | Required            | 5                                      |\n\t\t  | NeRF                                   | MLP                 | Not Required   | Required            | 8                                      |\n\t\t  | NeRF in the Wild                       | MLP                 | Not Required   | Required            | 7                                      |\n\t\t  | KiloNeRF                               | MLP                 | Not Required   | Required            | 8                                      |\n\t\t  | FastNeRF                               | MLP                 | Not Required   | Required            | 9                                      |\n\t\t  | Plenoctrees                            | MLP                 | Not Required   | Required            | 8                                      |\n\t\t  | Instant Neural Graphics Primitives     | MLP                 | Not Required   | Required            | 9                                      |\n\t\t  | Scene Representation Networks          | MLP                 | Not Required   | Required            | 7                                      |\n\t\t  | Extracting Motion and Appearance       | MLP, CNN, Transformer | Required   | Required            | 6                                      |\n\t\t  | Instant 3D                             | MLP                 | Not Required   | Required            | 8                                      |\n\t\t  | Neural Point Cloud Rendering           | CNN, U-Net          | Not Required   | Required            | 6                                      |\n\t\t  | Deep Shading                           | CNN                 | Not Required   | Required            | 6                                      |\n\t\t  | Neural Reflectance Fields              | CNN                 | Required       | Not Required        | 7                                      |\n\t\t  | Deep Illumination                      | GAN, U-Net          | Not Required   | Required            | 5                                      |\n\t\t  | Common Objects in 3D                   | MLP, Transformer    | Required       | Required            | 7                                      |\n\t\t  | GeoNeRF                                | Transformer         | Required       | Required            | 7                                      |\n\t\t  | Gen-NeRF                               | Transformer         | Required       | Required            | 7                                      |\n\t- [playcanvas/supersplat: 3D Gaussian Splat Editor](https://github.com/playcanvas/supersplat/) [[Gaussian splatting and Similar]]\n- [Long Volumetric Video](https://zju3dv.github.io/longvolcap/) [[Gaussian splatting and Similar]]\n- [AniGS](https://lingtengqiu.github.io/2024/AniGS/) [[Humans, Avatars , Character]] [[Gaussian splatting and Similar]]\n-\n\n## Current Landscape (2025)\n\n- Industry adoption and implementations\n  - Metaverse platforms continue to evolve with focus on interoperability and open standards\n  - Web3 integration accelerating with decentralised identity and asset ownership\n  - Enterprise adoption growing in virtual collaboration, training, and digital twins\n  - UK companies increasingly active in metaverse development and immersive technologies\n\n- Technical capabilities\n  - Real-time rendering at photorealistic quality levels\n  - Low-latency networking enabling seamless multi-user experiences\n  - AI-driven content generation and procedural world building\n  - Spatial audio and haptics enhancing immersion\n\n- UK and North England context\n  - Manchester: Digital Innovation Factory supports metaverse startups and research\n  - Leeds: Holovis leads in immersive experiences for entertainment and training\n  - Newcastle: University research in spatial computing and interactive systems\n  - Sheffield: Advanced manufacturing using digital twin technology\n\n- Standards and frameworks\n  - Metaverse Standards Forum driving interoperability protocols\n  - WebXR enabling browser-based immersive experiences\n  - glTF and USD for 3D asset interchange\n  - Open Metaverse Interoperability Group defining cross-platform standards\n\n## Metadata\n\n- **Last Updated**: 2025-11-16\n- **Review Status**: Automated remediation with 2025 context\n- **Verification**: Academic sources verified\n- **Regional Context**: UK/North England where applicable",
  "backlinks": [
    "TELE-052-neural-radiance-fields",
    "TELE-100-ai-avatars",
    "TELE-001-telepresence"
  ],
  "wiki_links": [
    "3D and 4D",
    "TELE-053-volumetric-video-conferencing",
    "PhotorealisticTelepresence",
    "MultiViewImages",
    "DifferentiableRendering",
    "SLAM",
    "Human tracking and SLAM capture",
    "Motion Capture",
    "TELE-052-neural-radiance-fields",
    "NeuralRendering",
    "TELE-020-virtual-reality-telepresence",
    "GPUAcceleration",
    "Scene Capture and Reconstruction",
    "RealTimeRendering",
    "GradientDescent",
    "Update Cycle",
    "NovelViewSynthesis",
    "TELE-060-instant-ngp",
    "Gaussian splatting and Similar",
    "Humans, Avatars , Character",
    "TELE-050-neural-rendering-telepresence",
    "3DReconstruction"
  ],
  "ontology": {
    "term_id": "TELE-051",
    "preferred_term": "3D Gaussian Splatting",
    "alt_terms": [],
    "iri": "http://narrativegoldmine.com/telecollaboration#ThreeDGaussianSplatting",
    "source_domain": "tc",
    "domain": "tc",
    "domain_full_name": "Telecollaboration",
    "definition": "\"A neural rendering technique that represents 3D scenes as collections of millions of 3D Gaussian primitives with learnable positions, colours, opacities, and covariances, enabling photorealistic real-time rendering at 100+ frames per second through GPU-accelerated rasterisation, revolutionising telepresence and immersive collaboration with unprecedented visual fidelity.\"",
    "scope_note": null,
    "status": "active",
    "maturity": "developing",
    "version": null,
    "public_access": true,
    "last_updated": null,
    "authority_score": null,
    "quality_score": null,
    "cross_domain_links": null,
    "owl_class": "tc:ThreeDGaussianSplatting",
    "owl_physicality": "ConceptualEntity",
    "owl_role": "Process",
    "owl_inferred_class": null,
    "is_subclass_of": [
      "NeuralRendering",
      "TELE-050-neural-rendering-telepresence",
      "3DReconstruction"
    ],
    "has_part": [],
    "is_part_of": [],
    "requires": [
      "MultiViewImages",
      "DifferentiableRendering",
      "GPUAcceleration",
      "GradientDescent"
    ],
    "depends_on": [],
    "enables": [
      "PhotorealisticTelepresence",
      "NovelViewSynthesis",
      "RealTimeRendering"
    ],
    "relates_to": [],
    "bridges_to": [],
    "bridges_from": [],
    "domain_extensions": {},
    "belongs_to_domain": [],
    "implemented_in_layer": [],
    "source": [],
    "other_relationships": {
      "related-to": [
        "TELE-053-volumetric-video-conferencing",
        "TELE-052-neural-radiance-fields",
        "TELE-060-instant-ngp"
      ]
    },
    "validation": {
      "is_valid": false,
      "errors": [
        "Missing required property: last-updated",
        "term-id 'TELE-051' doesn't match domain 'tc' (expected TC-)"
      ]
    }
  }
}