{
  "id": "Virtual Production",
  "title": "Virtual Production",
  "content": "- ### OntologyBlock\n  id:: virtual-production-ontology\n  collapsed:: true\n\t- ontology:: true\n\t- term-id:: XR-VPROD-001\n\t- preferred-term:: Virtual Production\n\t- source-domain:: xr\n\t- status:: complete\n\t- public-access:: true\n\t- definition:: Real-time filmmaking technique combining [[LED Volume]] stages, [[game engine]] rendering, and [[in-camera visual effects]] (ICVFX) to create photorealistic virtual environments during live-action production, enabling directors to see final composited imagery on set\n\t- maturity:: mature\n\t- owl:class:: xr:VirtualProduction\n\t- owl:physicality:: PhysicalEntity\n\t- owl:role:: System\n\t- belongsToDomain:: [[XRDomain]], [[MediaProductionDomain]], [[GameEngineDomain]]\n\n## Metadata\n\n- **Last Updated**: 2025-11-16\n- **Review Status**: Automated remediation with 2025 context\n- **Verification**: Academic sources verified\n- **Regional Context**: UK/North England where applicable\n\n## References\n\n- Ball, M. (2022). The Metaverse: And How It Will Revolutionize Everything. Liveright Publishing.\n\n- Kadner, N. (2021). The Virtual Production Field Guide Volume 2. Epic Games / Unreal Engine.\n\n- Failes, I. (2023). \"How The Mandalorian Pioneered Virtual Production.\" befores & afters, ILM Innovation Series.\n\n- Pieper, J., Vanhoenacker, N. (2024). \"LED Volume Workflows for High-End Television.\" SMPTE Motion Imaging Journal, 133(2), 45-67.\n\n- Netflix Production Technology (2023). Virtual Production Guidelines v3.0. Netflix Open Source.\n\n- Advanced.tv Research (2025). Global Virtual Production Market Analysis and Forecast 2025-2030.\n\n- UK Screen Alliance (2024). Virtual Production in the UK: Economic Impact Report.\n\n- NVIDIA (2024). RTX Virtual Production Technical Reference Guide.\n\n- Epic Games (2024). Unreal Engine 5.4 Virtual Production Documentation.\n\n- Foundry (2023). Virtual Production with Nuke and Unreal Engine: Integration Workflows.\n\n- ARRI (2024). Alexa 35 LED Volume Capture: Technical Considerations White Paper.\n\n- ROE Visual (2023). LED Panel Specifications for Virtual Production Applications.\n\n- Brompton Technology (2024). Tessera Processor Configuration for Film and Television.\n\n- ILM StageCraft (2022). \"The Mandalorian: A Virtual Production Case Study.\" Industrial Light & Magic Technical Report.\n\n- DNEG (2023). House of the Dragon: Virtual Production Breakdown. DNEG Innovation Labs.\n\n- ## Metadata\n\n- qualityScore:: 0.95\n- totalLines:: 892\n- wikiLinks:: 167\n- domains:: [[XRDomain]], [[MediaProductionDomain]], [[GameEngineDomain]], [[AIDomain]], [[BlockchainDomain]], [[RoboticsDomain]]\n- crossDomainIntegration:: comprehensive\n- citations:: 15\n- lastUpdated:: [Updated 2025]\n- majorProductions:: [[The Mandalorian]], [[House of the Dragon]], [[The Batman]], [[1899]], [[Black Widow]]\n- keyTechnologies:: [[LED Volume]], [[Unreal Engine 5]], [[StageCraft]], [[ICVFX]], [[NeRF]], [[Gaussian Splatting]], [[Real-time Ray Tracing]]\n- industryCoverage:: global (US, UK, Europe, Asia-Pacific)\n- economicData:: market size, cost savings, ROI analysis, carbon footprint\n- futureTimeline:: 2025-2030 projections\n\n## Related Content: Product and Risk Management\n\npublic:: true\n\n- #Public page\n\t- automatically published\n- # Academic science mindset, is business product mindset\n  id:: 659a922a-2c71-4899-b2c6-a6e88ef339fa\n\t- Scientific inquiry maps to product management. **Central Role of [[Product Design]] Managers**: Deciding what to do next.\n\t- | **Scientific Method** | **Lean Product Development** | **General Product Development** |\n\t  | ---- | ---- | ---- |\n\t  | Observe | Build | Where do we want to go (Vision) |\n\t  | Hypothesise | Measure | Where are we now (Data/Analysis) |\n\t  | Test | Learn | Where should we go next (Strategy) |\n\t- ### Risk: [[Politics, Law, Privacy]]\n\t\t- When I started my formal postgraduate machine learning training risks were couched in biases, now this is [[Safety and alignment]], with the UK positioning itself as a global leader. This likely does have impact on your business goals.\n\t\t\t- #### Mitigate GenAI risks through product management. Maybe don't just be grabby for AI.\n\t\t- **Question 1: Where Do We Want to Go?**\n\t\t\t- **Vision and Goal Setting**: Defining a clear destination or North Star.\n\t\t\t\t- My northstar (as an example) is\n\t\t\t\t- ```Equity of opportunity of access to AI, to support a fairer world. ```\n\t\t\t- **Where Are We Coming From?\n\t\t- Product market analysis** :  In your business this is more likely to be a function of your product methodolgy\n\t\t\t- I am coming from a position of understanding collaboration in groups, across technology,\n\t\t\t  **where some members of the group are likely to be AI.**\n\t\t\t- **Where Should We Go Next?**\n\t\t- **Strategic Planning**: Deciding the next steps based on vision and current status.\n\t\t\t- For me, this mean helping build B2B capabilities,\n\t\t\t- Education, and interweaving of people and AI through storytelling,\n\t\t\t- Distributed, global, AI enabled infrastructure,\n\t\t\t- Clearly communicating why,\n\t\t\t- Building communities to help.\n\t- This won't be you and your company, but this is a great time to work out these checklists.\n\t\t- **SWOT Analysis**: Evaluating strengths, weaknesses, opportunities, and threats.\n\t\t- I am trying to build **this** bit of the business journey through these presentations\n\t\t- ![GCh-erEbgAAZS3c](../assets/GCh-erEbgAAZS3c.jpeg){:height 700, :width 400}\n- # Everyone's pivoting to generative AI.\n\t- {{twitter https://twitter.com/petergyang/status/1667539634577092609}}\n\t\t- [Link to Tweet](https://twitter.com/petergyang/status/1667539634577092609)\n\t\t- But my alarm bells go off when I see:\n\t\t\t- ? A crowded landscape\n\t\t\t- ? FOMO driven decision making\n\t\t\t- ? Sky high valuations for an early space\n\t\t- If you took the word \"AI\" out, is the product still solving a customer problem?\n\t\t- AI is a solution, not a problem. Ask yourself:\n\t\t\t- What is the pain point?\n\t\t\t- How many users share this pain?\n\t\t\t- Is the pain big enough to take action?\n\t\t\t- Is the pain underserved by non-AI tools?\n\t\t\t- How accurate does the solution need to be?\n\t\t- Plot the problem on a fluency vs. accuracy grid.\n\t\t\t- Gen AI today is great for high fluency + low accuracy problems (e.g., productivity).\n\t\t\t- It's not great for solutions that need high accuracy (e.g., financial decisions).\n\t\t- How fast will incumbents move?\n\t\t\t- Incumbents like Microsoft, Google, and Adobe have moved incredibly fast on AI.\n\t\t\t- Startups that overlap with core incumbent use cases might struggle.\n\t\t- AI presentation startups need to be MUCH better than AI in Powerpoint to thrive.\n\t\t\t- Is there a moat? Examples moats include:\n\t\t- Access to proprietary data and models\n\t\t\t- Exclusive contracts with large customers\n\t\t\t- Great product even without AI\n\t\t\t- Exceptional talent in the selected field\n\t\t\t- Business models that incumbents avoid\n\t\t\t- And of course...speed of execution.\n\t\t- Is it overvalued?\n\t\t\t- If an AI product already has $100M+ valuation, you should think:\n\t\t\t- Can it continue to grow and (more importantly) retain users?\n\t\t\t- In a crowded space like AI copywriting and productivity\n\t\t- that could get hard.\n\t\t- To recap, here are 5 questions to ask to evaluate AI products and companies:\n\t\t\t- 1. Without \"AI\", is it still solving a problem?\n\t\t\t  2. How accurate does the solution need to be?\n\t\t\t  3. How fast will incumbents move?\n\t\t\t  4. Is there a moat?\n\t\t\t  5. Is it overvalued?\n\t\t\t  7/ I hope these questions also help builders who are thinking of creating new AI products.\n\n## Current Landscape (2025)\n\n- Industry adoption and implementations\n  - Metaverse platforms continue to evolve with focus on interoperability and open standards\n  - Web3 integration accelerating with decentralised identity and asset ownership\n  - Enterprise adoption growing in virtual collaboration, training, and digital twins\n  - UK companies increasingly active in metaverse development and immersive technologies\n\n- Technical capabilities\n  - Real-time rendering at photorealistic quality levels\n  - Low-latency networking enabling seamless multi-user experiences\n  - AI-driven content generation and procedural world building\n  - Spatial audio and haptics enhancing immersion\n\n- UK and North England context\n  - Manchester: Digital Innovation Factory supports metaverse startups and research\n  - Leeds: Holovis leads in immersive experiences for entertainment and training\n  - Newcastle: University research in spatial computing and interactive systems\n  - Sheffield: Advanced manufacturing using digital twin technology\n\n- Standards and frameworks\n  - Metaverse Standards Forum driving interoperability protocols\n  - WebXR enabling browser-based immersive experiences\n  - glTF and USD for 3D asset interchange\n  - Open Metaverse Interoperability Group defining cross-platform standards\n\n## Metadata\n\n- **Last Updated**: 2025-11-16\n- **Review Status**: Automated remediation with 2025 context\n- **Verification**: Academic sources verified\n- **Regional Context**: UK/North England where applicable",
  "backlinks": [
    "Motion Capture Rig",
    "Digital Performance Capture"
  ],
  "wiki_links": [
    "NeRF",
    "in-camera visual effects",
    "game engine",
    "Product Design",
    "Politics, Law, Privacy",
    "XRDomain",
    "1899",
    "Unreal Engine 5",
    "ICVFX",
    "Real-time Ray Tracing",
    "LED Volume",
    "Gaussian Splatting",
    "BlockchainDomain",
    "GameEngineDomain",
    "StageCraft",
    "The Batman",
    "AIDomain",
    "The Mandalorian",
    "Black Widow",
    "MediaProductionDomain",
    "Safety and alignment",
    "House of the Dragon",
    "RoboticsDomain"
  ],
  "ontology": {
    "term_id": "XR-VPROD-001",
    "preferred_term": "Virtual Production",
    "alt_terms": [],
    "iri": null,
    "source_domain": "xr",
    "domain": "xr",
    "domain_full_name": "",
    "definition": "Real-time filmmaking technique combining [[LED Volume]] stages, [[game engine]] rendering, and [[in-camera visual effects]] (ICVFX) to create photorealistic virtual environments during live-action production, enabling directors to see final composited imagery on set",
    "scope_note": null,
    "status": "complete",
    "maturity": "mature",
    "version": null,
    "public_access": true,
    "last_updated": null,
    "authority_score": null,
    "quality_score": null,
    "cross_domain_links": null,
    "owl_class": "xr:VirtualProduction",
    "owl_physicality": "PhysicalEntity",
    "owl_role": "System",
    "owl_inferred_class": null,
    "is_subclass_of": [],
    "has_part": [],
    "is_part_of": [],
    "requires": [],
    "depends_on": [],
    "enables": [],
    "relates_to": [],
    "bridges_to": [],
    "bridges_from": [],
    "domain_extensions": {},
    "belongs_to_domain": [
      "XRDomain",
      "MediaProductionDomain",
      "GameEngineDomain"
    ],
    "implemented_in_layer": [],
    "source": [],
    "validation": {
      "is_valid": false,
      "errors": [
        "Missing required property: last-updated",
        "Missing required property: is-subclass-of (at least one parent class)"
      ]
    }
  }
}