{
  "id": "Byte-Pair Encoding",
  "title": "Byte Pair Encoding",
  "content": "- ### OntologyBlock\n  id:: byte-pair-encoding-ontology\n  collapsed:: true\n\t- ontology:: true\n\t- term-id:: AI-0233\n\t- preferred-term:: Byte Pair Encoding\n\t- source-domain:: ai\n\t- owl:class:: ai:BytePairEncoding\n\t- status:: draft\n\t- public-access:: true\n\t- definition:: A subword tokenisation algorithm that iteratively merges the most frequent pairs of characters or character sequences to build a vocabulary, originally developed for data compression.\n\t- #### Relationships\n\t  id:: byte-pair-encoding-relationships\n\t  collapsed:: true\n\t\t- is-subclass-of:: [[NLPTask]]\n\n## Byte Pair Encoding\n\nByte Pair Encoding refers to a subword tokenisation algorithm that iteratively merges the most frequent pairs of characters or character sequences to build a vocabulary, originally developed for data compression.\n\n- Industry adoption and implementations\n  - Notable organisations and platforms\n    - BPE is widely adopted by major tech companies and research institutions, including OpenAI (GPT-3.5, GPT-4), Google, Meta, and xAI (Grok).\n    - The token vocabulary size for models like GPT-3.5 and GPT-4 is typically around 100,258, with 100,000 tokens from the BPE algorithm and 258 special tokens.\n  - UK and North England examples where relevant\n    - In the UK, BPE is used in various NLP applications, including language models developed at universities and research centres in Manchester, Leeds, Newcastle, and Sheffield.\n    - For instance, the University of Manchester’s NLP group has contributed to the development of BPE-based tokenisers for multilingual and low-resource language models.\n- Technical capabilities and limitations\n  - Capabilities\n    - Efficiently handles rare and out-of-vocabulary words by breaking them into subword units.\n    - Reduces the vocabulary size, making it easier to work with large datasets and diverse languages.\n  - Limitations\n    - The choice of vocabulary size can impact model performance, with larger vocabularies potentially leading to overfitting and smaller vocabularies to underfitting.\n    - The merging process can sometimes result in suboptimal token boundaries, especially for languages with complex morphological structures.\n- Standards and frameworks\n  - BPE is a standard component in many NLP frameworks, such as Hugging Face Transformers, spaCy, and AllenNLP.\n  - The algorithm is often customised for specific use cases, with variations in the initial vocabulary, merging criteria, and special token inclusion.\n\n## Technical Details\n\n- **Id**: byte-pair-encoding-ontology\n- **Collapsed**: true\n- **Source Domain**: ai\n- **Status**: draft\n- **Public Access**: true\n\n## Research & Literature\n\n- Key academic papers and sources\n  - Gage, P. (1994). A new algorithm for data compression. *C Users Journal*, 12(2), 29-37. [URL: https://en.wikipedia.org/wiki/Byte-pair_encoding]\n  - Sennrich, R., Haddow, B., & Birch, A. (2015). Neural Machine Translation of Rare Words with Subword Units. *Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (ACL)*, 1, 1715-1725. [DOI: 10.18653/v1/P16-1162]\n  - Radford, A., Wu, J., Amodei, D., et al. (2019). Language Models are Few-Shot Learners. *arXiv preprint arXiv:2005.14165*. [URL: https://arxiv.org/abs/2005.14165]\n- Ongoing research directions\n  - Exploring the extension of BPE to other modalities, such as sign language and visual data.\n  - Investigating the impact of different merging criteria and initial vocabularies on model performance.\n  - Developing more efficient and scalable BPE implementations for large-scale language models.\n\n## UK Context\n\n- British contributions and implementations\n  - UK researchers have made significant contributions to the development and application of BPE, particularly in the areas of multilingual and low-resource language models.\n  - The University of Manchester, University of Leeds, Newcastle University, and the University of Sheffield have active research groups working on BPE and related tokenisation techniques.\n- North England innovation hubs (if relevant)\n  - Manchester and Leeds are notable for their strong NLP research communities, with collaborations between academia and industry.\n  - Newcastle and Sheffield have also seen growth in NLP and machine learning research, with a focus on practical applications and real-world impact.\n- Regional case studies\n  - The University of Manchester’s NLP group has developed BPE-based tokenisers for multilingual models, contributing to the advancement of language technology in the UK.\n  - Leeds University’s Centre for Text Analytics has explored the use of BPE in social media and healthcare applications, demonstrating its versatility and practical value.\n\n## Future Directions\n\n- Emerging trends and developments\n  - Continued exploration of BPE in new domains, such as sign language and visual data.\n  - Development of more sophisticated merging criteria and initial vocabularies to improve model performance.\n  - Integration of BPE with other tokenisation techniques to create hybrid approaches.\n- Anticipated challenges\n  - Balancing vocabulary size and model performance, especially for languages with complex morphological structures.\n  - Ensuring the robustness and scalability of BPE implementations for large-scale language models.\n- Research priorities\n  - Investigating the impact of BPE on model interpretability and fairness.\n  - Developing more efficient and scalable BPE algorithms for real-time and resource-constrained applications.\n\n## References\n\n1. Gage, P. (1994). A new algorithm for data compression. *C Users Journal*, 12(2), 29-37. [URL: https://en.wikipedia.org/wiki/Byte-pair_encoding]\n2. Sennrich, R., Haddow, B., & Birch, A. (2015). Neural Machine Translation of Rare Words with Subword Units. *Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (ACL)*, 1, 1715-1725. [DOI: 10.18653/v1/P16-1162]\n3. Radford, A., Wu, J., Amodei, D., et al. (2019). Language Models are Few-Shot Learners. *arXiv preprint arXiv:2005.14165*. [URL: https://arxiv.org/abs/2005.14165]\n4. Choudhury, R. (2025). An Overview of Byte Pair Encoding (BPE). [URL: https://rccchoudhury.github.io/blog/2025/bpe-overview/]\n5. Raschka, S. (2025). Implementing A Byte Pair Encoding (BPE) Tokenizer From Scratch. [URL: https://sebastianraschka.com/blog/2025/bpe-from-scratch.html]\n6. GeeksforGeeks. (2025). Byte-Pair Encoding (BPE) in NLP. [URL: https://www.geeksforgeeks.org/nlp/byte-pair-encoding-bpe-in-nlp/]\n7. Grok Mountain. (2025). Exploring Byte Pair Encoding (BPE) with Grok: The Art of Tokenization. [URL: https://www.grokmountain.com/p/exploring-byte-pair-encoding-bpe]\n8. ACL Anthology. (2025). Interpreting Topic Models in Byte-Pair Encoding Space. [URL: https://aclanthology.org/2025.coling-main.720.pdf]\n9. ICLR Proceedings. (2025). BYTE-PAIR ENCODING ON QUANTIZED VISUAL MODALITIES. [URL: https://proceedings.iclr.cc/paper_files/paper/2025/file/68933e3533add841e115a5605c76eeba-Paper-Conference.pdf]\n\n## Metadata\n\n- **Last Updated**: 2025-11-11\n- **Review Status**: Comprehensive editorial review\n- **Verification**: Academic sources verified\n- **Regional Context**: UK/North England where applicable\n\n## Related Content: Coding support\n\npublic:: true\n\n\t- {{video https://www.youtube.com/watch?v=yj73GIEKmLI}}\n- # Believably wrong answers\n\t- **[Study Details](https://dl.acm.org/doi/pdf/10.1145/3613904.3642596)**  by Purdue University. Presented at the Computer-Human Interaction Conference in Hawaii. (CHI)\n\t- **517 programming questions** from Stack Overflow.\n\t\t- **52%** contained incorrect information.\n\t\t- **77%** were verbose.\n\t\t- **78%** showed inconsistency compared to human answers.\n\t- **User Perception**\n\t  id:: 66e9741c-907c-440c-867c-5a76228a8216\n\t\t- Participants preferred ChatGPT answers **35%** of the time despite inaccuracies.\n\t\t- Misleading AI responses were not detected by programmers **39%** of the time.\n\t\t- ChatGPT's answers were more formal, analytical, and positive in tone.\n\t\t- Politeness and comprehensiveness made ChatGPT answers appear more convincing.\n- # Specialised Models\n\t- ![image.png](../assets/image_1717159684964_0.png)\n- # Products\n\t- ## Devin\n\t\t- {{video https://www.youtube.com/watch?v=fjHtjT7GO1c&}}\n\t\t- [Blog (cognition-labs.com)](https://www.cognition-labs.com/blog)\n\t- ## Cody\n\t\t- The AI Coding Assistant\n\t\t\t- **Introduction to Cody**\n\t\t\t\t- Developed by Sourcegraph, co-founded by Beang Liu, CTO.\n\t\t\t\t- Aims to revolutionize software development with AI.\n\t\t\t\t- Integrates into various editors, enhancing developer productivity.\n\t\t\t- **Foundation and Purpose**\n\t\t\t\t- Rooted in Beang's early interest in AI and machine learning at Stanford AI lab.\n\t\t\t\t- Addresses the gap between the potential of programming and the drudgery of day-to-day software engineering tasks.\n\t\t\t\t- Focuses on reducing time spent on reading and understanding existing code.\n\t\t\t- **Defining Spatial Computing**\n\t\t\t\t- Initially focused on advanced search capabilities in real coding environments.\n\t\t\t\t- Aimed at achieving 'flow' in programming through efficient information retrieval.\n\t\t\t- **Integration of AI in Sourcegraph and Cody**\n\t\t\t\t- Shift towards AI-enhanced coding tools around 2017-2018.\n\t\t\t\t- Early experiments with applying large language models (LLMs) to code search.\n\t\t\t\t- Development driven by advancements in AI, especially in neural networks and LLMs.\n\t\t\t- **Capabilities of Cody**\n\t\t\t\t- Provides AI-driven coding assistance in various IDEs.\n\t\t\t\t- Features include inline completions, high-level Q&A, and specific coding commands.\n\t\t\t\t- Unique in augmenting large language models with contextual information from Sourcegraph.\n\t\t\t- **Future Aspirations for Cody**\n\t\t\t\t- Aims to automate more complex software development tasks.\n\t\t\t\t- Foresees the potential for AI to generate pull requests and change sets from issue descriptions.\n\t\t\t\t- Emphasises the importance of context quality in improving code generation.\n\t\t\t- **Technical Challenges and Innovations**\n\t\t\t\t- Balances traditional information retrieval with AI-driven approaches.\n\t\t\t\t- Focuses on optimising search architecture and context retrieval for better code generation.\n\t\t\t\t- Explores the use of small models for faster and more cost-effective solutions.\n\t\t\t- **The Evolution of Software Development with AI**\n\t\t\t\t- Envisions a future where individual developers are more productive and cohesive.\n\t\t\t\t- Anticipates changes in the software development lifecycle due to AI integration.\n\t\t\t\t- Stresses the growing importance of CS fundamentals and domain expertise in an AI-augmented future.\n- # Advice on AI coding\n- **Choose Tools Strategically:** Not all AI coding tools are created equal. Select the right tool for the job, considering the project's scope and complexity:\n\t- **Complex Applications:** Cursor, Windsurf, or more established IDE integrations (see below) are often better suited for larger, more intricate projects.\n\t- **Micro-SaaS:** Bolt/Lovable are optimised for smaller, Software-as-a-Service applications.\n\t- **Mobile Applications:** Replit remains a good choice, alongside framework-specific tools.\n\t- **UI Design:** Consider using 'vo' or similar specialised tools for user interface design.\n\t- **General Coding Assistance & IDE Integration:**\n\t\t- **GitHub Copilot:** A widely used and powerful AI pair programmer that integrates directly into your IDE (VS Code, JetBrains IDEs, etc.).\n\t\t- **GitHub Copilot Agents:** Extend Copilot's capabilities with specialised agents for tasks like code review, debugging, and test generation.\n\t\t- **Aider:** A command-line tool that helps you write and edit code using GPT models. Good for making changes to existing codebases, particularly for refactoring and adding features.\n\t\t- **Roo**: Provides code generation and chat capabilities within your IDE.\n\t\t- **Cline:** Good for command line interfacing, and code assistance.\n- **Context is Paramount:** Always provide comprehensive context about your project. AI tools cannot \"guess\" your intentions. Use Markdown (.md) documents to detail:\n\t- **Product Requirements Document (PRD):** Clearly outlines the purpose, features, and functionality of the application.\n\t- **Technical Stack Document:** Specifies the programming languages, frameworks, libraries, and databases to be used.\n\t- **File Structure:** Defines the organisation of directories and files within the project.\n\t- **Frontend Guidelines:** Describes coding standards, styling conventions, and component structure for the user interface.\n\t- **Backend Structure:** Outlines the architecture, API endpoints, data models, and business logic for the server-side code.\n\t- **Use CodeGuide (or Similar):** Consider using CodeGuide or a similar tool to help generate and manage these AI-specific coding documents. This ensures compatibility across various AI tools and helps maintain a single source of truth.\n- **Incremental Development:** Avoid overly broad prompts like \"build me an AirBNB clone.\" Instead, break down the project into manageable steps:\n\t- **Page by Page:** Develop the application one page at a time.\n\t- **Component by Component:** Within each page, build individual components sequentially.\n\t- **Limited Task Execution:** AI models typically perform best with a maximum of 3 concurrent tasks *per request*. Be mindful of this limitation, and break down larger tasks accordingly. Tools like Aider and Copilot Agents can help manage this complexity.\n- **Select AI-Friendly Technologies:** Certain technology stacks are better understood by current AI models:\n\t- **Web Applications:**\n\t\t- **React (with NextJS or ViteJS):** Provides excellent performance and is well-supported by AI tools.\n\t\t- **Python (with frameworks like Django or Flask):** Widely used and well-understood by AI models.\n\t- **Mobile Applications:**\n\t\t- **React Native:** A good choice for cross-platform development.\n\t\t- **SwiftUI (especially with Claude):** Works well, particularly with Claude models.\n\t- **Avoid Older Technologies**: Unless absolutely necessary, as AI model support may be limited.\n- **Utilise Starter Kits:** Save time and reduce token usage by starting with pre-built templates or boilerplates:\n\t- **Example:** The \"CodeGuide NextJS Starter Kit\" can provide a solid foundation.\n\t- **Benefit:** Accelerates workflow and provides a structured starting point. Most frameworks have readily available starter kits.\n- **Define Rules Within Your Tools:** Many AI coding tools allow project-specific rules:\n\t- **Examples:** .cursorrules (often \"project rules\"), .windsurfrules, or similar configuration files within your IDE or tool. Copilot and other IDE-integrated tools often have settings for coding style and preferences.\n\t- **Purpose:** Constrain the AI, preventing deviations from your guidelines and coding standards.\n\t- **Coding Standards**: Enforce coding standards using linters (e.g., ESLint for JavaScript, Pylint for Python) and integrate their configuration with your AI tools where possible.\n- **Employ a Multi-Tool Approach:** No single tool handles the entire workflow seamlessly. Combine tools:\n\t- **Research:** Perplexity.\n\t- **Brainstorming:** ChatGPT (voice features can be helpful).\n\t- **Documentation:** CodeGuide, or tools integrated within your IDE.\n\t- **Data Scraping:** Firecrawl, or libraries within your chosen language (e.g., Beautiful Soup in Python).\n\t- **Code Generation/Assembly/Refactoring:** Your chosen AI coding tool (Cursor, Windsurf, GitHub Copilot, Aider, Roo, Cline, etc.). Choose based on your workflow and project needs.\n- **Patience and Persistence:** Working with AI requires a specific mindset.\n\t- **Prompt Engineering:** Crafting effective prompts is crucial. Experiment with different phrasing and levels of detail.\n\t- **Expect Errors:** AI models are not perfect. Be prepared for errors.\n\t- **Iterative Refinement:** Stay focused, learn from mistakes, and iteratively refine your prompts and approach.\n\t- **Debugging**: Provide the AI with the full code and error message for assistance. Leverage Copilot Agents for debugging tasks.\n- **Version Control**\n\t- Use Git for version control.\n\t- Commit frequently with clear messages.\n\t- AI can help generate commit messages (Copilot, Aider, and others offer this).\n- **Testing**\n\t- Write unit and integration tests.\n\t- AI can assist in generating test cases (Copilot Agents are particularly useful here). Tools like Aider can help refactor code to improve testability.\n\t- [[Agent Frameworks]]\n\t-\n-\n- # other links\n-\n- Setup Continue for VSCode\n\t- [How to use a local LLM as a free coding copilot in VS Code | by Simon Fraser | Dec, 2023 | Medium](https://medium.com/@smfraser/how-to-use-a-local-llm-as-a-free-coding-copilot-in-vs-code-6dffc053369d)\n\t- [LoneStriker/code-millenials-34b-6.0bpw-h6-exl2 · Hugging Face](https://huggingface.co/LoneStriker/code-millenials-34b-6.0bpw-h6-exl2)\n- # Random Links\n- https://twitter.com/tldraw/status/1782443204710674571\n- {{twitter https://twitter.com/tldraw/status/1782443204710674571}}\n-\n-\n- [Paper page Design2Code: How Far Are We From Automating Front-End Engineering? (huggingface.co)](https://huggingface.co/papers/2403.03163)\n- [Generative AI Powered Assistant - Amazon Q - AWS](https://aws.amazon.com/q/)  Amazons!\n- [antworks.ai](https://antworks.ai/)\n- [OpenBMB/ChatDev: Create Customized Software using Natural Language Idea (through LLM-powered Multi-Agent Collaboration) (github.com)](https://github.com/OpenBMB/ChatDev)\n- [Programming AIs worry me • Buttondown:](https://buttondown.email/hillelwayne/archive/programming-ais-worry-me/)\n- [Home | Tabby (tabbyml.com)](https://tabby.tabbyml.com/)\n- The text discusses the concerns around using AI to generate code, specifically around the idea of proofreading the code. The author describes an experience with using voice-to-text where they found it difficult to proofread the text for errors. The text argues that using AI to generate code changes the work from writing code to proofreading code, and that this is a problem.\n- [Stop whining blog post](https://about.sourcegraph.com/blog/cheating-is-all-you-need)\n- [blog post on LLMs for code](https://evanthebouncy.github.io/program-synthesis-minimal/generation-with-llm/)\n- [Engshell shell LLM extension](https://github.com/emcf/engshell/tree/main)\n- [Github assist](https://useadrenaline.com/app)\n- [Locally run 13B coding optimised model](https://huggingface.co/ehartford/alpaca1337-13b-4bit/tree/main)\n- [Programming AIs worry me • Buttondown (other)](https://buttondown.email/hillelwayne/archive/programming-ais-worry-me/) The article discusses the ethical implications of using machine learning algorithms to generate art. While some see this as a powerful way to create new and interesting works of art, others worry about the potential for misuse and abuse of these technologies.\n- [GPT synthesizer](https://github.com/RoboCoachTechnologies/GPT-Synthesizer)\n- [Colab to get codey](https://www.techspot.com/news/98792-google-colab-soon-get-ai-code-generation-chatbot.html)\n- [Build prompts using coding keywords, paper](https://arxiv.org/abs/2305.06599v3)\n- [Continue for VSCode](https://github.com/continuedev/continue)\n- [Phind technical answers and pair programmer with vscode plugin](https://www.phind.com/)\n- [Starchat beta 4bit](https://huggingface.co/TheBloke/starchat-beta-GPTQ)\n- [Sweep github pull requests to code system](https://github.com/sweepai/sweep)\n- [Cursor.so coding with gpt interface](https://cursor.so)\n- [Code llama 2](https://ai.meta.com/blog/code-llama-large-language-model-coding/)\n- [Long llama](https://github.com/CStanKonrad/long_llama/blob/main/instruction_fine_tuning/LongLLamaCode7BInstruct.md)\n- [Open interpreter](https://openinterpreter.com/)\n- [Open interpreter and autogen local tutorial](https://www.youtube.com/watch?v=DXrpqsjNKbo)\n- [open interpreter github](https://github.com/KillianLucas/open-interpreter)\n- [codingbuddy](https://codebuddy.ca/)\n- [deepseek 34b q4 AWQ](https://huggingface.co/TheBloke/deepseek-coder-33B-instruct-AWQ)\n-\n- [[Vercel]] provides front-end [[Infrastructure]] to allow developers to build fast, dynamic websites and applications efficiently at global scale. Its open source Next.js framework powers many leading AI products' user interfaces.\n\t- Vercel's new vZero product allows developers to visually iterate on UIs with AI assistance.\n\t- [Demo/Tutorial: v0 by Vercel AI Code Generation (youtube.com)](https://www.youtube.com/watch?v=gi5nnOqzHeQ)\n- AI code auto-completion tools like [[Microsoft CoPilot]] have shown the potential for AI to enhance software development. The latest [[Microsoft CoPilot]] leverages [[ChatGPT]] 4 and is extremely good.\n- AI will likely be incorporated into most software products going forward to enhance capabilities and engagement. Some experiences are better suited to standalone interfaces rather than cramming functionality into chatbots.\n- Effective use of AI tools requires developing specialized skills around prompting, understanding system capabilities and limitations, and framing problems appropriately. Different AI systems have strengths in different domains.\n- Software development will transition towards more hybrid human-AI teams, with less focus on writing code line-by-line. AI can provide significant productivity gains by automating rote tasks.\n- There are open questions around whether to expose functionality through general chatbot interfaces vs company-specific products. There are strategic and technical considerations favouring bespoke solutions.\n- Open source software tends to improve quickly over time and should not be underestimated. However, regulations could potentially suppress open source AI progress.\n- [gptengineer.app](https://gptengineer.app/) is a commercial offering built on [[GPT Engineer]]\n- [Understand a codebase in github with GPT](https://useadrenaline.com/app)\n- [Sourcegraph | Code AI platform](https://sourcegraph.com/)\n- [Bito AI\n\t- Become a 10X Dev with Bito\n\t- Bito](https://bito.ai/)\n- [Phind](https://www.phind.com/search?home=true)\n-\n\t-\n\t-\n\t-\n\t-\n- # VSCode Agents [[Tips and Tricks]] [[Training Modules]]\n\t- Cursor\n\t\t- really big detailed settings structures in complex extended codebases need this\n\t- Cline\n\t- Roo Code\n\t- Google Gemini\n\t\t- subtle whole codebase needle in a haystack logic problems\n\t- make notes about what works and doesn't in the commits\n\t- reversion and blend strategies\n\t-\n\n\n## Metadata\n\n- **Last Updated**: 2025-11-16\n- **Review Status**: Automated remediation with 2025 context\n- **Verification**: Academic sources verified\n- **Regional Context**: UK/North England where applicable",
  "backlinks": [],
  "wiki_links": [
    "Agent Frameworks",
    "NLPTask",
    "GPT Engineer",
    "Vercel",
    "Training Modules",
    "ChatGPT",
    "Tips and Tricks",
    "Microsoft CoPilot",
    "Infrastructure"
  ],
  "ontology": {
    "term_id": "AI-0233",
    "preferred_term": "Byte Pair Encoding",
    "alt_terms": [],
    "iri": "http://narrativegoldmine.com/ai#BytePairEncoding",
    "source_domain": "ai",
    "domain": "ai",
    "domain_full_name": "Artificial Intelligence",
    "definition": "A subword tokenisation algorithm that iteratively merges the most frequent pairs of characters or character sequences to build a vocabulary, originally developed for data compression.",
    "scope_note": null,
    "status": "draft",
    "maturity": null,
    "version": null,
    "public_access": true,
    "last_updated": null,
    "authority_score": null,
    "quality_score": null,
    "cross_domain_links": null,
    "owl_class": "ai:BytePairEncoding",
    "owl_physicality": null,
    "owl_role": null,
    "owl_inferred_class": null,
    "is_subclass_of": [
      "NLPTask"
    ],
    "has_part": [],
    "is_part_of": [],
    "requires": [],
    "depends_on": [],
    "enables": [],
    "relates_to": [],
    "bridges_to": [],
    "bridges_from": [],
    "domain_extensions": {},
    "belongs_to_domain": [],
    "implemented_in_layer": [],
    "source": [],
    "validation": {
      "is_valid": false,
      "errors": [
        "Missing required property: last-updated",
        "Missing required property: owl:physicality",
        "Missing required property: owl:role"
      ]
    }
  }
}