{
  "id": "Self-Attention",
  "title": "Self Attention",
  "content": "- ### OntologyBlock\n  id:: self-attention-ontology\n  collapsed:: true\n\t- ontology:: true\n\t- term-id:: AI-0197\n\t- preferred-term:: Self Attention\n\t- source-domain:: metaverse\n\t- status:: draft\n\t- public-access:: true\n\n\n\n\n### OWL Classification\n\t- owl:class:: mv:SelfAttention\n\t- owl:physicality:: ConceptualEntity\n\t- owl:role:: Concept\n\n### Domain & Architecture\n\t- belongsToDomain:: [[MetaverseDomain]]\n\t- maturity:: draft\n\n### Relationships\n- is-subclass-of:: [[AttentionMechanism]]\n\n## Characteristics\n\n- **Intra-Sequence Dependencies**: Each token computes attention over all other tokens in the same sequence\n- **Query-Key-Value Framework**: Uses query, key, and value representations for attention computation\n- **Permutation Invariance**: Requires positional encoding as self-attention is inherently permutation-invariant\n- **Computational Complexity**: O(n²) complexity with sequence length, challenging for very long sequences\n\n## Academic Foundations\n\n**Primary Source**: Vaswani et al., \"Attention Is All You Need\", arXiv:1706.03762 (2017)\n\n**Key Innovation**: Demonstrated that self-attention alone, without recurrence, could model long-range dependencies effectively.\n\n## Technical Context\n\nSelf-attention models compute representations by allowing each token to attend to all other tokens using query-key compatibility scores. This mechanism enables capturing complex dependencies without the sequential constraints of recurrent architectures.\n\n## Ontological Relationships\n\n- **Broader Term**: Attention Mechanism\n- **Related Terms**: Multi-Head Attention, Scaled Dot-Product Attention, Cross-Attention\n- **Component Of**: Transformer Architecture\n\n## Usage Context\n\n\"Self-attention models compute representations by allowing each token to attend to all other tokens using query-key compatibility scores.\"\n\n## OWL Functional Syntax\n\n```clojure\n(Declaration (Class :SelfAttention))\n(AnnotationAssertion rdfs:label :SelfAttention \"Self-Attention\"@en)\n(AnnotationAssertion rdfs:comment :SelfAttention\n  \"An attention mechanism where every token in a sequence attends to every other token in the same sequence.\"@en)\n(AnnotationAssertion :hasSource :SelfAttention\n  \"Vaswani et al., 'Attention Is All You Need', arXiv:1706.03762 (2017)\"@en)\n\n;; Taxonomic relationships\n(SubClassOf :SelfAttention :AttentionMechanism)\n\n;; Mechanism components\n(SubClassOf :SelfAttention\n  (ObjectSomeValuesFrom :usesComponent :QueryKeyValueFramework))\n(SubClassOf :SelfAttention\n  (ObjectSomeValuesFrom :requires :PositionalEncoding))\n\n;; Computational properties\n(SubClassOf :SelfAttention\n  (ObjectSomeValuesFrom :computesAttentionOver :InputSequence))\n(SubClassOf :SelfAttention\n  (ObjectSomeValuesFrom :produces :ContextualisedRepresentation))\n\n;; Architectural characteristics\n(DataPropertyAssertion :hasComplexityClass :SelfAttention \"O(n²)\"^^xsd:string)\n(DataPropertyAssertion :isPermutationInvariant :SelfAttention \"true\"^^xsd:boolean)\n(DataPropertyAssertion :isBidirectional :SelfAttention \"true\"^^xsd:boolean)\n(DataPropertyAssertion :capturesLongRangeDependencies :SelfAttention \"true\"^^xsd:boolean)\n\n;; Related mechanisms\n(DisjointClasses :SelfAttention :CrossAttention)\n\n;; Used in architectures\n(SubClassOf :TransformerArchitecture\n  (ObjectSomeValuesFrom :implementsMechanism :SelfAttention))\n```\n\n## References\n\n- Vaswani, A., et al. (2017). \"Attention Is All You Need\". arXiv:1706.03762\n- Bahdanau, D., et al. (2014). \"Neural Machine Translation by Jointly Learning to Align and Translate\". arXiv:1409.0473\n\n---\n\n*Ontology Term managed by AI-Grounded Ontology Working Group*\n*UK English Spelling Standards Applied*\n\t- maturity:: draft\n\t- owl:class:: mv:SelfAttention\n\t- owl:physicality:: ConceptualEntity\n\t- owl:role:: Concept\n\t- belongsToDomain:: [[MetaverseDomain]]",
  "backlinks": [],
  "wiki_links": [
    "MetaverseDomain",
    "AttentionMechanism"
  ],
  "ontology": {
    "term_id": "AI-0197",
    "preferred_term": "Self Attention",
    "alt_terms": [],
    "iri": "http://narrativegoldmine.com/metaverse#SelfAttention",
    "source_domain": "metaverse",
    "domain": "metaverse",
    "domain_full_name": "",
    "definition": null,
    "scope_note": null,
    "status": "draft",
    "maturity": "draft",
    "version": null,
    "public_access": true,
    "last_updated": null,
    "authority_score": null,
    "quality_score": null,
    "cross_domain_links": null,
    "owl_class": "mv:SelfAttention",
    "owl_physicality": "ConceptualEntity",
    "owl_role": "Concept",
    "owl_inferred_class": null,
    "is_subclass_of": [],
    "has_part": [],
    "is_part_of": [],
    "requires": [],
    "depends_on": [],
    "enables": [],
    "relates_to": [],
    "bridges_to": [],
    "bridges_from": [],
    "domain_extensions": {},
    "belongs_to_domain": [
      "MetaverseDomain",
      "MetaverseDomain"
    ],
    "implemented_in_layer": [],
    "source": [],
    "validation": {
      "is_valid": false,
      "errors": [
        "Missing required property: last-updated",
        "Missing required property: definition",
        "Missing required property: is-subclass-of (at least one parent class)",
        "owl:class namespace 'mv' doesn't match source-domain 'metaverse'"
      ]
    }
  }
}