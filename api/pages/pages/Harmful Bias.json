{
  "id": "Harmful Bias",
  "title": "Harmful Bias",
  "content": "- ### OntologyBlock\n  id:: harmful-bias-ontology\n  collapsed:: true\n\t- ontology:: true\n\t- term-id:: AI-0083\n\t- preferred-term:: Harmful Bias\n\t- source-domain:: ai\n\t- status:: draft\n\t- public-access:: true\n\n\n### Relationships\n- is-subclass-of:: [[AIFairness]]\n\n# Harmful Bias in AI Systems â€“ Updated Ontology Entry\n\n## Academic Context\n\n- Systematic unfair discrimination embedded within artificial intelligence systems that produces inaccurate or discriminatory outcomes\n  - Affects decisions across criminal justice, healthcare, hiring, lending, and facial recognition domains\n  - Represents a fundamental challenge to equitable AI deployment rather than a peripheral concern\n  - Originates from interconnected sources: training data, algorithmic design, and evaluation methodologies\n  - Can amplify existing societal inequalities at unprecedented speed and scale[1]\n\n## Current Landscape (2025)\n\n- Manifestations across critical sectors\n  - Criminal justice: Risk assessment algorithms disproportionately label individuals from minority groups as \"high-risk,\" perpetuating systemic racial bias[4]\n  - Healthcare: AI systems demonstrate reduced effectiveness for Black patients when trained on historical spending patterns rather than health needs[2]\n  - Hiring and recruitment: Algorithms trained on historical data favour male candidates over female candidates, replicating embedded gender discrimination[4]\n  - Facial recognition: Documented error rates significantly higher for people of colour compared to white individuals, with particular disparities affecting darker-skinned women[3]\n  - Generative AI systems: Text and image generators amplify gender and racial stereotypes, with text-to-image models showing only 16% female representation in financial analyst outputs despite women comprising 43.9% of the profession[5]\n\n- Technical sources of bias\n  - Historical bias: AI systems trained on historical datasets inherently reflect societal biases embedded within those datasets[6]\n  - Representation bias: Sampling and availability biases introduce distortions when datasets underrepresent women, disabled individuals, and other populations[6]\n  - Algorithmic bias: Design choices and proxy variables can introduce discrimination independent of training data quality[4]\n  - Evaluation bias: Assessment methodologies may fail to detect disparities across demographic groups\n\n- Governance and standards development\n  - NIST Special Publication 1270 (March 2022): \"Towards a Standard for Identifying and Managing Bias in Artificial Intelligence\" established foundational methods for identifying, measuring, and reducing harmful bias[1]\n  - European Data Protection Board frameworks: Comprehensive bias evaluation guidance addressing data, algorithmic, and evaluation sources[6]\n  - Ongoing development of assurance and governance practices across regulatory jurisdictions\n\n- UK and North England context\n  - Growing awareness among UK research institutions and technology policy bodies regarding algorithmic fairness\n  - Manchester, Leeds, and Newcastle host significant AI research communities increasingly focused on bias mitigation\n  - UK regulatory landscape evolving to address algorithmic accountability, particularly within public sector deployments\n\n## Research & Literature\n\n- Foundational studies and current research\n  - Buolamwini, J. (2017). \"Gender Shades: Intersectional Accuracy Disparities in Commercial Gender Classification.\" Presented at the Conference on Fairness, Accountability and Transparency. Demonstrated significant accuracy disparities in commercial gender classification systems, with highest error rates affecting darker-skinned females[3]\n  - Nicoletti, L. & Bass, D. (2023). Analysis of Stable Diffusion image generation revealing simultaneous amplification of gender and racial stereotypes[3]\n  - Luccioni, A. S., et al. (2023). Research on risks of biased generative AI in police \"virtual sketch artist\" applications, highlighting potential for increased harm to over-targeted populations[3]\n  - Garg, S., et al. (2018). \"Word Embeddings Quantify 100 Years of Gender and Ethnic Stereotypes.\" Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. Demonstrated historical bias embedded within word embeddings used in text generation systems[6]\n  - Paullada, A., et al. (2021). Research on availability bias in dataset selection and out-of-context reuse of datasets[6]\n\n- Ongoing research directions\n  - Linguistic bias in large language models and mitigation strategies\n  - Intersectional approaches to bias evaluation across multiple demographic dimensions\n  - Trust gaps in generative AI adoption, particularly among underrepresented groups\n  - Real-world consequences of biased outputs in high-stakes decision-making contexts\n\n## UK Context\n\n- British research contributions\n  - UK universities increasingly conducting fairness and bias research within AI systems\n  - Growing policy attention to algorithmic accountability within public sector (NHS, criminal justice, benefits administration)\n  - Manchester and Leeds emerging as regional hubs for AI ethics research and responsible AI development\n\n- Regional considerations\n  - North England institutions collaborating on fairness-aware machine learning research\n  - Public sector organisations in Manchester, Leeds, Newcastle, and Sheffield exploring bias auditing frameworks for algorithmic decision systems\n  - Particular attention to bias in welfare and social care algorithms affecting vulnerable populations\n\n## Future Directions\n\n- Emerging priorities\n  - Development of robust, standardised bias evaluation methodologies applicable across diverse AI applications\n  - Integration of fairness considerations into AI development lifecycles rather than post-hoc remediation\n  - Improved transparency mechanisms to address \"black box\" decision-making and enable meaningful audit trails\n  - Clearer accountability frameworks assigning responsibility among developers, deploying organisations, and regulators\n\n- Anticipated challenges\n  - Tension between model performance optimisation and fairness constraints\n  - Difficulty in defining and measuring fairness across intersecting demographic categories\n  - Resource constraints for comprehensive bias testing, particularly among smaller organisations\n  - Risk that technological framing of bias obscures underlying structural inequalities\n\n- Research priorities\n  - Longitudinal studies tracking real-world harms from biased AI systems\n  - Development of bias mitigation techniques that maintain model utility\n  - Interdisciplinary approaches combining computer science, social science, and ethics\n  - Investigation of how biased AI systems shape public perception and behaviour over time[5]\n\n## References\n\n[1] National Institute of Standards and Technology (2022). \"Towards a Standard for Identifying and Managing Bias in Artificial Intelligence.\" NIST Special Publication 1270.\n\n[2] AIMultiple Research (2024). \"Bias in AI: Examples and 6 Ways to Fix it.\" Available at: research.aimultiple.com/ai-bias/\n\n[3] MIT Sloan EdTech (2024). \"When AI Gets It Wrong: Addressing AI Hallucinations and Bias.\" Available at: mitsloanedtech.mit.edu/ai/basics/addressing-ai-hallucinations-and-bias/\n\n[4] TIME (2024). \"The Definition of AI Bias.\" Available at: time.com/collections/the-ai-dictionary-from-allbusiness-com/7273920/definition-of-ai-bias/\n\n[5] TechPolicy.Press (2024). \"AI bias is not ideological. It's science.\"\n\n[6] European Data Protection Board (2025). \"Bias evaluation.\" Available at: edpb.europa.eu/system/files/2025-01/d1-ai-bias-evaluation_en.pdf\n\n## Metadata\n\n- **Last Updated**: 2025-11-11\n- **Review Status**: Comprehensive editorial review\n- **Verification**: Academic sources verified\n- **Regional Context**: UK/North England where applicable",
  "backlinks": [],
  "wiki_links": [
    "AIFairness"
  ],
  "ontology": {
    "term_id": "AI-0083",
    "preferred_term": "Harmful Bias",
    "alt_terms": [],
    "iri": null,
    "source_domain": "ai",
    "domain": "ai",
    "domain_full_name": "Artificial Intelligence",
    "definition": null,
    "scope_note": null,
    "status": "draft",
    "maturity": null,
    "version": null,
    "public_access": true,
    "last_updated": null,
    "authority_score": null,
    "quality_score": null,
    "cross_domain_links": null,
    "owl_class": null,
    "owl_physicality": null,
    "owl_role": null,
    "owl_inferred_class": null,
    "is_subclass_of": [],
    "has_part": [],
    "is_part_of": [],
    "requires": [],
    "depends_on": [],
    "enables": [],
    "relates_to": [],
    "bridges_to": [],
    "bridges_from": [],
    "domain_extensions": {},
    "belongs_to_domain": [],
    "implemented_in_layer": [],
    "source": [],
    "validation": {
      "is_valid": false,
      "errors": [
        "Missing required property: last-updated",
        "Missing required property: definition",
        "Missing required property: owl:class",
        "Missing required property: owl:physicality",
        "Missing required property: owl:role",
        "Missing required property: is-subclass-of (at least one parent class)"
      ]
    }
  }
}