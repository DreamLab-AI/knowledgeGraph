{
  "id": "RoBERTa",
  "title": "RoBERTa",
  "content": "- ### OntologyBlock\n  id:: roberta-ontology\n  collapsed:: true\n\t- ontology:: true\n\t- term-id:: AI-0217\n\t- preferred-term:: RoBERTa\n\t- source-domain:: metaverse\n\t- status:: draft\n\t- public-access:: true\n\n\n\n\n### OWL Classification\n\t- owl:class:: mv:RoBERTa\n\t- owl:physicality:: ConceptualEntity\n\t- owl:role:: Concept\n\n### Domain & Architecture\n\t- belongsToDomain:: [[MetaverseDomain]]\n\t- maturity:: draft\n\n### Relationships\n- is-subclass-of:: [[BERT]]\n\n## OWL Formal Semantics\n\n```clojure\n;; OWL Functional Syntax\n\n(Declaration (Class :Roberta))\n\n;; Annotations\n(AnnotationAssertion rdfs:label :Roberta \"RoBERTa\"@en)\n(AnnotationAssertion rdfs:comment :Roberta \"Robustly Optimised BERT Approach: an optimised version of BERT that removes next sentence prediction, trains with larger batches and learning rates, and uses dynamic masking to improve performance.\"@en)\n\n;; Data Properties\n(AnnotationAssertion dcterms:identifier :Roberta \"AI-0217\"^^xsd:string)\n(DataPropertyAssertion :isAITechnology :Roberta \"true\"^^xsd:boolean)\n```\n\n## Characteristics\n\n- **Improved Training**: Longer training with larger batches\n- **Dynamic Masking**: Changes masked tokens across epochs\n- **No NSP**: Removes next sentence prediction objective\n- **Larger Dataset**: Trained on more data than original BERT\n\n## Academic Foundations\n\n**Primary Source**: Liu et al., \"RoBERTa: A Robustly Optimized BERT Pretraining Approach\", arXiv:1907.11692 (2019)\n\n**Key Findings**: Demonstrates that BERT was significantly undertrained and that careful hyperparameter tuning and training procedure choices matter substantially.\n\n## Technical Context\n\nRoBERTa shows that BERT's training can be substantially improved by removing the next sentence prediction task, training with dynamic masking, using larger mini-batches and learning rates, and training on more data for longer.\n\n## Ontological Relationships\n\n- **Broader Term**: Pre-trained Language Model\n- **Related Terms**: BERT, ALBERT, DeBERTa\n- **Base Architecture**: BERT with training optimisations\n\n## Usage Context\n\n\"RoBERTa achieves state-of-the-art results on GLUE, SQuAD, and RACE benchmarks by optimising BERT's pre-training procedure.\"\n\n## References\n\n- Liu, Y., et al. (2019). \"RoBERTa: A Robustly Optimized BERT Pretraining Approach\". arXiv:1907.11692\n\n---\n\n*Ontology Term managed by AI-Grounded Ontology Working Group*\n*UK English Spelling Standards Applied*\n\t- maturity:: draft\n\t- owl:class:: mv:RoBERTa\n\t- owl:physicality:: ConceptualEntity\n\t- owl:role:: Concept\n\t- belongsToDomain:: [[MetaverseDomain]]",
  "backlinks": [],
  "wiki_links": [
    "BERT",
    "MetaverseDomain"
  ],
  "ontology": {
    "term_id": "AI-0217",
    "preferred_term": "RoBERTa",
    "alt_terms": [],
    "iri": "http://narrativegoldmine.com/metaverse#RoBERTa",
    "source_domain": "metaverse",
    "domain": "metaverse",
    "domain_full_name": "",
    "definition": null,
    "scope_note": null,
    "status": "draft",
    "maturity": "draft",
    "version": null,
    "public_access": true,
    "last_updated": null,
    "authority_score": null,
    "quality_score": null,
    "cross_domain_links": null,
    "owl_class": "mv:RoBERTa",
    "owl_physicality": "ConceptualEntity",
    "owl_role": "Concept",
    "owl_inferred_class": null,
    "is_subclass_of": [],
    "has_part": [],
    "is_part_of": [],
    "requires": [],
    "depends_on": [],
    "enables": [],
    "relates_to": [],
    "bridges_to": [],
    "bridges_from": [],
    "domain_extensions": {},
    "belongs_to_domain": [
      "MetaverseDomain",
      "MetaverseDomain"
    ],
    "implemented_in_layer": [],
    "source": [],
    "validation": {
      "is_valid": false,
      "errors": [
        "Missing required property: last-updated",
        "Missing required property: definition",
        "Missing required property: is-subclass-of (at least one parent class)",
        "owl:class namespace 'mv' doesn't match source-domain 'metaverse'"
      ]
    }
  }
}