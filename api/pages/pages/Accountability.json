{
  "id": "Accountability",
  "title": "Accountability",
  "content": "- ### OntologyBlock\n  id:: accountability-ontology\n  collapsed:: true\n\t- ontology:: true\n\t- term-id:: AI-0067\n\t- preferred-term:: Accountability\n\t- source-domain:: ai\n\t- owl:class:: ai:Accountability\n\t- status:: draft\n\t- public-access:: true\n\t- definition:: The assignment of clear responsibilities for AI system development, deployment, and outcomes, coupled with mechanisms for oversight, redress, and remediation, ensuring that actors can be held answerable for system impacts and failures.\n\t- #### Relationships\n\t  id:: accountability-relationships\n\t  collapsed:: true\n\t\t- is-subclass-of:: [[Metaverse]]\n\t\t- requires:: [[Explainability (AI-0063)]]\n\t\t- requires:: [[Transparency (AI-0062)]]\n\t\t- enables:: [[Compliance]]\n\t\t- enables:: [[Risk Management (AI-0078)]]\n\t\t- enables:: [[Trust]]\n\t\t- is-enabled-by:: [[Compliance Audit Trail]]\n\t\t- is-enabled-by:: [[Human Oversight]]\n\t\t- is-enabled-by:: [[AI Impact Assessment]]\n\t\t- is-enabled-by:: [[Human-in-the-Loop]]\n\t\t- is-enabled-by:: [[Audit Trail]]\n\n## OWL Formal Semantics\n\n```clojure\n;; OWL Functional Syntax\n\n(Declaration (Class :Accountability))\n\n;; Annotations\n(AnnotationAssertion rdfs:label :Accountability \"Accountability\"@en)\n(AnnotationAssertion rdfs:comment :Accountability \"The assignment of clear responsibilities for AI system development, deployment, and outcomes, coupled with mechanisms for oversight, redress, and remediation, ensuring that actors can be held answerable for system impacts and failures.\"@en)\n\n;; Data Properties\n(AnnotationAssertion dcterms:identifier :Accountability \"AI-0067\"^^xsd:string)\n(DataPropertyAssertion :isAITechnology :Accountability \"true\"^^xsd:boolean)\n```\n\n## Formal Specification\n\n```yaml\nterm: Accountability\ndefinition: \"Clear assignment of responsibilities with mechanisms for oversight and redress\"\ndomain: AI Governance\ntype: Organizational Principle\ncomponents:\n  - responsibility_assignment\n  - oversight_mechanisms\n  - audit_trails\n  - redress_mechanisms\n  - remediation_processes\nlevels:\n  - individual_accountability\n  - organizational_accountability\n  - regulatory_accountability\nstakeholders: [developers, deployers, users, regulators, affected_individuals]\n```\n\n## Authoritative References\n\n### Primary Sources\n\n1. **ISO/IEC 23894:2023** - Information technology — Artificial intelligence — Guidance on risk management\n   - Section 6.3.2: \"Accountability in AI systems\"\n   - Assigns responsibilities throughout AI lifecycle\n   - Source: ISO/IEC JTC 1/SC 42\n\n2. **NIST AI Risk Management Framework (AI RMF 1.0)**, January 2023\n   - Section 2.2: \"Accountable and Transparent\"\n   - \"Processes are established to maintain accountability, responsibility, and transparency across the AI lifecycle\"\n   - Source: National Institute of Standards and Technology\n\n3. **EU AI Act** (Regulation 2024/1689), June 2024\n   - Article 26: \"Responsibilities along the AI value chain\"\n   - Article 72: \"Reporting of serious incidents\"\n   - Source: European Parliament and Council\n\n### Supporting Standards\n\n4. **ISO/IEC 38500:2024** - Information technology — Governance of information technology\n   - Principles applicable to AI governance\n\n5. **OECD AI Principles** (2019)\n   - Principle 1.5: \"Accountability\"\n   - \"AI actors should be accountable for the proper functioning of AI systems\"\n\n## Key Characteristics\n\n### Dimensions of Accountability\n\n#### 1. Legal Accountability\n\n- **Liability**: Legal responsibility for harms\n- **Compliance**: Adherence to regulations\n- **Enforcement**: Penalties for violations\n- **Example**: GDPR fines for data protection violations\n\n#### 2. Organizational Accountability\n\n- **Internal Governance**: Clear roles and responsibilities\n- **Oversight Boards**: AI ethics committees\n- **Policies and Procedures**: Documented processes\n- **Example**: Designated AI Ethics Officer\n\n#### 3. Professional Accountability\n\n- **Codes of Conduct**: Professional standards\n- **Peer Review**: Professional scrutiny\n- **Continuing Education**: Staying current\n- **Example**: IEEE Code of Ethics\n\n#### 4. Social Accountability\n\n- **Public Trust**: Maintaining societal confidence\n- **Stakeholder Engagement**: Involving affected parties\n- **Transparency**: Open communication\n- **Example**: Public reporting on AI impacts\n\n## Components of Accountability\n\n### 1. Responsibility Assignment\n\n**Roles and Responsibilities Matrix**\n\n| Role | Responsibilities |\n|------|-----------------|\n| **AI Developer** | Design, training, testing, documentation |\n| **Data Provider** | Data quality, provenance, consent |\n| **System Deployer** | Appropriate use, monitoring, incident response |\n| **Human Overseer** | Review decisions, intervene when needed |\n| **Senior Management** | Governance, resource allocation, culture |\n| **Board of Directors** | Strategic oversight, risk appetite |\n| **Regulator** | Compliance verification, enforcement |\n\n**EU AI Act Accountability**\n- **Provider**: Entity developing or having AI system developed\n- **Deployer**: Entity using AI system under its authority\n- **Importer/Distributor**: Additional responsibilities for third-party systems\n- **Affected Individual**: Rights to information and redress\n\n### 2. Oversight Mechanisms\n\n**Internal Oversight**\n- AI ethics committees\n- Technical review boards\n- Compliance officers\n- Internal audit functions\n\n**External Oversight**\n- Regulatory inspections\n- Third-party audits\n- Certification bodies\n- Academic scrutiny\n\n**Automated Oversight**\n- Continuous monitoring systems\n- Automated compliance checking\n- Performance dashboards\n- Alert mechanisms\n\n### 3. Audit Trails and Traceability\n\n**What to Log**\n- Data sources and versions\n- Model training parameters\n- Algorithm versions\n- Decision rationale\n- Human interventions\n- Incidents and responses\n\n**Technical Implementation**\n```yaml\naudit_log_entry:\n  timestamp: \"2025-10-27T14:32:15Z\"\n  user_id: \"analyst_42\"\n  action: \"model_prediction\"\n  input_data_hash: \"a3f5...\"\n  model_version: \"v2.1.4\"\n  output: {prediction: 0.87, confidence: 0.72}\n  override: false\n  human_review_required: true\n```\n\n### 4. Redress and Remediation\n\n**Complaint Mechanisms**\n- Clear process for raising concerns\n- Multiple channels (online, phone, in-person)\n- Timely acknowledgment\n\n**Review Process**\n- Human review of contested decisions\n- Independent appeal mechanisms\n- Transparent criteria\n\n**Remediation**\n- Correction of errors\n- Compensation for harms\n- System improvements based on incidents\n\n## Relationships\n\n- **Component Of**: AI Trustworthiness (AI-0061)\n- **Requires**: Transparency (AI-0062), Explainability (AI-0063)\n- **Enables**: Trust, Compliance, Risk Management (AI-0078)\n- **Supports**: Fairness (AI-0065), Safety (AI-0070)\n- **Related To**: Governance Framework (AI-0035), AI Audit (AI-0104)\n\n## Accountability Frameworks\n\n### ISO/IEC 23894:2023 Framework\n\n**Phases**:\n1. **Plan**: Define responsibilities, establish oversight\n2. **Do**: Implement controls, maintain records\n3. **Cheque**: Audit, monitor, review\n4. **Act**: Remediate, improve, learn\n\n### NIST AI RMF Accountability\n\n**Functions**:\n- **Govern**: Establish accountability structures\n- **Map**: Identify accountable parties for each risk\n- **Measure**: Track accountability metrics\n- **Manage**: Execute accountability mechanisms\n\n### EU AI Act Accountability\n\n**Article 26 Obligations**:\n- Providers: Conformity assessment, quality management, documentation\n- Deployers: Human oversight, monitoring, incident reporting\n- Shared: Cooperation on incident investigation\n\n## Challenges to Accountability\n\n### Many Hands Problem\n\n**Challenge**: Diffusion of responsibility across many actors\n- Data collectors, model developers, deployers, users\n- Complex supply chains\n- Shared responsibility\n\n**Solution**:\n- Clear contractual obligations\n- Joint liability frameworks\n- Coordination mechanisms\n\n### Automation and Opacity\n\n**Challenge**: Difficulty attributing autonomous system decisions\n- \"The algorithm made the decision\"\n- Opaque decision-making\n- Emergent behaviours\n\n**Solution**:\n- Human oversight requirements\n- Explainability mechanisms\n- Override capabilities\n\n### Temporal Distance\n\n**Challenge**: Gap between development and harm\n- Long latency between deployment and adverse effects\n- Changing personnel\n- Evolution of systems\n\n**Solution**:\n- Long-term documentation\n- Institutional memory\n- Ongoing monitoring\n\n### Jurisdictional Issues\n\n**Challenge**: Cross-border development and deployment\n- Multinational corporations\n- Cloud infrastructure\n- Varying legal frameworks\n\n**Solution**:\n- Harmonized international standards\n- Clear jurisdictional rules\n- Mutual recognition agreements\n\n## Domain-Specific Accountability\n\n### Healthcare\n\n**Accountability Framework**:\n- Physician ultimately responsible for clinical decisions\n- AI system as \"decision support\" tool\n- Clear documentation of AI use in medical records\n- Incident reporting to FDA/regulatory bodies\n\n**Liability**:\n- Medical malpractice standards apply\n- Product liability for device manufacturers\n- Informed consent requirements\n\n### Finance\n\n**Accountability Framework**:\n- Model Risk Management (SR 11-7)\n- Designated model validators\n- Model inventory and governance\n- Regular model reviews\n\n**Liability**:\n- Fair lending compliance\n- Consumer protection laws\n- Fiduciary duties\n\n### Autonomous Vehicles\n\n**Accountability Framework**:\n- Manufacturer liability for defects\n- Driver responsibility for supervision (Level 2-3)\n- Shared responsibility (Level 4-5)\n- Event data recorders\n\n**Liability**:\n- Product liability\n- Tort law adaptation\n- Insurance frameworks\n\n## Implementation Best Practices\n\n### 1. Establish Clear Governance\n\n```\nBoard of Directors\n    ↓\nAI Ethics Committee\n    ↓\nChief AI Officer\n    ↓\nAI Development Teams + Deployment Teams + Oversight Functions\n```\n\n### 2. Document Everything\n\n**Documentation Requirements**:\n- Model cards and datasheets\n- Risk assessments\n- Validation reports\n- Deployment plans\n- Incident logs\n- Audit reports\n\n### 3. Build Accountability into Systems\n\n**Technical Measures**:\n- Logging and auditability\n- Version control\n- A/B testing\n- Canary deployments\n- Kill switches\n\n**Process Measures**:\n- Code review\n- Model validation\n- Deployment approvals\n- Post-deployment monitoring\n\n### 4. Enable Redress\n\n**User Rights**:\n- Right to explanation\n- Right to human review\n- Right to appeal\n- Right to correction\n\n**Process**:\n- Clear complaint submission\n- Defined response timelines\n- Independent review\n- Transparent outcomes\n\n### 5. Continuous Improvement\n\n**Learning Loop**:\n- Incident analysis\n- Root cause investigation\n- Systemic improvements\n- Knowledge sharing\n\n## Accountability Metrics\n\n### Process Metrics\n\n1. **Governance Maturity**\n   - Existence of accountability structures\n   - Completeness of documentation\n   - Regular oversight meetings\n\n2. **Audit Completeness**\n   - Percentage of systems audited\n   - Audit findings closure rate\n   - Time to remediation\n\n3. **Incident Response**\n   - Time to detection\n   - Time to resolution\n   - Recurrence rate\n\n### Outcome Metrics\n\n1. **Redress Effectiveness**\n   - Complaint resolution rate\n   - User satisfaction with process\n   - Correction success rate\n\n2. **Compliance**\n   - Regulatory findings\n   - Violation rate\n   - Penalty amounts\n\n3. **Trust Indicators**\n   - Stakeholder confidence surveys\n   - Reputation metrics\n   - Market trust signals\n\n## Regulatory Requirements\n\n### EU AI Act\n\n**Article 72: Serious Incident Reporting**\n- Providers and deployers must report serious incidents\n- Timeline: \"without undue delay, and in any event within 15 days\"\n- Content: Description, affected persons, remediation taken\n\n**Article 26: Responsibility Allocation**\n- Clear delineation between providers and deployers\n- Contractual arrangements for responsibility sharing\n\n### GDPR\n\n**Article 5(2): Accountability Principle**\n- Controller responsible for compliance\n- \"...and be able to demonstrate compliance\"\n- Requires documentation and evidence\n\n**Article 24: Controller Responsibilities**\n- Implement appropriate technical and organizational measures\n- Demonstrate compliance\n- Review and update measures\n\n### Sector-Specific\n\n**Healthcare**: FDA 21 CFR Part 820 (Quality Management)\n**Finance**: SR 11-7 (Model Risk Management)\n**Aviation**: DO-178C (Software safety)\n\n## Case Studies\n\n### Success: Microsoft Tay Incident Response\n\n**Situation**: Chatbot learned offensive language from user interactions (2016)\n\n**Accountability Actions**:\n- Immediate shutdown (within 24 hours)\n- Public apology\n- Root cause analysis\n- Improved content filtering\n- Enhanced safeguards for future releases\n\n**Lesson**: Swift action, transparency, systemic improvement\n\n### Failure: Compass Recidivism Algorithm\n\n**Situation**: Bias in risk assessments, lack of transparency (ProPublica 2016)\n\n**Accountability Gaps**:\n- Proprietary \"black box\" algorithm\n- No explanation for individuals\n- No effective appeal mechanism\n- Diffused responsibility\n\n**Lesson**: Transparency and redress mechanisms essential\n\n## Best Practices Summary\n\n1. **Assign Clear Roles**\n   - Explicit responsibility matrices\n   - Documented accountability\n   - No ambiguity\n\n2. **Enable Oversight**\n   - Internal and external mechanisms\n   - Independent review\n   - Adequate resources\n\n3. **Maintain Audit Trails**\n   - Comprehensive logging\n   - Version control\n   - Traceability\n\n4. **Provide Redress**\n   - Accessible complaint mechanisms\n   - Timely human review\n   - Effective remediation\n\n5. **Foster Accountability Culture**\n   - Training and awareness\n   - Reward responsible behaviour\n   - Enforce consequences\n\n6. **Continuous Improvement**\n   - Learn from incidents\n   - Update processes\n   - Share learnings\n\n## Related Terms\n\n- **AI Trustworthiness** (AI-0061)\n- **Transparency** (AI-0062)\n- **Explainability** (AI-0063)\n- **Governance Framework** (AI-0035)\n- **AI Audit** (AI-0104)\n- **Human Oversight** (AI-0041)\n\n## Version History\n\n- **1.0** (2025-10-27): Initial definition based on ISO/IEC 23894:2023 and EU AI Act\n\n---\n\n*This definition emphasises that accountability is not merely theoretical responsibility but requires concrete mechanisms for oversight, traceability, and redress.*\n\n## 2024-2025: Operationalising Accountability Through Mandated Mechanisms\nid:: accountability-recent-developments\n\nThe years 2024 and 2025 witnessed accountability transition from aspirational principle to **concrete operational requirement**, driven by regulatory mandates, high-profile AI failures, and the emergence of standardised accountability frameworks adopted across industries and jurisdictions.\n\n### EU AI Act Accountability Architecture\n\nThe **EU AI Act**, effective 1st August 2024, established the world's first comprehensive **legal accountability framework** for AI systems, with phased implementation through August 2026 for high-risk systems and August 2027 for general-purpose AI. The Act assigns explicit responsibilities across the AI value chain:\n\n**Article 16** requires **providers** of high-risk AI systems to establish a **quality management system** encompassing compliance monitoring, post-market surveillance, and incident reporting. **Article 26** delineates responsibilities between providers (those who develop or commission AI systems) and **deployers** (those who use AI systems under their authority), with detailed contractual requirements for shared accountability.\n\n**Article 72** mandates **serious incident reporting**: providers and deployers must report incidents causing death, serious health damage, or fundamental rights violations \"without undue delay, and in any event within 15 days\" to national competent authorities. By late 2024, the European Commission established a **centralised AI incident database** receiving reports from across member states, providing unprecedented visibility into AI system failures.\n\n### ISO Accountability Standards Deployment\n\n**ISO/IEC 42001:2023**, the first international standard for AI management systems, gained rapid adoption throughout 2024 as organisations sought certification demonstrating accountability maturity. The standard requires **documented accountability structures**, including designated AI governance roles, clear responsibility matrices, and audit trails for all AI system decisions.\n\nBy mid-2025, major certification bodies including **BSI, TÜV, and SGS** had certified hundreds of organisations to ISO 42001, creating a **market signal** whereby certified organisations gained competitive advantages in regulated sectors and public procurement. However, critics noted that certification verified processes but did not guarantee outcomes, raising concerns about **accountability theatre** versus substantive accountability.\n\n### The Algorithmic Accountability Act (U.S.)\n\nWhilst not enacted into law by early 2025, the proposed **Algorithmic Accountability Act** introduced in the U.S. Congress in 2024 would require companies deploying high-impact automated decision systems to conduct **impact assessments** evaluating risks to accuracy, fairness, bias, discrimination, privacy, and security. The Act would mandate documentation, ongoing monitoring, and corrective action for identified deficiencies.\n\nSeveral U.S. states advanced their own accountability frameworks: **Colorado's AI Act** (enacted May 2024) requires deployers to notify consumers when consequential decisions involve automated systems and establishes a right to appeal such decisions. **California** and **New York** introduced similar bills in 2024, signalling a potential patchwork of state-level accountability requirements.\n\n### Technical Audit Trails and Explainability Requirements\n\n2024-2025 saw **audit trail requirements** evolve from optional best practice to mandatory compliance obligation. The EU AI Act's **Annex IV** specifies detailed **technical documentation** requirements including:\n\n- **Training data provenance** and validation procedures\n- **Model architecture** and training methodology\n- **Testing and validation** results across subpopulations\n- **Human oversight mechanisms** and override procedures\n- **Risk management** measures throughout the lifecycle\n\nLeading AI platforms including **AWS SageMaker**, **Google Vertex AI**, and **Microsoft Azure ML** introduced **automated compliance logging** features capturing model lineage, training parameters, and prediction explainability, with immutable audit trails meeting regulatory requirements.\n\n### Third-Party Auditing and Certification\n\nThe demand for **independent AI audits** surged in 2024, with organisations including **BSI, DEKRA, UL Solutions**, and specialist firms such as **ORCAA (O'Reilly Centre for Algorithmic Accountability)** offering AI system audits against standards including ISO 42001, NIST AI RMF, and sector-specific frameworks.\n\nHowever, the field faced **methodology fragmentation**: no consensus emerged on audit procedures, testing regimes, or pass/fail criteria. In response, the **IEEE P7001** standard for **transparency of autonomous systems** advanced towards finalisation in 2025, whilst **ISO/IEC 29119-11** for **AI testing** progressed through standardisation.\n\n### Accountability for Foundation Models\n\nThe emergence of **general-purpose AI** and **foundation models** in 2023-2024 created novel accountability challenges: these models are developed by one organisation but deployed by thousands of downstream users for myriad applications, creating **diffuse accountability**. Who is responsible when a foundation model fine-tuned by a third party produces biased outputs?\n\nThe EU AI Act addressed this through **Article 53**, requiring general-purpose AI providers to maintain **technical documentation**, conduct **model evaluations**, and provide downstream users with sufficient information to comply with their own obligations. In practice, this led major foundation model providers including **OpenAI**, **Anthropic**, and **Google DeepMind** to publish **model cards**, **system cards**, and **responsible AI guidance** for deployers.\n\nThe **Frontier AI Safety Regime**, announced by the U.K. government in November 2024, introduced **pre-deployment safety testing** requirements for frontier AI models, with accountability vested in model developers rather than downstream deployers, establishing a **producer liability** model for capable AI systems.\n\n### The \"Many Hands\" Problem in Practice\n\nThe challenge of **diffused responsibility** across AI development pipelines intensified in 2024-2025 as systems became more complex. A typical enterprise AI system might involve:\n\n- **Cloud infrastructure** providers (AWS, Azure, GCP)\n- **Foundation model** developers (OpenAI, Anthropic, etc.)\n- **MLOps platform** vendors (Databricks, Weights & Biases)\n- **Data providers** and annotators\n- **Integration specialists** and system integrators\n- **Deploying organisations** and their employees\n- **End users** making decisions based on AI outputs\n\nWhen failures occurred, accountability was contested: providers blamed deployers for misuse, deployers blamed providers for inadequate safeguards, and both blamed users for not exercising appropriate oversight. The EU AI Act's detailed responsibility allocation in **Article 26** aimed to resolve these disputes through contractual clarity, but early case law in 2024-2025 revealed ambiguities requiring judicial interpretation.\n\n### Remediation and Redress Mechanisms\n\n**Right to explanation** provisions, long theorised under GDPR Article 22, saw increased enforcement in 2024 as data protection authorities issued guidance and penalties for inadequate explanations. France's **CNIL** and Italy's **Garante** issued notable decisions requiring organisations to provide **meaningful explanations** of automated decisions, not merely generic descriptions of algorithmic processes.\n\nThe **right to human review**, mandated for high-risk AI systems under EU AI Act Article 14, posed operational challenges: organisations struggled to define what constituted \"meaningful\" human oversight versus **rubber-stamping** AI outputs. Research demonstrated that humans frequently deferred to AI recommendations (**automation bias**), undermining the intended accountability safeguard.\n\n### Incident Reporting and Transparency\n\nBy late 2024, **AI incident databases** proliferated, including the **AIAAIC Repository**, **AI Incident Database**, and regulatory databases under the EU AI Act and U.K. Frontier AI regime. These databases revealed patterns:\n\n- **Bias incidents** dominated reports (employment discrimination, credit denial)\n- **Security vulnerabilities** including prompt injection and model extraction\n- **Safety failures** in autonomous systems and robotics\n- **Privacy breaches** through model inversion and membership inference attacks\n\nTransparency regarding incidents improved substantially: whereas historically organisations concealed AI failures, regulatory reporting requirements and corporate accountability cultures encouraged disclosure, enabling systemic learning from failures.\n\t- maturity:: draft\n\t- owl:class:: mv:Accountability\n\t- owl:physicality:: ConceptualEntity\n\t- owl:role:: Concept\n\t- belongsToDomain:: [[MetaverseDomain]]\n\t- #### Relationships\n\t  id:: accountability-relationships\n\t\t- is-subclass-of:: [[Metaverse]]\n\t\t- is-enabled-by:: [[Human-in-the-Loop]], [[AI Impact Assessment]], [[Compliance Audit Trail]], [[Audit Trail]], [[Human Oversight]]\n\t\t- requires:: [[Transparency (AI-0062)]], [[Explainability (AI-0063)]]\n\t\t- enables:: [[Trust]], [[Compliance]], [[Risk Management (AI-0078)]]",
  "backlinks": [
    "Human-in-the-Loop",
    "AIEthics",
    "Audit Trail",
    "Stakeholder",
    "AI Governance Principle",
    "Loss Function",
    "Compliance Audit Trail"
  ],
  "wiki_links": [
    "Trust",
    "Audit Trail",
    "Human-in-the-Loop",
    "Transparency (AI-0062)",
    "Compliance",
    "Explainability (AI-0063)",
    "Human Oversight",
    "AI Impact Assessment",
    "Compliance Audit Trail",
    "MetaverseDomain",
    "Risk Management (AI-0078)",
    "Metaverse"
  ],
  "ontology": {
    "term_id": "AI-0067",
    "preferred_term": "Accountability",
    "alt_terms": [],
    "iri": "http://narrativegoldmine.com/ai#Accountability",
    "source_domain": "ai",
    "domain": "ai",
    "domain_full_name": "Artificial Intelligence",
    "definition": "The assignment of clear responsibilities for AI system development, deployment, and outcomes, coupled with mechanisms for oversight, redress, and remediation, ensuring that actors can be held answerable for system impacts and failures.",
    "scope_note": null,
    "status": "draft",
    "maturity": "draft",
    "version": null,
    "public_access": true,
    "last_updated": null,
    "authority_score": null,
    "quality_score": null,
    "cross_domain_links": null,
    "owl_class": "ai:Accountability",
    "owl_physicality": "ConceptualEntity",
    "owl_role": "Concept",
    "owl_inferred_class": null,
    "is_subclass_of": [
      "Metaverse"
    ],
    "has_part": [],
    "is_part_of": [],
    "requires": [
      "Explainability (AI-0063)",
      "Transparency (AI-0062)"
    ],
    "depends_on": [],
    "enables": [
      "Compliance",
      "Risk Management (AI-0078)",
      "Trust"
    ],
    "relates_to": [],
    "bridges_to": [],
    "bridges_from": [],
    "domain_extensions": {},
    "belongs_to_domain": [
      "MetaverseDomain"
    ],
    "implemented_in_layer": [],
    "source": [],
    "other_relationships": {
      "is-enabled-by": [
        "Compliance Audit Trail",
        "Human Oversight",
        "AI Impact Assessment",
        "Human-in-the-Loop",
        "Audit Trail"
      ],
      "belongsToDomain": [
        "MetaverseDomain"
      ]
    },
    "validation": {
      "is_valid": false,
      "errors": [
        "Missing required property: last-updated"
      ]
    }
  }
}