{
  "id": "AI-0425-privacy-impact-assessment",
  "title": "Privacy Impact Assessment",
  "content": "- ### OntologyBlock\n  id:: 0425-privacy-impact-assessment-ontology\n  collapsed:: true\n\t- ontology:: true\n\t- term-id:: AI-0425\n\t- preferred-term:: Privacy Impact Assessment\n\t- source-domain:: ai\n\t- status:: in\n\t- public-access:: true\n\t- definition:: Privacy Impact Assessment is a structured evaluation process identifying, analyzing, and mitigating privacy risks associated with data processing activities, particularly AI systems handling personal information, ensuring compliance with data protection regulations and protecting individual privacy rights. This assessment methodology follows defined stages including systematic description documenting processing operations (data flows, purposes, retention periods, recipients), necessity and proportionality assessment evaluating whether processing is essential for stated purposes and uses minimal data required, privacy risk identification analyzing potential harms including unauthorized access, discrimination, surveillance, function creep, and re-identification risks, risk severity and likelihood evaluation producing risk matrices categorizing threats as low, medium, high, or very high based on potential impact and probability, mitigation strategy design specifying technical and organizational measures reducing risks to acceptable levels, and residual risk assessment determining whether remaining risks after mitigation require consultation with data protection authorities per GDPR Article 36. Assessment triggers mandated by GDPR Article 35 include automated decision-making with legal or similarly significant effects on individuals, large-scale processing of special category data (health, biometric, genetic, racial or ethnic origin, political opinions, religious beliefs, trade union membership, sexual orientation), systematic monitoring of publicly accessible areas at large scale, innovative use of new technologies, and processing that prevents data subjects from exercising rights or using services. AI-specific considerations examine algorithmic bias risks affecting protected groups, model explainability limitations preventing meaningful transparency, data quality issues propagating errors or outdated information, automation risks removing meaningful human oversight, and scale effects where processing volumes amplify individual harms. Stakeholder involvement requires consulting data protection officers providing expert guidance, data subjects gathering perspectives from affected individuals, processing staff understanding operational realities, legal counsel ensuring regulatory compliance, and technical experts evaluating security and privacy controls. Documentation artifacts include assessment reports capturing analysis and decisions, risk registers tracking identified risks and mitigation status, consultation records documenting stakeholder input, approval signatures from accountable executives, and review schedules ensuring periodic reassessment as systems or regulations evolve, with assessments reviewed whenever material changes occur in processing purposes, data categories, technologies, or legal landscape.\n\t- source:: [[GDPR Article 35]], [[ISO 29134]], [[ICO DPIA Code]]\n\t- maturity:: mature\n\t- owl:class:: ai:PrivacyImpactAssessment\n\t- owl:physicality:: VirtualEntity\n\t- owl:role:: Process\n\t- owl:inferred-class:: aigo:VirtualProcess\n\t- belongsToDomain:: [[AIEthicsDomain]]\n\t- implementedInLayer:: [[ConceptualLayer]]",
  "backlinks": [],
  "wiki_links": [
    "ICO DPIA Code",
    "AIEthicsDomain",
    "ConceptualLayer",
    "GDPR Article 35",
    "ISO 29134"
  ],
  "ontology": {
    "term_id": "AI-0425",
    "preferred_term": "Privacy Impact Assessment",
    "alt_terms": [],
    "iri": "http://narrativegoldmine.com/ai#PrivacyImpactAssessment",
    "source_domain": "ai",
    "domain": "ai",
    "domain_full_name": "Artificial Intelligence",
    "definition": "Privacy Impact Assessment is a structured evaluation process identifying, analyzing, and mitigating privacy risks associated with data processing activities, particularly AI systems handling personal information, ensuring compliance with data protection regulations and protecting individual privacy rights. This assessment methodology follows defined stages including systematic description documenting processing operations (data flows, purposes, retention periods, recipients), necessity and proportionality assessment evaluating whether processing is essential for stated purposes and uses minimal data required, privacy risk identification analyzing potential harms including unauthorized access, discrimination, surveillance, function creep, and re-identification risks, risk severity and likelihood evaluation producing risk matrices categorizing threats as low, medium, high, or very high based on potential impact and probability, mitigation strategy design specifying technical and organizational measures reducing risks to acceptable levels, and residual risk assessment determining whether remaining risks after mitigation require consultation with data protection authorities per GDPR Article 36. Assessment triggers mandated by GDPR Article 35 include automated decision-making with legal or similarly significant effects on individuals, large-scale processing of special category data (health, biometric, genetic, racial or ethnic origin, political opinions, religious beliefs, trade union membership, sexual orientation), systematic monitoring of publicly accessible areas at large scale, innovative use of new technologies, and processing that prevents data subjects from exercising rights or using services. AI-specific considerations examine algorithmic bias risks affecting protected groups, model explainability limitations preventing meaningful transparency, data quality issues propagating errors or outdated information, automation risks removing meaningful human oversight, and scale effects where processing volumes amplify individual harms. Stakeholder involvement requires consulting data protection officers providing expert guidance, data subjects gathering perspectives from affected individuals, processing staff understanding operational realities, legal counsel ensuring regulatory compliance, and technical experts evaluating security and privacy controls. Documentation artifacts include assessment reports capturing analysis and decisions, risk registers tracking identified risks and mitigation status, consultation records documenting stakeholder input, approval signatures from accountable executives, and review schedules ensuring periodic reassessment as systems or regulations evolve, with assessments reviewed whenever material changes occur in processing purposes, data categories, technologies, or legal landscape.",
    "scope_note": null,
    "status": "in",
    "maturity": "mature",
    "version": null,
    "public_access": true,
    "last_updated": null,
    "authority_score": null,
    "quality_score": null,
    "cross_domain_links": null,
    "owl_class": "ai:PrivacyImpactAssessment",
    "owl_physicality": "VirtualEntity",
    "owl_role": "Process",
    "owl_inferred_class": "aigo:VirtualProcess",
    "is_subclass_of": [],
    "has_part": [],
    "is_part_of": [],
    "requires": [],
    "depends_on": [],
    "enables": [],
    "relates_to": [],
    "bridges_to": [],
    "bridges_from": [],
    "domain_extensions": {},
    "belongs_to_domain": [
      "AIEthicsDomain"
    ],
    "implemented_in_layer": [
      "ConceptualLayer"
    ],
    "source": [
      "GDPR Article 35",
      "ISO 29134",
      "ICO DPIA Code"
    ],
    "validation": {
      "is_valid": false,
      "errors": [
        "Missing required property: last-updated",
        "Missing required property: is-subclass-of (at least one parent class)"
      ]
    }
  }
}