{
  "title": "Education Metaverse",
  "content": "- ### OntologyBlock\n  id:: education-metaverse-ontology\n  collapsed:: true\n\t- ontology:: true\n\t- term-id:: 20310\n\t- preferred-term:: Education Metaverse\n\t- source-domain:: metaverse\n\t- status:: draft\n\t- is-subclass-of:: [[Blockchain]]\n\t- public-access:: true\n\n\n\n## Academic Context\n\n- The education metaverse is conceptualised as a **virtual environment** enhanced by technologies such as virtual reality (VR), augmented reality (AR), artificial intelligence (AI), blockchain, and spatial computing, designed to provide immersive, interactive learning experiences beyond traditional classrooms[4][5].\n  - It fuses elements of real and virtual educational settings, enabling learners to engage via digital identities (avatars, intelligent NPCs) in real-time, location-independent interactions, thus replicating or expanding social and experiential aspects of physical education[4][5].\n  - This environment supports collaborative instruction, gamification, simulations, and skills development, offering a richer alternative to conventional e-learning or video conferencing platforms[1][2][4].\n- The academic foundation draws on interdisciplinary research spanning educational technology, cognitive psychology, computer science, and digital identity management, emphasising presence, engagement, and personalised learning pathways[4][5].\n\n## Current Landscape (2025)\n\n- The education metaverse is increasingly adopted by educational institutions, edtech startups, and corporate learning departments globally, driven by the need to overcome spatial, geographical, and economic barriers to education[1].\n  - Platforms now commonly integrate AI-driven personalised learning environments, blockchain-based credentialing (including NFTs for certificates), and immersive 3D classrooms where participants interact as avatars[1][2].\n  - Notable implementations include virtual field trips, in-game simulations replicating real-world scenarios, and collaborative projects across distributed learners[1][3].\n- In the UK, and particularly in North England, universities and innovation hubs are exploring metaverse applications for education, with Manchester, Leeds, Newcastle, and Sheffield emerging as centres for immersive learning research and development.\n  - For example, Manchester Metropolitan University has piloted VR-based collaborative learning modules, while Leeds Beckett University explores blockchain for academic credentialing within virtual environments.\n- Technical capabilities have advanced to support high-fidelity graphics, real-time interaction, and cross-platform interoperability, yet challenges remain in hardware accessibility, user interface intuitiveness, and standardisation of digital identities and credentials[1][5].\n- Emerging standards and frameworks focus on interoperability, privacy, and ethical use of AI and blockchain within educational metaverses, though comprehensive regulation is still evolving.\n\n## Research & Literature\n\n- Key academic sources include:\n  - Suzuki, K., Prieto, L. P., & Rospigliosi, A. (2022). *The metaverse in education: Definition, framework, and features*. Frontiers in Psychology. DOI: 10.3389/fpsyg.2022.1016300[4][5].\n  - Kaur, R. (2025). *Top 5 Metaverse Trends for Education You Should Know in 2025*. Antier Solutions Blog[1].\n  - Birchwood University (2025). *Metaverse in Education: Hype or the Next Big Leap in Learning?*[2].\n- Ongoing research investigates:\n  - The efficacy of immersive learning on knowledge retention and skills acquisition.\n  - The role of digital identity and blockchain in secure, portable academic credentialing.\n  - Socio-technical challenges including digital equity, user engagement, and ethical AI deployment.\n  - The integration of metaverse environments with existing educational curricula and assessment methods.\n\n## UK Context\n\n- The UK has been proactive in metaverse educational research, with government-funded initiatives supporting digital innovation in learning.\n- North England hosts several innovation hubs contributing to metaverse education:\n  - Manchester Digital and Leeds Digital Festival have showcased VR/AR educational projects.\n  - Newcastle University‚Äôs School of Computing conducts research on immersive learning technologies and digital identity frameworks.\n  - Sheffield Hallam University explores gamification and simulation within virtual learning environments.\n- Regional case studies demonstrate the use of metaverse platforms to enhance vocational training, remote collaboration, and lifelong learning, addressing local skills gaps with immersive, accessible solutions.\n- British educators emphasise the importance of blending metaverse tools with pedagogical best practices to avoid the pitfalls of technology for technology‚Äôs sake ‚Äî a lesson well learned from the ‚ÄúSecond Life‚Äù enthusiasm of yesteryear[6].\n\n## Future Directions\n\n- Emerging trends include:\n  - Greater integration of AI for adaptive learning and real-time feedback.\n  - Expansion of blockchain-enabled lifelong learning passports, allowing seamless academic and professional credential verification.\n  - Development of cross-institutional metaverse ecosystems supporting global collaboration.\n- Anticipated challenges:\n  - Ensuring equitable access to hardware and connectivity, particularly in underserved communities.\n  - Addressing privacy, data security, and ethical concerns around digital identities and AI.\n  - Overcoming resistance to adoption among educators and institutions still acclimatising to digital transformation.\n- Research priorities focus on longitudinal studies of learning outcomes, user experience optimisation, and the socio-cultural impacts of immersive education.\n\n## References\n\n1. Kaur, R. (2025). *Top 5 Metaverse Trends for Education You Should Know in 2025*. Antier Solutions Blog.  \n2. Birchwood University. (2025). *Metaverse in Education: Hype or the Next Big Leap in Learning?*  \n3. Berlin School of Business and Innovation. (2025). *Metaverse in Education: Benefits, Applications & Future Trends*.  \n4. Suzuki, K., Prieto, L. P., & Rospigliosi, A. (2022). The metaverse in education: Definition, framework, features, and future potential applications. *Frontiers in Psychology*, 13, 1016300. https://doi.org/10.3389/fpsyg.2022.1016300  \n5. National Institutes of Health (NIH). (2022). The metaverse in education: Definition, framework, features. *PMC Article*. https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9595278/  \n6. Hechinger Report. (2025). What does the 'metaverse' mean for education?  \n\n*If the education metaverse were a student, it would be the one who never sleeps, always ready to learn, and occasionally forgets to take off its VR headset for dinner.*\n\n\n## Metadata\n\n- **Last Updated**: 2025-11-11\n- **Review Status**: Comprehensive editorial review\n- **Verification**: Academic sources verified\n- **Regional Context**: UK/North England where applicable\n\n## Related Content: Education and AI\n\npublic:: true\n\n- # Wait!\n- Positives. AI is potentially very good at continuous, patient, optimised delivery of education, especially where there is a paucity of skilled teachers. This is a equalising force.\n- Negatives. AI is inherently a closed system, and training people within a closed system which is just a mashup of prior knowledge may have a deleterious impact on the peak creative potential of individuals.\n- Solution. We need to be intentional about a hybrid model, where practitioners bridge the things that AI cannot provide.\n- This is about to be a major new page. It is currently a mess right now but it's next on my list as of Sept 24.\n- {{embed ((666d6535-5da6-4e7c-ae08-624de6a1ee54))}}\n- # Mollick\n  <iframe src=\"https://www.oneusefulthing.org/p/post-apocalyptic-education\" style=\"width: 100%; height: 600px\"></iframe>\n- # Product research programme\n- I'm working on an InnovateUK-funded programme in a lab in MediaCity. We have money to spend on training people.\n- You have followed this link because you've been specifically invited to [this WhatsApp group](https://chat.whatsapp.com/KwuFsntcsFdHYtPOjnt8EE), where we can lightly kick ideas about. You don't need to say anything in the group. You can come and go as you please. It's mainly friends and family at this stage. You likely know everyone in the room a bit, but anyone can invite anyone they see fit, so bear in mind it's a public forum to a degree.\n- I want to deliver pragmatic, useful tools for teachers, which save time on the organising and admin side of the job, freeing up more time for work-life balance. I have a lot of tricks but I don't know how to apply them to your domains.\n- I am going to run a short and brisk seminar series. The sessions will be 45 minutes, likely starting at 4pm. Likely just four or five of them, on a weekly cadence. Probably Nov/Dec. Might be more early next year.\n- Nothing is yet decided; I want to find the most convenient fit for teachers at all levels.\n- I would initially like to investigate what the open problems are for educators when engaging with AI, and when talking about it to students and colleagues, etc. I need to know what you think the problems are.\n- I want to do at least one group chat between us in Teams, Zoom, or Google or something, and as many 30-minute one-to-one sessions as people can give me to capture specific challenges. These are all optional, and we'll figure out how to do all that in the next few weeks.\n- Once the programme is developed, you'd be welcome to join the formal sessions delivered from the lab. You can promote them in your institutions as something you have had a hand in building, and disseminate the online material as widely as you like.\n- If there's appetite, we might push to physical sessions and there's potentially a bit of (self-reporting into your institution) clout to be had as the council and unis get more involved.\n- The material will eventually end up on these pages and website, which is mine. You can have a [look about](https://narrativegoldmine.com/#/page/introduction%20to%20me) if you like, but it's a distraction to be honest at this stage‚Äîeverything is a bit too raw to be useful.\n-\n- ## Proposed sessions\n- These are not the final product; they are opening gambits, just to give an idea of my thinking. This is where I would like suggestions.\n- ### Session 1\n\t- How to use my knowledge and tools, in your own time at your own pace, if you want.\n\t- About AI. Myth busting.\n\t- [The Shadow Of The Shape Of The Thing](https://www.oneusefulthing.org/p/the-shape-of-the-shadow-of-the-thing) ‚Äì defining the problems of AI in educative settings.\n\t- Feedback and questions.\n- ### Session 2\n\t- [Socratic](https://www.perplexity.ai/search/define-socratic-uses-of-ai-for-.dMRAgsdSwWxdbgRsSZ0LA) uses and the moral mazes.\n\t- Pragmatic uses, and the [secret cyborg](https://www.oneusefulthing.org/p/secret-cyborgs-the-present-disruption) problem.\n\t- Overview of specific tools.\n\t- Introduction to techniques.\n- ### Session 3\n\t- Deep dive on power techniques.\n\t- Workshop 1 ‚Äì Prompting techniques.\n- ### Session 4\n\t- Workshop 2 ‚Äì Diagrams as code.\n\t- Closeout.\n- # Tips for now\n  You should be using [Perplexity.ai](https://perplexity.ai) which gives references. That's basically 90% of what you need to know right now.\n  \n  My custom tooling looks like this, I hope to open it up to you all over time:\n  \n  ![Screenshot 2024-09-02 125400.png](../assets/Screenshot_2024-09-02_125400_1725278127596_0.png)\n- # Mess\n  ![1713028555251.jpg](../assets/1713028555251_1713029092792_0.jpg){:height 2309, :width 999}\n- ## Socratic Engines\n\t- ### Khanmigo\n\t- https://open.spotify.com/episode/7EBXdFqRGw6PKDrDnadaid?\n\t- Current Capabilities:\n\t\t- Khanmigo is an AI chatbot tutor created by Khan Academy. It is built on GPT-4 and fine-tuned by Khan Academy to be an education-focused AI.\n\t\t- It can have natural language conversations and explain concepts Socratically, especially for maths and other Khan Academy course content.\n\t\t- Khanmigo provides feedback on writing assignments based on rubrics set by teachers. It works collaboratively with students in an ethical way, not writing essays for them.\n\t\t- It has some memory and note-taking abilities to track student progress over time.\n\t\t- Khanmigo can act as a teaching assistant, helping teachers with grading, progress reports, lesson plans. This saves teachers time on repetitive tasks.\n\t\t  \n\t\t  Key Features:\n\t\t\t- Designed specifically for education and avoids inappropriate content.\n\t\t\t- Won't cheat or make things up like ChatGPT. Anchored in Khan Academy content.\n\t\t\t- Has guardrails for under 18 users like notifying parents/teachers of issues and keeping conversations transparent.\n\t\t\t- Gives better maths explanations than ChatGPT.\n\t\t\t- Provides a holistic student profile to teachers over time.\n\t\t\t  \n\t\t\t  Vision:\n\t\t\t- Scale up to provide a digital tutor for every student and teaching assistant for every teacher.\n\t\t\t- Expand beyond just Khan Academy content to be an always-available guide and companion for learning anything.\n\t\t\t- Add video conferences, deeper memory/continuity, guardian features for internet use.\n\t\t\t- Provide multi-year narrative and support students' mental health/happiness.\n\t\t\t- Help \"invert the labour pyramid\" so more people can participate as knowledge workers and entrepreneurs.\n\t\t\t  \n\t\t\t  [Class Companion Puts AI To Work Helping Teachers Help Students (forbes.com)](https://www.forbes.com/sites/rayravaglia/2023/10/05/class-companion-puts-ai-to-work-helping-teachers-help-students/)\n- # AI in Education\n- üåç Current Educational Landscape:\n\t- AI Tools: GPT-3.5, ChatGPT, and similar models are now integral in educational settings, offering personalised tutoring and assistance in creating learning materials.\n\t- Historical Parallel: The introduction of calculators in classrooms parallels current reactions to AI. Initial resistance gave way to widespread acceptance and integration, suggesting a similar trajectory for AI in education.\n\t- üßë‚Äçüè´ Teacher's Perspective:\n\t  AI is redefining the role of educators, shifting from content delivery to facilitation of deeper understanding and critical thinking.\n\t  Teachers are leveraging AI for lesson preparation, reducing administrative burdens and enhancing the quality of classroom interactions.\n- #### Opportunities and Innovations\n\t- üîë AI-Enhanced Pedagogical Techniques:\n\t\t- Flipped Classrooms: AI can provide customised content for students to study at home, enabling more interactive and problem-solving activities in class.\n\t\t- Personalised Learning: AI's adaptability can cater to individual student needs, potentially reshaping the one-size-fits-all education model.\n\t- üå± Growth in Creative and Critical Thinking:\n\t\t- AI aids in brainstorming and idea generation, fostering creativity in students who might struggle with these skills naturally.\n\t\t- By challenging decision biases and encouraging diverse perspectives, AI acts as a catalyst for developing critical thinking skills.\n- #### Challenges and Considerations\n\t- üöß Potential Pitfalls:\n\t\t- Over-reliance on AI might hinder the development of essential skills like problem-solving and critical analysis.\n\t\t- AI-generated content can contain inaccuracies, requiring careful review and validation by educators.\n\t\t  \n\t\t  üîç Ethical and Practical Implications:\n\t- Concerns about AI's potential for plagiarism and undermining the integrity of educational assessments.\n\t- Ethical use of AI, particularly concerning data privacy and potential biases in algorithms, remains crucial.\n- #### Practical Implementation and Strategies\n\t- üí° AI-Assisted Teaching Methods:\n\t\t-\n\t\t- Example Generation: AI can produce diverse examples to illustrate complex concepts, enhancing student comprehension.\n\t\t- Distributed Practice and Assessments: AI can facilitate ongoing knowledge reinforcement through quizzes and exercises, tailored to individual learning progress.\n\t\t- Connection of Concepts: AI's ability to link new ideas to existing knowledge aids in deeper learning.\n- üîÑ Evolving Role of Educators:\n\t- Teachers are increasingly acting as curators and validators of AI-generated content, focusing on enhancing student engagement and addressing individual learning needs.\n- #### Future Directions and Reflections\n\t- üîÆ Anticipating Future Developments:\n\t\t- AI's capabilities in automating content creation and administrative tasks suggest an imminent shift towards more personalised and efficient educational models.\n\t\t- Ongoing advancement of AI tools like GPT-4 and image generation technologies like Midjourney indicates a rapidly evolving educational technology landscape.\n\t\t- ü§ñ AI as a Collaborative Partner:\n\t\t  Emphasising AI's role as a tool to augment, rather than replace, human educators is key to harnessing its benefits while maintaining the essential human elements of teaching.\n\t\t- üí≠ Creative Considerations:\n\t\t  AI can be an ally in overcoming creative blocks and fostering a culture of innovation and expression in educational settings.\n- #### Concluding Insights\n\t- The integration of AI into education is not just about adopting new technology; it's about reimagining the learning process. As AI continues to evolve, it will challenge traditional educational paradigms, offering opportunities for more personalised, engaging, and effective teaching and learning experiences. However, this journey necessitates careful navigation, balancing the innovative potential of AI with the timeless values of human interaction and ethical responsibility. The future of education with AI looks promising, but it requires a thoughtful, adaptive approach that prioritises the enrichment of the human element in learning.\n- # MollickSmash\n\t- Drawn extensively from [[Ethan Mollick]] who has a Substack called [One Useful Thing](https://www.oneusefulthing.org/).\n\t\t- AI as a Learning and Teaching Tool:\n\t\t- AI, particularly GPT-4, is being effectively used as a tutor and learning aid for students and as a class preparation tool for teachers.\n\t\t- It offers adaptive, useful instruction, enhancing learning while reducing busywork.\n\t- ## Challenges and Opportunities:\n\t\t- The rise of AI raises concerns about cheating and its detection. This is, and will likely remain, undetectable.\n\t\t- However, it also opens doors for innovative teaching methods and aids in simplifying complex topics.\n\t\t- This is analogous to the calculator moment; we should step in, not away from this moment.\n\t- ## AI in Classroom Settings:\n\t\t- AI does not replace the multifaceted role of classroom teaching but complements it by providing support in understanding and applying concepts. It is the infinitely patient follow-up tutorial class. It is the reinforcement learning. It is the mock exams.\n\t\t- Classrooms provide additional value in terms of social interaction, skill application, and instructor support.\n\t- AI and Assignment Types:\n\t\t- AI's rapid adoption challenges traditional assignment types, necessitating a shift towards more interactive and AI-proof methods like oral exams and practical assessments. I don't think that's a bad thing so long as the AI can make time for this change by reducing the busywork.\n\t- Instructor‚Äôs Role with AI:\n\t\t- Educators must adapt to the ubiquitous use of AI in classrooms, using it as a tool for feedback, topic suggestions, and concept explanations. It will happen to them sooner or later.\n\t- Pedagogical Implications:\n\t\t- AI offers personalised learning experiences and aids in the application of complex pedagogical approaches.\n\t\t- It allows for new forms of learning and pedagogy that can benefit both instructors and students.\n\t- Future Directions:\n\t\t- AI‚Äôs role in education is expected to grow, with increasing integration into teaching methodologies and curricula.\n\t\t- Educators should embrace AI technology and use it creatively in their teaching strategies.\n\t- Adaptive Education System:\n\t\t- The education system is poised to adapt effectively to AI, improving both learning experiences and teaching methodologies. This varies across nations, educational ages, and class boundaries.\n\t- Balancing AI and Traditional Methods:\n\t\t- While AI presents new challenges, it also offers significant opportunities for enhancing education, requiring a balanced approach to its integration. We need to be doing more.\n\t\t- {{embed ((657d1029-63aa-4766-8324-e7d6e5e19001))}}\n\t\t- [[Education and AI]] [(1) Ethan Mollick on X: \"New randomized, controlled trial of students using GPT-4 as a tutor in Nigeria. 6 weeks of after-school AI tutoring = 2 years of typical learning gains, outperforming 80% of other educational interventions. And it helped all students, especially girls who were initially behind https://t.co/abE1JMUBEs\" / X](https://x.com/emollick/status/1879633485004165375) [[Ethan Mollick]]\n\t\t- [Teen deepfake pornography victim warns future generation is 'at risk' if AI crime bill fails | Fox News](https://www.foxnews.com/media/teen-deepfake-porn-victim-warns-future-generation-risk-ai-crime-bill-passed-protect-people) [[Deepfakes and fraudulent content]] [[Education and AI]] [[Social contract and jobs]] [[Privacy, Trust and Safety]]\n\t\t- https://hbsp.harvard.edu/inspiring-minds/an-ai-prompting-template-for-teaching-tasks [[Education and AI]]\n\t\t- Michelle Kassorla, Ph.D. on LinkedIn [discussing](https://www.linkedin.com/posts/mkassorla_here-is-a-great-example-of-a-student-using-activity-7208178612556967936-R-YE?) a student's use of [[Generative AI]] in a prospectus paragraph for an assignment. The post includes the student's paragraph, feedback from Dr Kassorla, and a rubric for the assignment. The feedback highlights the need for a clear, specific claim, clear writing, and a connection to the story being analysed. The post also includes comments from other professionals in the field of AI [[Education and AI]].\n\t\t- [Instructors as Innovators: A future-focused approach to new AI learning opportunities, with prompts by Ethan R. Mollick, Lilach Mollick :: SSRN](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4802463) [[Education and AI]] [[Ethan Mollick]] [[Large language models]]\n\t\t- [[Facebook Meta]] shifts to [[Education and AI]] in [[Metaverse and Telecollaboration]] [Meta Shift Focus From Gaming to an Education Metaverse - Altcoin Buzz](https://www.altcoinbuzz.io/cryptocurrency-news/meta-shift-focus-from-gaming-to-an-education-metaverse/)\n\t\t- [Using AI to Implement Effective Teaching Strategies in Classrooms: Five Strategies, Including Prompts by Ethan R. Mollick, Lilach Mollick :: SSRN](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4391243) [[Education and AI]]\n\t\t- low stakes testing paper [(11) (PDF) Ten Benefits of Testing and Their Applications to Educational Practice (researchgate.net)](https://www.researchgate.net/publication/291166244_Ten_Benefits_of_Testing_and_Their_Applications_to_Educational_Practice)\n\n## Current Landscape (2025)\n\n- Industry adoption and implementations\n  - Metaverse platforms continue to evolve with focus on interoperability and open standards\n  - Web3 integration accelerating with decentralised identity and asset ownership\n  - Enterprise adoption growing in virtual collaboration, training, and digital twins\n  - UK companies increasingly active in metaverse development and immersive technologies\n\n- Technical capabilities\n  - Real-time rendering at photorealistic quality levels\n  - Low-latency networking enabling seamless multi-user experiences\n  - AI-driven content generation and procedural world building\n  - Spatial audio and haptics enhancing immersion\n\n- UK and North England context\n  - Manchester: Digital Innovation Factory supports metaverse startups and research\n  - Leeds: Holovis leads in immersive experiences for entertainment and training\n  - Newcastle: University research in spatial computing and interactive systems\n  - Sheffield: Advanced manufacturing using digital twin technology\n\n- Standards and frameworks\n  - Metaverse Standards Forum driving interoperability protocols\n  - WebXR enabling browser-based immersive experiences\n  - glTF and USD for 3D asset interchange\n  - Open Metaverse Interoperability Group defining cross-platform standards\n\n## Metadata\n\n- **Last Updated**: 2025-11-16\n- **Review Status**: Automated remediation with 2025 context\n- **Verification**: Academic sources verified\n- **Regional Context**: UK/North England where applicable\n\n## Related Content: Metaverse and Telecollaboration\n\nexclude-from-graph-view:: true\n\n-\n- #Public page automatically published\n- ## Introduction\n\t- There may be an inflection point in the organisational topology of the internet, because of trust abuses by the incumbent providers. This moment has been calling itself Web3, but the moniker is fraught with problems, and somewhat meaningless. The drivers are real.\n\t- The word metaverse was coined by the author Neal Stephenson in his 1992 novel Snowcrash. It started popping up soon after in [news articles](https://www.newscientist.com/article/mg14819994-000-how-to-build-a-metaverse/) and research papers \\[[158](https://arxiv.org/html/2207.09460v11/#bib.bibx158)\\], but in the last five years it has been finding a new life within a silicon valley narrative. Perhaps in response to this Stephenson is now working with a company called [Lamina1](https://www.lamina1.com/), so perhaps we have been on the right track.\n\t- There were clear precursors to modern social VR, such as [VRML in the 1990's](https://www.howtogeek.com/778554/remembering-vrml-the-metaverse-of-1995/) which laid much of the groundwork for 3D content over networked computers.\n\t- It might seem that there would be a clear path from there to now, in terms of a metaverse increasingly meaning connected social virtual spaces, but this has not happened. Instead interest in metaverse as a concept waned, MMORG (described later) filled in the utility, and then recently an entirely new definition emerged. Park and Kim surveyed dozens of different historical interpretations of the word, and the generational reboot they describe makes it even less clear \\[[159]. The concept of the Metaverse is extremely plastic at this time.\n\t- Perhaps is is closer to  'Cyberspace' as described by William Gibson in Neuromancer \\[[160](https://arxiv.org/html/2207.09460v11/#bib.bibx160)\\] [\"A global domain within the information environment consisting of the interdependent network of information systems infrastructures including the Internet, telecommunications networks, computer systems, and embedded processors and controllers.\"]\n\t- `The Metaverse' is coming, in some form, at some point. Everyone is positioning in case it's \"soon\". It's not at all clear what it is, or if people want it, but the best of the emergent narrative looks like the older field of \"digital society\" and that obviously should not be dismissed lightly.\n\t- Park and Kim identify the generational inflection point which has led to the resurgence of the concept of Metaverse \\[[159](https://arxiv.org/html/2207.09460v11/#bib.bibx159)\\]: [\"Unlike previous studies on the Metaverse based on Second Life, the current Metaverse is based on the social value of Generation Z that online and offine selves are not different.\"]\n\t- Large scale `social' & immersive metaverse is suffering poor adoption, failing as it has in the past. It's likely that the market need has been overstated. More advanced and popular (closed) games based solutions do not serve societal or business needs.\n\t- Brett Leonard, writer director of Lawnmower Man talks about the pressing need to get out in front of moral questions in the development of metaverse applications. He stressed that wellbeing will be a crucial underpinning of the technology because of the inherent intimacy of immersion in virtual spaces. He suggests that emotional engagement with storied characters is needed to satisfy the human need for narrative, and that this should be utopian by design to stave off the worst of dystopian emergent characteristics of the technology.\n\t- Adoption and convergence requires low friction communication and economic activity, within groups, at a global scale. Cryptography and distributed software can assist us with globally 'true' persistence of digital data, so we will look to integrate this with our social XR. This focus on persistence, value, and trust means it's most appropriate to focus on business uses as there is more opportunity for value creation which will be important to bootstrap this technology.\n\t- I think that with sufficiently informed guiding constraints in place, and smaller group sizes (ie, not a large scale social metaverse), that there is a path forward.\n\t- This \"visual capitalist\" hosted Gartner article cuts to the core of the issue. As always with such things, the numbers are likely guessed by someone with strong priors, but Gartners aren't terrible at this stuff. The best thing about it is the extent to which it shows that the metaverse doesn't meaningfully exist, even after ALL the hype. Metaverse is basically [[Roblox]] at this time. Hyundai having built an isolated 'metaverse' being a headline item is terribly exposing.\n\t\t- <iframe src=\"https://www.visualcapitalist.com/sp/life-in-the-metaverse/\" style=\"width: 100%; height: 600px\"></iframe>\n- ![Figure 7.2: Elon Musk agrees with this on Twitter. It's notable that Musk is now Twitters' [biggest shareholder](https://twitter.com/paraga/status/1511320953598357505), and has been vocal about web censorship on the platform.](../assets/muskWeb3.png)\n-\n- ## [](https://github.com/flossverse#current-contenders-in-the-metaverse) Current contenders in the Metaverse\n\t- The closest contenders at this time are [[Roblox]] for social and play, VRChat for more serious users, and Nvidia [[Omniverse]] for high end business to business metaverse.\n\t- From a business perspective metaverse is the worst of the remote collaboration tool-kits, and undermines flow, productivity, and interpersonal trust. This hasn't stopped Apple, Microsoft, and Meta's heavy investment in their business technologies and marketing. Metaverse is probably technology for technologies sake at this time.\n\t- With that said Hennig-Thurau says the following in a [LinkedIn post](https://www.linkedin.com/feed/update/urn:li:activity:7020679507141361664/):\n\t\t- [Our research finds that the performance of social interactions in the VR metaverse varies for different outcomes and settings, with productivity and creativity being on par with Zoom (not higher, but also not lower) for the two experimental settings in which we studied these constructs. Thus, as of today, meeting in VR does not overcome all the limitations that we are facing when using Zoom or Teams. But most importantly (to us), we find clear evidence that when people get together in the metaverse via VR, it creates SUBSTANTIALLY higher levels of social presence among group members across ALL FIVE STUDY CONTEXTS, from idea generation to joint movie going. This is the main insight from our study and the stuff we believe future uses of social virtual reality can (and should) build on. We also explain that the effectiveness of VR meetings can be further increased, and also how this can be done (by selecting the most appropriate settings, people, avatars, hardware, environments etc.).] \\[[161]\n- ## [](https://github.com/flossverse#digital-society) Digital Society\n\t- Digital society may be a more tangible and less hyped term to build around, and extends out into the more compelling spatial and augmented reality technologies, web, and digital money and trust.\n\t- Emerging markets, less developed nations, indeed much of the world is excluded from many of the tools that are taken for granted in `Western' digital society. They do not necessarily have the identification, banking rails, or compute power to engage fully. Our focus is on Africa and India.\n- ## [](https://github.com/flossverse#crypto-and-blockchain) Crypto and Blockchain\n\t- Uniting these attempts, with portable (transferable) \"goods\" across digital society possibly requires a global ledger (blockchain), indeed this is the basis of the Web3 interpretation. Crypto has undergone it's own boom and bust cycle on this topic, and is still seeing adoption both inside out outside of the metaverse context. There are other potential options based around simpler PKI infrastructure.\n\t- Representations of dollars and pounds can ride securely on top of such networks as stablecoins, and this is getting easier to integrate, though there are risks. This has the potential to open up global collaborative working practices, inclusive of emerging markets.\n\t- Crypto is a nightmare; rife with scams, poor technology choices, limited life, and incorrect assumptions. The only thing blockchain / crypto can do well is \"money like networks\", which is a cornerstone of human interaction, and the killer application. We believe that Bitcoin is the signal, and crypto is the noise, but even this is a risky proposition.\n- ## [](https://github.com/flossverse#open-digital-society) Open Digital Society\n\t- Industry has noted the risk, and failures of Meta across both metaverse, and digital currency, and have latched onto \"open metaverse\" as a narrative, to de-risk their interest. The current open metaverse is muddy and confused.\n\t- A truth seems to have been missed; that open metaverse should mean open source metaverse. There are some options, but they are under developed. We would like to contribute to this by applying our decades of telecollaboration research.\n\t- The UK seems to be endorsing significant controls and restrictions on internet usage including metaverse applications. This compliance overhead will price small companies out of large scale social experiences. Company walled gardens are less impacted (as per the slack service model), and this is an opportunity if tied to real business use cases.\n- ## [](https://github.com/flossverse#augmented-intelligence) Augmented Intelligence\n\t- AI & machine learning and especially generative art is further blurring these boundaries. A better term for AI/ML is supported creativity and/or augmented intelligence. While current models such as GPT3.5 and LAION based generative systems are already causing a global stir, and large language models are already forcing global debate about general AI.\n\t- Trust, accessibility, governance, and safeguarding, are hard problems, and made more complex by unrecorded social flow in immersive social VR. The challenge is to build a topologically flat, inclusive, permissionless, federated, and open metaverse, with economically empowered ML and AI actors, which can mediate governance issues, transparently, according to well constructed custom schemas, between cryptographically verifiable economic users (human or AI).\n\t- New open source [supported creativity, augmented intelligence] tooling from Stability and Llama potentially removes many of the problems with accessibility, creativity, language barriers, safeguarding, and governance. This is a huge, complex, and fast moving area, but tremendously exciting. Using new image generation ML it may be possible to build new kind of collaborative global networks for creative industries, ideating in simplistic immersive spaces while instantly creating scenes which can be stylised using verbal commands in real-time. This may open up and enfranchise fresh ideas from a wider cultural pool.\n\t- Such teams could be far more ad-hoc by experimenting with the designs outlined in this book. This kind of genuine digital society use case is something sorely lacking in large scale attempts such as Meta Horizons. It need not be complex or large scale, but it must be secure, trusted, and task appropriate. We think we can deliver this and conversations with the industry suggest that there is excent and cautious appetite.\n- ## Toward an open metaverse\n\t- The push toward open standards is being joined (somewhat late) by credible and established bodies [like the IEEE](https://spectrum.ieee.org/laying-foundation-for-extended-reality). It's such a fast moving and under explored set of problems that this movement toward standards will take a long time to even find it's feet. Hopefully it's clear to the reader that this kind of development guides the work here. In the wider \"real-time social VR\" various companies have attempted to build closed ecosystems, for years. These now look more like attempts at digital society, but are closer to isolated metaverses, or more usefully isolated digital ecosystems. This is still happening.\n\t- There's every chance that the Apple Vision Pro will remain closed, as this tends to be their business model. Theo Priestly, CEO at Metanomics [points out](https://www.linkedin.com/feed/update/urn:li:activity:6977366421034967040/) that Chinese Giant Tencent are doing similar, and he cited this image of building a closed but tightly linked suite of businesses into something that looks like a metaverse.\n\t- The levels of investment which are being hung under the metaverse moniker [are mind blowing](https://www.scmp.com/tech/policy/article/3194092/chinas-iphone-production-hub-henan-bets-its-future-metaverse), but that is not what we want to discuss as an end point for this book.\n\t- ![Figure 7.1: [McCormick attempts to guess the Tencent metaverse](https://www.notboring.co/p/tencents-dreams)](../assets/tencent.png)\n\t- For our purposes in this product design the interface between the previous chapter (NFTs) and this metaverse chapter is crucial. Punk6529 is a pseudonymous twitter account and thought leader in the \"crypto\" space. The text below encapsulates much of the reasoning that led to this book and product exploration, and is paraphrased [from this thread](https://twitter.com/punk6529/status/1536046831045685248) for our purposes.\n\t\t- Bit by bit, the visualization layer of the internet will get better until it is unrecognisably better (+/10 years). As the visualization layer of the internet gets better, digital objects will become more useful and more important. Avatars (2D and 3D), art, schoolwork, work work, 3D virtual spaces and hundreds of other things. Not only will the objects themselves become more important, they will lead to different emergent behaviours. We see this already with avatars and mixed eponymous/pseudonymous/anonymous communities. Yes, it is the internet plumbing underneath, but just like social media changed human behaviour on the internet, metaverse type experiences will further change it. NFT Twitter + Discord + various virtual worlds is a form of early metaverse. I feel like I am entering a different world here, not just some websites. The most important question for the health of the internet/metaverse/human society in the 2030s will be decided now. And that question is: \"who stores the definitive ownership records of those digital objects\". There are two answers: a company's database OR a blockchain. If we end up with \"a company's database\" we will end up with all the web dysfunctions, but worse. SMTP is an open protocol that anyone can use so we don't have societal level fights on \"who is allowed to use email\". Short messaging online ended up becoming Twitter. So we end up having the most absurd, surreal discussions on the topic of \"who is allowed to use short-messaging\" being dependant on \"who is the CEO of Twitter\". There is no way this is the correct architecture for our progressively more digital economy.... If this is your first time around here, we are fighting for an open metaverse.\"\n\t- It seems that industry shares much of this opinion regarding an open metaverse. The proposal of a persistent interactive digital universe online is [so] vast that major players recognise that they will not be able to monopolise this space, though Facebook/Meta are clearly attempting to. The [Metaverse Standards Forum](https://metaverse-standards.org/news/press-releases/leading-standards-organizations-and-companies-unite-to-drive-open-metaverse-interoperability/) is clearly an attempt by the other industry players to catch up and then get out ahead of Meta in this regard. It's also possible to view this as just another land grab, but through the vehicle of a standards body. Time will tell. They say:\n\t\t- \"Announced today, The Metaverse Standards Forum brings together leading standards organizations and companies for industry-wide cooperation on interoperability standards needed to build the open metaverse. The Forum will explore where the lack of interoperability is holding back metaverse deployment and how the work of Standards Developing Organizations (SDOs) defining and evolving needed standards may be coordinated and accelerated. Open to any organization at no cost, the Forum will focus on pragmatic, action-based projects such as implementation prototyping, hackathons, plugfests, and open-source tooling to accelerate the testing and adoption of metaverse standards, while also developing consistent terminology and deployment guidelines.\"\n- {{embed ((65cbd03c-03a4-4ca3-8e9d-984692e29412))}}\n- There is a lot of work for the creative and technical industries to do to integrate human narrative creativity this nascent metaverse, and it‚Äôs not even completely\n  clear that this is possible, or even what people want.\n-\n- Collaborative mixed reality---------------------------\n- ### Toward an open metaverse\n- The Openstand principles are a great starting place for what an openmetaverse might mean. [Theyare](https://open-stand.org/about-us/principles/):\n-\n\t- Cooperation: Respectful cooperation between standards organizations, whereby each respects the autonomy, integrity, processes, and intellectual property rules of the others.\n-\n\t- Adherence to Principles: Adherence to the five fundamental principles of standards development:\n-\n\t- Due process. Decisions are made with equity and fairness among        participants. No one party dominates or guides standards        development. Standards processes are transparent and        opportunities exist to appeal decisions. Processes for periodic        standards review and updating are well defined.\n-\n\t- Broad consensus. Processes allow for all views to be considered        and addressed, such that agreement can be found across a range        of interests.\n-\n\t- Transparency. Standards organizations provide advance public        notice of proposed standards development activities, the scope        of work to be undertaken, and conditions for participation.        Easily accessible records of decisions and the materials used in        reaching those decisions are provided. Public comment periods        are provided before final standards approval and adoption.\n-\n\t- Balance. Standards activities are not exclusively dominated by        any particular person, company or interest group.\n-\n\t- Openness. Standards processes are open to all interested and        informed parties.\n-\n\t- Collective Empowerment: Commitment by affirming standards organizations and their participants to collective empowerment by striving for standards that:\n-\n\t- are chosen and defined based on technical merit, as judged by        the contributed expertise of each participant;\n-\n\t- provide global interoperability, scalability, stability, and        resiliency;\n-\n\t- enable global competition;\n-\n\t- serve as building blocks for further innovation;\n-\n\t- contribute to the creation of global communities, benefiting        humanity.\n-\n\t- Availability: Standards specifications are made accessible to all for implementation and deployment. Affirming standards organizations have defined procedures to develop specifications that can be implemented under fair terms. Given market diversity, fair terms may vary from royalty-free to fair, reasonable, and non-discriminatory terms (FRAND).\n-\n\t- Voluntary Adoption: Standards are voluntarily adopted and success is determined by the market.\n- The push toward open standards is being joined (somewhat late) bycredible and established bodies [like theIEEE](https://spectrum.ieee.org/laying-foundation-for-extended-reality).It‚Äôs such a fast moving and under explored set of problems that thismovement toward standards will take a long time to even find it‚Äôs feet.Hopefully it‚Äôs clear to the reader that this kind of development guidesthe work here. In the wider ‚Äúreal-time social VR‚Äù various companies haveattempted to build closed ecosystems, for years. These now look morelike attempts at digital society, but are closer to isolated metaverses,or more usefully isolated digital ecosystems. This is still happening.There‚Äôs every chance that when Apple make their augmented reality playthis year or next they will keep their system closed off as this tendsto be their business model. Theo Priestly, CEO at Metanomics [pointsout](https://www.linkedin.com/feed/update/urn:li:activity:6977366421034967040/)that Chinese Giant Tencent are doing similar, and he cited Figure<a href=\"#fig:tencent\" data-reference-type=\"ref\" data-reference=\"fig:tencent\">7.1</a>;building a closed but tightly linked suite of businesses into somethingthat looks like a metaverse. The levels of investment which are beinghung under the metaverse moniker [are mindblowing](https://www.scmp.com/tech/policy/article/3194092/chinas-iphone-production-hub-henan-bets-its-future-metaverse),but that is not what we want to discuss as an end point for this book.\n  ![]./assets/e63f7f380108db361a71cb5cb9351e68f7c23a21.png\n  <a href=\"https://www.notboring.co/p/tencents-dreams\">McCormick attempts to guess the Tencent metaverse</a>\n- For our purposes in this product design the interface between theprevious chapter (NFTs) and this metaverse chapter is crucial. Punk6529is a pseudonymous twitter account and thought leader in the ‚Äúcrypto‚Äùspace. The text below encapsulates much of the reasoning that led tothis book and product exploration, and is paraphrased [from thisthread](https://twitter.com/punk6529/status/1536046831045685248) for ourpurposes.\n- itBit by bit, the visualization layer of the internet will get betteruntil it is unrecognisably better (+/- 10 years). As the visualizationlayer of the internet gets better, digital objects will become moreuseful and more important. Avatars (2D and 3D), art, schoolwork, workwork, 3D virtual spaces and hundreds of other things. Not only will theobjects themselves become more important, they will lead to differentemergent behaviours. We see this already with avatars and mixedeponymous/pseudonymous/anonymous communities. Yes, it is the internetplumbing underneath, but just like social media changed human behaviouron the internet, metaverse type experiences will further change it. NFTTwitter + Discord + various virtual worlds is a form of early metaverse.I feel like I am entering a different world here, not just somewebsites. The most important question for the health of theinternet/metaverse/human society in the 2030s will be decided now. Andthat question is: \"who stores the definitive ownership records of thosedigital objects\". There are two answers: a company‚Äôs database OR ablockchain. If we end up with \"a company‚Äôs database\" we will end up withall the web dysfunctions, but worse. SMTP is an open protocol thatanyone can use so we don‚Äôt have societal level fights on \"who is allowedto use email\". Short messaging online ended up becoming Twitter. So weend up having the most absurd, surreal discussions on the topic of \"whois allowed to use short-messaging\" being dependant on \"who is the CEO ofTwitter\". There is no way this is the correct architecture for ourprogressively more digital economy.... If this is your first time aroundhere, we are fighting for an open metaverse.‚Äù\n- It seems that industry shares much of this opinion regarding an openmetaverse. The proposal of a persistent interactive digital universeonline is **so** vast that major players recognise that they will not beable to monopolise this space, though Facebook/Meta are clearlyattempting to. The [Metaverse StandardsForum](https://metaverse-standards.org/news/press-releases/leading-standards-organizations-and-companies-unite-to-drive-open-metaverse-interoperability/)is clearly an attempt by the other industry players to catch up and thenget out ahead of Meta in this regard. It‚Äôs also possible to view this asjust another land grab, but through the vehicle of a standards body.Time will tell. They say:\n- it‚ÄúAnnounced today, The Metaverse Standards Forum brings togetherleading standards organizations and companies for industry-widecooperation on interoperability standards needed to build the openmetaverse. The Forum will explore where the lack of interoperability isholding back metaverse deployment and how the work of StandardsDeveloping Organizations (SDOs) defining and evolving needed standardsmay be coordinated and accelerated. Open to any organization at no cost,the Forum will focus on pragmatic, action-based projects such asimplementation prototyping, hackathons, plugfests, and open-sourcetooling to accelerate the testing and adoption of metaverse standards,while also developing consistent terminology and deployment guidelines.‚Äù\n- This looks like it will be a useful project and community for thepurposes outlined in this book, but the technology is young enough (inthat it doesn‚Äôt really exist) for multiple approaches to be trailed.\n- Europe is making metaverse a priority with [The Virtual and AugmentedReality IndustrialCoalition](https://digital-strategy.ec.europa.eu/en/policies/virtual-and-augmented-reality-coalition).President von der Leyen‚Äôs State of the Union [letter of intentsays](https://state-of-the-union.ec.europa.eu/system/files/2022-09/SOTEU_2022_Letter_of_Intent_EN_0.pdf):‚ÄúWe will continue looking at new digital opportunities and trends, suchas the metaverse.‚Äù\n- #### Primitives\n- OpenAI identified the following 5 points about metaverse, in response tothe query ‚ÄúWhat are 5 key points I should know when studying metaverse?‚Äù\n-\n\t- Metaverse is a virtual reality platform that allows users to interact with each other and with digital objects in a virtual space.\n-\n\t- Metaverse is a decentralized platform, meaning that there is no central authority or server that controls the platform.\n-\n\t- Metaverse is an open platform, meaning that anyone can develop applications for the platform.\n-\n\t- Metaverse is a secure platform, meaning that all data and transactions are encrypted and secure.\n-\n\t- Metaverse is a scalable platform, meaning that it can support a large number of users and a large number of transactions.\n- This is an unexpectedly great answer, probably the cleanest we havefound. The [Metaverse Standard Forum](https://metaverse-standards.org/)highlights the following, which reads like the output from a brainstormbetween academia and industry stakeholders.\n-\n\t- collaborative spatial computing\n-\n\t- interactive 3D graphics\n-\n\t- augmented and virtual reality\n-\n\t- photorealistic content authoring\n-\n\t- geospatial systems\n-\n\t- end-user content tooling\n-\n\t- digital twins\n-\n\t- real-time collaboration\n-\n\t- physical simulation\n-\n\t- online economies\n-\n\t- multi-user gaming\n-\n\t- new levels of scale and immersiveness.\n- It‚Äôs not a useless list by any means, but it lacks the kind of productfocus we need for detailed exploration of value and trust transfer.\n- Mystakidis identifies the following:[[mystakidis2022metaverse]]\n-\n\t- Principles\n-\n\t- Interoperable\n-\n\t- Open\n-\n\t- Hardware agnostic\n-\n\t- Network\n-\n\t- Technologies\n-\n\t- Virtual reality\n-\n\t- Augmented reality\n-\n\t- Mixed reality\n-\n\t- Affordances\n-\n\t- Immersive\n-\n\t- Embodiment\n-\n\t- Presence\n-\n\t- Identity construction\n-\n\t- Challenges\n-\n\t- Physical well-being\n-\n\t- Psychology\n-\n\t- Ethics\n-\n\t- Privacy\n- This is quite an academic list. A lot of these words will be explored inthe next section which is more of an academic literature review.\n- Nevelsteen attempted to identify key elements for a ‚Äòvirtual work‚Äô in2018 and these are relevant now, and described rigorously in theappendix of his paper:[[nevelsteen2018virtual]]\n-\n\t- Shared Temporality, meaning that the distributed users of the virtual world share the same frame of time.\n-\n\t- Real time which he defines as ‚Äúnot turn based‚Äù.\n-\n\t- Shared Spatiality, which he says can include an ‚Äòallegory‚Äô of a space, as in text adventures. It seems this might extend to a spoken interface to a mixed reality metaverse.\n-\n\t- ONE Shard is a description of the WLAN network architecture, and conforms to servers in a connected open metaverse.\n-\n\t- Many human agents simply means that more than one person can be represented in the virtual world and corresponds to ‚Äòsocial‚Äô in our description.\n-\n\t- Many Software Agents corresponds to AI actors in our descriptions. Non playing characters would be the gaming equivalent.\n-\n\t- Virtual Interaction pertains to any ability of a user to interact actively with the persistent virtual scene, and is pretty much a given these days.\n-\n\t- Nonpausable isn‚Äôt even a word, but is pretty self explanatory.\n-\n\t- Persistence means that if human participants leave then the data of the virtual world continues. This applies to the scenes, the data representing actions, and objects and actors in the worlds.\n-\n\t- Avatar is interesting as it might seem that having avatar representations of connected human participants is a given. In fact the shared spaces employed by Nvidia for digital engineering do not.\n- Turning to industry; John Riccitiello, CEO of Unity Technologies saysthat metaverse is it\n- ‚ÄúThe next generation of the internet that is:\n-\n\t- always real-time\n-\n\t- mostly 3D\n-\n\t- mostly interactive\n-\n\t- mostly social\n-\n\t- mostly persistent‚Äù\n- Expanding this slightly we will us the following primitives of what wethink are important for a metaverse:\n-\n\t- Fusing of digital and real life\n-\n\t- Social first\n-\n\t- Real time interactive 3d graphics first\n-\n\t- Persistent\n-\n\t- Supports ownership\n-\n\t- Supports user generated content[[ondrejka2004escaping]]\n-\n\t- Open and extensible\n-\n\t- Low friction economic actors and actions\n-\n\t- Trusted / secure\n-\n\t- Convergence of film and games\n-\n\t- Blurring of IP boundaries\n-\n\t- Blurring of narrative flow\n-\n\t- Multimodal and hardware agnostic\n-\n\t- Mobile first experiences\n-\n\t- Safeguarding, and governance\n- There is a **lot** of work for the creative and technical industries todo to integrate human narrative creativity this nascent metaverse, andit‚Äôs not even completely clear that this is possible, or even whatpeople want.\n- ### History\n- The word metaverse was coined by the author Neal Stephenson in his 1992novel Snowcrash. It started popping up soon after in [newsarticles](https://www.newscientist.com/article/mg14819994-000-how-to-build-a-metaverse/)and research papers,[[mclellan1993avatars]] but in the last five years ithas been finding a new life within a silicon valley narrative. Perhapsin response to this Stephenson is now working with a company called[Lamina1](https://www.lamina1.com/) which actually looks a lot like therest of this book, so perhaps we have been on the right track.\n- There were clear precursors to modern social VR, such as [VRML in the1990‚Äôs](https://www.howtogeek.com/778554/remembering-vrml-the-metaverse-of-1995/)which laid much of the groundwork for 3D content over networkedcomputers.\n- It might seem that there would be a clear path from there to now, interms of a metaverse increasingly meaning connected social virtualspaces, but this has not happened. Instead interest in metaverse as aconcept waned, MMORG (described later) filled in the utility, and thenrecently an entirely new definition emerged. Park and Kim surveyeddozens of different historical interpretations of the word, and thegenerational reboot they describe makes it even lessclear.[[park2022metaverse]] The concept of the Metaverse is extremelyplastic at this time (Figure<a href=\"#fig:muskWeb3\" data-reference-type=\"ref\" data-reference=\"fig:muskWeb3\">7.2</a>).\n- It‚Äôs arguable that what will be expanding in this chapter is moreappropriately ‚ÄòCyberspace‚Äô as described by William Gibson inNeuromancer[[gibson2019neuromancer]] it‚ÄúA global domain within theinformation environment consisting of the interdependent network ofinformation systems infrastructures including the Internet,telecommunications networks, computer systems, and embedded processorsand controllers.‚Äù\n- Park and Kim identify the generational inflection point which has led tothe resurgence of the concept of Metaverse:[[park2022metaverse]]it‚ÄúUnlike previous studies on the Metaverse based on Second Life, thecurrent Metaverse is based on the social value of Generation Z thatonline and offine selves are not different.‚Äù\n- Brett Leonard, writer director of Lawnmower Man talks about the pressingneed to get out in front of moral questions in the development ofmetaverse applications. He stressed that wellbeing will be a crucialunderpinning of the technology because of the inherent intimacy ofimmersion in virtual spaces. He suggests that emotional engagement withstoried characters is needed to satisfy the human need for narrative,and that this should be utopian by design to stave off the worst ofdystopian emergent characteristics of the technology.\n- The book will aim to build toward an understanding of metaverse as auseful social mixed reality, that allows low friction communication andeconomic activity, within groups, at a global scale. Cryptography anddistributed software can assist us with globally ‚Äòtrue‚Äô persistence ofdigital data, so we will look to integrate this with our social XR. Thisfocus on persistence, value, and trust means it‚Äôs most appropriate tofocus on business uses as there is more opportunity for value creationwhich will be important to bootstrap this technology.\n- Elsewhere in the book we state that metaverse is the worst of thetele-collaboration tool-kits, and in general we ‚Äòbelieve‚Äô this to betrue at this time. With that said Hennig-Thurau says the following in a[LinkedInpost](https://www.linkedin.com/feed/update/urn:li:activity:7020679507141361664/):itOur research finds that the performance of social interactions in theVR metaverse varies for different outcomes and settings, withproductivity and creativity being on par with Zoom (not higher, but alsonot lower) for the two experimental settings in which we studied theseconstructs. Thus, as of today, meeting in VR does not overcome all thelimitations that we are facing when using Zoom or Teams. But mostimportantly (to us), we find clear evidence that when people gettogether in the metaverse via VR, it creates SUBSTANTIALLY higher levelsof social presence among group members across ALL FIVE STUDY CONTEXTS,from idea generation to joint movie going. This is the main insight fromour study and the stuff we believe future uses of social virtual realitycan (and should) build on. We also explain that the effectiveness of VRmeetings can be further increased, and also how this can be done (byselecting the most appropriate settings, people, avatars, hardware,environments etc.).[[hennig2022social]]\n- We agree that with sufficiently informed guiding constraints in place,and smaller group sizes (ie, not a large scale social metaverse), thatthere is a path forward.\n- This chapter will first attempt to frame the context for telepresence(the academic term for communicating through technology), and thenexplain the increasingly polarised options for metaverse. It‚Äôs useful toprecisely identify the primitives of the product we would like to seehere, so this chapter is far more a review of academic literature in thefield, culminating in a proposed framework.\n  ![]./assets/05c60abfdb5138796c3e168be7f5b9653d60edbb.png\n  Elon Musk agrees with this on Twitter. It‚Äôs notable that Musk is now Twitters‚Äô <a href=\"https://twitter.com/paraga/status/1511320953598357505\">biggest shareholder</a>, and has been vocal about web censorship on the platform.\n- ### Video conferencing, the status quo\n- This section has been adapted and updated for open source release, fromthe authors PhD thesis, with the permission of the University ofSalford.\n- Video-conferencing has become more popular as technology improves, as itgets better integrated with ubiquitous cloud business support suites,and as a function of the global pandemic and changing work patterns.There is obviously increasing demands for real-time communication acrossgreater distances.\n- The full effects of video-conferencing on human communication are stillbeing explored, as seen in the experimental [‚ÄúTogetherMode‚Äù](https://news.microsoft.com/innovation-stories/microsoft-teams-together-mode/)within Microsoft Teams. Video-conferencing is presumed to be a somewhatricher form of communication than email and telephone, but not quite asinformative as face-to-face communication.\n- In this section we look at the influence of eye contact on communicationand how video-conferencing mediates both verbal and non-verbalinteractions. Facilitation of eye contact is a challenge that must beaddressed so that video-conferencing can approach the rich interactionsof face-to-face communication. This is an even bigger problem in theemerging metaverse systems, so it‚Äôs important that we examine thehistory and trajectory.\n- There is a tension emerging for companies who do not necessarily need toemploy remote meeting technology, but also cannot afford to ignore thecompetitive advantages that such systems bring. In an experimentpreformed well before the 2020 global pandemic at CTrip, Bloom et aldescribe how home working led to a 13% performance increase, of whichabout 9% was from working more minutes per shift (fewer breaks andsick-days) and 4% from more calls per minute (attributed to a quieterworking environment).[[Bloom2015]] Home workers also reported improvedwork satisfaction and experienced less turnover, but their promotionrate conditional on performance fell. This speaks to a lack ofmanagement capability with such systemic change. It‚Äôs clearly a complexand still barely understood change within business and management.\n- Due to the success of the experiment, CTrip rolled-out the option towork from home to the whole company, and allowed the experimentalemployees to re-select between the home or office. Interestingly, overhalf of them switched, which led to the gains almost doubling to 22%.This highlights the benefits of learning and selection effects whenadopting modern management practices like working from home.Increasingly this is becoming a choice issue for prospective employees,and an advantage for hiring managers to be able to offer it.\n- More recent research by Barrero, Bloom and Davies found that workingfrom home is likely to be ‚Äústicky.‚Äù[[barrero2021working]] They found:\n-\n\t- better-than-expected WFH experiences,\n-\n\t- new investments in physical and human capital that enable WFH,\n-\n\t- greatly diminished stigma associated with WFH,\n-\n\t- lingering concerns about crowds and contagion risks,\n-\n\t- a pandemic-driven surge in technological innovations that support WFH.\n- More recently Enterprise Collaboration Systems (ECS) provide richdocument management, sharing, and collaboration functionality across anorganisation. The enterprise ECS system may integrate collaborativevideo.[[prakash2020characteristic]] This is for instance the case withMicrosoft Teams / Sharepoint. This integration of ECS should beconsidered when thinking about social VR systems which wish to supportbusiness, value, and trust. It is very much the case that largetechnology providers are attempting to integrate their ‚Äòbusiness backend‚Äô systems into their emerging metaverse systems. Open sourceequivalents are currently lacking.\n- #### Pandemic drives adoption\n- The ongoing global COVID-19 pandemic is [changing how peoplework](https://blog.yelp.com/news/the-future-of-work-is-remote/), towarda new global ‚Äònormal‚Äô. Some ways of working are overdue transformation,and will be naturally disrupted. In the UK at least it seems that theremay be real appetite to shift away from old practises. This upheavalwill inevitably present both challenges and opportunities.\n- Highly technical workforces, especially, can [operate fromanywhere](https://globalworkplaceanalytics.com/telecommuting-statistics).The post pandemic world seems to have stronger national border controls,with a resultant shortage of highly technical staff. This has forced thehand of global business toward [internationally distributedteams](https://www.lifeatspotify.com/being-here/work-from-anywhere).\n- If only a small percentage of companies allow the option of remoteworking, then they gain a structural advantage, enjoying benefits ofreduced travel, lower workplace infection risk across all disease, andglobal agility for the personnel. Building and estate costs willcertainly be reduced. More diversity may be possible. Issues such assexual harassment and bullying may be reduced. With reduced overheadsproduct quality may increase. If customers are happier with theirservices, then over time this ‚Äòpush‚Äô may mean an enormous shift awayfrom centralised working practises toward distributed working.\n- Technologies which support this working style were still in theirinfancy at the beginning of the pandemic. The rush to ‚ÄòZoom‚Äô, apreviously relatively unknown and insecure[[aiken2020zooming]] webmeeting product, shows how naive businesses were in this space.\n- Connection of multiple users is now far better supported, with Zoom and[MircosoftTeams](https://www.microsoft.com/en-us/Investor/earnings/FY-2021-Q1/press-release-webcast)alone supporting hundreds of millions of chats a day. This is a 20xincrease on market leader Skype‚Äôs 2013 figure of [280million](https://www.microsoft.com/en-us/Investor/earnings/FY-2013-Q1/press-release-webcast)connections per month. Such technologies extend traditional telephony toprovide important multi sensory cues. However, these technologiesdemonstrate shortfalls compared to a live face-to-face meeting, which isgenerally agreed to be optimal for human-human interaction.[[Wolff2008]]\n- ## KPMG CEO Survey Highlights\n- [KPMG](https://fortune.com/company/kpmg/) surveyed U.S. CEOs from companies with over $500 million in revenue, finding that only [one-third anticipate a full office return](https://kpmg.com/kpmg-us/content/dam/kpmg/pdf/2024/KPMG-2024-US-CEO-Outlook-Pulse-Survey.pdf) in the next three years.\n- A drastic shift from last year's outlook where 62% believed remote work would end by 2026, as detailed in this [Fortune article](https://fortune.com/2023/10/05/ceo-rto-kpmg-research-return-to-work-full-time-5-days/).\n- The trend towards hybrid work models is growing, with nearly half of the CEOs supporting this flexible arrangement, up from 34% last year. ## Corporate Resistance and Work Model Evolution\n- Deutsche Bank and [Amazon](https://fortune.com/company/amazon-com/) faced significant resistance to their return-to-office mandates, with Deutsche Bank staff voicing displeasure internally, and 30,000 Amazon employees [signing a petition](https://www.businessinsider.com/amazons-hr-boss-rejects-employee-remote-work-petition-2023-3) against the in-office requirements.\n- Research indicates that nearly half of companies enforcing office returns observed increased employee attrition, supporting a shift towards hybrid models as a middle ground solution.\n- ## Future Workforce Dynamics\n- Insights from Amrit Sandhar of &Evolve and Lewis Maleh of Bentley Lewis highlight the ongoing transition to more flexible work arrangements. Maleh notes a significant rise in job postings for remote or hybrid roles, underscoring a broader acceptance of flexibility in work environments as crucial for attracting and retaining talent.\n- While the research community and business are learning how to adaptworking practises to web based telepresence,[[oeppen2020human]] thereremains little technology support for ad-hoc serendipitous meetingsbetween small groups. It‚Äôs possible that Metaverse applications can helpto fill this gap, by gamification of social spaces, but the underdiscussed problems with video conferencing are likely to be even worsein such systems.\n- Chris Herd of ‚ÄúFirstBase‚Äù (who admittedly have a bias) provides somefascinating speculations:\n- it\n- ‚ÄúI‚Äôve spoken to 2,000+ companies with 40M+ employees about remote workin the last 12 months A few predictions of what will happen before 2030:\n-\n\t- Rural Living: World-class people will move to smaller cities, have a lower cost of living & higher quality of life.\n-\n\t- These regions must innovate quickly to attract that wealth. Better schools, faster internet connections are a must.\n-\n\t- Async Work: Offices are instantaneous gratification distraction factories where synchronous work makes it impossible to get stuff done.\n-\n\t- Tools that enable asynchronous work are the most important thing globally remote teams need. A lot of startups will try to tackle this.\n-\n\t- Hobbie Renaissance: Remote working will lead to a rise in people participating in hobbies and activities which link them to people in their local community.\n-\n\t- This will lead to deeper, more meaningful relationships which overcome societal issues of loneliness and isolation.\n-\n\t- Diversity & Inclusion: The most diverse and inclusive teams in history will emerge rapidly Companies who embrace it have a first-mover advantage to attract great talent globally. Companies who don‚Äôt will lose their best people to their biggest competitors.\n-\n\t- Output Focus: Time will be replaced as the main KPI for judging performance by productivity and output.\n-\n\t- Great workers will be the ones who deliver what they promise consistently\n-\n\t- Advancement decisions will be decided by capability rather than who you drink beer with after work.\n-\n\t- Private Equity: The hottest trend of the next decade for private equity will see them purchase companies, make them remote-first The cost saving in real-estate at scale will be eye-watering. The productivity gains will be the final nail in the coffin for the office Working Too Much: Companies worry that the workers won‚Äôt work enough when operating remotely.\n-\n\t- The opposite will be true and become a big problem.\n-\n\t- Remote workers burning out because they work too much will have to be addressed.\n-\n\t- Remote Retreats: Purpose-built destinations that allow for entire companies to fly into a campus for a synchronous week.\n-\n\t- Likely staffed with facilitators and educators who train staff on how to maximize effectiveness.\n-\n\t- Life-Work Balance: The rise of remote will lead to people re-prioritizing what is important to them.\n-\n\t- Organizing your work around your life will be the first noticeable switch. People realizing they are more than their job will lead to deeper purpose in other areas.\n-\n\t- Bullshit Tasks: The need to pad out your 8 hour day will evaporate, replaced by clear tasks and responsibilities.\n-\n\t- Workers will do what needs to be done rather than wasting their trying to look busy with the rest of the office\n- ‚Äù\n- #### Point to Point Video Conferencing\n- O‚ÄôMalley et al. showed that face-to-face and video mediated employedvisual cues for mutual understanding, and that addition of video to theaudio channel aided confidence and mutual understanding. However, videomediated did not provide the clear cues of beingco-located.[[OMalley1996]]\n- Dourish et al. make a case for not using face-to-face as a baseline forcomparison, but rather that analysis of the efficacy of remotetele-collaboration tools should be made in a wider context of connectedmultimedia tools and ‚Äòemergent communicative practises.‚Äô[[Dourish1996]]While this is an interesting viewpoint it does not necessarily map wellto a recreation of the ad-hoc meeting.\n- There is established literature on human sensitivity to eye contact inboth 2D and 3D VC,[[Criminisi2003; @Van_Eijk2010]] with an acceptedminimum of 5-10 degrees before observers can reliably sense they are notbeing looked at.[[Chen2002]] Roberts et al. suggested that at the limitof social gaze distance (¬†4m) the maximum angular separation betweenpeople standing shoulder to shoulder in the real world would be around 4degrees[[Roberts2013]].\n- Sellen found limited impact on turn passing when adding a visual channelto audio between two people when using Hydra, an early system whichprovided multiple video conference displays in an intuitive spatialdistribution[[Sellen1992]]. She did however, find that the design of thevideo system affected the ability to hold multi-partyconversations.[[Sellen1995]]\n- Monk and Gale describe in detail experiments which they used forexamining gaze awareness in communication which is mediated andunmediated by technology. They found that gaze awareness increasedmessage understanding.[[Monk2002]]\n- Both Kuster et al. and Gemmel et al. have successfuly demonstratedsoftware systems which can adjust eye gaze to correct for off axiscapture in real time video systems[[Gemmell2000; @Kuster2012]].\n- Shahid et al. conducted a study on pairs of children playing games withand without video mediation and concluded that the availability ofmutual gaze affordance enriched social presence and fun, while itsabsence dramatically affects the quality of the interaction. They usedthe ‚ÄòNetworked Minds‚Äô, a social presence questionnaire.\n- #### Triadic and Small Group\n- Early enthusiasm in the 1970‚Äôs for video conferencing, as a medium forsmall group interaction quickly turned to disillusionment. It was agreedafter a flurry of initial research that the systems at the time offeredno particular advantage over audio only communication, and atconsiderable cost.[[Williams1977]]\n- Something in the breakdown of normal visual cues seems to impact theability of the technology to support flowing group interaction.Nonetheless, some non-verbal communication is supported in VC withlimited success.\n- Additional screens and cameras can partially overcome the limitation ofno multi-party support (that of addressing a room full of people on asingle screen) by making available more bidirectional channels. Forinstance, every remote user can be a head on a screen with acorresponding camera. The positioning of the screens must thennecessarily match the physical organization of the remote room.\n- Egido provides an early review of the failure of VC for group activity,with the ‚Äúmisrepresentation of the technology as a substitute forface-to-face\" still being valid today.[[Edigo1988]]\n- Commercial systems such as Cisco Telepresence Rooms cluster theircameras above the centre screen of three for meetings using theirtelecollaboration product, while admitting that this only works well forthe central seat of the three screens. They also group multiple peopleon a single screen in what Workhoven et al. dub a ‚Äúnon-isotropic\"configuration.[[Pejsa2016]] They maintain that this is a suitable tradeoff as the focus of the meeting is more generally toward the importantcontributor in the central seat. This does not necessarily follow forless formal meeting paradigms.\n- In small groups, it is more difficult to align non-verbal cues betweenall parties, and at the same time, it is more important because thehand-offs between parties are more numerous and important in groups. Abreakdown in conversational flow in such circumstances is harder tosolve. A perception of the next person to talk must be resolved for allparties and agreed upon to some extent.\n- However, most of the conventional single camera, and expensive multicamera VC systems, suffer a fundamental limitation in that the offsetbetween the camera sight lines and the lines of actual sight introduceincongruities that the brain must compensate for.[[Wolff2008]]\n- #### Other Systems to Support Business\n- There have been many attempts to support group working and rich datasharing between dispersed groups in a business setting. So called ‚Äôsmartspaces‚Äô allow interaction with different displays for differentactivities and add in some ability to communicate with remote or evenmobile collaborators on shared documents,[[Bardram2012]] with additionalchallenges for multi-disciplinary groups who are perhaps less familiarwith one or more of the technology barriers involved.[[Adamczyk2007]]\n- Early systems like clearboard[[Ishii1993]] demonstrated the potential forsmart whiteboards with a webcam component for peer-to-peer collaborativeworking. Indeed it is possible to support this modality with Skype and asmartboard system (and up to deployments such as Accessgrid). Theyremain relatively unpopular however.\n- #### Mona Lisa Type Effects\n- Almost all traditional group video meeting tools suffer from theso-called Mona Lisa effect which describes the phenomenon where theapparent gaze of a portrait or 2 dimensional image always appears tolook at the observer regardless of the observer‚Äôsposition.[[Vishwanath2005; @Anstis1969; @Wollaston1824]] This situationmanifests when the painted or imaged subject is looking into the cameraor at the eyes of the painter.[[Loomis2008; @Fullwood2006]]\n- Single user-to-user systems based around bidirectional video implicitlyalign the user‚Äôs gaze by constraining the camera to roughly the samelocation as the display. When viewed away from this ideal axis, itcreates the feeling of being looked at regardless of where this observeris,[[Moubayed2012; @Vishwanath2005; @Anstis1969; @Wollaston1824]] or the‚Äúcollapsed view effect‚Äù[[Nguyen2005]] where perception of gazetransmitted from a 2 dimensional image or video is dependent on theincidence of originating gaze to the transmission medium.\n- Multiple individuals using one such channel can feel as if they arebeing looked at simultaneously, leading to a breakdown in the normalnon-verbal communication which mediates turn passing.[[Vertegaal2002]]There is research investigating this sensitivity when the gaze ismediated by a technology, finding that ‚Äúdisparity between the opticalaxis of the camera and the looking direction of a looker should be atmost 1.2 degrees in the horizontal direction, and 1.7 degrees invertical direction to support eye contact\".[[Van_Eijk2010; @Bock2008]] Itseems that humans assume that they are being looked at unless they aresure that they are not.[[Chen2002]]\n- To be clear, there are technological solutions to this problem, but it‚Äôsuseful in the context of discussing metaverse to know that this problemexists. It‚Äôs known that there are cognitive dissonances around panes ofvideo conference images, but it seems that the effect is truely limitedto 2D surfaces. A 3D projection surface (a physical model of a human)designed to address this problem completely removed the Mona Lisaeffect.[[Moubayed2012]]\n- Metaverse then perhaps offers the promise of solving this, making morenatural interaction possible, but it‚Äôs clearly a long way fromdelivering on those promises right now. We need to understand what‚Äôsimportant and try to map these into a metaverse product.\n- ### What‚Äôs important for human communication\n- #### Vocal\n- The ubiquitous technology to mediate conversation is, of course, thetelephone. The [2021 Ericsson mobilityreport](https://www.ericsson.com/en/reports-and-papers/mobility-report/reports/november-2021)states that there are around 8 billion mobile subscriptions globally.More people have access to mobile phones than to working toilets[according toUNICEF](https://www.unicef.org/innovation/stories/more-cellphones-toilets).\n- Joupii and Pan designed a system which focused attention on spatiallycorrect high definition audio. They found ‚Äúsignificant improvement overtraditional audio conferencing technology, primarily due to theincreased dynamic range and directionality..[[Jouppi2002]] Aoki et al.also describe an audio only system with support for spatialcues.[[Aoki2003]]\n- In the following sections we will attempt to rigorously identify justwhat is important for our proposed application of business centriccommunication, supportive of trust, and thereby value transfer.\n- In his book ‚ÄòBodily Communication‚Äô[[Argyle1988]] Michael Argyle dividesvocal signals into the following categories:\n- 1.  Verbal\n- 2.  Non-Verbal Vocalisations\n- 1.  Linked to Speech\n- 1.  Prosodic\n- 2.  Synchronising\n- 3.  Speech Disturbances\n- 2.  Independent of Speech\n- 1.  Emotional Noises\n- 2.  Paralinguistic (emotion and interpersonal attitudes)\n- 3.  Personal voice and quality of accent\n- Additional to the semantic content of verbal communication there is arich layer of meaning in pauses, gaps, and overlaps[[Heldner2010]] whichhelp to mediate who is speaking and who is listening in multi-partyconversation. This mediation of turn passing, to facilitate flow, is byno means a given and is highly dependent on context and otherfactors.[[Kleinke1986]] Interruptions are also a major factor in turnpassing.\n- This extra-verbal content[[Ting-Toomey2012]] extends into physical cues,so-called ‚Äònonverbal‚Äô cues, and there are utterances which link theverbal and non-verbal.[[Otsuka2005]] This will be discussed later, but toan extent, it is impossible to discuss verbal communication withoutregard to the implicit support which exists around the words themselves.\n- In the context of all technology-mediated conversation the extra-verbalis easily compromised if technology used to support communication over adistance does not convey the information, or conveys it badly. This canintroduce additional complexity.[[Otsuka2005]]\n- These support structures are pretty much lacking in metaverse XRsystems. The goal then here perhaps is to examine the state-of-the-art,and remove as many of the known barriers as possible. Such a processmight better support trust, which might better support the kind ofeconomic and activity we seek to engineer.\n- When examining just verbal / audio communication technology it can beassumed that the physical non-verbal cues are lost, though notnecessarily unused. In the absence of non-verbal cues it falls to timelyvocal signals to take up the slack when framing and organising the turnpassing. For the synchronising of vocal signals between the parties tobe effective the systemic delays must remain small. System latency, theinherent delays added by the communication technology, can allow slipsor a complete breakdown of ‚Äôflow‚Äô.[[katagiri2007aiduti]] This problem canbe felt in current social VR platforms, though people don‚Äôt necessarilyidentify the cause of the breakdown correctly. In the main they feel tothe users like a bad ‚Äúaudio-only‚Äù teleconference.\n- With that said, the transmission of verbal / audio remains the mostcritical element for interpersonal communication as the most essentialmeaning is encoded semantically. There is a debate about ratios of howmuch information is conveyed through the various humanchannels,[[Loomis2012]] but it is reasonable to infer from its ubiquitythat support for audio is essential for meaningful communication over adistance. We have seen that it must be timely, to prevent a breakdown offraming, and preferably have sufficient fidelity to convey sub-vocalutterances.\n- For social immersive VR for business users, a real-time network such aswebsockets, RTP, or UDP seems essential, much better microphones areimportant, and the system should support both angular spatialisation,and respond to distance between interlocutors.\n- #### Nonverbal\n- We have already seen that verbal exchanges take place in a wider contextof sub vocal and physical cues. In addition, the spatial relationshipbetween the parties, their focus of attention, their gestures andactions, and the wider context of their environment all play a part incommunication.[[Goodwin2000]] These are identified as follows by Gilliesand Slater[[Gillies2005]] in their paper on virtual agents.\n-\n\t- Posture and gesture\n-\n\t- Facial expression\n-\n\t- Gaze\n-\n\t- Proxemics\n-\n\t- Head position and orientation\n-\n\t- Interactional synchrony\n- This is clearly important for our proposed collaborative mixed realityapplication. Below we will examine these six areas by looking across thewider available research.\n- ##### Gaze\n- Of particular importance is judgement of eye gaze which is normallyfast, accurate and automatic, operating at multiple levels of cognitionthrough multiplecues.[[Argyle1988; @Argyle1976; @Argyle1965; @Argyle1976; @Argyle1969; @Kendon1967; @Monk2002]]\n- Gaze in particular aids smooth turn passing[[Hedge1978]][[Novick1996]] andlack of support for eye gaze has been found to decrease the efficiencyof turn passing by 25%.[[Vertegaal2000]]\n- There are clear patterns to eye gaze in groups, with the person talking,or being talked to, probably also being lookedat[[Vertegaal2001]][[Langton2000]]. To facilitate this groups will tend toposition themselves to maximally enable observation of the gaze of theother parties.[[Kendon1967]] This intersects with proxemics which will bediscussed shortly. In general people look most when they are listening,with short glances of 3-10 seconds.[[Argyle1965]] Colburn et al. suggestthat gaze direction and the perception of the gaze of others directlyimpacts social cognition[[Colburn2000]] and this has been supported in afollow up study.[[Macrae2002]]\n- The importance of gaze is clearly so significant in evolutionary termsthat human acuity for eye direction is considered high at ¬†30 secarc[[Symons2004]] with straight binocular gaze judged more accuratelythan straight monocular gaze,[[Kluttz2009]] when using stereo vision.\n- Regarding the judgement of the gaze of others, Symons et al. suggestedthat ‚Äúpeople are remarkably sensitive to shifts in a person‚Äôs eye gaze‚Äùin triadic conversation.[[Symons2004]] This perception of the gaze ofothers operates at a low level and is automatic. Langton et al. citeresearch stating that the gaze of others is ‚Äúable to trigger reflexiveshifts of an observer‚Äôs visual attention‚Äù and further discuss the deepbiological underpinnings of gaze processing.[[Langton2000]]\n- When discussing technology-mediated systems, Vertegaal & Ding suggestedthat understanding the effects of gaze on triadic conversation is‚Äúcrucial for the design of teleconferencing systems and collaborativevirtual environments,‚Äù[[Vertegaal2002]] and further found correlationbetween the amount of gaze, and amount of speech. Vertegaal & Slagtersuggest that ‚Äúgaze function(s) as an indicator of conversationalattention in multiparty conversations.‚Äù[[Vertegaal2001]] It seems like iswe are to have useful markets within social immersive environments thensupport for natural gaze effects should be a priority.\n- Wilson et al. found that subjects can ‚Äúdiscriminate gaze focused onadjacent faces up to \\[3.5m\\].‚Äù[[Wilson2000]] This perhaps gives us atestable benchmark within a metaverse application which is eye gazeenabled. In this regard Schrammel et al. investigated to what extentembodied agents can elicit the same responses in eye gazedetection.[[Schrammel2007]]\n- Vertegaal et al. found that task performace was 46% better when gaze wassynchronised in their telepresence scenario. As they point out, gazesynchonisation (temporal and spatial) is ‚Äòcommendable‚Äô in all such groupsituations, but the precise utility will depend upon thetask.[[Vertegaal2002]]\n- There has been some success in the automatic detection of the focus ofattention of participants in multi partymeetings.[[Stiefelhagen2001; @Stiefelhagen2002]] More recently, eyetracking technologies allow the recording and replaying of accurate eyegaze information[[Steptoe2009]] alongside information about pupildilation toward determination of honesty and socialpresence.[[Steptoe2010]] It seems there are trust and honesty issuesconflated with how collaborants in a virtual space are represented.\n- In summary, gaze awareness does not just mediate verbal communicationbut rather is a complex channel of communication in its own right.Importantly, gaze has a controlling impact on those who are involved inthe communication at any one time, including and excluding even beyondthe current participants. Perhaps the systems we propose in this bookneed to demand eye gaze support, but it is clear that it should berecommended, and that the software selected should support thetechnology integration in principle.\n- ##### Mutual Gaze\n- Aygyle and Cook established early work around gaze and mutual gaze, withtheir seminal book of the same title,[[Argyle1976]] additionallydetailing confounding factors around limitations and inaccuracies inobservance of gaze and how this varies withdistance.[[Argyle1969; @Argyle1988; @Cook1977]]\n- Mutual gaze is considered to be the most sophisticated form of gazeawareness with significant impact on dyadic conversationespecially.[[Cook1977; @Kleinke1986; @Fagel2010]] The effects seem moreprofound than just helping to mediate flow and attention, with mutualeye gaze aiding in memory recall and the formation ofimpressions.[[Bohannon2013]]\n- While reconnection of mutual eye gaze through a technology boundary doesnot seem completely necessary it is potentially important, with impacton subtle elements of one-to-one communication, and thereforediscrimination of eye gaze direction should be bi-directional ifpossible, and if possible have sufficient accuracy to judge direct eyecontact. In their review Bohannon et al. said that the issue ofrejoining eye contact must be addressed in order to fully realise therichness of simulating face-to-face encounters.[[Bohannon2013]]\n- Mutual gaze is a challenging affordance as bi-directional connection ofgaze is not a trivial problem. It‚Äôs perhaps best to view this as at the‚Äòedge‚Äô of our requirements for a metaverse.\n- ##### Mutual Gaze in Telepresence\n- We have seen that transmission of attention can broadly impactcommunication in subtle ways, impacting empathy, trust, cognition, andco-working patterns. Mutual gaze (looking into one another‚Äôs eyes), iscurrently the high water mark for technology-mediated conversation.\n- Many attempts have been made to re-unite mutual eye gaze when usingtele-conferencing systems. In their 2015 review of approachesRegenbrecht and Langlotz found that none of the methods they examinedwere completely ideal.[[Regenbrecht2015]] They found most promise in 2Dand 3D interpolation techniques, which will be discussed in detaillater, but they opined that such systems were very much ongoing researchand lacked sufficient optimisation.\n- A popular approach uses the so called ‚ÄôPeppers Ghost‚Äôphenomenon,[[Steinmeyer2013]] where a semi silvered mirror presents animage to the eye of the observer, but allows a camera to view throughfrom behind the angled mirror surface. The earliest example of this isRosental‚Äôs two way television system in 1947,[[Rosenthal1947]] thoughBuxton et al. ‚ÄòReciprocal Video Tunnel‚Äô from 1992 is more oftencited.[[Buxton1992]] This optical characteristic isn‚Äôt supported byretroreflective projection technology, and besides requires carefulcontrol of light levels either side of the semi-silvered surface.\n- The early GAZE-2 system (which makes use of Pepper‚Äôs ghost) is novel inthat it uses an eye tracker to select the correct camera from severaltrained on the remote user. This ensures that the correct returned gaze(within the ability of the system) is returned to the correct user onthe other end of the network.[[Vertegaal2003]] Mutual gaze capability islater highlighted as an affordance supported or unsupported by keyresearch and commercial systems.\n- ##### Head Orientation\n- Orientation of the head (judged by the breaking of bilateral symmetryand alignment of nose) is a key factor when judging attention.Perception of head orientation can be judged to within a couple ofdegrees.[[Wilson2000]]\n- It has been established that head gaze can be detected all the way outto the extremis of peripheral vision, with accurate eye gaze assessmentonly achievable in central vision.[[Loomis2008]] This is less of use forour metaverses at this time, because user field of view is almost alwaysrestricted in such systems. More usefully, features of illumination canalter the apparent orientation of the head.[[Troje1998]]\n- Head motion over head orientation is a more nuanced propostion and canbe considered a micro gesture.[[Boker2011]] Head tracking systems withinhead mounted displays can certainly detect these tiny movements, butit‚Äôs clear that not all of this resolution is passed into shared virtualsettings through avatars. It would be beneficial to be able to fine tunethis feature within any software selected.\n- It is possible that 3D displays are better suited to perception of headgaze since it is suggested that they are more suitable for ‚Äúshapeunderstanding tasks‚Äù[[St_John2001]]\n- Bailenson, Baell, and Blascovich found that giving avatars rendered headmovements in a shared virtual environment decreased the amount oftalking, possibly as the extra channel of head gaze was opened up. Theyalso reported that subjectively, communication wasenhanced.[[Bailenson2002]]\n- Clearly head orientation is an important indicator of the direction ofattention of members of a group and can be discerned even in peripheralvision. This allows the focus of several parties to be followedsimultaneously and is an important affordance to replicate on anymulti-party communication system.\n- ##### Combined Head and Eye Gaze\n- Rienks et al. found that head orientation alone does not provide areliable cue for identification of the speaker in a multipartysetting.[[Rienks2010]] Stiefelhagen & Zhu found ‚Äúthat head orientationcontributes 68.9% to the overall gaze direction onaverage,‚Äù[[Stiefelhagen2002]] though head and eye gaze seem to be judgedinterdependently.[[Kluttz2009]] Langton noted that head and eye gaze are‚Äúmutually influential in the analysis of socialattention,‚Äù[[Langton2000]] and it is clear that transmission of ‚Äòheadgaze‚Äô by any mediating system, enhances rather than replaces timelydetection of subtle cues. Combined head and eye gaze give the best ofboth worlds and extend the lateral field of view in which attention canbe reliably conveyed to others.[[Loomis2008]]\n- ##### Other Upper Body: Overview\n- While it is well evidenced that there are advantages to accurateconnection of the gaze between conversationalpartners,[[Argyle1969; @Kleinke1986]] there is also a body of evidencethat physical communication channels extend beyond theface[[Kleinke1986; @Nguyen2009]] and include both micro (shrugs, handsand arms), and macro movement of the upper body.[[Ekman1993]]Goldin-Meadow suggests that gesturing aids conversational flow byresolving mismatches and aiding cognition.[[Goldin-Meadow1999]]\n- In their technology-mediated experiment which compared face to upperbody and face on a flat screen, Nguyen and Canny found that ‚Äúupper-bodyframing improves empathy measures and gives results not significantlydifferent from face-to-face under several empathymeasures.‚Äù[[Nguyen2009]]\n- The upper body can be broken up as follows:\n- bfFacial  Much emotional context can be described by facial expression (display)alone,[[Ekman1993; @Chovil1991]] with smooth transition betweenexpressions seemingly important.[[schiano2004]] This suggests thatmediating technologies should support high temporal resolution, or atleast that there is a minimum resolution between which transitionsbetween expressions become too ‚Äôcategorical‚Äô. Some aspects ofconversational flow appear to be mediated in part by facialexpression.[[ohba1998]] There are gender differences in the perception offacial affect.[[Hofmann2006]]\n- bfGesturing  (such as pointing at objects) paves the way for more complex channels ofhuman communication and is a basic and ubiquitous channel.[[Iverson2005]]Conversational hand gestures provide a powerful additional augmentationto verbal content.[[Krauss1996]]\n- bfPosture  Some emotions can be conveyed through upper body configurations alone.Argyle details some of these[[Argyle1988]] and makes reference to theposture of the body and the arrangement of the arms (i.e. folded acrossthe chest). These are clearly important cues. Kleinsmith andBianchi-Berthouze assert that \"some affective expressions may be bettercommunicated by the body than the face\".[[Kleinsmith2013]]\n- bfBody Torque  In multi-party conversation, body torque, that is the rotation of thetrunk from front facing, can convey aspects of attention andfocus.[[Schegloff1998]]\n- In summary, visual cues which manifest on the upper body and face canconvey meaning, mediate conversation, direct attention, and augmentverbal utterances.\n- ##### Effect of Shared Objects on Gaze\n- Ou et al. detail shared task eye gaze behaviour ‚Äúin which helpers seekvisual evidence for workers‚Äô understanding when they lack confidence ofthat understanding, either from a shared, or commonvocabulary.‚Äù[[Ou2005]]\n- Murray et al. found that in virtual environments, eye gaze is crucialfor discerning what a subject is looking at.[[Murray2009]] This work isshown in Figure<a href=\"#fig:murrayeyegaze\" data-reference-type=\"ref\" data-reference=\"fig:murrayeyegaze\">7.3</a>.\n- It is established that conversation around a shared object or task,especially a complex one, mitigates gaze between parties[[Argyle1976]]and this suggests that in some situations around shared tasks inmetaverses it may be appropriate to reduce fidelity of representation ofthe avatars.\n- <span class=\"image\">Eye tracked eye gaze awareness in VR. Murray et al.used immersive and semi immersive systems alongside eye trackers toexamine the ability of two avatars to detect the gaze awareness of asimilarly immersed collaborator.</span>\n- ##### Tabletop and Shared Task\n- In early telepresence research Buxton and William argued throughexamples that ‚Äúeffective telepresence depends on quality sharing of bothperson and task space.[[Buxton1992]]\n- In their triadic shared virtual workspace Tang et al. found difficultyin reading shared text using a ‚Äòround the table‚Äô configuration, a markedpreference for working collaboratively on the same side of the table.They also found additional confusion as to the identity of remoteparticipants.[[Tang2010]] Tse et al. found that pairs can work well overa shared digital tabletop, successfully overcoming a single userinterface to interleave tasks.[[Tse2007]]\n- Tang et al. demonstrate that collaborators engage and disengage around agroup activity through several distinct, recognizable mechanisms withunique characteristics.[[Tang2006]] They state that tabletop interfacesshould offer a variety of tools to facilitate this fluidity.\n- Camblend is a shared workspace with panoramic high resolution video. Itmaintains some spatial cues between locations by keeping a shared objectin the video feeds.[[Norris2013; @Norris2012]] Participants successfullyresolved co-orientation within the system.\n- The t-room system implemented by Luff et al. surrounds co-locatedparticipants standing at a shared digital table with life sized body andhead video representations of remote collaborators[[Luff2011]] but foundthat there were incongruities in the spatial and temporal matchingbetween the collaborators which broke the flow of conversation.Tuddenham et al. found that co-located collaborators naturally devolved‚Äôterritory‚Äô of working when sharing a task space, and that this did nothappen the same way with a tele-present collaborator.[[Tuddenham2009]]Instead remote collaboration adapted to use a patchwork of ownership ofa shared task. It seems obvious to say that task ownership is a functionof working space, but it is interesting that the research found nomeasurable difference in performance when the patchwork coping strategywas employed.\n- The nature of a shared collaborative task and/or interface directlyimpacts the style of interaction between collaborators. This will have abearing on the choice of task forexperimentation.[[Jamil2011; @Jetter2011]]\n- ### Psychology of Technology-Mediated Interaction\n- #### Proxemics\n- Proxemics is the formal study of the regions of interpersonal spacebegun in the late 50‚Äôs by Hall and Sommers and building toward TheHidden Dimension,[[Hall1969]] which details bands of space (Figure<a href=\"#fig:proxemics\" data-reference-type=\"ref\" data-reference=\"fig:proxemics\">7.4</a>)that are implicitly and instinctively created by humans and which have adirect bearing on communication.\n  ![]./assets/93e5e31635612a45cbb73bc9a52a19c538eeea0c.png\n  Bands of social space around a person Image CC0 <a href=\"https://en.wikipedia.org/wiki/Proxemics\">from wikipedia</a>.\n- Distance between conversational partners, and affiliation, also have abearing on the level of eye contact[[Argyle1965]] with a natural distanceequilibrium being established and developed throughout, through both eyecontact and a variety of subtle factors. Argyle & Ingham provide levelsof expected gaze and mutual gaze against distance.[[Argyle1969]] Theseboundaries are altered by ethnicity[[Watson1966; @Argyle1988]] andsomewhat by gender,[[Bruno2013]] and age.[[Slessor2008; @Hofmann2006]]\n- Even with significant abstraction by communication systems (such asSecondLife) social norms around personal spacepersist.[[Yee2007; @Bailenson2001; @Bailenson2003]] Bailenson &Blascovich found that even in Immersive Collaborative VirtualEnvironments (ICVE‚Äôs) ‚Äúparticipants respected personal space of thehumanoid representation‚Äù[[Bailenson2001]] implying that this is a deeplyheld ‚Äôlow-level‚Äô psychophysical reaction.[[Blascovich2002]] The degree towhich this applies to non-humanoid avatars seems under explored.\n- Maeda et al.[[Maeda2004]] found that seating position impacts the levelof engagement in teleconferencing. Taken together with the potential forreconfiguration within the group as well as perhaps signalling for theattention of participants outside of the confines of the group in anopen business metaverse setting.\n- When considering the attention of engaging with people outside theconfines of a meeting Hager et al. found that gross expressions can beresolved by humans over long distances.[[Hager1979; @Argyle1988]] Itseems that social interaction begins around 7.5m in the so-called‚Äòpublic space.‚Äô[[Hall1969]] Recreating this affordance in a metaversewould be a function of the display resolution, and seems another‚Äòstretch goal‚Äô rather than a core requirement.\n- #### Attention\n- The study of attention is a discrete branch of psychology. It is thestudy of cognitive selection toward a subjective or objective sub focus,to the relative exclusion of other stimulae. It has been defined as ‚Äúarange of neural operations that selectively enhance processing ofinformation.‚Äù[[Carlston2013]] In the context of interpersonalcommunication it can be refined to apply to selectively favouring aconversational agent or object or task above other stimuli in thecontextual frame.\n- Humans can readily determine the focus of attention of others in theirspace[[Stiefelhagen2001]] and preservation of the spatial cues whichsupport this are important for technology-mediatedcommunication[[Sellen1992]][[Stiefelhagen2002]].\n- The interplay between conversational partners, especially the reciprocalperception of attention, is dubbed the perceptualcrossing.[[Deckers2013; @Gibson1963]]\n- This is a complex field of study with gender, age, and ethnicity allimpacting the behaviour of interpersonalattention.[[Bente1998; @Slessor2008; @Argyle1988; @Hofmann2006; @Pan2008]]Vertegaal has done a great deal of work on awareness and attention intechnology-mediated situations and the work of his group is citedthroughout this chapter.[[Vertegaal1997]] As an example it is still sucha challenge to ‚Äúget‚Äù attention through mediated channels ofcommunication, that some research[[Fels2000; @Sellen1992]] and manycommercial systems such as ‚Äòblackboard collaborate‚Äô, Zoom, and Teams usetell tale signals (such as a microphone icon) to indicate when aparticipant is actively contributing. Some are automatic, but many arestill manual, requiring that a user effectively hold up a virtual handto signal their wish to communicate.\n- Langton et al. cite research stating that the gaze of others is ‚Äúable totrigger reflexive shifts of an observer‚Äôs visual attention‚Äù.\n- Regarding the attention of others, Fagal et el demonstrated that eyevisibility impacts collaborative task performance when considering ashared task.[[Fagel2010]] Novick et al. performed analysis on taskhand-off gaze patterns which is useful for extension into shared taskproduct design.[[Novick1996]]\n- #### Behaviour\n- Hedge et al. suggested that gaze interactions between strangers andfriends may be different which could have an impact on the kinds ofinteractions a metaverse might best support.[[Hedge1978]] Voida et al.elaborate that prior relationships can cause ‚Äúinternal fault lines‚Äù ingroup working.[[Voida2012]] When new relationships are formed the‚Äúprimary concern is one of uncertainty reduction or increasingpredictability about the behaviour of both themselves and others in theinteraction.‚Äù[[Berger1975]] This concept of smoothness in theconversation is a recurring theme, with better engineered systemsintroducing less extraneous artefacts into the communication, and sodisturbing the flow less. Immersive metaverse are rife with artefacts.\n- In a similar vein the actor-observer effect describes the mismatchbetween expectations which can creep into conversation. Conversationsmediated by technology can be especially prone to diverging perceptionsof the causes of behaviour.[[Jones1971]] Basically this meansmisunderstandings happen, and are harder to resolve with more mediatingtechnology.\n- Interacting subjects progress conversation through so-called‚Äòperception-action‚Äô loops which are open to predictive modelling throughdiscrete hidden Markov models.[[Mihoub2015]] This might allow product OKRtesting of the effectiveness of engineered systems.[[doerr2018measure]]\n- It may be that the perception-behaviour link where unconscious mirroringof posture bolsters empathy between conversational partners, especiallywhen working collaboratively,[[Chartrand1999]] and the extent to whichposture is represented through a communication medium may be important.\n- Landsberger posited the Hawthorne effect.[[Parsons1974]] Put simply thisis a short term increase in productivity that may occur as a result ofbeing watched or appreciated. The impression of being watched changesgaze patterns during experimentation, with even implied observationthrough an eye tracker modifying behaviour.[[Risko2011]]\n- There are also some fascinating findings around the neural correlates ofgratitude, which turn out not to be linked to gratitude felt by aparticipant, but rather the observation of gratitude received within asocial context.[[fox2015neural]] These findings have potentially usefulimplications for the behaviours of AI actors and avatars within animmersive social scene.\n- There is much historic work describing ‚Äúthe anatomy ofcooperation\",[[Kollock1998]] and this might better inform how educationalor instructional tasks are built in metaverse applications.\n- Cuddihy and Walters defined an early model for assessing desktopinteraction mechanisms for social virtual environments.[[Cuddihy2000]]\n- ##### Perception Of Honesty\n- Hancock et al. state that we are most likely to lie, and to be lied to,on the telephone.[[Hancock2004]] Technology used for communicationimpacts interpersonal honesty. It seems that at some level humans knowthis; lack of eye contact leads to feelings of deception, impactingtrust.[[Holm2010]] This has a major impact on immersive social XR, whichoften does not support mutual gaze. Trust is crucial for businessinteractions.\n- Further there are universal expressions, micro-expressions, and blinkrate which can betray hidden emotions,[[Porter2008]] though the effectsare subtle and there is a general lack of awareness by humans of theirabilities in this regard.[[Holm2010]] Absence of support for suchinstinctive cues inhibits trust.[[Roberts2015]] Support for these rapidand transient facial features demands high resolution reproduction inboth resolution and time domains. There is detectable difference in aparticipant‚Äôs ability to detect deception when between video conferencemediated communication and that mediated by avatars.[[Steptoe2010]]Systems should aim for maximally faithful reproduction.\n- #### Presence, Co-presence, and Social Presence\n- Presence is a heavily cited historic indicator of engagement in virtualreality, though the precise meaning has been interpreted differently bydifferent specialisms.[[Beck2011; @Schuemie2001]] It is generally agreedto be the ‚Äôsense of being‚Äô in a virtual environment.[[Slater1999]] Slaterextends this to include the ‚Äúextent to which the VE becomes dominant\".\n- Beck et al. reviewed 108 articles and synthesised an ontology ofpresence[[Beck2011]] which at its simplest is as follows:\n- 1.  Sentient presence\n- 1.  Physical interaction\n- 2.  Mental interaction\n- 2.  Non-sentient\n- 1.  Physical immersion\n- 2.  Mental immersion = psychological state\n- When presence is applied to interaction it may be split intoTelepresence, and Co/Social presence.[[Heeter1992; @Biocca1997]]Co-presence and/or social presence is the sense of ‚Äúbeing there withanother\", and describes the automatic responses to complex socialcues.[[Fulk1987; @Haythornthwaite1995]] Social presence (and co-presence)refers in this research context to social presence which is mediated bytechnology (even extending to text based chat[[Gunawardena1997]]), andhas its foundations in psychological mechanisms which engender mutualismin the ‚Äòreal‚Äô. This is analysed in depth by Nowak.[[Nowak2001]] Anexamination of telepresence, co-presence and social presence necessarilyrevisits some of the knowledge already elaborated.\n- The boundaries between the three are blurred in research withconflicting results presented.[[Bulu2012]] Biocca et al. attempted toenumerate the different levels and interpretations surrounding thesevague words,[[Biocca2003]] and to distill them into a more robust theorywhich better lends itself to measurement. They suggest a solidunderstanding of the surrounding psychological requirements which needsupport in a mediated setting, and then a scope that is detailed andlimited to the mediated situation.\n- Since ‚Äòsocial presence‚Äô has been subject to varieddefinitions[[Biocca2003]] it is useful here to consider a singledefinition from the literature which defines it as ‚Äúthe ability ofparticipants in the community of inquiry to project their personalcharacteristics into the community, thereby presenting themselves to theother participants as real people..‚Äù[[Garrison1999; @Beck2011]] Similarlyto specifically define co-presence for this research it is taken to bethe degree to which participants in a virtual environment are‚Äúaccesible, available, and subject to one another\".[[Biocca2003]]\n- Social presence has received much attention and there are establishedquestionnaires used in the field for measurement of the levels ofperceived social presence yet the definitions here also remain broad,with some confusion about what is being measured.[[Biocca2003]]\n- Telepresence meanwhile is interaction with a different (usually remote)environment which may or may not be virtual, and may or may not containa separate social/co-presence component.\n- Even in simple videoconferencing Bondareva and Bouwhuis stated (as partof an experimental design) that the following determinants are importantto create social presence.[[Bondareva2004; @Jouppi2002]]\n- 1.  Direct eye contact is preserved\n- 2.  Wide visual field\n- 3.  Both remote participants appear life size\n- 4.  Possibility for participants to see the upper body of the interlocutor\n- 5.  High quality image and correct colour reproduction\n- 6.  Audio with high S/N ratio\n- 7.  Directional sound field\n- 8.  Minimization of the video and audio signal asynchrony\n- 9.  Availability of a shared working space.\n- Bondareva et al. went on to describe a person-to-person telepresencesystem with a semi-silvered mirror to reconnect eye gaze, which theyclaimed increased social presence indicators. Interestingly they chose achecklist of interpersonal interactions which they used againstrecordings of conversations through the system.[[Bondareva2004]]\n- The idea of social presence as an indicator of the efficacy of thesystem, suggests the use of social presence questionnaires in theevaluation of the system.[[Biocca2003]] Subjective questionnaires arehowever troublesome in measuring effectiveness of virtual agents andembodiments, with even nonsensical questions producing seemingly validresults.[[Slater2004]] Usoh et al. found that ‚Äôthe real‚Äô produced onlymarginally higher presence results than the virtual.[[Usoh2000]] It wouldbe difficult to test products this way.\n- Nowak states that ‚ÄúA satisfactory level of co-presence with another mindcan be achieved with conscious awareness that the interaction ismediated\" and asserts that while the mediation may influence the degreeof co-presence it is not a prohibiting factor.[[Nowak2001]]\n- Baren and IJsselsteijn[[Van_Baren2004; @Harms2004]] list 20 usefulpresence questionnaires in 2004 of which ‚ÄúNetworked Minds\" seemed mostappropriate for the research. Hauber et al. employed the ‚ÄúNetworkedMinds\" Social Presence questionnaire experimentally and found that whilethe measure could successfully discriminate between triadic conversationthat is mediated or unmediated by technology, it could not find adifference between 2D and 3D mediatedinterfaces.[[Hauber2005; @Gunawardena1997]]\n- In summary, social presence and co-presence are important historicmeasures of the efficacy of a communication system. Use of the term inliterature peaked between 1999 and 2006 according to Google‚Äôs ngramviewer and has been slowly falling off since. The questionnairemethodology has been challenged in recent research and while moreobjective measurement may be appropriate, the networked minds questionsseem to be able to differentiate real from virtualinteractions.[[Harms2004]]\n- ### Other Systems to Support Business\n- There have been many attempts to support group working and rich datasharing between dispersed groups in a business setting. So called ‚Äôsmartspaces‚Äô allow interaction with different displays for differentactivities and add in some ability to communicate with remote or evenmobile collaborators on shared documents,[[Bardram2012]] with additionalchallenges for multi-disciplinary groups who are perhaps less familiarwith one or more of the technology barriers involved.[[Adamczyk2007]]\n- Early systems like clearboard[[Ishii1993]] demonstrated the potential forsmart whiteboards with a webcam component for peer to peer collaborativeworking. Indeed it is possible to support this modality with Skype and asmartboard system (and up to deployments such as Accessgrid). Theyremain relatively unpopular however.\n- Displays need not be limited to 2 dimensional screens and can beenhanced in various ways.\n- Stereoscopy allows an illusion of depth to be added to a 2D image byexploiting the stereo depth processing characteristics of the humanvision system. This technical approach is not perfect as it does notfully recreate the convergence and focus expected by the eyes and brain.\n- There are multiple approaches to separating the left and right eyeimages, these primarily being active (where a signal selectively blanksthe input to left then right eyes in synchronicity with the display),passive, where either selective spectrum or selective polarisation oflight allow different portions of a display access to different eyes, orphysical arrangements which present different displays (or slices oflight as in lenticular systems) to different eyes.\n- These barrier stereoscopy / lenticular displays use vertical lightbarriers built into the display to create multiple discrete channels ofdisplay which are accessed by moving horizontally with respect to thedisplay. In this way it is possible to generate either a left/right eyeimage pair for ‚Äôautostereoscopic‚Äô viewing, or with the addition of headtracking and small motors. With these techniques multiple viewpoint oran adaptive realtime viewpoint update can be presented without theglasses required for active or passive stereoscopic systems.\n- #### Spatially Faithful Group\n- Hauber et al. combined videoconferencing, tabletop, and social presenceanalysis and tested the addition of 3D. They found a nuanced responsewhen comparing 2D and 3D approaches to spatiality: 3D showed improvedpresence over 2D (chiefly through gaze support), while 2D demonstratedimproved task performance because of task focus.[[Hauber2006]]\n- I3DVC reconstructs participants from multiple cameras and places themisotropically (spatially faithful).[[Kauff2002; @Kauff2002a]] The systemuses a large projection screen, a custom table, and carefully definedseating positions. They discussed an ‚Äúextended perception space\" whichused identical equipment in the remote spaces in a tightly coupledcollaborative ‚Äòbooth‚Äô. It employed head tracking and multi camerareconstruction alongside large screens built into the booth. This systemexemplified the physical restrictions which are required to limit theproblems of looking into another space through the screen. Fuchs et al.demonstrated a similar system over a wide area network but achieved onlylimited resolution and frame rate with the technology of theday.[[Fuchs2002]]\n- University of Southern California used a technically demanding real-timeset-up with 3D face scanning and an autostereoscopic 3D display togenerate multiple ‚Äòface tracked‚Äô viewpoints.[[Jones2009]] This had thedisadvantage of displaying a disembodied head.\n- MAJIC is an early comparable system to support small groups with lifesize spatially correct video, but without multiple viewpoints onto theremote collaborators it was a one to ‚Äôsome‚Äô system rather than ‚Äôsome‚Äô toone. Additionally users were rooted to definedlocations.[[Ichikawa1995; @Okada1994]]\n- There seems to be less interest recently in large display screens forspatially correct viewpoints between groups. The hardware is technicallydemanding and there may have been sufficient research done to limitinvestment in research questions. This doesn‚Äôt mean that there is nofuture for metaverse applications. Imagine one of the new XR studiowalls such as that used to film the Mandalorian. With application oftelepresence research it would be possible to bring external metaverseparticipants into the ‚Äòbackstage‚Äô virtual scene. These avatars would beable to explore the scene invisible to the actors, but could be givenaccess to visual feeds from the stage side. This is a hybridvirtual/real metaverse with a well researched and understood boundaryinterface. It would be possible to give different access privileges todifferent levels of paying ‚Äòfilm studio tourist‚Äô or investor, with VIPsperhaps commanding a view onto the live filming. At the nadir of this itmay be possible to bring producers and directors directly into thevirtual studio as avatars on the screen boundary, with a spatiallyfaithful view onto the set. For the purposes of this book it‚Äôs alsoworth noting that NFTs of the experience and corresponding virtualobjects from the scene could be monetised and sold within the metaverse.\n- ##### Multiview\n- In order to reconnect directional cues of all kinds it is necessary foreach party in the group to have a spatially correct view of the remoteuser which is particular for them. This requires a multi-view display,which has applications beyond telepresence but are used extensively inresearch which attempts to address these issues.\n- Nguyen and Canny demonstrated the ‚ÄòMultiview‚Äô system.[[Nguyen2005]]Multiview is a spatially segmented system, that is, it presentsdifferent views to people standing in different locationssimultaneously. They found similar task performance in trust tasks toface-to-face meetings, while a similar approach without spatialsegmentation was seen to negatively impact performance.\n- In addition to spatial segmentation of viewpoints[[Gotsch2018]] it ispossible to isolate viewpoints in the time domain. Different trackedusers can be presented with their individual view of a virtual scene fora few milliseconds per eye, before another viewpoint is shown to anotheruser. Up to six such viewpoints are supported in the c1x6system[[Kulik2011]] Similarly MM+Space offered 4 Degree-Of-FreedomKinetic Display to recreate Multiparty Conversation Spaces[[Otsuka2013]]\n- #### Holography and Volumetric\n- Blanche et al. have done a great deal of research into holographic andvolumetric displays using lasers, rotating surfaces, and light fieldtechnology.[[Blanche2010; @Tay2008]] They are actively seeking to usetheir technologies for telepresence and their work is very interesting.\n- Similarly Jones et al. ‚ÄúHeadSPIN\" is a one-to-many 3D videoteleconferencing system[[Jones2009]] which uses a rotating display torender the holographic head of a remote party. They achievetransmissible and usable framerate using structured light scanning of aremote collaborator as they view a 2D screen which they say shows aspatially correct view of the onlooking parties.\n- Eldes et al. used a rotating display to present multi-viewautostereoscopic projected images to users.[[Eldes2013]]\n- Seelinder is an interesting system which uses parallax barriers torender a head which an onlooking viewer can walk around. The system uses360 high resolution still images which means a new spatially segmentedview of the head every 1 degreesof arc. They claim the system is capableof playback of video and this head in a jar multi-view system clearlyhas merit but is comparatively small, and as yet untested fortelepresence.[[Yendo2010]]\n- These systems do not satisfy the requirement to render upper body forthe viewers and are not situated (as described soon).\n- There‚Äôs a future possible where real-time scanned avatar representationin persistent shared metaverse environments will be able to supportbusiness, but the camera rigs which currently generate such models aretoo bulky and involved for a good costs benefit analysis. It is morelikely that recent advances in LIDAR phone scanning show the way. Theallow realistic avatars to be quickly created for animation withinmetaverse scenes.[[authenticVolume2022]]\n- ##### Project Skyline\n- Project Starline, is a next-generation video conferencing technologythat aims to create a sense of presence, making you feel like you‚Äôresitting across the table from someone. It uses advanced hardware andsoftware to achieve this.\n-\n\t- The newer Starline booth is a refined version of earlier models and looks like a large 65-inch display on a stand. It contains color cameras, depth sensors, microphones, and speakers. Additionally, there are lights on the back of the display that serve as a key light for the person on the call. These lights are mounted around the person and used to create a depth map of the subject and the room they‚Äôre in.\n-\n\t- The display creates an immersive 3D depth effect. It uses a barrier lenticular light field display that shows a different image to your left eye and to your right eye. This effect lets you compute depth on the fly while doing all the head tracking in real time. The display technology in Project Starline is significantly smoother and more realistic than what you would experience with traditional 3D movies.\n-\n\t- The computing side of Project Starline is responsible for rendering the people using the system into realistic 3D models in real-time. It uses AI and depth information gathered by the cameras to map the exact shape, depth, texture, and lighting of the person. The result is an ultra-realistic 3D representation of the person on the other end of the call.\n-\n\t- The system features spatial audio such that the perceived audio changes based on where you are leaning or moving, creating an even more immersive and realistic experience.\n- At this point, Google has been working with several companies who areusing these booths for meetings, and it‚Äôs hoped that as the technologybecomes cheaper and more refined, it they assert that it couldrevolutionize the way we communicate, though the cost of the system and‚Äòsingle user to single user‚Äô restriction is likely to be a blocker tocrucial business adoption.\n- #### Simulated Humans\n- ##### Uncanniness\n- When employing simulation representations of humans it may be the casethat there is an element of weirdness to some of these systems,especially those that currently represent a head without a body. Morihas demonstrated The Uncanny Valley[[Mori1970]] effect in which imperfectrepresentations of humans elicit revulsion in certain observers. Thisprovides a toolkit for inspecting potentially ‚Äòweird‚Äô representations,especially if they are ‚Äòeerie‚Äô and is testable through Mori‚Äôs GODSPEEDquestionnaire.\n- With an improved analysis of the shape of the likeability curveestimated later showing a more nuanced response from respondents whereanthropomorphism of characters demonstrated increased likeability evenagainst a human baseline.[[Bartneck2007; @Bartneck2009]]\n- A mismatch in the human realism of face and voice also produces anUncanny Valley response.[[Mitchell2011]]\n- However, there is a possibility that Mori‚Äôs hypothesis may be toosimplistic for practical everyday use in CG and robotics research sinceanthropomorphism can be ascribed to many and interdependent featuressuch as movement and content of interaction.[[Bartneck2009]]\n- Bartneck et al. also performed tests which suggest that the originalUncanny Valley assertions may be incorrect, and that it may beinappropriate to map human responses to human simulacrum to such asimplistic scale. They suggest that the measure has been a convenient‚Äòescape route‚Äô for researchers.[[Bartneck2009]] Their suggestion that themeasure should not hold back the development of more realistic robotsholds less bearing for the main thrust of this telepresence researchwhich seeks to capture issues with imperfect video representation ratherthan test the validity of an approximation.\n- Interestingly Ho et al. performed tests on a variety of facialrepresentations using images. They found that facial performance is a‚Äòdouble edged sword‚Äô with realism being important to roboticrepresentations, but there also being a significant Uncanny Valleyeffect around ‚Äòeerie, creepy, and strange‚Äô which can be avoided by gooddesign.[[Ho2008]]\n- More humanlike representations exhibiting higher realism produce morepositive social interactions when subjective measures are used[[Yee2007]]but not when objective measures are used. This suggests thatquestionnaires may be more important when assessing potentialuncanniness.\n- A far more objective method would be to measure user responses tohumans, robots, and representations with functional near-infraredspectroscopy and while this has been attempted it is early exploratoryresearch,[[Strait2014]] an emotional response to ‚Äòeerie‚Äô was discovered.\n- ##### Embodiment through robots\n- Virtuality human representation extends beyond simple displays intorobotic embodiments (which need not be humanoid[[Marti2005]]), shapemapped projection dubbed ‚Äúshader lamps\", and hybridisations of the two.\n- Robots which carry a videoconference style screen showing a head can addmobility and this extends the availablecues.[[Adalgeirsson2010; @Lee2011; @Tsui2011; @Paulos1998; @Kristoffersson2013]]Interestingly Desai and Uhlik maintain that the overriding modalityshould be high quality audio.[[Desai2011]]\n- Tsui et al. asked 96 participants to rate how personal and interactivethey found interfaces to be. Interestingly they rated videoconferencingas both more personal and more interactive than telepresence robots,suggesting that there is a problem with the overall representation orembodiment.[[Tsui2012]]\n- Kristoffersson et al. applied the Networked Minds questionnaire to judgepresence of a telepresence robot for participants with little or noexperience of videoconferencing. Their results were encouraging, thoughthey identified that the acuity of the audio channel needingimprovement.[[Kristoffersson2011]]\n- There are a very few lifelike robots which can be used for telepresence,and even these are judged to be uncanny.[[Sakamoto2007]] This is only anissue for a human likeness since anthropomorphic proxies such as robotsand toys perform well.[[Mori1970]]\n- ##### Physical & Hybrid embodiment\n- Embodiment through hybridisation of real-time video and physicalanimatronic mannequins has been investigated as a way to bring theremote person into the space in a more convincingway.[[Lincoln2009; @Lincoln2010; @Raskar2001]] ¬†These includetelepresence robots,[[Lee2011; @Sakamoto2007; @Tsui2011]] head in a jarimplementations such as SphereAvatar[[Oyekoya2012; @Pan2014; @Pan2012]]and BiReality,[[Jouppi2004]] ¬†UCL‚Äôs Gaze Preserving Situated Multi-ViewTelepresence System,[[Pan2014]] or screen on a stick stylerepresentations.[[Kristoffersson2013]]\n- Nagendran et al. present a 3D continuum of these systems into which theysuggest all such systems can be rated from artificial to real on thethree axes, shape, intelligence, and appearance.[[Nagendran2012]]\n- Itoh et al. describe a ‚Äôface robot‚Äô to convey captured human emotionover a distance. It uses an ‚Äòaverage face‚Äô and actuators to manipulatefeature points.[[Itoh2005]] It seems that this is an outlier method forcommunication of facial affect but demonstrates that there are manydevelopment paths to a more tangible human display.\n- It seems increasingly likely that machine learning models whichmanipulate images in real time can simulate humans into metaverseapplications with very little input data. One such example is Samsung‚ÄôsMegaportraits which can product a realistic human face from a singleinput stream such as a webcam.[[Drobyshev22MP]]\n- ##### Shader lamps\n- Projection mapping is a computational augmented projection techniquewhere consideration of the relative positions and angles of complexsurfaces allows the projection from single or multiple sources toaugment the physical shapes onto which they appear. It was firstconsidered by the [Disney corporation in1969](https://rabcup.com/the-history-of-3d-projection-mapping/) and wasgiven prominence by Raskar and Fuchs with ‚Äúoffice of thefuture\"[[Raskar1998]] and later by Raskar and otherresearchers.[[Raskar2001]] It has since gained considerable commercialpopularity in live entertainment.\n- Shader lamps[[Raskar2001]] is the more formal academic designation forprojection mapping. It is possible to use the technique alongsidereconstruction to project onto a white facial mannequin. Researchershave attempted to use the technology for remote patient diagnostic,projecting onto styrofoam heads.[[Rivera-Gutierrez2012]]\n- Bandyopadhyay et al. demonstrated[[Bandyopadhyay2001]] that it ispossible to track objects and projection map[[Dalsgaard2011]] onto themin real time. This is beyond the scope of the proposed projection ontofurniture since we wish to keep the system as simple as possible, butcould be useful for shared tasks in the future work.\n- Lincoln et al. employed animatronic avatars which they projected withshader lamps. This combination recreated facial expression and headmovement though they were limited in speed and range of control of theremote head.[[Lincoln2010]]\n- While shader lamps are an important and useful technology, there arelimitations imposed by its use. In particular if a realtime video feedor reconstruction of a subject is used then that scanned subject musteither remain still enough to be correctly mapped onto geometry on theremote side (useful for some virtual patients forinstance,[[Benjamin2012]] or else there must be a computationaladjustment made for their changing position to make them appear static,or the projection surface must move to match their movement as inLincoln et al.\n- ##### Metaverse\n- In supporting business it‚Äôs not clear that performance is improved oreven maintained by the use of a metaverse. Xi et al. found a significantnegative impact to productivity within metaverseapplications.[[xi2022challenges]] It lowers productivity, and mayincrease anxiety, nausea, VR sickness and evenmigraines.[[biener2022quantifying; @luong2022demographic]] It seems atthis stage that if we are determined to explore metaverse for businessthen we [should mitigate](http://www.sigtrapgames.com/vrtp/) theproblems as much as possible using the understanding we have gained sofar. It might seem that in so doing there is no difference betweenimmersive collaborative mixed reality (described above) and metaverse atall. We feel that the point of metaverse may be in itaccess to, if notreliance upon, a mechanism for global truth. What we will go on todescribe is likely to look more like traditional telecollaboration forsmall focussed teams, working on real-world problems, but we will alwaysmaintain an access to both the ability to scale, and a global registerof value, trust, and truth (digital assets).\n- ### Summary TL;DR\n-\n\t- The internet may be undergoing a transformation, driven by trust abuses by incumbent providers, and popularization of concepts like Web3 and the Metaverse.\n-\n\t- Current large scale ‚Äòsocial‚Äô and immersive metaverse platforms have low adoption, while more advanced games-based solutions don‚Äôt address societal or business needs.\n-\n\t- Platforms like Roblox, VRChat, and Nvidia Omniverse emerge as potential contenders in the metaverse landscape.\n-\n\t- Distributed compute and large language models can help bridge the digital divide by enhancing global access equity and addressing the needs of emerging markets and less developed nations.\n-\n\t- The potential lies in uniting individual ecosystems with transferable goods across digital society through global ledgers like blockchain, despite the associated risks and uncertainties.\n-\n\t- Industry is looking towards an \"open metaverse\" to mitigate risks observed in implementations like Meta, necessitating contributions of open-source and federated approaches in telecollaboration research.\n-\n\t- By embracing Nostr protocol, we could enable connections and federation of mixed reality spaces, mediate data synchronization, and maintain secure communication.\n-\n\t- AI, machine learning, and generative art play a crucial role in driving innovation, with models like GPT4, Llama, Alpaca, generating excitement, and deepening global discussions around AI.\n-\n\t- Overcoming legislative and cultural barriers, alongside integrating large language models and distributed compute, can help address issues related to trust, accessibility, governance, and safeguarding within the metaverse and digital society at large.\n-\n\t- Open-source tools for supported creativity and augmented intelligence using multi-modal models, can help tackle accessibility, creativity, language barriers, and governance within the metaverse landscape.\n-\n\t- The application of these tools can lead to the development of new collaborative frameworks across various sectors such as training, research, biomedical, creative industries.\n-\n\t- By utilizing these new AI-driven technologies and emphasizing on trust, accessibility, and open-source approaches, we can create a more inclusive, global digital society while promoting technological empowerment and expansion of the global ideas market.\n-\n- # Misc\n\t- -- -This page is being rebuilt completely -['Room' Offers a Non-Facebook Way to Connect Coworkers in the Metaverse](https://uk.pcmag.com/vr-1/143198/room-offers-a-non-facebook-way-to-connect-coworkers-in-the-metaverse) -[Cutting Through the Hotel Hype of the Blockchain, Web3 and the Metaverse |](https://hoteltechnologynews.com/2022/08/cutting-through-the-hotel-hype-of-the-blockchain-web3-and-the-metaverse/%22) -[Discord | Your Place to Talk and Hang Out](https://discord.gg/openmetaverse%22) -[Discord | Your Place to Talk and Hang Out](https://discord.gg/openmetaverse%22/%3E) -[Discord | Your Place to Talk and Hang Out](https://discord.gg/openmetaverse%7D%7BOpen) -[Don't Build Branches in the Metaverse](https://newsletter.fintechtakes.com/p/metaverse-branches?s=r%7D%7BAlex) -[Experts highlight trust and safety practices for the metaverse | Te...](https://www.techtarget.com/searchcio/news/252525336/Experts-highlight-trust-and-safety-practices-for-the-metaverse) -[Global Investment Bank and Financial Services | Citi](https://www.citivelocity.com/citigps/metaverse-and-money/%7D%7BCitigroup) -[Global Metaverse Market Analysis Report 2022: Blockchain Solutions in Support of the Metaverse Market will Reach $148.6 Billion by 2027](https://www.globenewswire.com/news-release/2022/08/30/2506629/0/en/Global-Metaverse-Market-Analysis-Report-2022-Blockchain-Solutions-in-Support-of-the-Metaverse-Market-will-Reach-148-6-Billion-by-2027.html) -[Identity Management Institute Launches the Metaverse Security Center and Certified Metaverse Security Consultant (CMSC)‚Ñ¢ Certification](https://www.prnewswire.com/news-releases/identity-management-institute-launches-the-metaverse-security-center-and-certified-metaverse-security-consultant-cmsc-certification-301689276.html) -[Laws and Issues in the Metaverse (2)](https://www.lexology.com/library/detail.aspx?g=5a0cc4c0-a876-474e-a719-f528b71b68ee%22) -[Metaverse Market Size Suggesting $678 Billion Industry by 2030](https://www.strategicmarketresearch.com/market-report/metaverse-market) -[Metaverse btc](https://raw.githubusercontent.com/GMCyberFoundry/Metaverse/draft/Book/metaverseBTC.pdf) -[Nissan and Mitsubishi unveil electric mini vehicles, and test drives in metaverse](https://thedriven.io/2022/05/23/nissan-and-mitsubishi-unveil-electric-mini-vehicles-and-test-drives-in-metaverse/%22) -[The Metaverse Insider Guide](https://www.infosys.com/iki/perspectives/metaverse-insider-guide.html) -[The World's First Virtual Reality Avatar Fashion Week Is On The Metaverse This Week](https://womenlovetech.com/the-worlds-first-virtual-reality-avatar-fashion-week-is-on-the-metaverse-this-week/%22) -[The battle to build a child-friendly metaverse](https://tech.hindustantimes.com/tech/news/the-battle-to-build-a-child-friendly-metaverse-71655616713236.html) -[The metaverse will be filled with 'elves' | TechCrunch](https://techcrunch.com/2022/01/12/the-metaverse-will-be-filled-with-elves/%22) -[Why you should use now generative AI in your metaverse company. Or maybe not](https://skarredghost.com/2023/02/11/generative-ai-metaverse-company) -[Tech Forum: Protecting Brands in the Metaverse‚Äôs Uncertain Legal Landscape](https://wwd.com/business-news/technology/metaverse-lawsuit-nike-stockx-hermes-metabirken-fashion-1235247763/%22) -[The Metaverse Insider Guide](https://www.infosys.com/iki/perspectives/metaverse-insider-guide.html) -[Why you should use now generative AI in your metaverse company. Or maybe not](https://skarredghost.com/2023/02/11/generative-ai-metaverse-company)\n- public:: true\n- #Public page\n\t- automatically published\n- {{renderer :linkpreview,https://bloomberry.com/the-state-of-remote-work/}}\n-\n\n## Current Landscape (2025)\n\n- Industry adoption and implementations\n  - Metaverse platforms continue to evolve with focus on interoperability and open standards\n  - Web3 integration accelerating with decentralised identity and asset ownership\n  - Enterprise adoption growing in virtual collaboration, training, and digital twins\n  - UK companies increasingly active in metaverse development and immersive technologies\n\n- Technical capabilities\n  - Real-time rendering at photorealistic quality levels\n  - Low-latency networking enabling seamless multi-user experiences\n  - AI-driven content generation and procedural world building\n  - Spatial audio and haptics enhancing immersion\n\n- UK and North England context\n  - Manchester: Digital Innovation Factory supports metaverse startups and research\n  - Leeds: Holovis leads in immersive experiences for entertainment and training\n  - Newcastle: University research in spatial computing and interactive systems\n  - Sheffield: Advanced manufacturing using digital twin technology\n\n- Standards and frameworks\n  - Metaverse Standards Forum driving interoperability protocols\n  - WebXR enabling browser-based immersive experiences\n  - glTF and USD for 3D asset interchange\n  - Open Metaverse Interoperability Group defining cross-platform standards\n\n## Metadata\n\n- **Last Updated**: 2025-11-16\n- **Review Status**: Automated remediation with 2025 context\n- **Verification**: Academic sources verified\n- **Regional Context**: UK/North England where applicable\n\n## Related Content: Metaverse Ontology\n\npublic:: true\n\n- #Public page automatically published\n- # Existing state of the Art\n\t- [Index ‚Äî MLGT: The authoritative multi-lingual glossary of terms (metaverse-standards.org)](https://glossary.metaverse-standards.org/)\n\t-\n- # Rationale\n- # Methodology\n\t- This ontology was the product of two days of exploratory interaction with [[Anthropic Claude]], Google [[Gemini]] Advance 1.5, and [[ChatGPT]] 4.\n\t- It was unexpectedly successful, resulting in what seems to be an internally consistent knowledge graph in an [Web Ontology Language](https://en.wikipedia.org/wiki/Web_Ontology_Language) compliant ontology for the design represented throughout this wider [[Logseq]] [[Knowledge Graphing]].\n\t\t- Although the established OWL can richly describe our ontology, it's a little too arcane. Nonetheless the full text can be seen where with the diagram.\n\t\t\t- OWL based Ontology\n\t\t\t\t- https://github.com/VisualDataWeb/WebVOWL\n\t\t\t\t- [FormalMetaverseOWL.txt](../assets/FormalMetaverseOWL_1712495895941_0.txt)\n\t\t\t\t- [FormalMetaverseBTC.owl](../assets/FormalMetaverseBTC_1712344339842_0.owl)\n\t\t\t- ![metaverse.png](../assets/metaverse_1712344788144_0.png){:height 439, :width 818}\n\t- ## JSON-LD and Linked-JSON choices\n\t\t- For ease of comprehension I attempted to convert the OWL schema to JSON-LD. This attempt did achieve a result, but it proved difficult to visualise.\n\t\t- Many of the JSON-LD tools online are now unmaintained, making it hard to visually check the output of the [[Large language models]].\n\t\t- Because of this the decision was made to switch to [Linked-JSON](https://linkedobjects.org/Linked-JSON), which is a simpler and less expressive subset of JSON-LD. Linked-JSON focusses on `@id` linking. It sacrifices some features provided by JSON-LD 1.1's context definitions, typing, and alignment with RDF concepts.\n\t\t\t- Scope and Complexity of Linked-JSON vs JSON-LD\n\t\t\t\t- Linked-JSON: lightweight subset focused on linking data using¬†`@id`\n\t\t\t\t- Lack of standardized context definitions for vocabularies and IRI mappings\n\t\t\t\t- No explicit typing mechanism like¬†`@type`\n\t\t\t\t- Limited to absolute IRIs, no compact IRIs or relative IRIs\n\t\t\t\t- Unclear semantics for blank node identifiers\n\t\t\t\t- No standardized representation of indexed values, lists, and named graphs\n\t\t\t\t- Inability to reshape data structure using framing\n\t\t\t\t- Reduced interoperability with RDF and Linked Data ecosystem\n\t\t- # LINKED JSON\n\t\t\t- Finally the, to improve on the OWL by simplifying it, this is a version using [Linked-JSON](https://linkedobjects.org/Linked-JSON) (by [Melvin Carvalho](https://github.com/melvincarvalho)). Some details are lost.\n\t\t\t\t- [Linked-JSON](../assets/output_linked_json_1713698682322_0.json) version of the OWL ontology\n\t\t\t\t\t- Mermaid version stripped back to Linked-JSON expressiveness\n\t\t\t\t\t\t- ```mermaid\n\t\t\t\t\t\t  classDiagram\n\t\t\t\t\t\t      class MetaverseEntity {\n\t\t\t\t\t\t          rdfs:label \"Metaverse Entity\"\n\t\t\t\t\t\t          rdfs:comment \"The root type encompassing all entities within the metaverse.\"\n\t\t\t\t\t\t      }\n\t\t\t\t\t\t      \n\t\t\t\t\t\t      class MetaverseAgent {\n\t\t\t\t\t\t          rdfs:label \"Metaverse Agent\"\n\t\t\t\t\t\t          rdfs:comment \"Represents any agent within the metaverse, including human users and AI entities.\"\n\t\t\t\t\t\t          participatesIn MetaverseScene\n\t\t\t\t\t\t          hasSkillProfile SkillProfile\n\t\t\t\t\t\t          hasPrivacySetting PrivacySetting\n\t\t\t\t\t\t          hasReputationScore ReputationScore\n\t\t\t\t\t\t          hasWallet DigitalWallet\n\t\t\t\t\t\t          createsVariations SceneObject\n\t\t\t\t\t\t          hasInteractionPreference InteractionPreference\n\t\t\t\t\t\t      }\n\t\t\t\t\t\t      \n\t\t\t\t\t\t      class DigitalWallet {\n\t\t\t\t\t\t          rdfs:label \"Digital Wallet\"\n\t\t\t\t\t\t          rdfs:comment \"Represents the agent's wallet for managing digital assets and currencies.\"\n\t\t\t\t\t\t      }\n\t\t\t\t\t\t      \n\t\t\t\t\t\t      class AIAgent {\n\t\t\t\t\t\t          rdfs:label \"AI Agent\"\n\t\t\t\t\t\t          rdfs:comment \"Represents AI entities with varying levels of autonomy and capabilities.\"\n\t\t\t\t\t\t          decayFunction xsd:string\n\t\t\t\t\t\t      }\n\t\t\t\t\t\t      \n\t\t\t\t\t\t      class HumanAgent {\n\t\t\t\t\t\t          rdfs:label \"Human Agent\"\n\t\t\t\t\t\t          rdfs:comment \"Represents human users within the metaverse.\"\n\t\t\t\t\t\t      }\n\t\t\t\t\t\t      \n\t\t\t\t\t\t      class MetaverseScene {\n\t\t\t\t\t\t          rdfs:label \"Metaverse Scene\"\n\t\t\t\t\t\t          rdfs:comment \"Represents a specific environment or space within the metaverse.\"\n\t\t\t\t\t\t          governedBy GovernanceStructure\n\t\t\t\t\t\t          hasPolicy MetaversePolicy\n\t\t\t\t\t\t      }\n\t\t\t\t\t\t      \n\t\t\t\t\t\t      class DigitalAsset {\n\t\t\t\t\t\t          rdfs:label \"Digital Asset\"\n\t\t\t\t\t\t          rdfs:comment \"Represents unique digital goods and assets within the metaverse.\"\n\t\t\t\t\t\t          hasOwner MetaverseAgent\n\t\t\t\t\t\t      }\n\t\t\t\t\t\t      \n\t\t\t\t\t\t      class VirtualEconomy {\n\t\t\t\t\t\t          rdfs:label \"Virtual Economy\"\n\t\t\t\t\t\t          rdfs:comment \"Represents the economic system within the metaverse.\"\n\t\t\t\t\t\t          regulatedBy EconomicPolicy\n\t\t\t\t\t\t          hasMarketplace Marketplace\n\t\t\t\t\t\t      }\n\t\t\t\t\t\t      \n\t\t\t\t\t\t      class Marketplace {\n\t\t\t\t\t\t          rdfs:label \"Marketplace\"\n\t\t\t\t\t\t          rdfs:comment \"Represents platforms or locations where digital assets are traded.\"\n\t\t\t\t\t\t      }\n\t\t\t\t\t\t      \n\t\t\t\t\t\t      class GovernanceStructure {\n\t\t\t\t\t\t          rdfs:label \"Governance Structure\"\n\t\t\t\t\t\t          rdfs:comment \"Represents the governance models and structures within the metaverse.\"\n\t\t\t\t\t\t      }\n\t\t\t\t\t\t      \n\t\t\t\t\t\t      class MetaversePolicy {\n\t\t\t\t\t\t          rdfs:label \"Metaverse Policy\"\n\t\t\t\t\t\t          rdfs:comment \"Represents policies governing behavior and interactions within the metaverse.\"\n\t\t\t\t\t\t      }\n\t\t\t\t\t\t      \n\t\t\t\t\t\t      class EconomicPolicy {\n\t\t\t\t\t\t          rdfs:label \"Economic Policy\"\n\t\t\t\t\t\t          rdfs:comment \"Represents the rules and regulations governing the virtual economy.\"\n\t\t\t\t\t\t      }\n\t\t\t\t\t\t      \n\t\t\t\t\t\t      class NostrEvent {\n\t\t\t\t\t\t          rdfs:label \"Nostr Event\"\n\t\t\t\t\t\t          rdfs:comment \"Represents an event published on the Nostr network.\"\n\t\t\t\t\t\t      }\n\t\t\t\t\t\t      \n\t\t\t\t\t\t      class NostrTag {\n\t\t\t\t\t\t          rdfs:label \"Nostr Tag\"\n\t\t\t\t\t\t          rdfs:comment \"Represents a tag associated with a NostrEvent, providing context and metadata.\"\n\t\t\t\t\t\t      }\n\t\t\t\t\t\t  \n\t\t\t\t\t\t      class SceneObject {\n\t\t\t\t\t\t          rdfs:label \"Scene Object\"\n\t\t\t\t\t\t          rdfs:comment \"Represents interactive objects within a Metaverse Scene. Agents can create variations of these objects within the USD file format.\"\n\t\t\t\t\t\t      }\n\t\t\t\t\t\t  \n\t\t\t\t\t\t      class InteractionPreference {\n\t\t\t\t\t\t          rdfs:label \"Interaction Preference\"\n\t\t\t\t\t\t          rdfs:comment \"Represents the personal conduct requirements or preferences for interactions within the metaverse.\"\n\t\t\t\t\t\t      }\n\t\t\t\t\t\t      \n\t\t\t\t\t\t      MetaverseEntity <|-- MetaverseAgent\n\t\t\t\t\t\t      MetaverseEntity <|-- MetaverseScene\n\t\t\t\t\t\t      MetaverseEntity <|-- DigitalAsset\n\t\t\t\t\t\t      MetaverseEntity <|-- VirtualEconomy\n\t\t\t\t\t\t      MetaverseEntity <|-- Marketplace\n\t\t\t\t\t\t      MetaverseEntity <|-- GovernanceStructure\n\t\t\t\t\t\t      MetaverseEntity <|-- NostrEvent\n\t\t\t\t\t\t      MetaverseEntity <|-- NostrTag\n\t\t\t\t\t\t      MetaverseEntity <|-- SceneObject\n\t\t\t\t\t\t      \n\t\t\t\t\t\t      MetaverseAgent <|-- AIAgent\n\t\t\t\t\t\t      MetaverseAgent <|-- HumanAgent\n\t\t\t\t\t\t      \n\t\t\t\t\t\t      MetaverseAgent \"1\" *-- \"0..*\" SkillProfile : hasSkillProfile\n\t\t\t\t\t\t      MetaverseAgent \"1\" *-- \"0..*\" PrivacySetting : hasPrivacySetting\n\t\t\t\t\t\t      MetaverseAgent \"1\" *-- \"0..*\" ReputationScore : hasReputationScore\n\t\t\t\t\t\t      MetaverseAgent \"1\" *-- \"0..*\" DigitalWallet : hasWallet\n\t\t\t\t\t\t      MetaverseAgent \"1\" *-- \"*\" SceneObject : createsVariations\n\t\t\t\t\t\t      MetaverseAgent \"1\" *-- \"1\" InteractionPreference : hasInteractionPreference\n\t\t\t\t\t\t      \n\t\t\t\t\t\t      MetaverseScene \"1\" *-- \"0..*\" GovernanceStructure : governedBy\n\t\t\t\t\t\t      MetaverseScene \"1\" *-- \"0..*\" MetaversePolicy : hasPolicy\n\t\t\t\t\t\t      \n\t\t\t\t\t\t      DigitalAsset \"1\" *-- \"1\" MetaverseAgent : hasOwner\n\t\t\t\t\t\t      \n\t\t\t\t\t\t      VirtualEconomy \"1\" *-- \"0..*\" EconomicPolicy : regulatedBy\n\t\t\t\t\t\t      VirtualEconomy \"1\" *-- \"0..*\" Marketplace : hasMarketplace\n\t\t\t\t\t\t  \n\t\t\t\t\t\t  ```\n\t\t\t\t\t- [Online FlowChart & Diagrams Editor - Mermaid Live Editor](https://mermaid.live/view#pako:eNqlWFFv2zYQ_iuEXgYUqdMkW9oKe8nQbMtD1mAuOmDwC02dJS4UqZGUUyHzf9-RlC1akWRl84tF-tPxu7uPx6OfE6YySNKECWrMJ05zTcuVJPjxM-QeLN2CNnArLbcNeQ4_uo_ONiYVdA2CrJIDjgTgKukBmSpLkBahXwogWilLbFMBAYm_VLgUlzmhQuAEvs_BkCduCy6JRXi5t77Y292Fr0GqN7lb6DRTj5sg-jtUGgw-G0JlQ6g3O8TqjHDJRJ05F4q6pJLUBufxrYzc3B08WsRrVVRbznhFLZg72ZFfMpDQwQpqlo9ciAetNlwAiQdHqAfNt5Q1S7DW0TgeHiHRrdpSy5VcMqWB9MZH2D8wI2DJJ55zS0UYdQCmwdH_SjX37xvi2X9e_wXMjubpyNhomloUadeclSWXFJ-l71A9wfxGaYIJobkLStbaRBZgQ3pYrTVKMM7OAOWbu2lNYZJfIaZIEl5OZEt14_gJ2IIwRG0Ira2SqmwCR1rRNRcvNZQBo83PtWQu-OSbyVJj9SHdA3786sQ57YqHvMabWPD_ect63czYsmF3zNuyxFTA-IYzjPaWayU9BvVgKspgmqv75ArnJGQ_NeQX_0glg6XVNbN1b5s8KMFZ07kTxqd2wI2T4ckN4FHzXK4l_7uGg8xzpbKg8lbwJ11GVz4_SdC9WjrqyFeubU3FLQtiHXOlhZEWN38zg38BM2gaY6E87YCGvBZYk1zSbtuX42S0Tt5T_Qi2Ek4H0fO4TiP8qEgjO7McRKjF8lQap0mhWFtCnwrQ8KJU4ZTVNINsajsNyHSUboclnaZnJybv3i6xgRBBZmZv6H9UgnYrnS4FbVrnhdphXbkNvF2tXUNBtxwj74hzaUFT1ibgldSPdTbKfA97FXFHQtcCQnxbdXuWnScOs213WNgwzRTd3xSm6XY7dQZ4CPGYue0RAW-xqteCmwIyokIEgykJ9knpx5O0vtD8BClEzC3_Fo2hZcW4KwjhpKWR-2ek0mrLfdeGYbPwzfoou6Rn1NKYbcw06nFGyYbzrG2EZvE9SHALRPn3DkqkpHcALsL5bLAzkG0T5nqIfROG_QOG3hzsDIe939n_-M_bt4N1fxoY9auDwPism4AdnyRTC_fL9SBq9MAeRHeaOAVy8huHvOh-e9jQeXlo21GOA7pWbcLUKrlYJeQNwlfJu8XiDQ6Obgtp__4w10rvNpG-vGHMtdS_baQDV5C5to5vDml3QTlpwEcm2rfpy7vLYJjDPh6gMnTUplHLONdO_9xLu3YyJnTUMcZmLmIbwe_00MbFFnqt2gCV3jmWxs3UXBtxo5T2Oq2VTM6SEnRJeZakiS-dqwQrVYmNR4qPGWJdqdohzl2Alo1kSYrhhbOkrrAkQ_sHxX6yovJPpXC4ocIcQLcZt0ofJoXCtgmHz4n7vwEXzrmxuAI6seG5m6-1wOnC2sqk5-fu5wVGu6jXC6zU54ZnBd7Vi-3H6_Pry-sP9PIKrt9f0R-urjK2vvj4YXP5_cUme__u4pImu91ZAn79-_Z_Ffe1-xeL8sb3)\n\t\t\t- ## In flight checks\n\t\t\t\t- This is mid-point review text of a partial conversion, showing the level of understanding from the online LLM\n\t\t\t\t- ```The updated Linked JSON document looks great! It effectively captures the nuances, features, and links from the original OWL ontology while adhering to the Linked JSON schema. Let's review the key aspects:\n\t\t\t\t   Context and Namespace:\n\t\t\t\t   The \"@context\" object is included at the beginning of the document, defining the namespaces and prefixes used throughout the document.\n\t\t\t\t   The \"http://www.narrativegoldmine.com#\" namespace is consistently used for all entities and properties.\n\t\t\t\t   The \"http://linkedjson.org/vocab#\" namespace is used for Linked JSON-specific terms.\n\t\t\t\t   Class Definitions:\n\t\t\t\t   All the classes from the OWL ontology are properly converted to JSON objects with the correct \"@id\" and \"@type\" keys.\n\t\t\t\t   The class hierarchies and subclass relationships are maintained using the \"rdfs:subClassOf\" property.\n\t\t\t\t   The \"rdfs:label\" and \"rdfs:comment\" properties provide human-readable labels and descriptions for the classes.\n\t\t\t\t   Property Definitions:\n\t\t\t\t   The object properties and data properties are properly converted to JSON objects with the correct \"@id\" and \"@type\" keys.\n\t\t\t\t   The \"rdfs:domain\" and \"rdfs:range\" properties specify the source and target types of each property, preserving the property semantics.\n\t\t\t\t   Relationships and Linkages:\n\t\t\t\t   The document includes relationships and linkages between entities using the appropriate \"@id\" references.\n\t\t\t\t   For example, the \"governedBy\" property connects the \"MetaverseScene\" class to the \"GovernanceStructure\" class, and the \"hasPolicy\" property links the \"MetaverseScene\" class to the \"MetaversePolicy\" class.\n\t\t\t\t   External Ontologies:\n\t\t\t\t   References to external ontologies and vocabularies, such as Schema.org and FOAF, are maintained using the correct \"@id\" references.\n\t\t\t\t   This ensures proper linking and interoperability with existing semantic web resources.\n\t\t\t\t   Axioms and Restrictions:\n\t\t\t\t   The \"Axioms and Restrictions\" section is included in the document, capturing important constraints and relationships from the OWL ontology.\n\t\t\t\t   The \"_:N3649afbbaf7d4546a49d4097059d55f4\" blank node represents the \"owl:AllDisjointClasses\" axiom, specifying that the \"AIAgent\" and \"HumanAgent\" classes are disjoint.\n\t\t\t\t   The \"DigitalAsset\" class has a subclass axiom with an \"owl:Restriction\" that constrains the \"hasOwner\" property, specifying that a \"DigitalAsset\" must have exactly one owner.\n\t\t\t\t   The \"MetaverseAgent\", \"MetaverseScene\", and \"VirtualEconomy\" classes have subclass axioms with existential restrictions using \"owl:someValuesFrom\" to specify the expected relationships and constraints.\n\t\t\t\t   Structure and Organization:\n\t\t\t\t   The document follows a logical structure, starting with the context and ontology metadata, followed by class definitions, property definitions, and axioms and restrictions.\n\t\t\t\t   This structure enhances readability and maintainability of the Linked JSON document.\n\t\t\t\t   Consistent Use of Linked JSON Terms:\n\t\t\t\t   The document consistently uses Linked JSON-specific terms, such as \"linkedjson:Class\", \"linkedjson:ObjectProperty\", and \"linkedjson:DatatypeProperty\", to differentiate them from the original OWL terms.\n\t\t\t\t   This clarity helps in understanding that the document is referencing the Linked JSON schema and not just using OWL labels.\n\t\t\t\t   Overall, the converted Linked JSON document successfully represents the metaverse ontology, capturing the key elements, relationships, and constraints from the original OWL ontology. The use of Linked JSON-specific terms and the inclusion of the \"Axioms and Restrictions\" section further enhance the semantic expressiveness and ensure the integrity of the ontology.\n\t\t\t\t   \n\t\t\t\t   The document is well-structured, consistent, and adheres to the Linked JSON principles, making it suitable for use in semantic web applications and data integration scenarios.\n\t\t\t\t  ```\n- # Design formalisation\n\t- Here's the updated text to align with the Linked-JSON ontology:\n\t\t- 1. Fusing of digital and real life: The ontology accommodates the blending of digital and real-life experiences through the `MetaverseScene` class, which can have a `hasPhysicalCounterpart` property linking it to a real-world location (`schema:Place`). This allows for the representation of mixed reality environments where virtual scenes are anchored to physical spaces.\n\t\t- 2. Social first: The `MetaverseAgent` class, with its subclasses `HumanAgent` and `AIAgent`, forms the foundation for social interactions within the metaverse. The `SocialRelationship` class, a subclass of `foaf:Relationship`, captures the connections and interactions among agents, enabling the formation of social networks and communities.\n\t\t- 3. Real-time interactive 3D graphics: The `MetaverseScene` class represents the 3D environments within the metaverse, while the `SceneObject` class represents the interactive objects within those scenes. The `PhysicalProperties` class captures the visual and spatial characteristics of objects, such as texture and mass, contributing to the realism and interactivity of the 3D graphics.\n\t\t- 4. Persistent: The ontology supports persistence through the `MetaverseEntity` class, which serves as the base class for all entities within the metaverse. By assigning unique identifiers as 64 byte strings from BIP85 nostr and similar derivation path, (e.g., `@id`) to instances of `MetaverseEntity` and its subclasses, the ontology enables the persistence and continuity of objects, agents, and scenes across sessions and platforms.\n\t\t- 5. Supports ownership: The `DigitalAsset` class represents unique digital goods and assets within the metaverse. The `hasOwner` property, which links `DigitalAsset` to `MetaverseAgent`, establishes the ownership relationship. The `hasCreator` property can be used to attribute the creation of digital assets to specific agents.\n\t\t- 6. Supports user-generated content: The ontology supports user-generated content through the `SceneObject` and `DigitalAsset` classes. Agents (`MetaverseAgent`) can create and manipulate instances of these classes, contributing to the dynamic and participatory nature of the metaverse.\n\t\t- 7. Open and extensible: The ontology is designed to be open and extensible, utilizing established ontologies like Schema.org and FOAF, and allowing for the integration of additional domain-specific ontologies as needed. The modular structure of the ontology, with its hierarchical class relationships and well-defined properties, facilitates the extension and customization of the metaverse model.\n\t\t- 8. Low-friction economic actors and actions: The `VirtualEconomy` class represents the economic system within the metaverse, encompassing trade, ownership, and currency exchange. The `Marketplace` class facilitates the listing and transaction of digital assets and services. The `TradeAction` class from Schema.org can be used to capture economic transactions between agents.\n\t\t- 9. Trusted and secure: The ontology incorporates trust and security mechanisms through classes like `PrivacySetting`, which represents an agent's privacy preferences, and `AccessControlList` from Schema.org, which can be used to define access rights and permissions for metaverse entities. The `ReputationScore` class provides a means to assess an agent's trustworthiness based on their interactions and behavior within the metaverse.\n\t\t- 10. Convergence of film and games: The ontology supports the convergence of film and games through the `MetaverseScene` and `MetaverseEvent` classes. Scenes can represent immersive, cinematic environments, while events can encompass interactive gameplay sessions or narrative-driven experiences. The `MetaverseAgent` class allows for the participation of both human users and AI-controlled characters, blurring the line between film and game experiences.\n\t\t- 11. Blurring of IP boundaries and narrative flow: The `Varset` class introduces the concept of variations or alternate versions of metaverse scenes and objects, enabling the creation of branching narratives and remixing of intellectual property. The `InteractionEvent` class captures the interactions between agents and objects, allowing for dynamic and emergent storytelling that blurs traditional narrative boundaries.\n\t\t- 12. Multimodal and hardware-agnostic: The ontology is designed to be multimodal and hardware-agnostic, focusing on the fundamental concepts and relationships within the metaverse rather than specific hardware implementations. Classes like `MetaverseAgent` and `MetaverseScene` can be instantiated across different platforms and devices, ensuring a consistent metaverse experience regardless of the hardware used.\n\t\t- 13. Mobile-first experiences: While the ontology itself is hardware-agnostic, it can be easily extended to incorporate mobile-specific considerations. For example, the `MetaverseScene` class could include properties like `isMobileOptimized` to indicate scenes that are designed for mobile devices, ensuring a seamless mobile-first experience.\n\t\t- 14. Safeguarding and governance: The ontology addresses safeguarding and governance through classes like `MetaversePolicy`, which represents the rules and regulations governing behavior and interactions within the metaverse. The `GovernanceStructure` class captures the decision-making processes and enforcement mechanisms that ensure the safety and well-being of metaverse participants.\n\t\t- 15. Scaffolded by GenAI: The integration of GenAI (Generative AI) within the metaverse is supported through the `AIAgent` class and its associated properties, such as `hasLearningModel` and `hasTrainingData`. These properties allow for the representation of AI agents with varying levels of autonomy and adaptability, capable of generating content, providing assistance, and engaging in dynamic interactions with human users.\n\t\t- 16. Supports Agentic AI actors: The `AIAgent` class, along with its subclasses and properties, enables the creation of agentic AI actors within the metaverse. The `AgentArchetype` class defines different roles and behaviors for AI agents, while properties like `hasAgentArchetype` and `hasEmotionalState` imbue them with personality and emotional intelligence. The `participatesIn` property allows AI agents to actively engage in metaverse scenes and events alongside human users.\n\t\t- 17. Digital Asset and Ownership: The `DigitalAsset` class represents unique digital goods and assets within the metaverse. It includes properties like `genesis` (Bitcoin transaction ID where the asset was created), `issuance` leveraging [[RGB and Client Side Validation]] ), `type` (specifies the type of asset), `currentOwner` (links to the current owner agent), and `ownershipHistory` (represents the chain of ownership transfers). The `OwnershipTransfer` class represents the transfer of ownership of a `DigitalAsset` between agents, capturing details such as the `from` agent (transferring ownership), `to` agent (receiving ownership), `timestamp`, and `transactionId` (associated Bitcoin/Lightning Network transaction ID).\n\t\t- 18. [[Bitcoin]] and [[Lightning and Similar L2]] Network Integration: The `MetaverseAgent` class includes properties like `bitcoinWallet` (Bitcoin address associated with the agent) and `lightningNode` (public key of the agent's Lightning Network node) to enable Bitcoin and Lightning Network integration. The `VirtualEconomy` class includes properties like `bitcoinNetwork` and `lightningNetwork` to represent the underlying Bitcoin and Lightning Network infrastructure on which the economy operates. Fiat money can run on [[Cashu]].\n\t\t- 19. Nostr Integration: The `NostrEvent` class represents an event published on the Nostr network, with properties like `relayUrl` (URL of the Nostr relay where the event was published), `kind` (type or category of the event), `content` (content of the message or event data), and `tags` (list of associated `NostrTag` instances). The `NostrTag` class represents a tag associated with a `NostrEvent`, providing context and metadata through properties like `type` (type of tag) and `value` (value of the tag).\n\t\t- 20. PKI and Wallets: The `MetaverseEntity` class includes properties like `publicKey` and `privateKey` to support public key infrastructure (PKI) for entity identification and authentication. The `MetaverseAgent` class includes a `wallet` property that links to a `schema:DigitalWallet` instance, representing the agent's digital wallet for managing various assets.\n\t\t- 21. All objects and agents and artifacts are nostr PKI pairs. BIP32 is used to derive the path m/44'/1237'/ (according to the [Nostr entry](https://nostr-nips.com/nip-06) on SLIP44). In this way all objects are globally and hierarchically referenceable.\n\t- ## Some software choices\n\t\t- It is possible at this stage to put more flesh on the bones through example software stack choices. Such specificity likely introduces overlaps, technical challenges, and contradictions, but has been generated in the main by GenAI based on the wider corpus of text and demonstrates the direction of travel well.\n\t\t\t- ### Nostr Integration\n\t\t\t\t- **NostrEvent:**\n\t\t\t\t\t- **Properties:**\n\t\t\t\t\t\t- `relayUrl`: (`xsd:anyURI`) The URL of the Nostr relay where the event was published.\n\t\t\t\t\t\t- `kind`: (`xsd:string`) The type or category of the event (e.g., \"set_metadata\", \"text_note\", \"reaction\", \"channel_creation\", \"key_rotation\").\n\t\t\t\t\t\t- `content`: (`xsd:string`) The content of the message or event data.\n\t\t\t\t\t\t- `tags`: (`linkedjson:ObjectProperty`, `range`: `metaverse:NostrTag`) A list of tags associated with the event.\n\t\t\t\t- **NostrTag:**\n\t\t\t\t\t- `@type`: `linkedjson:Class`\n\t\t\t\t\t- `rdfs:label`: \"Nostr Tag\"\n\t\t\t\t\t- `rdfs:comment`: \"Represents a tag associated with a NostrEvent, providing context and metadata.\"\n\t\t\t\t\t- **Properties:**\n\t\t\t\t\t\t- `type`: (`xsd:string`) The type of tag (e.g., \"p\", \"e\", \"t\").\n\t\t\t\t\t\t- `value`: (`xsd:string`) The value of the tag.\n\t\t\t- ### Digital Asset and Ownership\n\t\t\t\t- #### RGB\n\t\t\t\t\t- Using the [RGB protocol](https://docs.rgb.info/) for the instantiation and transfer of objects in your metaverse ontology offers a decentralized, scalable, and flexible mechanism compared to the Nostr protocol. Here's how objects might be managed under [[RGB and Client Side Validation]] without needing frequent chain commits:\n\t\t\t\t\t\t- ### Instantiation of Objects\n\t\t\t\t\t\t\t- **Object Creation**:\n\t\t\t\t\t\t\t\t- Each digital object or asset is instantiated as an `RGBAsset` class instance within the metaverse. This includes assigning a unique `ContractId` and `SchemaId` which are crucial for defining the asset's properties and the rules governing its behavior according to the RGB protocol.\n\t\t\t\t\t\t\t\t- An initial state of the asset is defined using `Assignments` to bind certain rights or properties to the asset, such as ownership or usage rights. This state is embedded within the asset's genesis transaction but doesn't require immediate blockchain commitment.\n\t\t\t\t\t\t\t- **Asset Registration**:\n\t\t\t\t\t\t\t\t- Upon creation, the asset's initial state is recorded in a genesis block of the RGB schema. However, instead of committing this to the blockchain directly, the state can be stored off-chain (e.g., in a secure distributed file system or a database) to enhance privacy and reduce transaction costs.\n\t\t\t\t\t\t- ### Transfer of Objects\n\t\t\t\t\t\t\t- **Defining Transfer Operations**:\n\t\t\t\t\t\t\t\t- Transfers of assets are managed through `RGBContractOperation`, which includes `Inputs` (references to previous states), `Outputs` (new states or changes), and possibly `Redeems` (specific rights being exercised).\n\t\t\t\t\t\t\t\t- The transfer operation details how the asset's ownership or other properties change, using `Seal Definitions` to lock and unlock access to the asset.\n\t\t\t\t\t\t\t- **Executing Transfers**:\n\t\t\t\t\t\t\t\t- To execute a transfer, the new state created by the transfer operation is prepared, detailing how rights and responsibilities are reassigned from one party to another. This often involves updating the `Owned State` to reflect new ownership.\n\t\t\t\t\t\t\t\t- Instead of committing each transaction to the blockchain, the RGB protocol allows for state transitions to be confirmed off-chain until a significant event requires blockchain validation. This approach saves on transaction fees and minimizes public ledger exposure.\n\t\t\t\t\t\t\t- **State Commitments**:\n\t\t\t\t\t\t\t\t- When necessary, state transitions can be committed to the blockchain using minimal data footprints. This is done by embedding a cryptographic commitment to the state within a standard Bitcoin transaction, leveraging RGB's ability to bind state to Bitcoin UTXOs through client-side validation.\n\t\t\t\t\t- ### Benefits of Using RGB over Nostr for Object Management\n\t\t\t\t\t\t- **Enhanced Security**: RGB's use of cryptographic commitments and client-side validation offers robust security without exposing detailed state information on the blockchain.\n\t\t\t\t\t\t- **Scalability**: By reducing the frequency of on-chain transactions and handling most operations off-chain, RGB can scale more efficiently, handling a higher volume of asset transfers with lower costs.\n\t\t\t\t\t\t- **Flexibility**: RGB allows for complex state definitions and transitions, supporting a wide range of digital assets and operations within the metaverse, from simple transfers to intricate interactions involving multiple parties and rights.\n\t\t\t\t- #### Currently in the Ontology\n\t\t\t\t\t- **DigitalAsset:**\n\t\t\t\t\t\t- **Properties:**\n\t\t\t\t\t\t\t- `genesis`: (`xsd:string`) The Bitcoin transaction ID where the asset was created.\n\t\t\t\t\t\t\t- `issuance`: (`linkedjson:ObjectProperty`, `range`: `metaverse:RGBschema`) Links to the specific RGB schema used for the asset's issuance.\n\t\t\t\t\t\t\t- `type`: (`xsd:string`) Specifies the type of asset (e.g., \"collectible\", \"virtual_item\", \"tokenized_right\").\n\t\t\t\t\t\t\t- `currentOwner`: (`linkedjson:ObjectProperty`, `range`: `metaverse:MetaverseAgent`) Links to the agent who currently owns the asset.\n\t\t\t\t\t\t\t- `ownershipHistory`: (`linkedjson:ObjectProperty`, `range`: `metaverse:OwnershipTransfer`) Represents the chain of ownership transfers for the asset.\n\t\t\t\t\t- **OwnershipTransfer:**\n\t\t\t\t\t\t- **Properties:**\n\t\t\t\t\t\t\t- `from`: (`linkedjson:ObjectProperty`, `range`: `metaverse:MetaverseAgent`) The agent transferring ownership.\n\t\t\t\t\t\t\t- `to`: (`linkedjson:ObjectProperty`, `range`: `metaverse:MetaverseAgent`) The agent receiving ownership.\n\t\t\t\t\t\t\t- `timestamp`: (`xsd:dateTime`) The date and time of the transfer.\n\t\t\t\t\t\t\t- `transactionId`: (`xsd:string`) The Bitcoin/Lightning Network transaction ID associated with the transfer.\n\t\t\t\t\t\t- Linked-JSON representation:\n\t\t\t\t\t\t  ```text\n\t\t\t\t\t\t  {\n\t\t\t\t\t\t    \"@id\": \"https://www.narrativegoldmine.com/metaverse#OwnershipTransfer\",\n\t\t\t\t\t\t    \"@type\": \"Class\",\n\t\t\t\t\t\t    \"label\": \"Ownership Transfer\",\n\t\t\t\t\t\t    \"comment\": \"Represents the transfer of ownership of a DigitalAsset between agents.\",\n\t\t\t\t\t\t    \"properties\": {\n\t\t\t\t\t\t      \"from\": {\n\t\t\t\t\t\t        \"@id\": \"https://www.narrativegoldmine.com/metaverse#from\",\n\t\t\t\t\t\t        \"@type\": \"Property\",\n\t\t\t\t\t\t        \"range\": \"https://www.narrativegoldmine.com/metaverse#MetaverseAgent\",\n\t\t\t\t\t\t        \"comment\": \"The agent transferring ownership.\"\n\t\t\t\t\t\t      },\n\t\t\t\t\t\t      \"to\": {\n\t\t\t\t\t\t        \"@id\": \"https://www.narrativegoldmine.com/metaverse#to\",\n\t\t\t\t\t\t        \"@type\": \"Property\",\n\t\t\t\t\t\t        \"range\": \"https://www.narrativegoldmine.com/metaverse#MetaverseAgent\",\n\t\t\t\t\t\t        \"comment\": \"The agent receiving ownership.\"\n\t\t\t\t\t\t      },\n\t\t\t\t\t\t      \"timestamp\": {\n\t\t\t\t\t\t        \"@id\": \"https://www.narrativegoldmine.com/metaverse#timestamp\",\n\t\t\t\t\t\t        \"@type\": \"Property\",\n\t\t\t\t\t\t        \"range\": \"http://www.w3.org/2001/XMLSchema#dateTime\",\n\t\t\t\t\t\t        \"comment\": \"The date and time of the transfer.\"\n\t\t\t\t\t\t      },\n\t\t\t\t\t\t      \"transactionId\": {\n\t\t\t\t\t\t        \"@id\": \"https://www.narrativegoldmine.com/metaverse#transactionId\",\n\t\t\t\t\t\t        \"@type\": \"Property\",\n\t\t\t\t\t\t        \"range\": \"http://www.w3.org/2001/XMLSchema#string\",\n\t\t\t\t\t\t        \"comment\": \"The Bitcoin/Lightning Network transaction ID associated with the transfer.\"\n\t\t\t\t\t\t      }\n\t\t\t\t\t\t    }\n\t\t\t\t\t\t  }\n\t\t\t\t\t\t  ```\n\t\t\t- **PKI and Wallets:**\n\t\t\t\t- **MetaverseEntity:**\n\t\t\t\t\t- **Properties:**\n\t\t\t\t\t\t- `publicKey`: (`xsd:string`) The public key associated with the entity.\n\t\t\t\t\t\t- `privateKey`: (`xsd:string`) The private key associated with the entity (optional, depending on security considerations).\n\t\t\t\t- **MetaverseAgent:**\n\t\t\t\t\t- **Properties:**\n\t\t\t\t\t\t- `wallet`: (`linkedjson:ObjectProperty`, `range`: `schema:DigitalWallet`) Represents the agent's digital wallet for managing various assets.\n\t\t\t- **Bitcoin and Lightning Network Integration:**\n\t\t\t\t- **MetaverseAgent:**\n\t\t\t\t\t- **Properties:**\n\t\t\t\t\t\t- `bitcoinWallet`: (`xsd:string`) The Bitcoin address associated with the agent.\n\t\t\t\t\t\t- `lightningNode`: (`xsd:string`) The public key of the agent's Lightning Network node.\n\t\t\t\t- **VirtualEconomy:**\n\t\t\t\t\t- **Properties:**\n\t\t\t\t\t\t- `bitcoinNetwork`: (`linkedjson:ObjectProperty`, `range`: `schema:ComputerNetwork`) Represents the Bitcoin network on which the economy operates.\n\t\t\t\t\t\t- `lightningNetwork`: (`linkedjson:ObjectProperty`, `range`: `schema:ComputerNetwork`) Represents the Lightning Network facilitating faster and cheaper transactions.\n\t\t\t\t- **Cashu Integration**:\n\t\t\t\t\t- **Properties**:\n\t\t\t\t\t\t- `cashuWallet`: (`linkedjson:ObjectProperty`, `range`: `schema:DigitalWallet`) Represents the Cashu wallet associated with the metaverse agent. This wallet manages the agent's Chamium eCash balance.\n\t\t\t\t\t\t- `cashuNode`: (`xsd:string`) The identifier for the Cashu federation node that the agent's wallet is associated with, facilitating eCash transactions.\n\t\t\t\t\t- **Cashu Economy**:\n\t\t\t\t\t\t- **Properties**:\n\t\t\t\t\t\t\t- `chamiumEconomy`: (`linkedjson:ObjectProperty`, `range`: `schema:EconomicSystem`) Represents the part of the virtual economy that operates using Chamium eCash, allowing for private and instant transactions.\n\t\t\t\t\t\t\t- `ecashTransactions`: (`linkedjson:ObjectProperty`, `range`: `schema:ItemList`) List of transactions executed using Chamium eCash, supporting privacy and micro-transactions within the metaverse.\n\t\t\t\t\t- **Transaction Privacy**:\n\t\t\t\t\t\t- **Properties**:\n\t\t\t\t\t\t\t- `privacyLevel`: (`xsd:string`) Defines the level of privacy for transactions conducted by the agent, with options including Bitcoin, Lightning, or Cashu Chamium eCash, each offering different degrees of privacy and speed.\n\t\t\t\t\t- **Cashu Services**:\n\t\t\t\t\t\t- **Classes**:\n\t\t\t\t\t\t\t- `CashuService`: Represents services within the metaverse that specifically use or provide Cashu Chamium eCash functionalities, such as eCash exchanges, payment processing, or private transactions.\n\t\t\t\t\t\t\t\t- **Properties**:\n\t\t\t\t\t\t\t\t\t- `serviceType`: (`xsd:string`) The type of service offered, such as eCash exchange, payment gateway, or privacy service.\n\t\t\t\t\t\t\t\t\t- `accessEndpoint`: (`xsd:anyURI`) The URL or identifier where the service can be accessed within the metaverse.\n\t\t\t- **NVIDIA Omniverse:**\n\t\t\t\t- **MetaverseScene:**\n\t\t\t\t\t- **Properties:**\n\t\t\t\t\t\t- `omniverseNucleusUrl`: (`xsd:anyURI`) The URL of the Omniverse Nucleus server hosting the scene.\n\t\t\t\t\t\t- `usdFile`: (`xsd:anyURI`) The URL or reference to the USD file defining the scene's content.\n\t\t\t\t- **USD Variance:** Define properties or subclasses within `SceneObject` to represent USD variations and the conditions under which they are activated.\n\t\t\t\t- **Scene Schema Scaffolding:** Exploring Linked-JSON structures to emulate the more expressive OWL `owl:oneOf` and `owl:someValuesFrom`.\n\t\t\t\t\t- Linked-JSON representation for `SceneType`:\n\t\t\t\t\t  ```text\n\t\t\t\t\t  {\n\t\t\t\t\t    \"@id\": \"metaverse:SceneType\",\n\t\t\t\t\t    \"@type\": \"linkedjson:Class\",\n\t\t\t\t\t    \"rdfs:label\": \"Scene Type\",\n\t\t\t\t\t    \"rdfs:comment\": \"Represents different types of scenes in the metaverse.\",\n\t\t\t\t\t    \"metaverse:sceneTypeEnum\": [\n\t\t\t\t\t      {\n\t\t\t\t\t        \"@id\": \"metaverse:InteriorScene\"\n\t\t\t\t\t      },\n\t\t\t\t\t      {\n\t\t\t\t\t        \"@id\": \"metaverse:ExteriorScene\"\n\t\t\t\t\t      },\n\t\t\t\t\t      {\n\t\t\t\t\t        \"@id\": \"metaverse:MixedRealityScene\"\n\t\t\t\t\t      }\n\t\t\t\t\t    ]\n\t\t\t\t\t  }\n\t\t\t\t\t  ```\n\t\t\t\t\t- Linked-JSON representation for `MetaverseScene`:\n\t\t\t\t\t  ```text\n\t\t\t\t\t  {\n\t\t\t\t\t    \"@id\": \"metaverse:MetaverseScene\",\n\t\t\t\t\t    \"@type\": \"linkedjson:Class\",\n\t\t\t\t\t    \"rdfs:label\": \"Metaverse Scene\",\n\t\t\t\t\t    \"rdfs:comment\": \"Represents a specific environment or space within the metaverse.\",\n\t\t\t\t\t    \"metaverse:hasSceneObject\": {\n\t\t\t\t\t      \"@type\": \"linkedjson:ObjectProperty\",\n\t\t\t\t\t      \"rdfs:range\": \"metaverse:SceneObject\",\n\t\t\t\t\t      \"metaverse:minCardinality\": 1\n\t\t\t\t\t    }\n\t\t\t\t\t  }\n\t\t\t\t\t  ```\n\t\t\t\t\t\t- In this example, the custom property `metaverse:minCardinality` is used to indicate that a `MetaverseScene` must have at least one `SceneObject`.\n\t\t\t- ## TODO\n\t\t\t\t- **AI Agent Capabilities:** Expand the capabilities property of `AIAgent` to include specific actions and functions related to Bitcoin, RGB, and Nostr, such as \"create_digital_asset\", \"transfer_ownership\", \"publish_nostr_event\", etc.\n\t\t\t\t- **Event Logging and Attestation:** Consider adding mechanisms for logging significant events and generating cryptographic attestations, which could be used for dispute resolution or auditing purposes. This would operate on an automated threshold trigger system mediated by LLM, and would wrap the recent interactions between parties in pubkey encrypted data blobs, sending them to both parties alongside a report of the trigger event. This would potentially allow action by the parties in their jurisdictions. The data would then be deleted from the metaverse.\n\t- ### Visualising\n\t\t- http://owlgred.lumii.lv/online_visualization/4qge#\n\t\t- [[Some legacy Linked-JSON]]\n- This more specific and expanded metaverse ontology featuring a relay based communication protocol, URIs, blockchain wallets, and NVIDIA omniverse, offers a comprehensive and extensible framework generated with the help of GenAI. It offers a glimpse of the potential for automating ontological descriptions for emergent and novel social, digital, collaborative spaces.\n- By incorporating a wide range of classes, properties, and relationships, the ontology starts to enable formalisation of:\n\t- Agents: Human users and AI entities, their attributes, skills, and relationships.\n\t- Scenes and Objects: Virtual environments, their characteristics, and interactive elements.\n\t- Digital Assets and Economy: Creation, ownership, and exchange of virtual goods and currencies.\n\t- Events and Interactions: Social gatherings, communication, and collaborative activities.\n\t- Governance and Policies: Decision-making processes, rules, and regulations within the metaverse.\n\t- Infrastructure and Technology: Hardware, software, and networking components enabling the metaverse.\n\t-\n- # AI Agent Refactor\n- ```mermaid\n  classDiagram\n  \n  %% -----------------------------------------------------------------------\n  %% 1. CORE: Agent-Oriented Ontology\n  %% -----------------------------------------------------------------------\n      class AgentOntologyEntity {\n          rdfs:label \"Agent Ontology Entity\"\n          rdfs:comment \"Root class for all concepts in the agent-focused ontology.\"\n      }\n  \n      class Agent {\n          rdfs:label \"Agent\"\n          rdfs:comment \"A general actor capable of autonomous or semi-autonomous behavior, including humans and AIs.\"\n          hasSkillProfile SkillProfile\n          hasPrivacySetting PrivacySetting\n          hasReputationScore ReputationScore\n          hasWallet DigitalWallet\n          hasInteractionPreference InteractionPreference\n      }\n  \n      class AIAgent {\n          rdfs:label \"AI Agent\"\n          rdfs:comment \"An agent driven by AI models, with specialized capabilities for autonomy and decision-making.\"\n          hasAIModel AIModel\n          decayFunction xsd:string\n      }\n  \n      class HumanAgent {\n          rdfs:label \"Human Agent\"\n          rdfs:comment \"A real human user. May be linked to a WebID for decentralized identity and authentication.\"\n      }\n  \n      class AIModel {\n          rdfs:label \"AI Model\"\n          rdfs:comment \"The underlying neural network or algorithm powering an AI Agent's intelligence.\"\n      }\n  \n      class SkillProfile {\n          rdfs:label \"Skill Profile\"\n          rdfs:comment \"Describes the abilities or competencies an agent (human or AI) can exhibit.\"\n      }\n  \n      class PrivacySetting {\n          rdfs:label \"Privacy Setting\"\n          rdfs:comment \"Rules and preferences regarding data usage, sharing, and retention for an agent.\"\n      }\n  \n      class ReputationScore {\n          rdfs:label \"Reputation Score\"\n          rdfs:comment \"Metric or rating reflecting an agent's trust or standing in a community.\"\n      }\n  \n      class DigitalWallet {\n          rdfs:label \"Digital Wallet\"\n          rdfs:comment \"Holds digital assets, cryptocurrencies, or tokens owned by an agent.\"\n      }\n  \n      class InteractionPreference {\n          rdfs:label \"Interaction Preference\"\n          rdfs:comment \"Specifies how an agent prefers to interact, moderate content, or engage with others.\"\n      }\n  \n  \n  %% -----------------------------------------------------------------------\n  %% 2. WEBID SUBCATEGORY: Decentralized Identity & Events\n  %% -----------------------------------------------------------------------\n      class WebIDEntity {\n          rdfs:label \"WebID Entity\"\n          rdfs:comment \"A conceptual parent for all objects in the WebID ecosystem.\"\n      }\n  \n      class Person {\n          rdfs:label \"Person\"\n          rdfs:comment \"Represents a decentralized identity and profile using WebID.\"\n          webid xsd:anyURI \"A unique WebID URI identifying this individual\"\n          name xsd:string \"A handle or short name (distinct from display_name)\"\n          display_name xsd:string \"A more user-facing display name\"\n          about xsd:string \"Short bio/about text\"\n          picture xsd:anyURI \"URI of the user‚Äôs profile image\"\n          banner xsd:anyURI \"URI of the user‚Äôs banner image\"\n          website xsd:anyURI \"User‚Äôs personal website\"\n      }\n  \n      class WebIDEvent {\n          rdfs:label \"WebID Event\"\n          rdfs:comment \"A generic event referencing a WebID identity, with timestamps, content, and possible signatures.\"\n          authorWebID xsd:anyURI \"The WebID of the event author\"\n          created_at xsd:dateTime \"Timestamp of creation\"\n          content xsd:string \"Event content or message\"\n          signature xsd:string \"Digital signature (e.g., for verifiability)\"\n      }\n  \n      class WebIDService {\n          rdfs:label \"WebID Service\"\n          rdfs:comment \"A decentralized or semi-decentralized service endpoint that stores or routes WebID events.\"\n          serviceURI xsd:anyURI \"The endpoint (e.g., HTTPS, WebSocket) providing service functionality\"\n      }\n  \n  \n  %% -----------------------------------------------------------------------\n  %% 3. IMMERSIVE REAL-TIME SUBCATEGORY\n  %% -----------------------------------------------------------------------\n      class ImmersiveRealTimeEntity {\n          rdfs:label \"Immersive Real-Time Entity\"\n          rdfs:comment \"Base class for objects and concepts within immersive real-time environments (3D spaces, XR, etc.).\"\n      }\n  \n      class ImmersiveScene {\n          rdfs:label \"Immersive Scene\"\n          rdfs:comment \"A real-time interactive environment or 'scene' supporting multi-agent presence.\"\n          governedBy GovernanceStructure\n          hasPolicy ImmersivePolicy\n      }\n  \n      class GovernanceStructure {\n          rdfs:label \"Governance Structure\"\n          rdfs:comment \"Rules, roles, and processes governing an immersive scene or community.\"\n      }\n  \n      class ImmersivePolicy {\n          rdfs:label \"Immersive Policy\"\n          rdfs:comment \"Behavioral and content guidelines within a particular immersive real-time environment.\"\n      }\n  \n      class SceneObject {\n          rdfs:label \"Scene Object\"\n          rdfs:comment \"Any interactive or visible object in an immersive real-time scene.\"\n      }\n  \n      class DigitalAsset {\n          rdfs:label \"Digital Asset\"\n          rdfs:comment \"A unique digital good or token that can be owned or traded in immersive or other digital contexts.\"\n          hasOwner Agent\n      }\n  \n      class VirtualEconomy {\n          rdfs:label \"Virtual Economy\"\n          rdfs:comment \"A system enabling trade and currency exchange in immersive or other digital environments.\"\n          regulatedBy EconomicPolicy\n          hasMarketplace Marketplace\n      }\n  \n      class Marketplace {\n          rdfs:label \"Marketplace\"\n          rdfs:comment \"A platform or location to trade digital assets (NFTs, tokens, etc.).\"\n      }\n  \n      class EconomicPolicy {\n          rdfs:label \"Economic Policy\"\n          rdfs:comment \"Regulations defining currency issuance, exchange rules, and other economic mechanics.\"\n      }\n  \n  \n  %% -----------------------------------------------------------------------\n  %% 4. RELATIONSHIPS & INHERITANCE\n  %% -----------------------------------------------------------------------\n  \n      %% A. Ontology Roots & Subcategories\n      AgentOntologyEntity <|-- Agent\n      AgentOntologyEntity <|-- WebIDEntity\n      AgentOntologyEntity <|-- ImmersiveRealTimeEntity\n  \n      %% B. Agent Hierarchy\n      Agent <|-- AIAgent\n      Agent <|-- HumanAgent\n  \n      %% C. WebID\n      WebIDEntity <|-- Person\n      WebIDEntity <|-- WebIDEvent\n      WebIDEntity <|-- WebIDService\n      HumanAgent -- Person : \"may link to/represent\"\n  \n      %% D. Immersive Real-Time\n      ImmersiveRealTimeEntity <|-- ImmersiveScene\n      ImmersiveRealTimeEntity <|-- DigitalAsset\n      ImmersiveRealTimeEntity <|-- VirtualEconomy\n      ImmersiveRealTimeEntity <|-- Marketplace\n      ImmersiveRealTimeEntity <|-- GovernanceStructure\n      ImmersiveRealTimeEntity <|-- ImmersivePolicy\n      ImmersiveRealTimeEntity <|-- SceneObject\n  \n      %% E. Detailed Links\n      AIAgent \"1\" *-- \"1\" AIModel : hasAIModel\n      Agent \"1\" *-- \"0..*\" SkillProfile : hasSkillProfile\n      Agent \"1\" *-- \"0..*\" PrivacySetting : hasPrivacySetting\n      Agent \"1\" *-- \"0..*\" ReputationScore : hasReputationScore\n      Agent \"1\" *-- \"0..*\" DigitalWallet : hasWallet\n      Agent \"1\" *-- \"1\" InteractionPreference : hasInteractionPreference\n  \n      %% Example of AI Agents creating scene objects (optional)\n      AIAgent \"1\" *-- \"*\" SceneObject : canCreateVariations\n  \n      ImmersiveScene \"1\" *-- \"0..*\" GovernanceStructure : governedBy\n      ImmersiveScene \"1\" *-- \"0..*\" ImmersivePolicy : hasPolicy\n  \n      DigitalAsset \"1\" *-- \"1\" Agent : hasOwner\n  \n      VirtualEconomy \"1\" *-- \"0..*\" EconomicPolicy : regulatedBy\n      VirtualEconomy \"1\" *-- \"0..*\" Marketplace : hasMarketplace\n  \n  ```\n-\n- # Mycelium Experiment\n\t- This idea is developed further here:\n\t\t- [[Agentic Mycelia]]\n\t\t- This is an [[Ontology conversation with AIs]]\n\n## Current Landscape (2025)\n\n- Industry adoption and implementations\n  - Metaverse platforms continue to evolve with focus on interoperability and open standards\n  - Web3 integration accelerating with decentralised identity and asset ownership\n  - Enterprise adoption growing in virtual collaboration, training, and digital twins\n  - UK companies increasingly active in metaverse development and immersive technologies\n\n- Technical capabilities\n  - Real-time rendering at photorealistic quality levels\n  - Low-latency networking enabling seamless multi-user experiences\n  - AI-driven content generation and procedural world building\n  - Spatial audio and haptics enhancing immersion\n\n- UK and North England context\n  - Manchester: Digital Innovation Factory supports metaverse startups and research\n  - Leeds: Holovis leads in immersive experiences for entertainment and training\n  - Newcastle: University research in spatial computing and interactive systems\n  - Sheffield: Advanced manufacturing using digital twin technology\n\n- Standards and frameworks\n  - Metaverse Standards Forum driving interoperability protocols\n  - WebXR enabling browser-based immersive experiences\n  - glTF and USD for 3D asset interchange\n  - Open Metaverse Interoperability Group defining cross-platform standards\n\n## Metadata\n\n- **Last Updated**: 2025-11-16\n- **Review Status**: Automated remediation with 2025 context\n- **Verification**: Academic sources verified\n- **Regional Context**: UK/North England where applicable",
  "properties": {
    "id": "education-metaverse-ontology",
    "collapsed": "true",
    "- ontology": "true",
    "- term-id": "20310",
    "- preferred-term": "Education Metaverse",
    "- source-domain": "metaverse",
    "- status": "draft",
    "- is-subclass-of": "[[Blockchain]]",
    "- public-access": "true"
  },
  "backlinks": [],
  "wiki_links": [
    "Heeter1992; @Biocca1997",
    "RGB and Client Side Validation",
    "Jones2009",
    "authenticVolume2022",
    "Jones1971",
    "Pejsa2016",
    "Van_Baren2004; @Harms2004",
    "Krauss1996",
    "Yee2007; @Bailenson2001; @Bailenson2003",
    "mystakidis2022metaverse",
    "Hauber2006",
    "Ichikawa1995; @Okada1994",
    "Bardram2012",
    "Parsons1974",
    "Murray2009",
    "Iverson2005",
    "Macrae2002",
    "Tang2010",
    "Blanche2010; @Tay2008",
    "Desai2011",
    "Vertegaal2002",
    "Hall1969",
    "Steinmeyer2013",
    "Some legacy Linked-JSON",
    "Steptoe2009",
    "Carlston2013",
    "Stiefelhagen2001; @Stiefelhagen2002",
    "Kleinke1986; @Nguyen2009",
    "Gemmell2000; @Kuster2012",
    "Gillies2005",
    "Argyle1976",
    "Moubayed2012",
    "Yendo2010",
    "Ou2005",
    "Loomis2008; @Fullwood2006",
    "Risko2011",
    "Beck2011; @Schuemie2001",
    "Fagel2010",
    "Itoh2005",
    "Lee2011; @Sakamoto2007; @Tsui2011",
    "Garrison1999; @Beck2011",
    "Moubayed2012; @Vishwanath2005; @Anstis1969; @Wollaston1824",
    "Sellen1992",
    "Anthropic Claude",
    "ChatGPT",
    "Goldin-Meadow1999",
    "Tse2007",
    "Bulu2012",
    "Bente1998; @Slessor2008; @Argyle1988; @Hofmann2006; @Pan2008",
    "Logseq",
    "Loomis2012",
    "Bloom2015",
    "Social contract and jobs",
    "Kulik2011",
    "Kendon1967",
    "St_John2001",
    "Stiefelhagen2002",
    "Wilson2000",
    "Ekman1993; @Chovil1991",
    "Bartneck2007; @Bartneck2009",
    "Ishii1993",
    "doerr2018measure",
    "nevelsteen2018virtual",
    "Stiefelhagen2001",
    "Van_Eijk2010; @Bock2008",
    "Otsuka2013",
    "Argyle1988; @Argyle1976; @Argyle1965; @Argyle1976; @Argyle1969; @Kendon1967; @Monk2002",
    "Berger1975",
    "gibson2019neuromancer",
    "Vertegaal2000",
    "Otsuka2005",
    "Biocca2003",
    "Hofmann2006",
    "Agentic Mycelia",
    "Yee2007",
    "Goodwin2000",
    "Nagendran2012",
    "Criminisi2003; @Van_Eijk2010",
    "Oyekoya2012; @Pan2014; @Pan2012",
    "Large language models",
    "Nguyen2005",
    "Chartrand1999",
    "Gunawardena1997",
    "Bartneck2009",
    "Raskar2001",
    "Slater1999",
    "Argyle1969; @Kleinke1986",
    "fox2015neural",
    "Sellen1995",
    "Vertegaal1997",
    "Boker2011",
    "Vertegaal2001",
    "Pan2014",
    "Fulk1987; @Haythornthwaite1995",
    "Rivera-Gutierrez2012",
    "Dalsgaard2011",
    "Slater2004",
    "Bailenson2001",
    "Rosenthal1947",
    "Roberts2015",
    "Argyle1988",
    "Edigo1988",
    "Facebook Meta",
    "Marti2005",
    "Ontology conversation with AIs",
    "park2022metaverse",
    "Watson1966; @Argyle1988",
    "Blascovich2002",
    "aiken2020zooming",
    "Tuddenham2009",
    "Williams1977",
    "barrero2021working",
    "ondrejka2004escaping",
    "Kleinsmith2013",
    "Jouppi2002",
    "Cashu",
    "Schegloff1998",
    "Deepfakes and fraudulent content",
    "Roblox",
    "Maeda2004",
    "Sakamoto2007",
    "Holm2010",
    "Bondareva2004; @Jouppi2002",
    "Luff2011",
    "Privacy, Trust and Safety",
    "Fels2000; @Sellen1992",
    "Porter2008",
    "Heldner2010",
    "Bohannon2013",
    "Chen2002",
    "Langton2000",
    "Kauff2002; @Kauff2002a",
    "Lincoln2010",
    "Hancock2004",
    "Bandyopadhyay2001",
    "Ting-Toomey2012",
    "Argyle1969",
    "Hager1979; @Argyle1988",
    "Tsui2012",
    "Aoki2003",
    "Regenbrecht2015",
    "katagiri2007aiduti",
    "Nowak2001",
    "Adamczyk2007",
    "Eldes2013",
    "Symons2004",
    "schiano2004",
    "Bondareva2004",
    "Troje1998",
    "Jouppi2004",
    "Ethan Mollick",
    "Metaverse and Telecollaboration",
    "Omniverse",
    "ohba1998",
    "Blockchain",
    "Hedge1978",
    "Knowledge Graphing",
    "Cuddihy2000",
    "Bitcoin",
    "Loomis2008",
    "Hauber2005; @Gunawardena1997",
    "Mori1970",
    "Argyle1965",
    "mclellan1993avatars",
    "Generative AI",
    "Ho2008",
    "Mitchell2011",
    "Cook1977; @Kleinke1986; @Fagel2010",
    "Rienks2010",
    "Beck2011",
    "Bruno2013",
    "Dourish1996",
    "Harms2004",
    "Lightning and Similar L2",
    "Kleinke1986",
    "Voida2012",
    "Monk2002",
    "Gemini",
    "Ekman1993",
    "Nguyen2009",
    "prakash2020characteristic",
    "Colburn2000",
    "Jamil2011; @Jetter2011",
    "Wolff2008",
    "Slessor2008; @Hofmann2006",
    "Novick1996",
    "Lincoln2009; @Lincoln2010; @Raskar2001",
    "hennig2022social",
    "Bailenson2002",
    "Usoh2000",
    "Raskar1998",
    "OMalley1996",
    "Deckers2013; @Gibson1963",
    "Mihoub2015",
    "Vertegaal2003",
    "Benjamin2012",
    "Education and AI",
    "Kristoffersson2011",
    "Schrammel2007",
    "oeppen2020human",
    "Adalgeirsson2010; @Lee2011; @Tsui2011; @Paulos1998; @Kristoffersson2013",
    "xi2022challenges",
    "Kollock1998",
    "Norris2013; @Norris2012",
    "Roberts2013",
    "Kristoffersson2013",
    "Gotsch2018",
    "biener2022quantifying; @luong2022demographic",
    "Strait2014",
    "Drobyshev22MP",
    "Steptoe2010",
    "Vishwanath2005; @Anstis1969; @Wollaston1824",
    "Buxton1992",
    "Argyle1969; @Argyle1988; @Cook1977",
    "Tang2006",
    "Fuchs2002",
    "Kluttz2009"
  ],
  "ontology": {
    "term_id": "20310",
    "preferred_term": "Education Metaverse",
    "definition": "",
    "source_domain": "metaverse",
    "maturity_level": null,
    "authority_score": null
  }
}