{
  "title": "Stable Diffusion",
  "content": "- ### OntologyBlock\n  id:: stable-diffusion-ontology\n  collapsed:: true\n\t- ontology:: true\n\t- term-id:: mv-819761654713\n\t- preferred-term:: Stable Diffusion\n\t- source-domain:: metaverse\n\t- status:: active\n\t- public-access:: true\n\t- definition:: A component of the metaverse ecosystem focusing on stable diffusion.\n\t- maturity:: mature\n\t- authority-score:: 0.85\n\t- owl:class:: mv:StableDiffusion\n\t- owl:physicality:: ConceptualEntity\n\t- owl:role:: Concept\n\t- belongsToDomain:: [[MetaverseDomain]]\n\t- #### Relationships\n\t  id:: stable-diffusion-relationships\n\t\t- enables:: [[ImmersiveExperience]], [[Presence]], [[SpatialComputing]]\n\t\t- requires:: [[DisplayTechnology]], [[TrackingSystem]], [[RenderingEngine]]\n\t\t- bridges-to:: [[HumanComputerInteraction]], [[ComputerVision]], [[Robotics]]\n\n\t- #### OWL Axioms\n\t  id:: stable-diffusion-owl-axioms\n\t  collapsed:: true\n\t\t- ```clojure\n\t\t  Declaration(Class(mv:StableDiffusion))\n\n\t\t  # Classification\n\t\t  SubClassOf(mv:StableDiffusion mv:ConceptualEntity)\n\t\t  SubClassOf(mv:StableDiffusion mv:Concept)\n\n\t\t  # Domain classification\n\t\t  SubClassOf(mv:StableDiffusion\n\t\t    ObjectSomeValuesFrom(mv:belongsToDomain mv:MetaverseDomain)\n\t\t  )\n\n\t\t  # Annotations\n\t\t  AnnotationAssertion(rdfs:label mv:StableDiffusion \"Stable Diffusion\"@en)\n\t\t  AnnotationAssertion(rdfs:comment mv:StableDiffusion \"A component of the metaverse ecosystem focusing on stable diffusion.\"@en)\n\t\t  AnnotationAssertion(dcterms:identifier mv:StableDiffusion \"mv-819761654713\"^^xsd:string)\n\t\t  ```\n\npublic:: true\n\n- #Public page automatically published\n- Stable Diffusion has emerged as a transformative force in generative AI, mainly for text to image synthesis. This open source model was developed by UK company 'Stability AI', has democratised access to high quality image workflows, empowering artists, creatives, and professionals.\n- # Why Stable Diffusion?\n  id:: 66408f9e-30e0-442b-9aba-9eb51e36a739\n\t- ## Image, Video and 3D\n\t\t- [[Stable Diffusion]] and [[Stable Video Diffusion]] allow a lot of control, but at a cost of complexity.\n\t\t  collapsed:: true\n\t\t- ## Stable Diffusion 1.5, XL, and 3\n\t\t\t- UK company with global impact. It is likely now winding up it's operations after difficulty generating revenue in the hyper competitive GenAI market.\n\t\t\t\t- **Introduction**: Open-source model by StabilityAI\n\t\t\t\t\t- **Cost**: Free to run on own hardware; nominal fee for online tools.\n\t\t\t\t\t- **User Interface**: User-friendly through platforms like Leonardo.AI.\n\t\t\t\t\t- **Strengths**: Unlimited control, good image quality, no censorship.\n\t\t\t\t\t- **Weaknesses**: Requires decent hardware, steep learning curve. Questions about Stability business.\n\t\t\t\t\t- **Skill Level**: Intermediate to advanced.\n\t\t\t- ### Text-to-Image Generation\n\t\t\t\t- Stable Diffusion generates realistic and imaginative images from descriptive text prompts. This core functionality allows users to translate their creative visions into visual form with remarkable accuracy and detail. Whether it's a photorealistic portrait, a surreal landscape, or an abstract concept, Stable Diffusion can bring your ideas to life with just a few words.\n\t\t\t\t- A lot of the products you see on the market are either wrappers for the big AI companies, or else leveraging Stability models on rented cloud compute.\n\t\t\t\t\t- ![ComfyUI_temp_exgja_00013_.png](../assets/ComfyUI_temp_exgja_00013_1702592236908_0.png){:width 800}\n\t\t\t\t\t- ![Rife_00027.mp4](../assets/Rife_00027_1702831574413_0.mp4){:width 300}\n\t- ## Open Source\n\t\t- Stable Diffusion's open-source nature sets it apart from many other generative AI models.\n\t\t- Users have free access to the model's weights and a lot of modular code, allowing them to modify, distribute, and build upon it.\n\t\t- This openness fosters collaboration, innovation, and community driven development.\n\t\t- Ensures that the technology is not controlled by a select few entities.\n\t\t- For brands and private companies this allows private development of digital assets.\n\t- ## User Friendly Interfaces\n\t\t- Platforms like [Leonardo.AI](https://leonardo.ai/), [RunDiffusion](https://rundiffusion.com/) and [Automatic1111's WebUI](https://github.com/AUTOMATIC1111/stable-diffusion-webui) provide intuitive and user friendly interfaces for interacting with Stable Diffusion.\n\t\t- ### Rundiffusion\n\t\t\t- <iframe src=\"https://rundiffusion.com/\" style=\"width: 800px; height: 600px\"></iframe>\n\t\t- These interfaces offer a range of options for customizing parameters, fine tuning models, and experimenting with different artistic styles.\n\t- ### Customisation\n\t\t- Stable Diffusion's flexibility extends to its ability to be fine-tuned on custom datasets.\n\t\t- Techniques like  [[KOHYA Dreambooth and similar]] and  [[LoRA DoRA etc]] training   allow users to tailor the model to their specific needs and generate images that align with their unique artistic visions or domain-specific requirements.\n\t\t  :LOGBOOK:\n\t\t  CLOCK: [2024-05-12 Sun 11:12:30]--[2024-05-12 Sun 11:12:31] =>  00:00:01\n\t\t  :END:\n\t\t\t- ![ComfyUI_temp_ayipz_00012_.png](../assets/ComfyUI_temp_ayipz_00012_1702330298489_0.png){:width 300, :height 402}\n\t\t- This opens up a world of possibilities for creating personalised images,\n\t\t\t- Generating images of specific objects or individuals,\n\t\t\t- Developing models for specialised domains like  [[Fashion]]  or architectural design.\n\t- ### Community Support\n\t\t- One of Stable Diffusion's greatest strengths is its vibrant and active community.\n\t\t- Much of this happens on Discord and Reddit\n\t\t\t- [(1832) Discord | #ad_resources | banodoco](https://discord.com/channels/1076117621407223829/1149372684220768367)\n\t\t\t- ![image.png](../assets/image_1715715718817_0.png){:width 800}\n\t\t\t- [comfyui (reddit.com)](https://www.reddit.com/r/comfyui/)\n\t\t\t- ![image.png](../assets/image_1715715792127_0.png){:width 800}\n\t\t\t- The [StableDiffusion subreddit](https://www.reddit.com/r/StableDiffusion/)\n\t\t\t- The [Stability AI Discord](https://discord.gg/stabilityai) serve as hubs for sharing creations, resources, and tutorials.\n\t\t\t- This collaborative environment fosters learning, inspiration, and rapid innovation\n\t\t- <iframe src=\"https://openaijourney.com/comfyui-guide/\" style=\"width: 800px; height: 600px\"></iframe>\n\t\t- <iframe src=\"https://comfyworkflows.com\" style=\"width: 900px; height: 600px\"></iframe>\n\t- ## Core Models\n\t\t- ~~Stable Diffusion 1.4~~\n\t\t- ### Stable Diffusion 1.5\n\t\t\t- [Available on GitHub](https://github.com/CompVis/stable-diffusion), this model is optimized for speed and efficiency,\n\t\t\t- Suitable for generating images quickly, especially on less powerful hardware.\n\t\t\t- Highest model diversity\n\t\t- ~~Stable Diffusion 2.1~~\n\t\t- ### SDXL\n\t\t\t- Higher resolution, better prompt control\n\t\t\t- Will often mess up human bodies due to constrained training\n\t\t\t- More resource intensive\n\t\t\t- Less compatible extensions\n\t\t- ### CosXL\n\t\t\t- Likely the last update from the team, most of whom have left following the departure of founder [Emad Mostaque](https://en.wikipedia.org/wiki/Emad_Mostaque).\n\t\t\t- This is a \"best practice\" update to SDXL which allows higher contrast.\n\t\t- ### Zero123 & SV3D\n\t\t\t- {{video https://youtu.be/gl1s7f0mV2E?t=677}}\n\t\t- ### ~~Stable Cascade~~\n\t\t\t- Only a partial release.\n\t\t\t- Not great adoption.\n\t\t\t- Better prompt adherence.\n\t\t- ### Stable Diffusion 3\n\t\t\t- [Temporary Stable Diffusion 3 Ban | Civitai](https://civitai.com/articles/5732/temporary-stable-diffusion-3-ban)\n\t\t\t- Might be ok in the end.\n\t\t\t- Whole new architecture.\n\t\t\t- Excellent prompt following.\n\t\t\t- Terrible human anatomy.\n\t- # Community models\n\t\t- Models and inspiration from CivitAI, which is very often \"not safe for work\" so do exercise caution.\n\t\t\t- <iframe src=\"https://civitai.com/images\" style=\"width: 100%; height: 600px\"></iframe>\n- ### Prompt Engineering: The Art of Guiding AI Creativity\n\t- Effective prompt engineering is crucial for unlocking the full potential of Stable Diffusion. Different models demand different styles\n\t- Here are some tips to enhance your prompts:\n\t\t- #### Specificity:\n\t\t\t- Use specific keywords and descriptive phrases to clearly convey your desired image to the AI model.\n\t\t\t- The more precise and detailed your prompt, the better the model can understand your intent and generate images that match your vision.\n\t\t- #### Negative Prompts:\n\t\t\t- Utilize negative prompts to exclude unwanted elements or styles from the generated image.\n\t\t\t- This allows you to refine the output and avoid generating images with undesirable features.\n\t\t- #### Compositional Control:\n\t\t\t- Employ prompt scheduling and area prompting to create complex compositions and focus on specific details.\n\t\t\t- These techniques allow you to control the timing and location of different elements within the image, resulting in more intricate and visually compelling outputs.\n\t\t- #### Extensions:\n\t\t\t- Leverage extensions like \"Test My Prompt\" to understand the impact of each word in your prompt and refine your wording for better results. This extension helps you analyse how the model interprets different words and phrases, allowing you to optimize your prompts for the desired outcome.\n\t\t- #### Experimentation:\n\t\t\t- Don't be afraid to experiment with different models, fine tuning techniques, and prompt styles to discover new possibilities and achieve your desired artistic outcomes.\n\t\t\t- The beauty of Stable Diffusion lies in its flexibility and the endless creative potential it offers.\n- ### Applications Across Industries:\n\t- Stable Diffusion's versatility has led to its adoption across various industries:\n\t\t- #### Digital Art Creation:\n\t\t\t- Artists are using Stable Diffusion to create stunning and innovative digital artworks, pushing the boundaries of artistic expression and exploring new creative frontiers.\n\t\t\t  Concept Visualization:\n\t\t- #### Designers and engineers\n\t\t\t- Use Stable Diffusion to quickly generate visual representations of their ideas, facilitating rapid prototyping and concept development. This allows for faster iteration and improved communication within design teams.\n\t\t\t  Character Design:\n\t\t- #### Game developers and animators\n\t\t\t- Leverage Stable Diffusion to create unique and memorable characters, streamlining the design process and reducing the time and resources required for character creation.\n\t\t\t  Illustration:\n\t\t- #### Illustrators\n\t\t\t- Can use Stable Diffusion to generate high-quality illustrations for books, magazines, and other media, offering a faster and more efficient way to produce visually compelling artwork.\n\t\t- #### Virtual Production:\n\t\t\t- Filmmakers and VFX artists can use Stable Diffusion to generate realistic backgrounds and environments for virtual production shoots, offering a cost-effective and efficient alternative to traditional green screen techniques.\n- ### Addressing Hardware Limitations:\n  While Stable Diffusion requires a decent GPU for optimal performance, several solutions are emerging to address hardware limitations:\n  Cloud-based Solutions: Platforms like [RunDiffusion](https://app.rundiffusion.com/)\n- https://www.forbes.com/sites/iainmartin/2024/03/20/key-stable-diffusion-researchers-leave-stability-ai-as-company-flounders/\n  Stable diffusion is a company that specializes in developing advanced artificial intelligence models. They are known for their expertise in creating generative models, which are capable of producing high-quality and realistic outputs in various domains such as image synthesis, language generation, and music composition. Stable Diffusion's cutting-edge research and innovative approaches have made significant contributions to the field of generative AI.\n- ## Stable diffusion\n\t- is a company that specializes in developing advanced artificial intelligence models. They are known for their expertise in creating generative models, which are capable of producing high-quality and realistic outputs in various domains such as image synthesis, language generation, and music composition. Stable Diffusion's cutting-edge research and innovative approaches have made significant contributions to the field of generative AI.\n\t- [Illustrated overview](https://jalammar.github.io/illustrated-stable-diffusion/)\n\t- Stable diffusion XL muse GPT [Stable Diffusion Muse SDXL GPT Prompt Generator | Civitai](https://civitai.com/articles/2928/stable-diffusion-muse-sdxl-gpt-prompt-generator)\n\t- [Automatic1111 GUI and user guide](https://www.thosesixfaces.com/post/stable-diffusion-getting-started-windows)\n\t- [citivia browser](https://github.com/Vetchems/sd-civitai-browser)\n\t- [Automatic WebUI](https://github.com/AUTOMATIC1111/stable-diffusion-webui)\n\t- Vlads next SD\n\t- [InvokeAI simple interface](https://invoke-ai.github.io/InvokeAI/)\n\t- ### Prompt engineering links\n\t\t- https://phraser.tech/\n\t\t- [Artist keywords that are known to work](https://docs.google.com/document/d/1SaQx1uJ9LBRS7c6OsZIaeanJGkUdsUBjk9X4dC59BaA/edit#)\n\t\t- https://promptomania.com/stable-diffusion-prompt-builder/\n\t\t- https://www.krea.ai/\n\t\t- [Lexica](https://lexica.art/)\n\t\t- [Dall-E prompt engineering](https://docs.google.com/document/d/11WlzjBT0xRpQhP9tFMtxzd0q6ANIdHPUBkMV-YB043U/edit#)\n\t\t- [public prompts guy](https://publicprompts.art/)\n\t\t- [Promptimize testing suite for prompts](https://github.com/preset-io/promptimize)\n\t\t- [Photoshop plugin](https://christiancantrell.com/#ai-ml)\n\t- ### Dreambooth retraining for faces\n\t\t- [windows instructions](https://pastebin.com/xcFpp9Mr)\n\t\t- [Discord server](https://discord.com/channels/1023277529424986162/)\n\t\t- [dreambooth for SD2](https://github.com/nitrosocke/dreambooth-training-guide/blob/main/README.md#how-to-fine-tune-stable-diffusion-20)\n\t- ### Birme image resizer\n\t\t- [2 hour tutorial](https://www.youtube.com/watch?v=Bdl-jWR3Ukc&t=34)\n\t\t- [inject your face into any model (dreambooth)](https://www.youtube.com/watch?v=s25hcW4zq4M)\n\t\t- [Guide for dreambooth](https://github.com/nitrosocke/dreambooth-training-guide)\n\t\t- [Shivram](https://github.com/ShivamShrirao/diffusers/tree/main/examples/dreambooth)\n\t\t- [Progen photorealism Miro guide](https://miro.com/app/board/uXjVPzJyAtU=/)\n\t\t- [rare dreambooth tokens](https://github.com/2kpr/dreambooth-tokens)\n\t\t- [Multi subject tokens](https://medium.com/@yushantripleseven/using-captions-with-dreambooth-joepenna-dreambooth-716f5b9e9866)\n\t\t- [tag editor](https://github.com/toshiaki1729/stable-diffusion-webui-dataset-tag-editor)\n\t\t- [SDXL dreambooth](https://medium.com/@yushantripleseven/dreambooth-training-sdxl-using-kohya-ss-windows-7d2491460608)\n\t\t- [Lora guide](https://civitai.com/articles/1771)\n\t\t- [stable swarm distributed comfyui](https://github.com/Stability-AI/StableSwarmUI)\n\t\t- [Textual inversion](https://www.reddit.com/r/StableDiffusion/comments/10gs4s2/new_expert_tutorial_for_textual_inversion_text/)\n\t\t- [Img2Img guide from reddit for face mapping](https://www.reddit.com/r/StableDiffusion/comments/xgurs3/testing_img2img_batch_processing_i_convert_this/)\n\t\t- [textual inversion cheaper training](https://github.com/rinongal/textual_inversion)\n\t\t- [CIO blog post](https://danieljeffries.substack.com/p/the-turning-point-for-truly-open?sd=pf)\n\t\t- [google stable diffusion](https://www.youtube.com/watch?v=lHcPtbZ0Mnc)\n\t\t- [Cross attention replace named items](https://github.com/bloc97/CrossAttentionControl)\n\t\t- [256 x faster speedup](https://the-decoder.com/stable-diffusion-could-soon-generate-images-much-faster/)\n\t\t- [VoltaML acceleration](https://github.com/VoltaML/voltaML-fast-stable-diffusion)\n\t\t- [Depth map into blender from SD2](https://www.youtube.com/watch?v=AeDngG9kQNI)\n\t\t- [midjourney tweaks](https://www.reddit.com/r/StableDiffusion/comments/z622mp/trained_midjourney_embedding_on_stable_diffusion/)\n\t\t- [and another](https://civitai.com/models/1253/anthro)\n\t\t- [Updates Pastebin](https://rentry.org/sdupdates3)\n\t\t- [Game development using SD](https://www.heroo.ai/)\n\t\t- [Wildcard manager using ChatGPT](https://github.com/mattjaybe/sd-wildcards)\n\t\t- [Depth2Img for text](https://www.reddit.com/r/StableDiffusion/comments/10c9kg8/depth2img_works_well_for_text_inputs/)\n\t\t- [train chat GPT to write prompts](https://dreamlike.art/guides/using-openai-chat-gpt-to-write-stable-diffusion-prompts)\n\t\t- [non destructive image manipulation using seeds](https://www.reddit.com/r/StableDiffusion/comments/10no6tp/non_destructive_image_variation_in_text2image/)\n\t\t- [Instruct pix2pix](https://www.reddit.com/r/StableDiffusion/comments/10l74sl/instruct_pix2pix_is_amazing_inpaintingimg2img/)\n\t\t- [reddit post](https://www.reddit.com/r/StableDiffusion/comments/10tjzmf/instructpix2pix_is_built_straight_into_the/)\n\t\t- [Attention heatmap for prompts (youtube)](https://www.youtube.com/watch?v=XiKyEKJrTLQ)\n\t\t- [enormous link roundup](https://rentry.org/RentrySD/)\n\t\t- [Prompt master variations management](https://github.com/hoblin/prompt-master)\n\t\t- [panoramic world builder](https://huggingface.co/congazverse/worldBuilder)\n\t\t- [GitHub AbdullahAlfaraj/Auto-Photoshop-StableDiffusion-Plugin: A user-friendly plug-in that makes it easy to generate stable diffusion images inside Photoshop using Automatic1111-sd-webui as a backend.](https://github.com/abdullahalfaraj/auto-photoshop-stablediffusion-plugin)\n\t\t- [GitHub ashawkey/stable-dreamfusion: A pytorch implementation of text-to-3D dreamfusion, powered by stable diffusion.](https://github.com/ashawkey/stable-dreamfusion)\n\t\t- [Fine tune stable diffusion](https://github.com/nitrosocke/dreambooth-training-guide/blob/main/readme.md#how-to-fine-tune-stable-diffusion-20)\n\t\t- [GitHub Sanster/lama-cleaner: Image inpainting tool powered by SOTA AI Model. Remove any unwanted object, defect, people from your pictures or erase and replace(powered by stable diffusion) any thing on your pictures.](https://github.com/sanster/lama-cleaner)\n\t\t- [holovolo immersive volumetric VR180 videos and photos, and 3D stable diffusion, for Quest and WebVR](https://holovolo.tv)\n\t\t- [The Illustrated Stable Diffusion Jay Alammar Visualizing machine learning one concept at a time.](https://jalammar.github.io/illustrated-stable-diffusion/)\n\t\t- [reddit educational links](https://www.reddit.com/r/StableDiffusion/comments/116ki29/comment/j97jac3/)\n\t\t- [Negative prompt hack tip](https://www.reddit.com/r/StableDiffusion/comments/11pcsxe/just_discovered_a_useful_trick_for_getting_good/)\n\t\t- [Modify images with text](https://github.com/justinpinkney/stable-diffusion/blob/main/notebooks/imagic.ipynb)\n\t\t- [Photorealism](https://www.reddit.com/r/StableDiffusion/comments/11u2p0u/lazy_guide_to_photorealistic_images/)\n\t\t- [sdtools image v 1.6](https://www.reddit.com/r/StableDiffusion/comments/127gck9/sdtools_v16/)\n\t\t- [Character plugin](https://github.com/alexv0iceh/AutoChar)\n\t- [[Checkpoints]]\n\t\t- [Stability specific tools](https://sdtools.org/)\n\t\t- Arible Prompt Database https://www.arible.co/prompts\n\t\t- [[Guide] Make your own Loras, easy and free | Stable Diffusion Other | Civitai: You don't need to download anything, this is a guide with online tools. Click \"Show more\" below.](https://civitai.com/models/22530)\n\t\t- [sdxl lora training](https://github.com/FurkanGozukara/Stable-Diffusion/blob/main/Tutorials/How-To-Install-And-Use-Kohya-GUI-And-Do-Ultra-Realistic-SDXL-Training-Tutorial.md)\n\t\t- [dylora scripts](https://github.com/facebookresearch/dadaptation/issues/24)\n\t\t- [kohya fork with scripts](https://github.com/bmaltais/kohya_ss#about-sdxl-training)\n\t\t- [lora of loras (compressed sets)](https://huggingface.co/FFusion/400GB-LoraXL/tree/main)\n\t\t- [chart of print size aspect ratios](https://www.reddit.com/r/StableDiffusion/comments/10wqv7r/when_it_comes_to_printing_converting_resolutions/)\n\t\t- [SDXL native text lora](https://civitai.com/models/176555/harrlogos-xl-finally-custom-text-generation-in-sd)\n\t\t- [SDXL lcm motion lora](https://huggingface.co/latent-consistency/lcm-lora-sdxl)\n\t\t- SDXL universal negative prompt\n\t\t\t- text, watermark, low-quality, signature, moiré pattern, downsampling, aliasing, distorted, blurry, glossy, blur, jpeg artifacts, compression artifacts, poorly drawn, low-resolution, bad, distortion, twisted, excessive, exaggerated pose, exaggerated limbs, grainy, symmetrical, duplicate, error, pattern, beginner, pixelated, fake, hyper, glitch, overexposed, high-contrast, bad-contrast\n\t\t- [SDXL prodigy training guide](https://civitai.com/articles/1022)\n\t\t- [Lora training interface for windows](https://github.com/bmaltais/kohya_ss)\n\t\t- [Refined model](https://civitai.com/models/8392/refined)\n\t\t- [Fine tuning with captioning and other fine tuning tricks, followfox](https://substack.com/profile/110613456-followfoxai)\n\t\t- [Negative embedding textual inversion for hands etc](https://huggingface.co/datasets/Nerfgun3/bad_prompt)\n\t\t- [GitHub kpthedev/ez-text2video: Easily run text-to-video diffusion with customized video length, fps, and dimensions on 4GB video cards, as well as on CPU.](https://github.com/kpthedev/ez-text2video)\n\t\t- [Gligen grounding capability for sd1.5](https://gligen.github.io/)\n\t\t- [This repository contains a ComfyUI Extension for Automated Text Generation. The extension provides nodes which can be used to automate the text generation process. The goal is to build a node-based Automated Text Generation AGI. This extension should ultimately combine all of the features of the existing text generation tools into one tool.](https://github.com/xXAdonesXx/NodeGPT)\n\t\t- [[R] Text-to-image Diffusion Models in Generative AI: A Survey: r/MachineLearning](https://www.reddit.com/r/MachineLearning/comments/12ehcez/r_texttoimage_diffusion_models_in_generative_ai_a/)\n\t\t- [Tutorial: Creating a Consistent Character as a Textual Inversion Embedding](https://github.com/BelieveDiffusion/tutorials/discussions/3)\n\t\t- [Segment anything webui](https://www.reddit.com/r/StableDiffusion/comments/12hkdy8/sd_webui_segment_everything/)\n\t\t- [segment anything training](https://github.com/NielsRogge/Transformers-Tutorials/tree/master/SAM)\n\t\t- [Nvidia stable diffusion segment through clip](https://github.com/NVlabs/ODISE)\n\t\t- [Overriding iphone footage with SD characters using controlnet](https://www.reddit.com/r/StableDiffusion/comments/12lg8mn/override_more_experiments_overriding_the_original/)\n\t\t- [Interactive photo manipulation GAN](https://huggingface.co/papers/2305.10973)\n\t\t- [3d plugin for Automatic1111](https://github.com/jtydhr88/sd-webui-3d-editor)\n\t\t- [Face replace plugin for automatic](https://github.com/Gourieff/sd-webui-roop-nsfw)\n\t\t- ## Images\n\t\t\t- [Colour palette extraction](https://github.com/mattdesl/gifenc)\n\t\t\t- [Text based real time image manipulation](https://arxiv.org/abs/2210.09276)\n\t\t\t- [Sketch guided text to image inference](https://sketch-guided-diffusion.github.io/)\n\t\t\t- [Google prompt to prompt image remodeller](https://www.youtube.com/watch?v=lHcPtbZ0Mnc)\n\t\t\t- [github](https://github.com/google/prompt-to-prompt)\n\t\t\t- [Img2Prompt](https://replicate.com/methexis-inc#)\n\t\t\t- [eDiffi nvidia text to image](https://deepimagination.cc/eDiffi/)\n\t\t\t- [Image to caption](https://laion.ai/blog/laion-coco/)\n\t\t\t- [lama image cleanup](https://github.com/Sanster/lama-cleaner)\n\t\t\t- [upscalers](https://upscale.wiki/wiki/Model_Database)\n\t\t\t- [upscayl](https://github.com/upscayl/upscayl)\n\t\t\t- [Google Muse](https://www.infoq.com/news/2023/01/google-muse-text-to-image/)\n\t\t\t- [Flair generate photo shoots of products](https://flair.ai/)\n\t\t\t- [Vector graphics from text](https://illustroke.com/)\n\t\t\t- [Simple stock image generator](https://stockimg.ai/)\n\t\t\t- [Patterned: Generates royalty-free patterns.](https://www.patterned.ai/)\n\t\t\t- [Cleanup.picture: Removes objects, defects, people or text from your images.](https://cleanup.pictures/)\n\t\t\t- [Looka: Generates brand names and logos.](https://looka.com/)\n\t\t\t- [CLIP interrogator and prompt engineering colab](https://github.com/pharmapsychotic/clip-interrogator)\n\t\t\t- [Prompt management engine (local and cloud) (promptlayer)](https://magniv.notion.site/PromptLayer-Docs-db0e6f50cacf4564a6d09824ba17a629)\n\t\t\t- [Composer stable diffusion TYPE model](https://github.com/damo-vilab/composer)\n\t\t\t- [Multi-diffusion panoramas](https://multidiffusion.github.io/)\n\t\t\t- [coherent panoramas paper](https://syncdiffusion.github.io/)\n\t\t\t- [UX design AI](https://www.usegalileo.ai/)\n\t\t\t- [pix2pix-3D: 3D-aware Conditional Image Synthesis](http://www.cs.cmu.edu/~pix2pix3D/)\n\t\t\t- [HuggingFace Demo for /ELITE: new fine-tuning technique that can be trained in less than a second/ now available: r/StableDiffusion](https://www.reddit.com/r/StableDiffusion/comments/11mzxyu/huggingface_demo_for_elite_new_finetuning/)\n\t\t\t- [GIGAgan](https://mingukkang.github.io/GigaGAN/)\n\t\t\t- [implementation](https://github.com/lucidrains/gigagan-pytorch)\n\t\t\t- [GitHub danielgatis/rembg: Rembg is a tool to remove images background (other)](https://github.com/danielgatis/rembg)\n\t\t\t- Other. The text is a description of a new product called the \"Meta 2\" which is a headset that allows users to interact with a computer using their hands.\n\t\t\t- [GitHub kanewallmann/Dreambooth-Stable-Diffusion: Implementation of Dreambooth with Stable Diffusion (tweaks focused on training faces)](https://github.com/kanewallmann/dreambooth-stable-diffusion)\n\t\t\t- [GitHub sedthh/pyxelate: Python class that generates pixel art from images (other)](https://github.com/sedthh/pyxelate)\n\t\t\t- [GitHub upscayl/upscayl: Free and Open Source AI Image Upscaler for Linux, MacOS and Windows built with Linux-First philosophy. (other)](https://github.com/upscayl/upscayl)\n\t\t\t- [GitHub YuxinWenRick/hard-prompts-made-easy: Contribute to YuxinWenRick/hard-prompts-made-easy development by creating an account on GitHub.](https://github.com/YuxinWenRick/hard-prompts-made-easy)\n\t\t- This repository contains a tool for gradient-based discrete optimization, which can be used to find the optimal solution for a given problem. The tool is designed to be easy to use, and includes a number of features to make the process of finding the optimal solution easier.\n\t\t\t- [Civitai Helper: SD Webui Civitai Extension | Stable Diffusion Other | Civitai: Now, we finally have a Civitai SD webui extension!! Update: 1.5.7 is here, if you're using localization extension, like Asian language UI, you need ...](https://civitai.com/models/16768/civitai-helper-sd-webui-civitai-extension)\n\t\t- The Civitai Helper is a Civitai extension that allows for stable diffusions of other Civitai extensions. It also includes an animation which rotates and scales the extension icon.\n\t\t- [GitHub YuxinWenRick/hard-prompts-made-easy: Contribute to YuxinWenRick/hard-prompts-made-easy development by creating an account on GitHub.](https://github.com/YuxinWenRick/hard-prompts-made-easy)\n\t\t- This repository contains code for a gradient-based discrete optimization method. The method is designed to make it easy to find hard prompts, which are useful for training machine learning models.\n\t\t- [StableSam meta segmentation plus SD inpainting](https://twitter.com/abhi1thakur/status/1645669023726592007)\n\t\t- New Feature: \"ZOOM ENHANCE\" for the A111 WebUI. Automatically fix small details like faces and hands! : r/StableDiffusion [https://www.reddit.com/r/StableDiffusion/comments/11pyiro/new_feature_zoom_enhance_for_the_a111_webui/](https://www.reddit.com/r/StableDiffusion/comments/11pyiro/new_feature_zoom_enhance_for_the_a111_webui/)\n\t\t- [Realtime scribble](https://github.com/houseofsecrets/SdPaint)\n\t\t- [latent labs 360 images lora](https://civitai.com/models/10753/latentlabs360)\n\t\t- Kandinsky model\n\t\t\t- [finetuned 2.1](https://www.reddit.com/r/StableDiffusion/comments/13hgpo2/kandinsky_21_fine_tune/)\n\t\t\t- [QR codes](https://www.youtube.com/watch?v=IntRn96C4l4)\n\t\t\t- [DragGan image editing through drag points](https://github.com/XingangPan/DragGAN)\n\t\t\t- [Faster CPP clip](https://github.com/monatis/clip.cpp)\n\t\t\t- [animateDiff](https://github.com/guoyww/animatediff/)\n\t\t\t- [AnimatediffSDXL lora](https://www.reddit.com/r/StableDiffusion/comments/17stnug/sdxl_animatediff_motion_lora_released/)\n\t\t\t- [diffbar image sharpen](https://github.com/XPixelGroup/DiffBIR?ref=aiartweekly)\n\t\t\t- [SD model mixer](https://github.com/wkpark/sd-webui-model-mixer)\n\t\t\t- Textual Inversion character creation [tutorials/consistent_character_embedding/README.md at main · BelieveDiffusion/tutorials (github.com)](https://github.com/BelieveDiffusion/tutorials/blob/main/consistent_character_embedding/README.md)\n\t\t\t- [%3 e](https://github.com/nitrosocke/dreambooth-training-guide/blob/main/README.md#how-to-fine-tune-stable-diffusion-20%22/%3E)\n\t\t\t- [AI Creating 'Art' Is An Ethical And [[Copyright]] Nightmare](https://kotaku.com/ai-art-dall-e-midjourney-stable-diffusion-[[copyright]]-1849388060)\n\t\t\t- [CompVis/stable-diffusion: A latent text-to-image diffusion model](https://github.com/CompVis/stable-diffusion)\n\t\t\t- [Consistency in Stable Diffusion Definitive Guide to Having Multiple Faces of the Same Character](https://www.youtube.com/watch?v=Ig1S2guCfKM%22%2F%3E)\n\t\t\t- [Consistent character embedding#readme%22](https://github.com/BelieveDiffusion/tutorials/tree/main/consistent_character_embedding#readme%22)\n\t\t\t- [Consistent character embedding#readme}{walkthrough](https://github.com/BelieveDiffusion/tutorials/tree/main/consistent_character_embedding#readme}{walkthrough)\n\t\t\t- [Controlnet for DensePose v1.0 | Stable Diffusion Controlnet | Civitai](https://civitai.com/models/120149/controlnet-for-densepose%22/%3E)\n\t\t\t- [From the StableDiffusion community on Reddit: New Feature: \"ZOOM ENHANCE\" for the A111 WebUI. Automatically fix small details like faces and hands!](https://www.reddit.com/r/StableDiffusion/comments/11pyiro/new_feature_zoom_enhance_for_the_a111_webui)\n\t\t\t- [From the StableDiffusion community on Reddit](https://www.reddit.com/r/StableDiffusion/comments/132rcou/30_stable_diffusion_tutorials_automatic1111_web)\n\t\t\t- [How to Inject Your Trained Subject e.g. Your Face Into Any Custom Stable Diffusion Model By Web UI](https://www.youtube.com/watch?v=s25hcW4zq4M%22%2F%3E)\n\t\t\t- [Imagic: Text-Based Real Image Editing with Diffusion Models](https://buff.ly/3VLGMzo)\n\t\t\t- [RODIN Diffusion](https://3d-avatar-diffusion.microsoft.com/?amp%3Butm_medium=email&amp%3Butm_source=Revue+newsletter#/%22/%3E)\n\t\t\t- [Readme](https://github.com/huggingface/diffusers/blob/main/examples/community/README.md#tensorrt-text2image-stable-diffusion-pipeline)\n\t\t\t- [Readme](https://github.com/nitrosocke/dreambooth-training-guide/blob/main/README.md#how-to-fine-tune-stable-diffusion-20%22)\n\t\t\t- [Spirited Away General Model (1.5) @Spirited | Stable Diffusion Checkpoint | Civitai](https://civitai.com/models/5378/spirited-away-general-model-15%22)\n\t\t\t- [Style-Info: An embedding for infographic style art 1.0 | Stable Diffusion Embedding | Civitai](https://civitai.com/models/5271/style-info-an-embedding-for-infographic-style-art%22/%3E)\n\t\t\t- [THE DECODER](https://the-decoder.com/stable-diffusion-could-soon-generate-images-much-faster/%7D%7B256)\n\t\t\t- [Tutorial: Creating a Consistent Character as a Textual Inversion Embedding · BelieveDiffusion tutorials · Discussion #3](https://github.com/BelieveDiffusion/tutorials/discussions/3%22%3E)\n\t\t\t- [Ultimate Guide to Upscale Images with AI in Stable Diffusion](https://onceuponanalgorithm.org/ultimate-guide-to-upscale-images-with-ai-in-stable-diffusion)\n\t\t\t- [What are Diffusion Models?](https://lilianweng.github.io/posts/2021-07-11-diffusion-models#classifier-free-guidance}{here}.)\n\t\t\t- [Wojak SDXL v1.0 | Stable Diffusion LoRA | Civitai](https://civitai.com/models/128046/wojak-sdxl%22)\n\t\t\t- https://www.reddit.com/r/StableDiffusion/comments/145d6by/scannable_cat_qr_art_with_ai_my_recent_attempt)\n\t\t\t- [https://www.reddit.com/r/StableDiffusion/comments/114dxgl/advanced_advice_for_model_training_finetuning_and/%22%3E%3Crichcontent](https://www.reddit.com/r/StableDiffusion/comments/114dxgl/advanced_advice_for_model_training_finetuning_and/%22%3E%3Crichcontent)\n\t\t\t- [wl-zhao/UniPC: [NeurIPS 2023] UniPC: A Unified Predictor-Corrector Framework for Fast Sampling of Diffusion Models](https://github.com/wl-zhao/UniPC)\n\t\t\t- [万象熔炉 | Anything V5/Ink ink | Stable Diffusion Checkpoint | Civitai](https://civitai.com/models/9409/anything-v5-or-anything-diffusion-original%22)\n\t\t\t- [Align your Latents: High-Resolution Video Synthesis with Latent Diffusion Models](https://buff.ly/41FgQrb%22)\n\t\t\t- [Anthro v1 | Stable Diffusion Embedding | Civitai](https://civitai.com/models/1253/anthro%22)\n\t\t\t- [Become A Stable Diffusion Prompt Master By Using DAAM Attention Heatmap For Each Used Token Word](https://www.youtube.com/watch?v=XiKyEKJrTLQ%7D%7BAttention)\n\t\t\t- [Consistent AI Characters with Different Poses Angles CharTurner Stable Diffusion](https://www.youtube.com/watch?v=-iwPVUzAWzk%22)\n\t\t\t- [From the StableDiffusion community on Reddit: Advanced advice for model training / fine-tuning and captioning](https://www.reddit.com/r/StableDiffusion/comments/114dxgl/advanced_advice_for_model_training_finetuning_and)\n\t\t\t- [From the StableDiffusion community on Reddit](https://www.reddit.com/r/StableDiffusion/comments/11mulj6/quality_improvements_to%22)\n\t\t\t- [Google's prompt-to-prompt AI for Stable Diffusion tutorial!](https://www.youtube.com/watch?v=lHcPtbZ0Mnc%7D%7BGoogle)\n\t\t\t- [Home](https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Developing-extensions%22)\n\t\t\t- [How to Make 360 VR Environments for Quest with AI Stable Diffusion and Blender Tutorial 2023](https://www.youtube.com/watch?v=t9zzcRsf0IA%22)\n\t\t\t- [Open Source AI and Stable Diffusion with Emad Mostaque](https://open.spotify.com/episode/3PCboPPIdkicl9eyW5Eaux?si=u6sUA8WdR9Wyw3NPLMdAXQ)\n\t\t\t- [Reddit Prove your humanity](https://www.reddit.com/r/StableDiffusion/comments/10no6tp/non_destructive_image_variation_in_text2image/%7D%7Bnon)\n\t\t\t- [Refined Refined v11 | Stable Diffusion Checkpoint | Civitai](https://civitai.com/models/8392/refined%22)\n\t\t\t- [Spirited Away General Model (1.5) @Spirited | Stable Diffusion Checkpoint | Civitai](https://civitai.com/models/5378/spirited-away-general-model-15%7D%7Bspirited)\n\t\t\t- [Stable Diffusion Outpainting Colab Tutorial](https://m.youtube.com/watch?list=PLpdmBGJ6ELUJOuL83tQKJm-ty4IJficHc&v=-8jmBGgGj2E)\n\t\t\t- [Style-Info: An embedding for infographic style art 1.0 | Stable Diffusion Embedding | Civitai](https://civitai.com/models/5271/style-info-an-embedding-for-infographic-style-art%7D%7BInfographic)\n\t\t\t- [Tutorial: Creating a Consistent Character as a Textual Inversion Embedding · BelieveDiffusion tutorials · Discussion #3](https://github.com/BelieveDiffusion/tutorials/discussions/3%22)\n\t\t\t- [Zero To Hero Stable Diffusion DreamBooth Tutorial By Using Automatic1111 Web UI Ultra Detailed](https://www.youtube.com/watch?t=34%7D%7B2&v=Bdl-jWR3Ukc%5C)\n\t\t\t- [altryne/awesome-ai-art-image-synthesis: A list of awesome tools, ideas, prompt engineering tools, colabs, models, and helpers for the prompt designer playing with aiArt and image synthesis. Covers Dalle2, MidJourney, StableDiffusion, and open source tools.](https://github.com/altryne/awesome-ai-art-image-synthesis)\n\t\t\t- [diStyApps/Stable-Diffusion-Pickle-Scanner-GUI: Pickle Scanner GUI](https://github.com/diStyApps/Stable-Diffusion-Pickle-Scanner-GUI)\n\t\t\t- [https://www.reddit.com/r/StableDiffusion/comments/100tp0v/protogenx34_has_absolutely_amazing_detail/%22](https://www.reddit.com/r/StableDiffusion/comments/100tp0v/protogenx34_has_absolutely_amazing_detail/%22)\n\t\t\t- [https://www.reddit.com/r/StableDiffusion/comments/10c9kg8/depth2img_works_well_for_text_inputs/%22](https://www.reddit.com/r/StableDiffusion/comments/10c9kg8/depth2img_works_well_for_text_inputs/%22)\n\t\t\t- [https://www.reddit.com/r/StableDiffusion/comments/10c9kg8/depth2img_works_well_for_text_inputs/%7D%7BDepth2Img](https://www.reddit.com/r/StableDiffusion/comments/10c9kg8/depth2img_works_well_for_text_inputs/%7D%7BDepth2Img)\n\t\t\t- [https://www.reddit.com/r/StableDiffusion/comments/10gs4s2/new_expert_tutorial_for_textual_inversion_text/%7D%7BTextual](https://www.reddit.com/r/StableDiffusion/comments/10gs4s2/new_expert_tutorial_for_textual_inversion_text/%7D%7BTextual)\n\t\t\t- [https://www.reddit.com/r/StableDiffusion/comments/10l74sl/instruct_pix2pix_is_amazing_inpaintingimg2img/%7D%7BInstruct](https://www.reddit.com/r/StableDiffusion/comments/10l74sl/instruct_pix2pix_is_amazing_inpaintingimg2img/%7D%7BInstruct)\n\t\t\t- [https://www.reddit.com/r/StableDiffusion/comments/10no6tp/non_destructive_image_variation_in_text2image/%22](https://www.reddit.com/r/StableDiffusion/comments/10no6tp/non_destructive_image_variation_in_text2image/%22)\n\t\t\t- [https://www.reddit.com/r/StableDiffusion/comments/10rr99t/mocap_unreal_engine_warpfusion/%7D%7BMoCap](https://www.reddit.com/r/StableDiffusion/comments/10rr99t/mocap_unreal_engine_warpfusion/%7D%7BMoCap)\n\t\t\t- [https://www.reddit.com/r/StableDiffusion/comments/10tjzmf/instructpix2pix_is_built_straight_into_the/%22](https://www.reddit.com/r/StableDiffusion/comments/10tjzmf/instructpix2pix_is_built_straight_into_the/%22)\n\t\t\t- [https://www.reddit.com/r/StableDiffusion/comments/10tjzmf/instructpix2pix_is_built_straight_into_the/%7D%7Breddit](https://www.reddit.com/r/StableDiffusion/comments/10tjzmf/instructpix2pix_is_built_straight_into_the/%7D%7Breddit)\n\t\t\t- [https://www.reddit.com/r/StableDiffusion/comments/1148x38/tencent_ai_just_release_their_method_and_code/%7D%7BTencent](https://www.reddit.com/r/StableDiffusion/comments/1148x38/tencent_ai_just_release_their_method_and_code/%7D%7BTencent)\n\t\t\t- [https://www.reddit.com/r/StableDiffusion/comments/114dxgl/advanced_advice_for_model_training_finetuning_and/%7D%7BAdvanced](https://www.reddit.com/r/StableDiffusion/comments/114dxgl/advanced_advice_for_model_training_finetuning_and/%7D%7BAdvanced)\n\t\t\t- [https://www.reddit.com/r/StableDiffusion/comments/114zmh3/controlnet_and_ebsynth_make_incredible_temporally/%22](https://www.reddit.com/r/StableDiffusion/comments/114zmh3/controlnet_and_ebsynth_make_incredible_temporally/%22)\n\t\t\t- [Stable Assistant — Stability AI](https://stability.ai/stable-assistant) [[Stable Diffusion]]\n\t\t\t- [[Controlnet and similar]] [[Stable Diffusion]] [xinsir/controlnet-union-sdxl-1.0 · Hugging Face]\n\t\t\t- [[AI Video]] [(1865) Discord | \"Steerable Motion 1.4 - now with unlimited input frames! (+ minor optimisations)\" | banodoco](https://discord.com/channels/1076117621407223829/1234280186892259368) [[Stable Diffusion]] [[Stable Video Diffusion]]\n\t\t\t- (https://huggingface.co/xinsir/controlnet-union-sdxl-1.0)\n\n## Current Landscape (2025)\n\n- Industry adoption and implementations\n  - Metaverse platforms continue to evolve with focus on interoperability and open standards\n  - Web3 integration accelerating with decentralised identity and asset ownership\n  - Enterprise adoption growing in virtual collaboration, training, and digital twins\n  - UK companies increasingly active in metaverse development and immersive technologies\n\n- Technical capabilities\n  - Real-time rendering at photorealistic quality levels\n  - Low-latency networking enabling seamless multi-user experiences\n  - AI-driven content generation and procedural world building\n  - Spatial audio and haptics enhancing immersion\n\n- UK and North England context\n  - Manchester: Digital Innovation Factory supports metaverse startups and research\n  - Leeds: Holovis leads in immersive experiences for entertainment and training\n  - Newcastle: University research in spatial computing and interactive systems\n  - Sheffield: Advanced manufacturing using digital twin technology\n\n- Standards and frameworks\n  - Metaverse Standards Forum driving interoperability protocols\n  - WebXR enabling browser-based immersive experiences\n  - glTF and USD for 3D asset interchange\n  - Open Metaverse Interoperability Group defining cross-platform standards\n\n## Metadata\n\n- **Last Updated**: 2025-11-16\n- **Review Status**: Automated remediation with 2025 context\n- **Verification**: Academic sources verified\n- **Regional Context**: UK/North England where applicable",
  "properties": {
    "id": "66408f9e-30e0-442b-9aba-9eb51e36a739",
    "collapsed": "true",
    "- ontology": "true",
    "- term-id": "mv-819761654713",
    "- preferred-term": "Stable Diffusion",
    "- source-domain": "metaverse",
    "- status": "active",
    "- public-access": "true",
    "- definition": "A component of the metaverse ecosystem focusing on stable diffusion.",
    "- maturity": "mature",
    "- authority-score": "0.85",
    "- owl:class": "mv:StableDiffusion",
    "- owl:physicality": "ConceptualEntity",
    "- owl:role": "Concept",
    "- belongsToDomain": "[[MetaverseDomain]]",
    "- enables": "[[ImmersiveExperience]], [[Presence]], [[SpatialComputing]]",
    "- requires": "[[DisplayTechnology]], [[TrackingSystem]], [[RenderingEngine]]",
    "- bridges-to": "[[HumanComputerInteraction]], [[ComputerVision]], [[Robotics]]",
    "public": "true"
  },
  "backlinks": [
    "Open Generative AI tools",
    "BC-0072-node",
    "Virtual Production",
    "Transformers",
    "BC 0115 minting",
    "Prompt Engineering",
    "AnimateDiff",
    "Interfaces",
    "BC-0035-difficulty",
    "ComfyUI",
    "Blender",
    "Banodoco",
    "Stable Diffusion",
    "Flux",
    "Upscaling"
  ],
  "wiki_links": [
    "RenderingEngine",
    "LoRA DoRA etc",
    "Robotics",
    "HumanComputerInteraction",
    "Copyright",
    "copyright",
    "Controlnet and similar",
    "Presence",
    "Stable Diffusion",
    "ImmersiveExperience",
    "ComputerVision",
    "KOHYA Dreambooth and similar",
    "SpatialComputing",
    "Stable Video Diffusion",
    "DisplayTechnology",
    "TrackingSystem",
    "AI Video",
    "Fashion",
    "MetaverseDomain",
    "Checkpoints"
  ],
  "ontology": {
    "term_id": "mv-819761654713",
    "preferred_term": "Stable Diffusion",
    "definition": "A component of the metaverse ecosystem focusing on stable diffusion.",
    "source_domain": "metaverse",
    "maturity_level": null,
    "authority_score": 0.85
  }
}