{
  "title": "Algorithmic Bias",
  "content": "- ### OntologyBlock\n  id:: 0378-algorithmic-bias-ontology\n  collapsed:: true\n\n  - **Identification**\n    - public-access:: true\n    - ontology:: true\n    - term-id:: AI-0378\n    - preferred-term:: Algorithmic Bias\n    - source-domain:: ai\n    - status:: in-progress\n    - version:: 1.0\n    - last-updated:: 2025-10-29\n\n  - **Definition**\n    - definition:: Algorithmic Bias refers to systematic and repeatable errors in AI systems that create unfair outcomes favoring or discriminating against particular groups or individuals. This bias manifests through multiple pathways including historical bias (reflecting past societal inequalities in training data), representation bias (unrepresentative or incomplete data samples), measurement bias (flawed proxy variables), aggregation bias (combining heterogeneous groups inappropriately), and feedback loops (where system outputs influence future inputs, amplifying initial biases). Algorithmic bias affects protected groups based on attributes such as race, gender, age, disability, or socioeconomic status, potentially resulting in discriminatory decisions in critical domains like hiring, lending, criminal justice, and healthcare. Detection requires statistical analysis, fairness auditing, and counterfactual testing, while mitigation involves pre-processing data corrections, in-processing fairness constraints, and post-processing prediction adjustments. The severity and legal implications of algorithmic bias are governed by anti-discrimination frameworks including the EU Anti-Discrimination Directives, UK Equality Act 2010, and US civil rights legislation.\n    - maturity:: mature\n    - source:: [[ISO/IEC TR 24027]], [[NIST SP 1270]], [[IEEE P7003-2021]]\n    - authority-score:: 0.95\n\n  - **Semantic Classification**\n    - owl:class:: aigo:AlgorithmicBias\n    - owl:physicality:: VirtualEntity\n    - owl:role:: Process\n    - owl:inferred-class:: aigo:VirtualProcess\n    - belongsToDomain:: [[AIEthicsDomain]]\n    - implementedInLayer:: [[ConceptualLayer]]\n\n  - #### Relationships\n    id:: 0378-algorithmic-bias-relationships\n\n  - #### OWL Axioms\n    id:: 0378-algorithmic-bias-owl-axioms\n    collapsed:: true\n    - ```clojure\n      (Declaration (Class :AlgorithmicBias))\n(SubClassOf :AlgorithmicBias :EthicalConcern)\n(SubClassOf :AlgorithmicBias :AIRisk)\n\n(AnnotationAssertion rdfs:label :AlgorithmicBias\n  \"Algorithmic Bias\"@en)\n(AnnotationAssertion rdfs:comment :AlgorithmicBias\n  \"Systematic and repeatable errors in AI systems that create unfair outcomes, including historical bias, representation bias, measurement bias, and feedback loops.\"@en)\n(AnnotationAssertion :dcterms:source :AlgorithmicBias\n  \"ISO/IEC TR 24027:2021, NIST SP 1270, IEEE P7003-2021\")\n\n;; Object Properties\n(Declaration (ObjectProperty :affectsGroup))\n(ObjectPropertyDomain :affectsGroup :AlgorithmicBias)\n(ObjectPropertyRange :affectsGroup :ProtectedGroup)\n\n(Declaration (ObjectProperty :originatesFrom))\n(ObjectPropertyDomain :originatesFrom :AlgorithmicBias)\n(ObjectPropertyRange :originatesFrom :BiasSource)\n\n(Declaration (ObjectProperty :manifestsIn))\n(ObjectPropertyDomain :manifestsIn :AlgorithmicBias)\n(ObjectPropertyRange :manifestsIn :AISystemComponent)\n\n(Declaration (ObjectProperty :amplifiedBy))\n(ObjectPropertyDomain :amplifiedBy :AlgorithmicBias)\n(ObjectPropertyRange :amplifiedBy :FeedbackMechanism)\n\n(Declaration (ObjectProperty :detectedBy))\n(ObjectPropertyDomain :detectedBy :AlgorithmicBias)\n(ObjectPropertyRange :detectedBy :BiasDetectionMethod)\n\n(Declaration (ObjectProperty :mitigatedBy))\n(ObjectPropertyDomain :mitigatedBy :AlgorithmicBias)\n(ObjectPropertyRange :mitigatedBy :BiasMitigationTechnique)\n\n;; Data Properties\n(Declaration (DataProperty :hasSeverity))\n(DataPropertyDomain :hasSeverity :AlgorithmicBias)\n(DataPropertyRange :hasSeverity xsd:string)\n(AnnotationAssertion rdfs:comment :hasSeverity\n  \"Severity level: low, medium, high, critical\"@en)\n\n(Declaration (DataProperty :hasImpactScope))\n(DataPropertyDomain :hasImpactScope :AlgorithmicBias)\n(DataPropertyRange :hasImpactScope xsd:string)\n\n(Declaration (DataProperty :isSystematic))\n(DataPropertyDomain :isSystematic :AlgorithmicBias)\n(DataPropertyRange :isSystematic xsd:boolean)\n\n(Declaration (DataProperty :hasLegalImplication))\n(DataPropertyDomain :hasLegalImplication :AlgorithmicBias)\n(DataPropertyRange :hasLegalImplication xsd:boolean)\n\n;; Bias Type Subclasses\n(Declaration (Class :HistoricalBias))\n(SubClassOf :HistoricalBias :AlgorithmicBias)\n(AnnotationAssertion rdfs:comment :HistoricalBias\n  \"Bias arising from historical societal inequalities reflected in training data\"@en)\n\n(Declaration (Class :RepresentationBias))\n(SubClassOf :RepresentationBias :AlgorithmicBias)\n(AnnotationAssertion rdfs:comment :RepresentationBias\n  \"Bias from unrepresentative or incomplete training data samples\"@en)\n\n(Declaration (Class :MeasurementBias))\n(SubClassOf :MeasurementBias :AlgorithmicBias)\n(AnnotationAssertion rdfs:comment :MeasurementBias\n  \"Bias from flawed measurement or proxy variables for ground truth\"@en)\n\n(Declaration (Class :AggregationBias))\n(SubClassOf :AggregationBias :AlgorithmicBias)\n(AnnotationAssertion rdfs:comment :AggregationBias\n  \"Bias from combining heterogeneous groups into single model\"@en)\n\n(Declaration (Class :EvaluationBias))\n(SubClassOf :EvaluationBias :AlgorithmicBias)\n(AnnotationAssertion rdfs:comment :EvaluationBias\n  \"Bias in benchmarks or test sets used for model evaluation\"@en)\n\n(Declaration (Class :DeploymentBias))\n(SubClassOf :DeploymentBias :AlgorithmicBias)\n(AnnotationAssertion rdfs:comment :DeploymentBias\n  \"Bias from misalignment between development and deployment contexts\"@en)\n\n(Declaration (Class :FeedbackLoopBias))\n(SubClassOf :FeedbackLoopBias :AlgorithmicBias)\n(AnnotationAssertion rdfs:comment :FeedbackLoopBias\n  \"Bias amplified through system outputs influencing future inputs\"@en)\n\n;; Complexity Constraints\n(SubClassOf :AlgorithmicBias\n  (ObjectSomeValuesFrom :affectsGroup :ProtectedGroup))\n(SubClassOf :AlgorithmicBias\n  (ObjectSomeValuesFrom :originatesFrom :BiasSource))\n(SubClassOf :AlgorithmicBias\n  (DataSomeValuesFrom :hasSeverity xsd:string))\n\n;; Disjoint Unions\n(DisjointUnion :BiasSource\n  :DataSource :AlgorithmDesign :HumanDecision :SystemicFactors)\n      ```\n\n- ## About Algorithmic Bias\n  id:: 0378-algorithmic-bias-about\n\n  - \n  -\n  \n\n\n## Current Landscape (2025)\n\n- Industry adoption and implementations\n  - Metaverse platforms continue to evolve with focus on interoperability and open standards\n  - Web3 integration accelerating with decentralised identity and asset ownership\n  - Enterprise adoption growing in virtual collaboration, training, and digital twins\n  - UK companies increasingly active in metaverse development and immersive technologies\n\n- Technical capabilities\n  - Real-time rendering at photorealistic quality levels\n  - Low-latency networking enabling seamless multi-user experiences\n  - AI-driven content generation and procedural world building\n  - Spatial audio and haptics enhancing immersion\n\n- UK and North England context\n  - Manchester: Digital Innovation Factory supports metaverse startups and research\n  - Leeds: Holovis leads in immersive experiences for entertainment and training\n  - Newcastle: University research in spatial computing and interactive systems\n  - Sheffield: Advanced manufacturing using digital twin technology\n\n- Standards and frameworks\n  - Metaverse Standards Forum driving interoperability protocols\n  - WebXR enabling browser-based immersive experiences\n  - glTF and USD for 3D asset interchange\n  - Open Metaverse Interoperability Group defining cross-platform standards\n\n## Metadata\n\n- **Last Updated**: 2025-11-16\n- **Review Status**: Automated remediation with 2025 context\n- **Verification**: Academic sources verified\n- **Regional Context**: UK/North England where applicable",
  "properties": {
    "id": "0378-algorithmic-bias-about",
    "collapsed": "true",
    "- public-access": "true",
    "- ontology": "true",
    "- term-id": "AI-0378",
    "- preferred-term": "Algorithmic Bias",
    "- source-domain": "ai",
    "- status": "in-progress",
    "- version": "1.0",
    "- last-updated": "2025-10-29",
    "- definition": "Algorithmic Bias refers to systematic and repeatable errors in AI systems that create unfair outcomes favoring or discriminating against particular groups or individuals. This bias manifests through multiple pathways including historical bias (reflecting past societal inequalities in training data), representation bias (unrepresentative or incomplete data samples), measurement bias (flawed proxy variables), aggregation bias (combining heterogeneous groups inappropriately), and feedback loops (where system outputs influence future inputs, amplifying initial biases). Algorithmic bias affects protected groups based on attributes such as race, gender, age, disability, or socioeconomic status, potentially resulting in discriminatory decisions in critical domains like hiring, lending, criminal justice, and healthcare. Detection requires statistical analysis, fairness auditing, and counterfactual testing, while mitigation involves pre-processing data corrections, in-processing fairness constraints, and post-processing prediction adjustments. The severity and legal implications of algorithmic bias are governed by anti-discrimination frameworks including the EU Anti-Discrimination Directives, UK Equality Act 2010, and US civil rights legislation.",
    "- maturity": "mature",
    "- source": "[[ISO/IEC TR 24027]], [[NIST SP 1270]], [[IEEE P7003-2021]]",
    "- authority-score": "0.95",
    "- owl:class": "aigo:AlgorithmicBias",
    "- owl:physicality": "VirtualEntity",
    "- owl:role": "Process",
    "- owl:inferred-class": "aigo:VirtualProcess",
    "- belongsToDomain": "[[AIEthicsDomain]]",
    "- implementedInLayer": "[[ConceptualLayer]]"
  },
  "backlinks": [],
  "wiki_links": [
    "AIEthicsDomain",
    "IEEE P7003-2021",
    "ISO/IEC TR 24027",
    "NIST SP 1270",
    "ConceptualLayer"
  ],
  "ontology": {
    "term_id": "AI-0378",
    "preferred_term": "Algorithmic Bias",
    "definition": "Algorithmic Bias refers to systematic and repeatable errors in AI systems that create unfair outcomes favoring or discriminating against particular groups or individuals. This bias manifests through multiple pathways including historical bias (reflecting past societal inequalities in training data), representation bias (unrepresentative or incomplete data samples), measurement bias (flawed proxy variables), aggregation bias (combining heterogeneous groups inappropriately), and feedback loops (where system outputs influence future inputs, amplifying initial biases). Algorithmic bias affects protected groups based on attributes such as race, gender, age, disability, or socioeconomic status, potentially resulting in discriminatory decisions in critical domains like hiring, lending, criminal justice, and healthcare. Detection requires statistical analysis, fairness auditing, and counterfactual testing, while mitigation involves pre-processing data corrections, in-processing fairness constraints, and post-processing prediction adjustments. The severity and legal implications of algorithmic bias are governed by anti-discrimination frameworks including the EU Anti-Discrimination Directives, UK Equality Act 2010, and US civil rights legislation.",
    "source_domain": "ai",
    "maturity_level": null,
    "authority_score": 0.95
  }
}