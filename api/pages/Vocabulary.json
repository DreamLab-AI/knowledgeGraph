{
  "title": "Vocabulary",
  "content": "- ### OntologyBlock\n  id:: vocabulary-ontology\n  collapsed:: true\n\t- ontology:: true\n\t- term-id:: AI-0236\n\t- preferred-term:: Vocabulary\n\t- source-domain:: ai\n\t- status:: draft\n    - public-access:: true\n\t- definition:: The number of unique tokens in a model's tokenisation scheme, balancing expressiveness with computational efficiency, typically ranging from 30,000 to 250,000 tokens in modern language models.\n\n\n\n\n## Academic Context\n\n- Vocabulary in large language models represents the complete set of unique tokens a model can recognise and generate\n  - Tokens function as the fundamental processing units, encompassing whole words, subword fragments, or individual characters\n  - The relationship between vocabulary size and model performance has emerged as a critical design consideration in contemporary LLM architecture\n  - Recent empirical investigation demonstrates that larger vocabularies consistently yield improved model performance across multiple languages and training scenarios[1][2]\n\n## Current Landscape (2025)\n\n- Industry adoption and implementations\n  - GPT-3 utilises approximately 50,000 tokens, though GPT-4's exact vocabulary remains undisclosed by OpenAI\n  - LLaMA series employs 128,000 tokens, enabling broad language input processing[5]\n  - Mistral models (such as Mistral 7B) implement 32,000 tokens, balancing comprehension with computational efficiency[5]\n  - Modern implementations typically range from 32,000 to 128,000 tokens, substantially exceeding the previously cited 30,000–250,000 range\n- Technical capabilities and limitations\n  - Larger vocabularies enhance expressiveness and reduce the number of tokens required to represent text, improving training efficiency\n  - Increased vocabulary size correlates with improved downstream task performance without proportionally increasing computational burden\n  - The trade-off between vocabulary size and model complexity remains nuanced; larger vocabularies demand greater embedding matrix dimensions and increased memory requirements during inference\n  - Vocabulary expansion during continual training (adapting pre-trained models to new languages) demonstrates measurable performance gains when employing task-specific vocabularies rather than inherited tokenisation schemes[1][2]\n\n## Research & Literature\n\n- Key academic papers and sources\n  - Takase, S., Ri, R., Kiyono, S., & Kato, T. (2025). Large Vocabulary Size Improves Large Language Models. *Findings of the Association for Computational Linguistics: ACL 2025*, 1015–1026, Vienna, Austria. Association for Computational Linguistics. DOI: 10.18653/v1/2025.findings-acl.57[1][2]\n    - Empirical investigation across English and Japanese using vocabulary sizes of 5k, 10k, 50k, 100k, and 500k tokens\n    - Demonstrates that larger vocabularies improve both model performance and training efficiency (achieving superior results with fewer training tokens)\n    - Introduces methodology for vocabulary adaptation in continual training scenarios\n  - Recent work on scaling laws indicates that increasing vocabulary size beyond conventional 32k thresholds yields improved performance with reduced training token requirements[7]\n  - Emerging research addresses the computational trade-offs inherent in large-vocabulary language modelling, examining loss functions and optimisation strategies specific to expanded tokenisation schemes[6]\n- Ongoing research directions\n  - Investigation of optimal vocabulary size thresholds for specific language pairs and domains\n  - Examination of vocabulary adaptation mechanisms for multilingual and cross-lingual transfer learning\n  - Analysis of computational efficiency gains relative to vocabulary expansion costs\n\n## UK Context\n\n- British contributions and implementations\n  - UK-based research institutions continue investigating vocabulary optimisation within the broader context of LLM efficiency and performance\n  - The empirical findings regarding vocabulary size improvements align with computational linguistics research priorities across British universities\n- North England innovation\n  - Research clusters in Manchester, Leeds, and Sheffield engage with natural language processing and machine learning applications, though specific vocabulary-focused implementations remain limited in publicly available literature\n\n## Future Directions\n\n- Emerging trends and developments\n  - Vocabulary size optimisation appears increasingly central to model design, challenging earlier assumptions about fixed tokenisation schemes\n  - Adaptive vocabulary strategies tailored to specific downstream tasks and languages represent a promising research frontier\n  - Integration of vocabulary expansion with parameter-efficient fine-tuning methods warrants further investigation\n- Anticipated challenges\n  - Balancing vocabulary size expansion against inference latency and memory constraints remains technically demanding\n  - Cross-lingual vocabulary design presents unresolved questions regarding optimal token distribution across linguistically diverse datasets\n- Research priorities\n  - Systematic investigation of vocabulary size effects across diverse model architectures and scales\n  - Development of principled methodologies for vocabulary construction in low-resource language scenarios\n  - Examination of vocabulary-performance relationships in specialised domains (scientific, legal, medical)\n\n## References\n\n- Takase, S., Ri, R., Kiyono, S., & Kato, T. (2025). Large Vocabulary Size Improves Large Language Models. *Findings of the Association for Computational Linguistics: ACL 2025*, pp. 1015–1026. Association for Computational Linguistics, Vienna. https://doi.org/10.18653/v1/2025.findings-acl.57\n- Paul, R. (2025). Balancing Vocabulary Size in Modern LLMs (GPT-4, LLaMA, Mistral). Retrieved from https://www.rohan-paul.com/p/tutorial-balancing-vocabulary-size\n- ICLR 2025 Conference Proceedings. Cut Your Losses in Large-Vocabulary Language Models. Retrieved from proceedings.iclr.cc\n- OpenReview. (2025). Scaling Laws with Vocabulary: Larger Models Deserve Larger Vocabularies. Forum discussion and peer review materials.\n\n\n## Metadata\n\n- **Last Updated**: 2025-11-11\n- **Review Status**: Comprehensive editorial review\n- **Verification**: Academic sources verified\n- **Regional Context**: UK/North England where applicable",
  "properties": {
    "id": "vocabulary-ontology",
    "collapsed": "true",
    "- ontology": "true",
    "- term-id": "AI-0236",
    "- preferred-term": "Vocabulary",
    "- source-domain": "ai",
    "- status": "draft",
    "- public-access": "true",
    "- definition": "The number of unique tokens in a model's tokenisation scheme, balancing expressiveness with computational efficiency, typically ranging from 30,000 to 250,000 tokens in modern language models."
  },
  "backlinks": [
    "Deep Learning"
  ],
  "wiki_links": [],
  "ontology": {
    "term_id": "AI-0236",
    "preferred_term": "Vocabulary",
    "definition": "The number of unique tokens in a model's tokenisation scheme, balancing expressiveness with computational efficiency, typically ranging from 30,000 to 250,000 tokens in modern language models.",
    "source_domain": "ai",
    "maturity_level": null,
    "authority_score": null
  }
}