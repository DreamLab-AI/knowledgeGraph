{
  "title": "Fairness Metrics",
  "content": "- ### OntologyBlock\n  id:: 0377-fairness-metrics-ontology\n  collapsed:: true\n\n  - **Identification**\n\n    - domain-prefix:: AI\n\n    - sequence-number:: 0377\n\n    - filename-history:: [\"AI-0377-fairness-metrics.md\"]\n    - public-access:: true\n    - ontology:: true\n    - term-id:: AI-0377\n    - preferred-term:: Fairness Metrics\n    - source-domain:: ai\n    - status:: in-progress\n    - version:: 1.0\n    - last-updated:: 2025-10-29\n\n  - **Definition**\n    - definition:: Fairness Metrics are quantitative measures and mathematical frameworks used to evaluate and ensure equitable treatment across different demographic groups in AI systems. These metrics provide objective, measurable criteria to assess whether an algorithmic system produces disparate impacts, maintains statistical parity, or achieves equalized odds across protected attributes such as race, gender, age, or disability status. Key fairness metrics include demographic parity (equal positive prediction rates across groups), equalized odds (equal true positive and false positive rates), equal opportunity (equal true positive rates), and predictive parity (equal precision across groups). The selection and application of fairness metrics depends on the specific context, stakeholder values, and regulatory requirements, as different metrics can conflict and no single metric satisfies all fairness criteria simultaneously. Implementation requires confusion matrix analysis, statistical testing, and careful consideration of base rate differences between groups, as formalized in IEEE P7003-2021 and NIST SP 1270 guidelines for algorithmic fairness assessment.\n    - maturity:: mature\n    - source:: [[IEEE P7003-2021]], [[ISO/IEC TR 24027]], [[NIST SP 1270]]\n    - authority-score:: 0.95\n\n  - **Semantic Classification**\n    - owl:class:: aigo:FairnessMetrics\n    - owl:physicality:: VirtualEntity\n    - owl:role:: Process\n    - owl:inferred-class:: aigo:VirtualProcess\n    - belongsToDomain:: [[AIEthicsDomain]]\n    - implementedInLayer:: [[ConceptualLayer]]\n\n  - #### Relationships\n    id:: 0377-fairness-metrics-relationships\n\n  - #### OWL Axioms\n    id:: 0377-fairness-metrics-owl-axioms\n    collapsed:: true\n    - ```clojure\n      (Declaration (Class :FairnessMetric))\n(SubClassOf :FairnessMetric :EvaluationMetric)\n(SubClassOf :FairnessMetric :EthicalAIComponent)\n\n(AnnotationAssertion rdfs:label :FairnessMetric\n  \"Fairness Metric\"@en)\n(AnnotationAssertion rdfs:comment :FairnessMetric\n  \"Quantitative measures for assessing algorithmic fairness across protected groups, including demographic parity, equalized odds, and equality of opportunity.\"@en)\n(AnnotationAssertion :dcterms:source :FairnessMetric\n  \"IEEE P7003-2021, ISO/IEC TR 24027:2021, NIST SP 1270\")\n\n;; Object Properties\n(Declaration (ObjectProperty :measures))\n(ObjectPropertyDomain :measures :FairnessMetric)\n(ObjectPropertyRange :measures :AlgorithmicFairness)\n\n(Declaration (ObjectProperty :detectsBias))\n(ObjectPropertyDomain :detectsBias :FairnessMetric)\n(ObjectPropertyRange :detectsBias :ProtectedAttribute)\n\n(Declaration (ObjectProperty :appliesTo))\n(ObjectPropertyDomain :appliesTo :FairnessMetric)\n(ObjectPropertyRange :appliesTo :AIModel)\n\n(Declaration (ObjectProperty :requiresConfusionMatrix))\n(SubObjectPropertyOf :requiresConfusionMatrix :dependsOn)\n\n;; Data Properties\n(Declaration (DataProperty :hasValueRange))\n(DataPropertyAssertion :hasValueRange :FairnessMetric\n  \"[0,1] for most metrics\"^^xsd:string)\n\n(Declaration (DataProperty :hasThreshold))\n(DataPropertyDomain :hasThreshold :FairnessMetric)\n(DataPropertyRange :hasThreshold xsd:decimal)\n\n(Declaration (DataProperty :requiresGroundTruth))\n(DataPropertyAssertion :requiresGroundTruth :FairnessMetric\n  \"true\"^^xsd:boolean)\n\n;; Subclass Definitions\n(Declaration (Class :DemographicParity))\n(SubClassOf :DemographicParity :FairnessMetric)\n(AnnotationAssertion rdfs:comment :DemographicParity\n  \"P(Ŷ=1|A=0) = P(Ŷ=1|A=1) where A is protected attribute and Ŷ is prediction\"@en)\n\n(Declaration (Class :EqualizedOdds))\n(SubClassOf :EqualizedOdds :FairnessMetric)\n(AnnotationAssertion rdfs:comment :EqualizedOdds\n  \"P(Ŷ=1|A=0,Y=y) = P(Ŷ=1|A=1,Y=y) for y ∈ {0,1}\"@en)\n\n(Declaration (Class :EqualOpportunity))\n(SubClassOf :EqualOpportunity :FairnessMetric)\n(AnnotationAssertion rdfs:comment :EqualOpportunity\n  \"P(Ŷ=1|A=0,Y=1) = P(Ŷ=1|A=1,Y=1) - equal true positive rates\"@en)\n\n(Declaration (Class :PredictiveParity))\n(SubClassOf :PredictiveParity :FairnessMetric)\n(AnnotationAssertion rdfs:comment :PredictiveParity\n  \"P(Y=1|Ŷ=1,A=0) = P(Y=1|Ŷ=1,A=1) - equal precision across groups\"@en)\n\n;; Disjoint Classes\n(DisjointClasses :DemographicParity :EqualizedOdds :EqualOpportunity)\n\n;; Domain Constraints\n(SubClassOf :FairnessMetric\n  (ObjectSomeValuesFrom :measures :AlgorithmicFairness))\n(SubClassOf :FairnessMetric\n  (ObjectSomeValuesFrom :detectsBias :ProtectedAttribute))\n(SubClassOf :FairnessMetric\n  (DataSomeValuesFrom :hasThreshold xsd:decimal))\n      ```\n\n- ## About Fairness Metrics\n  id:: 0377-fairness-metrics-about\n\n  - \n  -\n  \n\n\n\n## Academic Context\n\n- Brief contextual overview\n  - Fairness metrics are mathematical tools used to measure and address bias in artificial intelligence systems, ensuring equitable treatment across demographic groups\n  - These metrics are central to the ethical development and deployment of AI, reflecting ongoing debates about equality, equity, and justice in automated decision-making\n\n- Key developments and current state\n  - The field has matured from theoretical frameworks to practical implementation, with increasing emphasis on context-specific fairness standards\n  - Recent research highlights the impossibility of satisfying all fairness definitions simultaneously, necessitating careful trade-offs depending on application domain\n\n- Academic foundations\n  - Rooted in social science, statistics, and computer science, fairness metrics draw from concepts such as demographic parity, equal opportunity, and predictive parity\n  - Theoretical work continues to explore the compatibility and limitations of different fairness criteria\n\n## Current Landscape (2025)\n\n- Industry adoption and implementations\n  - Many organisations now embed fairness metrics into their AI governance strategies, using them to comply with regulations, build trust, and protect brand reputation\n  - Notable platforms include Iterate.ai, Shelf.io, and IEEE’s machine learning fairness standards\n\n- UK and North England examples where relevant\n  - UK-based companies and public sector bodies increasingly adopt fairness metrics, particularly in sectors such as finance, healthcare, and public services\n  - In North England, cities like Manchester, Leeds, Newcastle, and Sheffield host innovation hubs and research centres focused on ethical AI, including fairness and bias mitigation\n\n- Technical capabilities and limitations\n  - Fairness metrics can identify and quantify bias across groups, but they cannot eliminate all forms of unfairness due to inherent trade-offs between different fairness definitions\n  - Metrics are most effective when combined with transparency, explainability, and continuous monitoring\n\n- Standards and frameworks\n  - IEEE 3198-2025 provides a comprehensive standard for evaluating machine learning fairness, specifying methods, metrics, and test cases\n  - Other frameworks include the EU’s AI Act and the UK’s own regulatory guidance on automated decision-making\n\n## Research & Literature\n\n- Key academic papers and sources\n  - Barocas, S., Hardt, M., & Narayanan, A. (2019). Fairness and Machine Learning: Limitations and Opportunities. fairmlbook.org. https://fairmlbook.org/\n  - Mehrabi, N., Morstatter, F., Saxena, N., Lerman, K., & Galstyan, A. (2021). A Survey on Bias and Fairness in Machine Learning. ACM Computing Surveys, 54(6), 1–37. https://doi.org/10.1145/3457607\n  - Mitchell, S., Potash, E., Barocas, S., D’Amour, A., & Lum, K. (2021). Algorithmic Fairness: Choices, Assumptions, and Definitions. Annual Review of Statistics and Its Application, 8, 141–163. https://doi.org/10.1146/annurev-statistics-042720-020326\n\n- Ongoing research directions\n  - Contextual fairness standards tailored to specific domains (e.g., healthcare, criminal justice)\n  - Global and cultural variations in fairness perceptions and requirements\n  - Integration of fairness metrics with explainable AI and human-in-the-loop systems\n\n## UK Context\n\n- British contributions and implementations\n  - The UK has been active in developing regulatory frameworks and best practices for AI fairness, with contributions from academic institutions, industry, and government bodies\n  - The Centre for Data Ethics and Innovation (CDEI) and the Alan Turing Institute play key roles in shaping national policy and research\n\n- North England innovation hubs (if relevant)\n  - Manchester, Leeds, Newcastle, and Sheffield are home to several universities and research centres engaged in AI ethics and fairness, including the University of Manchester’s AI for Social Good initiative and Newcastle University’s Centre for Social Justice and Community Action\n\n- Regional case studies\n  - Local authorities in North England have piloted AI systems for social services, using fairness metrics to ensure equitable outcomes for diverse communities\n  - For example, a recent project in Leeds used fairness metrics to evaluate an AI-driven housing allocation system, highlighting the importance of context-specific fairness standards\n\n## Future Directions\n\n- Emerging trends and developments\n  - Increasing focus on domain-specific fairness standards and global harmonisation of regulatory approaches\n  - Growing interest in the role of cultural and societal factors in shaping fairness perceptions\n\n- Anticipated challenges\n  - Balancing competing fairness criteria and managing trade-offs in real-world applications\n  - Ensuring that fairness metrics are accessible and usable for non-expert stakeholders\n\n- Research priorities\n  - Developing more robust and context-aware fairness metrics\n  - Exploring the intersection of fairness, transparency, and accountability in AI systems\n  - Investigating the long-term societal impacts of fairness-aware AI\n\n## References\n\n1. Barocas, S., Hardt, M., & Narayanan, A. (2019). Fairness and Machine Learning: Limitations and Opportunities. fairmlbook.org. https://fairmlbook.org/\n2. Mehrabi, N., Morstatter, F., Saxena, N., Lerman, K., & Galstyan, A. (2021). A Survey on Bias and Fairness in Machine Learning. ACM Computing Surveys, 54(6), 1–37. https://doi.org/10.1145/3457607\n3. Mitchell, S., Potash, E., Barocas, S., D’Amour, A., & Lum, K. (2021). Algorithmic Fairness: Choices, Assumptions, and Definitions. Annual Review of Statistics and Its Application, 8, 141–163. https://doi.org/10.1146/annurev-statistics-042720-020326\n4. IEEE 3198-2025. IEEE Standard for Machine Learning Fairness. IEEE. https://standards.ieee.org/ieee/3198/11068/\n5. Centre for Data Ethics and Innovation (CDEI). (2023). AI Barometer Report. https://www.gov.uk/government/organisations/centre-for-data-ethics-and-innovation\n6. Alan Turing Institute. (2023). Fairness in AI. https://www.turing.ac.uk/research/research-programmes/fairness-ai\n7. University of Manchester. (2023). AI for Social Good. https://www.manchester.ac.uk/research/themes/ai-for-social-good/\n8. Newcastle University. (2023). Centre for Social Justice and Community Action. https://www.ncl.ac.uk/csjca/\n\n\n## Metadata\n\n\n- **Migration Status**: Ontology block enriched on 2025-11-12\n- **Last Updated**: 2025-11-12\n- **Review Status**: Comprehensive editorial review\n- **Verification**: Academic sources verified\n- **Regional Context**: UK/North England where applicable",
  "properties": {
    "id": "0377-fairness-metrics-about",
    "collapsed": "true",
    "- domain-prefix": "AI",
    "- sequence-number": "0377",
    "- filename-history": "[\"AI-0377-fairness-metrics.md\"]",
    "- public-access": "true",
    "- ontology": "true",
    "- term-id": "AI-0377",
    "- preferred-term": "Fairness Metrics",
    "- source-domain": "ai",
    "- status": "in-progress",
    "- version": "1.0",
    "- last-updated": "2025-10-29",
    "- definition": "Fairness Metrics are quantitative measures and mathematical frameworks used to evaluate and ensure equitable treatment across different demographic groups in AI systems. These metrics provide objective, measurable criteria to assess whether an algorithmic system produces disparate impacts, maintains statistical parity, or achieves equalized odds across protected attributes such as race, gender, age, or disability status. Key fairness metrics include demographic parity (equal positive prediction rates across groups), equalized odds (equal true positive and false positive rates), equal opportunity (equal true positive rates), and predictive parity (equal precision across groups). The selection and application of fairness metrics depends on the specific context, stakeholder values, and regulatory requirements, as different metrics can conflict and no single metric satisfies all fairness criteria simultaneously. Implementation requires confusion matrix analysis, statistical testing, and careful consideration of base rate differences between groups, as formalized in IEEE P7003-2021 and NIST SP 1270 guidelines for algorithmic fairness assessment.",
    "- maturity": "mature",
    "- source": "[[IEEE P7003-2021]], [[ISO/IEC TR 24027]], [[NIST SP 1270]]",
    "- authority-score": "0.95",
    "- owl:class": "aigo:FairnessMetrics",
    "- owl:physicality": "VirtualEntity",
    "- owl:role": "Process",
    "- owl:inferred-class": "aigo:VirtualProcess",
    "- belongsToDomain": "[[AIEthicsDomain]]",
    "- implementedInLayer": "[[ConceptualLayer]]"
  },
  "backlinks": [
    "AI Model Card",
    "AI-0386-fairness-auditing-tools",
    "ConceptualLayer",
    "AIEthicsDomain",
    "AI Governance Principle"
  ],
  "wiki_links": [
    "NIST SP 1270",
    "ConceptualLayer",
    "ISO/IEC TR 24027",
    "AIEthicsDomain",
    "IEEE P7003-2021"
  ],
  "ontology": {
    "term_id": "AI-0377",
    "preferred_term": "Fairness Metrics",
    "definition": "Fairness Metrics are quantitative measures and mathematical frameworks used to evaluate and ensure equitable treatment across different demographic groups in AI systems. These metrics provide objective, measurable criteria to assess whether an algorithmic system produces disparate impacts, maintains statistical parity, or achieves equalized odds across protected attributes such as race, gender, age, or disability status. Key fairness metrics include demographic parity (equal positive prediction rates across groups), equalized odds (equal true positive and false positive rates), equal opportunity (equal true positive rates), and predictive parity (equal precision across groups). The selection and application of fairness metrics depends on the specific context, stakeholder values, and regulatory requirements, as different metrics can conflict and no single metric satisfies all fairness criteria simultaneously. Implementation requires confusion matrix analysis, statistical testing, and careful consideration of base rate differences between groups, as formalized in IEEE P7003-2021 and NIST SP 1270 guidelines for algorithmic fairness assessment.",
    "source_domain": "ai",
    "maturity_level": null,
    "authority_score": 0.95
  }
}